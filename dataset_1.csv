,before_merge,after_merge,url,bug type,bug description,bug filename,bug function_name,bug lines,full_traceback,traceback_type,path_to_snippet_before_merge,path_to_snippet_after_merge
353547,"def remove_lb_backend_address_pool_address(cmd, resource_group_name, load_balancer_name,
                                           backend_address_pool_name, address_name):
    client = network_client_factory(cmd.cli_ctx).load_balancer_backend_address_pools
    address_pool = client.get(resource_group_name, load_balancer_name, backend_address_pool_name)
    lb_addresses = [addr for addr in address_pool.load_balancer_backend_addresses if addr.name != address_name]
    address_pool.load_balancer_backend_addresses = lb_addresses
    return client.create_or_update(resource_group_name, load_balancer_name, backend_address_pool_name, address_pool)
","def remove_lb_backend_address_pool_address(cmd, resource_group_name, load_balancer_name,
                                           backend_address_pool_name, address_name):
    client = network_client_factory(cmd.cli_ctx).load_balancer_backend_address_pools
    address_pool = client.get(resource_group_name, load_balancer_name, backend_address_pool_name)
    if address_pool.load_balancer_backend_addresses is None:
        address_pool.load_balancer_backend_addresses = []
    lb_addresses = [addr for addr in address_pool.load_balancer_backend_addresses if addr.name != address_name]
    address_pool.load_balancer_backend_addresses = lb_addresses
    return client.create_or_update(resource_group_name, load_balancer_name, backend_address_pool_name, address_pool)
",https://github.com/Azure/azure-cli/issues/14342,CWE-248: Uncaught Exception,Uncaught Exception when adding an address to a load balancer backend pool and that load balancer backend pool is None,src/azure-cli/azure/cli/command_modules/network/custom.py,remove_lb_backend_address_pool_address,[5],"john@Azure:~$ az network lb address-pool address add --lb-name myLB2 --pool-name myLB2bepool --resource-group myResourceGroup -n ""address-name"" --vnet virtualNetwork1 --ip-address 10.0.1.8
Command group 'network lb address-pool address' is in preview. It may be changed/removed in a future release.
The command failed with an unexpected error. Here is the traceback:

'NoneType' object has no attribute 'append'
Traceback (most recent call last):
File ""/opt/az/lib/python3.6/site-packages/knack/cli.py"", line 215, in invoke
cmd_result = self.invocation.execute(args)
File ""/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py"", line 654, in execute
raise ex
File ""/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py"", line 718, in _run_jobs_serially
results.append(self._run_job(expanded_arg, cmd_copy))
File ""/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py"", line 711, in _run_job
six.reraise(*sys.exc_info())
File ""/opt/az/lib/python3.6/site-packages/six.py"", line 703, in reraise
raise value
File ""/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py"", line 688, in _run_job
result = cmd_copy(params)
File ""/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py"", line 325, in __call__
return self.handler(*args, **kwargs)
File ""/opt/az/lib/python3.6/site-packages/azure/cli/core/__init__.py"", line 545, in default_command_handler
return op(**command_args)
File ""/opt/az/lib/python3.6/site-packages/azure/cli/command_modules/network/custom.py"", line 2989, in add_lb_backend_address_pool_address
address_pool.load_balancer_backend_addresses.append(new_address)
AttributeError: 'NoneType' object has no attribute 'append'",AttributeError,buggy_snippets_files/3bb7f7f09145626ed3e4ae2d9cd0e7aaf19ece09f8bd1ce16c29558b1c6d5248_before_merge.py,buggy_snippets_files/3bb7f7f09145626ed3e4ae2d9cd0e7aaf19ece09f8bd1ce16c29558b1c6d5248_after_merge.py
355403,"    def split_action(arguments):
        class SplitAction(argparse.Action): #pylint: disable=too-few-public-methods

            def __call__(self, parser, namespace, values, option_string=None):
                ''' The SplitAction will take the given ID parameter and spread the parsed
                parts of the id into the individual backing fields.

                Since the id value is expected to be of type `IterateValue`, all the backing
                (dest) fields will also be of type `IterateValue`
                '''
                try:
                    for value in [values] if isinstance(values, str) else values:
                        parts = parse_resource_id(value)
                        for arg in [arg for arg in arguments.values() if arg.id_part]:
                            existing_values = getattr(namespace, arg.name, None)
                            if existing_values is None:
                                existing_values = IterateValue()
                            existing_values.append(parts[arg.id_part])
                            setattr(namespace, arg.name, existing_values)
                except Exception as ex:
                    raise ValueError(ex)

        return SplitAction
","    def split_action(arguments):
        class SplitAction(argparse.Action): #pylint: disable=too-few-public-methods

            def __call__(self, parser, namespace, values, option_string=None):
                ''' The SplitAction will take the given ID parameter and spread the parsed
                parts of the id into the individual backing fields.

                Since the id value is expected to be of type `IterateValue`, all the backing
                (dest) fields will also be of type `IterateValue`
                '''
                try:
                    for value in [values] if isinstance(values, str) else values:
                        parts = parse_resource_id(value)
                        for arg in [arg for arg in arguments.values() if arg.id_part]:
                            existing_values = getattr(namespace, arg.name, None)
                            if existing_values is None:
                                existing_values = IterateValue()
                                existing_values.append(parts[arg.id_part])
                            elif isinstance(existing_values, str):
                                logger.warning(
                                    ""Property '%s=%s' being overriden by value '%s' from IDs parameter."", # pylint: disable=line-too-long
                                    arg.name, existing_values, parts[arg.id_part]
                                )
                                existing_values = IterateValue()
                                existing_values.append(parts[arg.id_part])
                            setattr(namespace, arg.name, existing_values)
                except Exception as ex:
                    raise ValueError(ex)

        return SplitAction
",https://github.com/Azure/azure-cli/issues/793,CWE-754: Improper Check for Unusual or Exceptional Conditions,"Attribute `arg.name` of `namespace` can be a string, but there is only one check if this attribute is None",src/azure-cli-core/azure/cli/core/commands/arm.py,add_id_parameters.split_action,[16],"'str' object has no attribute 'append'
Traceback (most recent call last):
File ""c:\users\trpresco\documents\github\azure-cli\src\azure\cli\commands\arm.py"", line 110, in __call__
existing_values.append(parts[arg.id_part])
AttributeError: 'str' object has no attribute 'append'",AttributeError,buggy_snippets_files/25709cdb193f8c883c904614aeeb0bad790585b490a77263c7925d83fdb5ae9c_before_merge.py,buggy_snippets_files/25709cdb193f8c883c904614aeeb0bad790585b490a77263c7925d83fdb5ae9c_after_merge.py
379188,"    def parse_series(self, data, **kwargs):
        log.debug('Parsing series: `%s` [options: %s]', data, kwargs)
        guessit_options = self._guessit_options(kwargs)
        valid = True
        if kwargs.get('name'):
            expected_titles = [kwargs['name']]
            if kwargs.get('alternate_names'):
                expected_titles.extend(kwargs['alternate_names'])
            # apostrophe support
            expected_titles = [title.replace('\'', '(?:\'|\\\'|\\\\\'|-|)?') for title in expected_titles]
            guessit_options['expected_title'] = ['re:' + title for title in expected_titles]
        if kwargs.get('id_regexps'):
            guessit_options['id_regexps'] = kwargs.get('id_regexps')
        start = preferred_clock()
        # If no series name is provided, we don't tell guessit what kind of match we are looking for
        # This prevents guessit from determining that too general of matches are series
        parse_type = 'episode' if kwargs.get('name') else None
        if parse_type:
            guessit_options['type'] = parse_type

        # NOTE: Guessit expects str on PY3 and unicode on PY2 hence the use of future.utils.native
        try:
            guess_result = guessit_api.guessit(native(data), options=guessit_options)
        except GuessitException:
            log.warning('Parsing %s with guessit failed. Most likely a unicode error.', data)
            guess_result = {}

        if guess_result.get('type') != 'episode':
            valid = False

        name = kwargs.get('name')
        country = guess_result.get('country')
        if not name:
            name = guess_result.get('title')
            if country and hasattr(country, 'alpha2'):
                name += ' (%s)' % country.alpha2
        elif guess_result.matches['title']:
            # Make sure the name match is up to FlexGet standards
            # Check there is no unmatched cruft before the matched name
            title_start = guess_result.matches['title'][0].start
            title_end = guess_result.matches['title'][0].end
            if title_start != 0:
                try:
                    pre_title = max((match[0].end for match in guess_result.matches.values() if
                                     match[0].end <= title_start))
                except ValueError:
                    pre_title = 0
                for char in reversed(data[pre_title:title_start]):
                    if char.isalnum() or char.isdigit():
                        return SeriesParseResult(data=data, valid=False)
                    if char.isspace() or char in '._':
                        continue
                    else:
                        break
            # Check the name doesn't end mid-word (guessit might put the border before or after the space after title)
            if data[title_end - 1].isalnum() and len(data) <= title_end or \
                    not self._is_valid_name(data, guessit_options=guessit_options):
                valid = False
            # If we are in exact mode, make sure there is nothing after the title
            if kwargs.get('strict_name'):
                post_title = sys.maxsize
                for match_type, matches in guess_result.matches.items():
                    if match_type in ['season', 'episode', 'date', 'regexpId']:
                        if matches[0].start < title_end:
                            continue
                        post_title = min(post_title, matches[0].start)
                        if matches[0].parent:
                            post_title = min(post_title, matches[0].parent.start)
                for char in data[title_end:post_title]:
                    if char.isalnum() or char.isdigit():
                        valid = False
        else:
            valid = False
        season = guess_result.get('season')
        episode = guess_result.get('episode')
        if episode is None and 'part' in guess_result:
            episode = guess_result['part']
        if isinstance(episode, list):
            # guessit >=2.1.4 returns a list for multi-packs, but we just want the first one and the number of eps
            episode = episode[0]
        date = guess_result.get('date')
        quality = self._quality(guess_result)
        proper_count = self._proper_count(guess_result)
        group = guess_result.get('release_group')
        # Validate group with from_group
        if not self._is_valid_groups(group, guessit_options.get('allow_groups', [])):
            valid = False
        # Validate country, TODO: LEGACY
        if country and name.endswith(')'):
            p_start = name.rfind('(')
            if p_start != -1:
                parenthetical = re.escape(name[p_start + 1:-1])
                if parenthetical and parenthetical.lower() != str(country).lower():
                    valid = False
        special = guess_result.get('episode_details', '').lower() == 'special'
        if 'episode' not in guess_result.values_list:
            episodes = len(guess_result.values_list.get('part', []))
        else:
            episodes = len(guess_result.values_list['episode'])
        if episodes > 3:
            valid = False
        identified_by = kwargs.get('identified_by', 'auto')
        identifier_type, identifier = None, None
        if identified_by in ['date', 'auto']:
            if date:
                identifier_type = 'date'
                identifier = date
        if not identifier_type and identified_by in ['ep', 'auto']:
            if episode is not None:
                if season is None and kwargs.get('allow_seasonless', True):
                    if 'part' in guess_result:
                        season = 1
                    else:
                        episode_raw = guess_result.matches['episode'][0].initiator.raw
                        if episode_raw and any(c.isalpha() and c.lower() != 'v' for c in episode_raw):
                            season = 1
                if season is not None:
                    identifier_type = 'ep'
                    identifier = (season, episode)

        if not identifier_type and identified_by in ['id', 'auto']:
            if guess_result.matches['regexpId']:
                identifier_type = 'id'
                identifier = '-'.join(match.value for match in guess_result.matches['regexpId'])
        if not identifier_type and identified_by in ['sequence', 'auto']:
            if episode is not None:
                identifier_type = 'sequence'
                identifier = episode
        if (not identifier_type or guessit_options.get('prefer_specials')) and (special or
                                                                        guessit_options.get('assume_special')):
            identifier_type = 'special'
            identifier = guess_result.get('episode_title', 'special')
        if not identifier_type:
            valid = False
        # TODO: Legacy - Complete == invalid
        if 'complete' in normalize_component(guess_result.get('other')):
            valid = False

        parsed = SeriesParseResult(
            data=data,
            name=name,
            episodes=episodes,
            identified_by=identified_by,
            id=identifier,
            id_type=identifier_type,
            quality=quality,
            proper_count=proper_count,
            special=special,
            group=group,
            valid=valid
        )

        log.debug('Parsing result: %s (in %s ms)', parsed, (preferred_clock() - start) * 1000)
        return parsed
","    def parse_series(self, data, **kwargs):
        log.debug('Parsing series: `%s` [options: %s]', data, kwargs)
        guessit_options = self._guessit_options(kwargs)
        valid = True
        if kwargs.get('name'):
            expected_titles = [kwargs['name']]
            if kwargs.get('alternate_names'):
                expected_titles.extend(kwargs['alternate_names'])
            # apostrophe support
            expected_titles = [title.replace('\'', '(?:\'|\\\'|\\\\\'|-|)?') for title in expected_titles]
            guessit_options['expected_title'] = ['re:' + title for title in expected_titles]
        if kwargs.get('id_regexps'):
            guessit_options['id_regexps'] = kwargs.get('id_regexps')
        start = preferred_clock()
        # If no series name is provided, we don't tell guessit what kind of match we are looking for
        # This prevents guessit from determining that too general of matches are series
        parse_type = 'episode' if kwargs.get('name') else None
        if parse_type:
            guessit_options['type'] = parse_type

        # NOTE: Guessit expects str on PY3 and unicode on PY2 hence the use of future.utils.native
        try:
            guess_result = guessit_api.guessit(native(data), options=guessit_options)
        except GuessitException:
            log.warning('Parsing %s with guessit failed. Most likely a unicode error.', data)
            guess_result = {}

        if guess_result.get('type') != 'episode':
            valid = False

        name = kwargs.get('name')
        country = guess_result.get('country')
        if not name:
            name = guess_result.get('title')
            if not name:
                valid = False
            elif country and hasattr(country, 'alpha2'):
                name += ' (%s)' % country.alpha2
        elif guess_result.matches['title']:
            # Make sure the name match is up to FlexGet standards
            # Check there is no unmatched cruft before the matched name
            title_start = guess_result.matches['title'][0].start
            title_end = guess_result.matches['title'][0].end
            if title_start != 0:
                try:
                    pre_title = max((match[0].end for match in guess_result.matches.values() if
                                     match[0].end <= title_start))
                except ValueError:
                    pre_title = 0
                for char in reversed(data[pre_title:title_start]):
                    if char.isalnum() or char.isdigit():
                        return SeriesParseResult(data=data, valid=False)
                    if char.isspace() or char in '._':
                        continue
                    else:
                        break
            # Check the name doesn't end mid-word (guessit might put the border before or after the space after title)
            if data[title_end - 1].isalnum() and len(data) <= title_end or \
                    not self._is_valid_name(data, guessit_options=guessit_options):
                valid = False
            # If we are in exact mode, make sure there is nothing after the title
            if kwargs.get('strict_name'):
                post_title = sys.maxsize
                for match_type, matches in guess_result.matches.items():
                    if match_type in ['season', 'episode', 'date', 'regexpId']:
                        if matches[0].start < title_end:
                            continue
                        post_title = min(post_title, matches[0].start)
                        if matches[0].parent:
                            post_title = min(post_title, matches[0].parent.start)
                for char in data[title_end:post_title]:
                    if char.isalnum() or char.isdigit():
                        valid = False
        else:
            valid = False
        season = guess_result.get('season')
        episode = guess_result.get('episode')
        if episode is None and 'part' in guess_result:
            episode = guess_result['part']
        if isinstance(episode, list):
            # guessit >=2.1.4 returns a list for multi-packs, but we just want the first one and the number of eps
            episode = episode[0]
        date = guess_result.get('date')
        quality = self._quality(guess_result)
        proper_count = self._proper_count(guess_result)
        group = guess_result.get('release_group')
        # Validate group with from_group
        if not self._is_valid_groups(group, guessit_options.get('allow_groups', [])):
            valid = False
        # Validate country, TODO: LEGACY
        if country and name.endswith(')'):
            p_start = name.rfind('(')
            if p_start != -1:
                parenthetical = re.escape(name[p_start + 1:-1])
                if parenthetical and parenthetical.lower() != str(country).lower():
                    valid = False
        special = guess_result.get('episode_details', '').lower() == 'special'
        if 'episode' not in guess_result.values_list:
            episodes = len(guess_result.values_list.get('part', []))
        else:
            episodes = len(guess_result.values_list['episode'])
        if episodes > 3:
            valid = False
        identified_by = kwargs.get('identified_by', 'auto')
        identifier_type, identifier = None, None
        if identified_by in ['date', 'auto']:
            if date:
                identifier_type = 'date'
                identifier = date
        if not identifier_type and identified_by in ['ep', 'auto']:
            if episode is not None:
                if season is None and kwargs.get('allow_seasonless', True):
                    if 'part' in guess_result:
                        season = 1
                    else:
                        episode_raw = guess_result.matches['episode'][0].initiator.raw
                        if episode_raw and any(c.isalpha() and c.lower() != 'v' for c in episode_raw):
                            season = 1
                if season is not None:
                    identifier_type = 'ep'
                    identifier = (season, episode)

        if not identifier_type and identified_by in ['id', 'auto']:
            if guess_result.matches['regexpId']:
                identifier_type = 'id'
                identifier = '-'.join(match.value for match in guess_result.matches['regexpId'])
        if not identifier_type and identified_by in ['sequence', 'auto']:
            if episode is not None:
                identifier_type = 'sequence'
                identifier = episode
        if (not identifier_type or guessit_options.get('prefer_specials')) and (special or
                                                                        guessit_options.get('assume_special')):
            identifier_type = 'special'
            identifier = guess_result.get('episode_title', 'special')
        if not identifier_type:
            valid = False
        # TODO: Legacy - Complete == invalid
        if 'complete' in normalize_component(guess_result.get('other')):
            valid = False

        parsed = SeriesParseResult(
            data=data,
            name=name,
            episodes=episodes,
            identified_by=identified_by,
            id=identifier,
            id_type=identifier_type,
            quality=quality,
            proper_count=proper_count,
            special=special,
            group=group,
            valid=valid
        )

        log.debug('Parsing result: %s (in %s ms)', parsed, (preferred_clock() - start) * 1000)
        return parsed
",https://github.com/Flexget/Flexget/issues/2276,CWE-754: Improper Check for Unusual or Exceptional Conditions,"`guess_result.get('title')` can return None, then `valid` must be `False`",flexget/plugins/parsers/parser_guessit.py,ParserGuessit.parse_series,,"2018-12-10 19:39 DEBUG    parser_guessit moving_anime    Parsing series: `Ep 02` [options: {'name': None, 'identified_by': 'auto', 'allow_seasonless': False}]
2018-12-10 19:39 DEBUG    parser_guessit moving_anime    Parsing result: <SeriesParseResult(data=Ep 02,name=None,id=2,season=0,season_pack=False,episode=2,quality=unknown,pro
per=0,special=False,status=OK)> (in 31.31600000006074 ms)
2018-12-10 19:39 CRITICAL task          moving_anime    BUG: Unhandled error in plugin metainfo_series: 'NoneType' object has no attribute 'split'
2018-12-10 19:39 CRITICAL manager       moving_anime    An unexpected crash has occurred. Writing crash report to /config/crash_report.2018.12.10.193956695553.log. Please ver
ify you are running the latest version of flexget by using ""flexget -V"" from CLI or by using version_checker plugin at http://flexget.com/wiki/Plugins/version_checker. You ar
e currently using version 2.17.18
2018-12-10 19:39 DEBUG    manager       moving_anime    Traceback:
Traceback (most recent call last):
File ""/usr/lib/python3.6/site-packages/flexget/task.py"", line 486, in __run_plugin
return method(*args, **kwargs)
File ""/usr/lib/python3.6/site-packages/flexget/event.py"", line 23, in __call__
return self.func(*args, **kwargs)
File ""/usr/lib/python3.6/site-packages/flexget/plugins/metainfo/series.py"", line 32, in on_task_metainfo
self.guess_entry(entry)
File ""/usr/lib/python3.6/site-packages/flexget/plugins/metainfo/series.py"", line 49, in guess_entry
parsed.name = normalize_name(remove_dirt(parsed.name))
File ""/usr/lib/python3.6/site-packages/flexget/plugins/parsers/parser_common.py"", line 103, in normalize_name
name = capwords(name)
File ""/usr/lib/python3.6/string.py"", line 48, in capwords
return (sep or ' ').join(x.capitalize() for x in s.split(sep))
AttributeError: 'NoneType' object has no attribute 'split'
2018-12-10 19:39 WARNING  task          moving_anime    Aborting task (plugin: metainfo_series)",AttributeError,buggy_snippets_files/f652403f87adbfe2ca2690e2d0db03e6edbb70094bb3bc2d900b9f74bb3b22d6_before_merge.py,buggy_snippets_files/f652403f87adbfe2ca2690e2d0db03e6edbb70094bb3bc2d900b9f74bb3b22d6_after_merge.py
99259,"    def __init__(self, **kwargs):
        # Save values so that we can revert to their initial values
        self.old_defaults = {}
        with Session() as lib:
            for key in kwargs:
                self.old_defaults[key] = lib.get_default(key)

        # call gmt set to change GMT defaults
        arg_str = "" "".join(
            [""{}={}"".format(key, value) for key, value in kwargs.items()]
        )
        with Session() as lib:
            lib.call_module(""set"", arg_str)
","    def __init__(self, **kwargs):
        # Save values so that we can revert to their initial values
        self.old_defaults = {}
        self.special_params = {
            ""FONT"": [
                ""FONT_ANNOT_PRIMARY"",
                ""FONT_ANNOT_SECONDARY"",
                ""FONT_HEADING"",
                ""FONT_LABEL"",
                ""FONT_TAG"",
                ""FONT_TITLE"",
            ],
            ""FONT_ANNOT"": [""FONT_ANNOT_PRIMARY"", ""FONT_ANNOT_SECONDARY""],
            ""FORMAT_TIME_MAP"": [""FORMAT_TIME_PRIMARY_MAP"", ""FORMAT_TIME_SECONDARY_MAP""],
            ""MAP_ANNOT_OFFSET"": [
                ""MAP_ANNOT_OFFSET_PRIMARY"",
                ""MAP_ANNOT_OFFSET_SECONDARY"",
            ],
            ""MAP_GRID_CROSS_SIZE"": [
                ""MAP_GRID_CROSS_SIZE_PRIMARY"",
                ""MAP_GRID_CROSS_SIZE_SECONDARY"",
            ],
            ""MAP_GRID_PEN"": [""MAP_GRID_PEN_PRIMARY"", ""MAP_GRID_PEN_SECONDARY""],
            ""MAP_TICK_LENGTH"": [""MAP_TICK_LENGTH_PRIMARY"", ""MAP_TICK_LENGTH_SECONDARY""],
            ""MAP_TICK_PEN"": [""MAP_TICK_PEN_PRIMARY"", ""MAP_TICK_PEN_SECONDARY""],
        }
        with Session() as lib:
            for key in kwargs:
                if key in self.special_params:
                    for k in self.special_params[key]:
                        self.old_defaults[k] = lib.get_default(k)
                else:
                    self.old_defaults[key] = lib.get_default(key)

        # call gmt set to change GMT defaults
        arg_str = "" "".join(
            [""{}={}"".format(key, value) for key, value in kwargs.items()]
        )
        with Session() as lib:
            lib.call_module(""set"", arg_str)
",https://github.com/GenericMappingTools/pygmt/issues/409,CWE-754: Improper Check for Unusual or Exceptional Conditions,There can be no value for key in default. No check for it,pygmt/modules.py,config.__init__,[6],"pygmt-session [ERROR]: Syntax error: Unrecognized keyword FONT
Traceback (most recent call last):
File ""/opt/miniconda3/envs/liam/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
exec(code_obj, self.user_global_ns, self.user_ns)
File ""<ipython-input-19-8e431d18430a>"", line 1, in <module>
pygmt.config(FONT='8p')
File ""/opt/miniconda3/envs/liam/lib/python3.7/site-packages/pygmt/modules.py"", line 168, in __init__
self.old_defaults[key] = lib.get_default(key)
File ""/opt/miniconda3/envs/liam/lib/python3.7/site-packages/pygmt/clib/session.py"", line 458, in get_default
name, status
pygmt.exceptions.GMTCLibError: Error getting default value for 'FONT' (error code 67).",pygmt.exceptions.GMTCLibError,buggy_snippets_files/db65c0082d70560caaba4022d944a79a61c6b46392261d530a8290c3da9acf3b_before_merge.py,buggy_snippets_files/db65c0082d70560caaba4022d944a79a61c6b46392261d530a8290c3da9acf3b_after_merge.py
403629,"    def dump_checkpoint(self, weights_only: bool = False) -> dict:
        """"""Creating model checkpoint.

        Args:
            weights_only: saving model weights only

        Return:
             structured dictionary
        """"""
        checkpoint = {
            'epoch': self.current_epoch + 1,
            'global_step': self.global_step + 1,
            'pytorch-lightning_version': pytorch_lightning.__version__,
        }

        if not weights_only:

            # TODO support more generic way for callbacks to persist a state_dict in a checkpoint
            checkpoint_callbacks = [c for c in self.callbacks if isinstance(c, ModelCheckpoint)]
            early_stopping_callbacks = [c for c in self.callbacks if isinstance(c, EarlyStopping)]

            if checkpoint_callbacks:
                # we add the official checkpoint callback to the end of the list
                # extra user provided callbacks will not be persisted yet
                checkpoint[ModelCheckpoint.CHECKPOINT_STATE_BEST_SCORE] = self.checkpoint_callback.best_model_score
                checkpoint[ModelCheckpoint.CHECKPOINT_STATE_BEST_PATH] = self.checkpoint_callback.best_model_path

            if early_stopping_callbacks and checkpoint_callbacks:
                # we add the official early stopping callback to the end of the list
                # extra user provided callbacks will not be persisted yet
                checkpoint['early_stop_callback_state_dict'] = early_stopping_callbacks[-1].state_dict()

            # save optimizers
            optimizer_states = []
            for i, optimizer in enumerate(self.optimizers):
                optimizer_states.append(optimizer.state_dict())
            checkpoint['optimizer_states'] = optimizer_states

            # save lr schedulers
            lr_schedulers = []
            for scheduler in self.lr_schedulers:
                lr_schedulers.append(scheduler['scheduler'].state_dict())
            checkpoint['lr_schedulers'] = lr_schedulers

            # save native amp scaling
            if self.amp_backend == AMPType.NATIVE and not self.use_tpu:
                checkpoint['native_amp_scaling_state'] = self.scaler.state_dict()
            elif self.amp_backend == AMPType.APEX:
                checkpoint['amp_scaling_state'] = amp.state_dict()

        # add the module_arguments and state_dict from the model
        model = self.get_model()

        checkpoint['state_dict'] = model.state_dict()

        if model.hparams:
            if hasattr(model, '_hparams_name'):
                checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_NAME] = model._hparams_name
            # add arguments to the checkpoint
            checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_KEY] = model.hparams
            if OMEGACONF_AVAILABLE:
                if isinstance(model.hparams, Container):
                    checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_TYPE] = type(model.hparams)

        # give the model a chance to add a few things
        model.on_save_checkpoint(checkpoint)

        return checkpoint
","    def dump_checkpoint(self, weights_only: bool = False) -> dict:
        """"""Creating model checkpoint.

        Args:
            weights_only: saving model weights only

        Return:
             structured dictionary
        """"""
        checkpoint = {
            'epoch': self.current_epoch + 1,
            'global_step': self.global_step + 1,
            'pytorch-lightning_version': pytorch_lightning.__version__,
        }

        if not weights_only:

            # TODO support more generic way for callbacks to persist a state_dict in a checkpoint
            checkpoint_callbacks = [c for c in self.callbacks if isinstance(c, ModelCheckpoint)]
            early_stopping_callbacks = [c for c in self.callbacks if isinstance(c, EarlyStopping)]

            if checkpoint_callbacks:
                # we add the official checkpoint callback to the end of the list
                # extra user provided callbacks will not be persisted yet
                checkpoint[ModelCheckpoint.CHECKPOINT_STATE_BEST_SCORE] = self.checkpoint_callback.best_model_score
                checkpoint[ModelCheckpoint.CHECKPOINT_STATE_BEST_PATH] = self.checkpoint_callback.best_model_path

            if early_stopping_callbacks and checkpoint_callbacks:
                # we add the official early stopping callback to the end of the list
                # extra user provided callbacks will not be persisted yet
                checkpoint['early_stop_callback_state_dict'] = early_stopping_callbacks[-1].state_dict()

            # save optimizers
            optimizer_states = []
            for i, optimizer in enumerate(self.optimizers):
                optimizer_states.append(optimizer.state_dict())
            checkpoint['optimizer_states'] = optimizer_states

            # save lr schedulers
            lr_schedulers = []
            for scheduler in self.lr_schedulers:
                lr_schedulers.append(scheduler['scheduler'].state_dict())
            checkpoint['lr_schedulers'] = lr_schedulers

            # save native amp scaling
            if self.amp_backend == AMPType.NATIVE and not self.use_tpu and self.scaler is not None:
                checkpoint['native_amp_scaling_state'] = self.scaler.state_dict()
            elif self.amp_backend == AMPType.APEX:
                checkpoint['amp_scaling_state'] = amp.state_dict()

        # add the module_arguments and state_dict from the model
        model = self.get_model()

        checkpoint['state_dict'] = model.state_dict()

        if model.hparams:
            if hasattr(model, '_hparams_name'):
                checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_NAME] = model._hparams_name
            # add arguments to the checkpoint
            checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_KEY] = model.hparams
            if OMEGACONF_AVAILABLE:
                if isinstance(model.hparams, Container):
                    checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_TYPE] = type(model.hparams)

        # give the model a chance to add a few things
        model.on_save_checkpoint(checkpoint)

        return checkpoint
",https://github.com/PyTorchLightning/pytorch-lightning/issues/2655,CWE-754: Improper Check for Unusual or Exceptional Conditions,There is no check if scaler is None,pytorch_lightning/trainer/training_io.py,TrainerIOMixin.dump_checkpoint,[46],"Running command:
python pipe/train_cnn.py
/home/gianluca/git/kaggle/siim-isic-melanoma-classification/.venv/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Checkpoint directory /home/gianluca/git/kaggle/siim-isic-melanoma-classification/models exists and is not empty with save_top_k != 0.All files in this directory will be deleted when
a checkpoint is saved!
warnings.warn(*args, **kwargs)
Using cache found in /home/gianluca/.cache/torch/hub/facebookresearch_WSL-Images_master
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
Using native 16bit precision.
Traceback (most recent call last):
File ""pipe/train_cnn.py"", line 237, in <module>
main(create_submission=True)
File ""pipe/train_cnn.py"", line 48, in main
preds, weight_fpath = train(fold_number=fold_number, folds=folds)
File ""pipe/train_cnn.py"", line 120, in train
trainer.fit(model)
File ""/home/gianluca/git/kaggle/siim-isic-melanoma-classification/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 956, in fit
self._run_lr_finder_internally(model)
File ""/home/gianluca/git/kaggle/siim-isic-melanoma-classification/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/lr_finder.py"", line 58, in _run_lr_finder_internally
lr_finder = self.lr_find(model)
File ""/home/gianluca/git/kaggle/siim-isic-melanoma-classification/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/lr_finder.py"", line 180, in lr_find
self.save_checkpoint(str(save_path))
File ""/home/gianluca/git/kaggle/siim-isic-melanoma-classification/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py"", line 268, in save_checkpoint
checkpoint = self.dump_checkpoint(weights_only)
File ""/home/gianluca/git/kaggle/siim-isic-melanoma-classification/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py"", line 362, in dump_checkpoint
checkpoint['native_amp_scaling_state'] = self.scaler.state_dict()
AttributeError: 'NoneType' object has no attribute 'state_dict'
ERROR: failed to reproduce 'train_cnn.dvc': stage: 'train_cnn.dvc' cmd 'python pipe/train_cnn.py' failed",AttributeError,buggy_snippets_files/9e8ea084968f1648a3f76bc45672796335a0cdfe7bbb94ec909aa1d154e0e289_before_merge.py,buggy_snippets_files/9e8ea084968f1648a3f76bc45672796335a0cdfe7bbb94ec909aa1d154e0e289_after_merge.py
403734,"    @classmethod
    def _load_model_state(cls, checkpoint: Dict[str, Any], *cls_args, **cls_kwargs):
        cls_spec = inspect.getfullargspec(cls.__init__)
        cls_init_args_name = inspect.signature(cls).parameters.keys()
        # pass in the values we saved automatically
        if cls.CHECKPOINT_HYPER_PARAMS_KEY in checkpoint:
            model_args = {}

            # add some back compatibility, the actual one shall be last
            for hparam_key in CHECKPOINT_PAST_HPARAMS_KEYS + (cls.CHECKPOINT_HYPER_PARAMS_KEY,):
                if hparam_key in checkpoint:
                    model_args.update(checkpoint[hparam_key])

            model_args = _convert_loaded_hparams(model_args, checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_TYPE))

            args_name = checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_NAME)

            if args_name == 'kwargs':
                # in case the class cannot take any extra argument filter only the possible
                cls_kwargs.update(**model_args)
            elif args_name:
                if args_name in cls_init_args_name:
                    cls_kwargs.update({args_name: model_args})
            else:
                cls_args = (model_args,) + cls_args

        if not cls_spec.varkw:
            # filter kwargs according to class init unless it allows any argument via kwargs
            cls_kwargs = {k: v for k, v in cls_kwargs.items() if k in cls_init_args_name}

        # prevent passing positional arguments if class does not accept any
        if len(cls_spec.args) <= 1 and not cls_spec.kwonlyargs:
            cls_args, cls_kwargs = [], {}
        model = cls(*cls_args, **cls_kwargs)
        # load the state_dict on the model automatically
        model.load_state_dict(checkpoint['state_dict'])

        # give model a chance to load something
        model.on_load_checkpoint(checkpoint)

        return model
","    @classmethod
    def _load_model_state(cls, checkpoint: Dict[str, Any], *cls_args, **cls_kwargs):
        cls_spec = inspect.getfullargspec(cls.__init__)
        cls_init_args_name = inspect.signature(cls).parameters.keys()
        # pass in the values we saved automatically
        if cls.CHECKPOINT_HYPER_PARAMS_KEY in checkpoint:
            model_args = {}

            # add some back compatibility, the actual one shall be last
            for hparam_key in CHECKPOINT_PAST_HPARAMS_KEYS + (cls.CHECKPOINT_HYPER_PARAMS_KEY,):
                if hparam_key in checkpoint:
                    model_args.update(checkpoint[hparam_key])

            model_args = _convert_loaded_hparams(model_args, checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_TYPE))

            args_name = checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_NAME)

            if args_name == 'kwargs':
                # in case the class cannot take any extra argument filter only the possible
                cls_kwargs.update(**model_args)
            elif args_name:
                if args_name in cls_init_args_name:
                    cls_kwargs.update({args_name: model_args})
            else:
                cls_args = (model_args,) + cls_args

        if not cls_spec.varkw:
            # filter kwargs according to class init unless it allows any argument via kwargs
            cls_kwargs = {k: v for k, v in cls_kwargs.items() if k in cls_init_args_name}

        # prevent passing positional arguments if class does not accept any
        if len(cls_spec.args) <= 1 and not cls_spec.varargs and not cls_spec.kwonlyargs:
            cls_args, cls_kwargs = [], {}

        model = cls(*cls_args, **cls_kwargs)
        # load the state_dict on the model automatically
        model.load_state_dict(checkpoint['state_dict'])

        # give model a chance to load something
        model.on_load_checkpoint(checkpoint)

        return model
",https://github.com/PyTorchLightning/pytorch-lightning/issues/2909,CWE-754: Improper Check for Unusual or Exceptional Conditions,No check if attribute varargs is None,pytorch_lightning/core/saving.py,ModelIO._load_model_state,[32],"Traceback (most recent call last):
File ""main.py"", line 64, in <module>
model = LitModel.load_from_checkpoint(hparams.checkpoint)
File ""/home/siahuat0727/.local/lib/python3.8/site-packages/pytorch_lightning/core/saving.py"", line 138, in load_from_checkpoint
model = cls._load_model_state(checkpoint, *args, **kwargs)
File ""/home/siahuat0727/.local/lib/python3.8/site-packages/pytorch_lightning/core/saving.py"", line 174, in _load_model_state
model = cls(*cls_args, **cls_kwargs)
File ""main.py"", line 46, in __init__
super().__init__(*args, **kwargs)
TypeError: __init__() missing 1 required positional argument: 'hparams'",TypeError,buggy_snippets_files/c057c0020bce902c0edfea386ead3bd98879012694be5e7467f919b5d7ee999d_before_merge.py,buggy_snippets_files/c057c0020bce902c0edfea386ead3bd98879012694be5e7467f919b5d7ee999d_after_merge.py
32993,"    def __init__(self, text: base.String,
                 request_contact: base.Boolean = None,
                 request_location: base.Boolean = None,
                 request_poll: KeyboardButtonPollType = None):
        super(KeyboardButton, self).__init__(text=text,
                                             request_contact=request_contact,
                                             request_location=request_location,
                                             request_poll=request_poll)
","    def __init__(self, text: base.String,
                 request_contact: base.Boolean = None,
                 request_location: base.Boolean = None,
                 request_poll: KeyboardButtonPollType = None,
                 **kwargs):
        super(KeyboardButton, self).__init__(text=text,
                                             request_contact=request_contact,
                                             request_location=request_location,
                                             request_poll=request_poll,
                                             **kwargs)
",https://github.com/aiogram/aiogram/issues/343,?,Method does not support field from parent object. Adding **kwargs solve that issue,aiogram/types/reply_keyboard.py,KeyboardButton.__init__,[4;8],"Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""/usr/lib/python3.8/site-packages/aiogram/types/base.py"", line 145, in to_object
return cls(**data)
File ""/usr/lib/python3.8/site-packages/aiogram/types/reply_keyboard.py"", line 35, in __init__
super(ReplyKeyboardMarkup, self).__init__(keyboard=keyboard, resize_keyboard=resize_keyboard,
File ""/usr/lib/python3.8/site-packages/aiogram/types/base.py"", line 91, in __init__
self.props[key].set_value(self, value, parent=self)
File ""/usr/lib/python3.8/site-packages/aiogram/types/fields.py"", line 56, in set_value
value = self.deserialize(value, parent)
File ""/usr/lib/python3.8/site-packages/aiogram/types/fields.py"", line 161, in deserialize
row_result.append(deserialize(item, parent=parent))
File ""/usr/lib/python3.8/site-packages/aiogram/types/fields.py"", line 112, in deserialize
return self.base_object(conf={'parent': parent}, **value)
TypeError: __init__() got an unexpected keyword argument 'conf'",TypeError,buggy_snippets_files/d5a8ce20a1579fbc69a6e6dbf46c1770d7e2bb0c62f331a76fe83228d209929d_before_merge.py,buggy_snippets_files/d5a8ce20a1579fbc69a6e6dbf46c1770d7e2bb0c62f331a76fe83228d209929d_after_merge.py
210989,"    def get_vars(self, play=None, host=None, task=None, include_hostvars=True, include_delegate_to=True, use_cache=True):
        '''
        Returns the variables, with optional ""context"" given via the parameters
        for the play, host, and task (which could possibly result in different
        sets of variables being returned due to the additional context).

        The order of precedence is:
        - play->roles->get_default_vars (if there is a play context)
        - group_vars_files[host] (if there is a host context)
        - host_vars_files[host] (if there is a host context)
        - host->get_vars (if there is a host context)
        - fact_cache[host] (if there is a host context)
        - play vars (if there is a play context)
        - play vars_files (if there's no host context, ignore
          file names that cannot be templated)
        - task->get_vars (if there is a task context)
        - vars_cache[host] (if there is a host context)
        - extra vars
        '''

        display.debug(""in VariableManager get_vars()"")

        all_vars = dict()
        magic_variables = self._get_magic_variables(
            play=play,
            host=host,
            task=task,
            include_hostvars=include_hostvars,
            include_delegate_to=include_delegate_to,
        )

        # default for all cases
        basedirs = [self._loader.get_basedir()]

        if play:
            # first we compile any vars specified in defaults/main.yml
            # for all roles within the specified play
            for role in play.get_roles():
                all_vars = combine_vars(all_vars, role.get_default_vars())

        if task:
            # set basedirs
            if C.PLAYBOOK_VARS_ROOT == 'all':  # should be default
                basedirs = task.get_search_path()
            elif C.PLAYBOOK_VARS_ROOT in ('bottom', 'playbook_dir'):  # only option in 2.4.0
                basedirs = [task.get_search_path()[0]]
            elif C.PLAYBOOK_VARS_ROOT != 'top':
                # preserves default basedirs, only option pre 2.3
                raise AnsibleError('Unkown playbook vars logic: %s' % C.PLAYBOOK_VARS_ROOT)

            # if we have a task in this context, and that task has a role, make
            # sure it sees its defaults above any other roles, as we previously
            # (v1) made sure each task had a copy of its roles default vars
            if task._role is not None and (play or task.action == 'include_role'):
                all_vars = combine_vars(all_vars, task._role.get_default_vars(dep_chain=task.get_dep_chain()))

        if host:
            # THE 'all' group and the rest of groups for a host, used below
            all_group = self._inventory.groups.get('all')
            host_groups = sort_groups([g for g in host.get_groups() if g.name not in ['all']])

            def _get_plugin_vars(plugin, path, entities):
                data = {}
                try:
                    data = plugin.get_vars(self._loader, path, entities)
                except AttributeError:
                    try:
                        for entity in entities:
                            if isinstance(entity, Host):
                                data.update(plugin.get_host_vars(entity.name))
                            else:
                                data.update(plugin.get_group_vars(entity.name))
                    except AttributeError:
                        if hasattr(plugin, 'run'):
                            raise AnsibleError(""Cannot use v1 type vars plugin %s from %s"" % (plugin._load_name, plugin._original_path))
                        else:
                            raise AnsibleError(""Invalid vars plugin %s from %s"" % (plugin._load_name, plugin._original_path))
                return data

            # internal fuctions that actually do the work
            def _plugins_inventory(entities):
                ''' merges all entities by inventory source '''
                data = {}
                for inventory_dir in self._inventory._sources:
                    if ',' in inventory_dir and not os.path.exists(inventory_dir):  # skip host lists
                        continue
                    elif not os.path.isdir(inventory_dir):  # always pass 'inventory directory'
                        inventory_dir = os.path.dirname(inventory_dir)

                    for plugin in vars_loader.all():

                        data = combine_vars(data, _get_plugin_vars(plugin, inventory_dir, entities))
                return data

            def _plugins_play(entities):
                ''' merges all entities adjacent to play '''
                data = {}
                for plugin in vars_loader.all():

                    for path in basedirs:
                        data = combine_vars(data, _get_plugin_vars(plugin, path, entities))
                return data

            # configurable functions that are sortable via config, rememer to add to _ALLOWED if expanding this list
            def all_inventory():
                return all_group.get_vars()

            def all_plugins_inventory():
                return _plugins_inventory([all_group])

            def all_plugins_play():
                return _plugins_play([all_group])

            def groups_inventory():
                ''' gets group vars from inventory '''
                return get_group_vars(host_groups)

            def groups_plugins_inventory():
                ''' gets plugin sources from inventory for groups '''
                return _plugins_inventory(host_groups)

            def groups_plugins_play():
                ''' gets plugin sources from play for groups '''
                return _plugins_play(host_groups)

            def plugins_by_groups():
                '''
                    merges all plugin sources by group,
                    This should be used instead, NOT in combination with the other groups_plugins* functions
                '''
                data = {}
                for group in host_groups:
                    data[group] = combine_vars(data[group], _plugins_inventory(group))
                    data[group] = combine_vars(data[group], _plugins_play(group))
                return data

            # Merge groups as per precedence config
            # only allow to call the functions we want exposed
            for entry in C.VARIABLE_PRECEDENCE:
                if entry in self._ALLOWED:
                    display.debug('Calling %s to load vars for %s' % (entry, host.name))
                    all_vars = combine_vars(all_vars, locals()[entry]())
                else:
                    display.warning('Ignoring unknown variable precedence entry: %s' % (entry))

            # host vars, from inventory, inventory adjacent and play adjacent via plugins
            all_vars = combine_vars(all_vars, host.get_vars())
            all_vars = combine_vars(all_vars, _plugins_inventory([host]))
            all_vars = combine_vars(all_vars, _plugins_play([host]))

            # finally, the facts caches for this host, if it exists
            try:
                facts = self._fact_cache.get(host.name, {})
                all_vars.update(namespace_facts(facts))

                # push facts to main namespace
                if C.INJECT_FACTS_AS_VARS:
                    all_vars = combine_vars(all_vars, wrap_var(facts))
                else:
                    # always 'promote' ansible_local
                    all_vars = combine_vars(all_vars, wrap_var({'ansible_local': facts.get('ansible_local', {})}))
            except KeyError:
                pass

        if play:
            all_vars = combine_vars(all_vars, play.get_vars())

            vars_files = play.get_vars_files()
            try:
                for vars_file_item in vars_files:
                    # create a set of temporary vars here, which incorporate the extra
                    # and magic vars so we can properly template the vars_files entries
                    temp_vars = combine_vars(all_vars, self._extra_vars)
                    temp_vars = combine_vars(temp_vars, magic_variables)
                    templar = Templar(loader=self._loader, variables=temp_vars)

                    # we assume each item in the list is itself a list, as we
                    # support ""conditional includes"" for vars_files, which mimics
                    # the with_first_found mechanism.
                    vars_file_list = vars_file_item
                    if not isinstance(vars_file_list, list):
                        vars_file_list = [vars_file_list]

                    # now we iterate through the (potential) files, and break out
                    # as soon as we read one from the list. If none are found, we
                    # raise an error, which is silently ignored at this point.
                    try:
                        for vars_file in vars_file_list:
                            vars_file = templar.template(vars_file)
                            try:
                                data = preprocess_vars(self._loader.load_from_file(vars_file, unsafe=True))
                                if data is not None:
                                    for item in data:
                                        all_vars = combine_vars(all_vars, item)
                                break
                            except AnsibleFileNotFound:
                                # we continue on loader failures
                                continue
                            except AnsibleParserError:
                                raise
                        else:
                            # if include_delegate_to is set to False, we ignore the missing
                            # vars file here because we're working on a delegated host
                            if include_delegate_to:
                                raise AnsibleFileNotFound(""vars file %s was not found"" % vars_file_item)
                    except (UndefinedError, AnsibleUndefinedVariable):
                        if host is not None and self._fact_cache.get(host.name, dict()).get('module_setup') and task is not None:
                            raise AnsibleUndefinedVariable(""an undefined variable was found when attempting to template the vars_files item '%s'""
                                                           % vars_file_item, obj=vars_file_item)
                        else:
                            # we do not have a full context here, and the missing variable could be because of that
                            # so just show a warning and continue
                            display.vvv(""skipping vars_file '%s' due to an undefined variable"" % vars_file_item)
                            continue

                    display.vvv(""Read vars_file '%s'"" % vars_file_item)
            except TypeError:
                raise AnsibleParserError(""Error while reading vars files - please supply a list of file names. ""
                                         ""Got '%s' of type %s"" % (vars_files, type(vars_files)))

            # By default, we now merge in all vars from all roles in the play,
            # unless the user has disabled this via a config option
            if not C.DEFAULT_PRIVATE_ROLE_VARS:
                for role in play.get_roles():
                    all_vars = combine_vars(all_vars, role.get_vars(include_params=False))

        # next, we merge in the vars from the role, which will specifically
        # follow the role dependency chain, and then we merge in the tasks
        # vars (which will look at parent blocks/task includes)
        if task:
            if task._role:
                all_vars = combine_vars(all_vars, task._role.get_vars(task.get_dep_chain(), include_params=False))
            all_vars = combine_vars(all_vars, task.get_vars())

        # next, we merge in the vars cache (include vars) and nonpersistent
        # facts cache (set_fact/register), in that order
        if host:
            # include_vars non-persistent cache
            all_vars = combine_vars(all_vars, self._vars_cache.get(host.get_name(), dict()))
            # fact non-persistent cache
            all_vars = combine_vars(all_vars, self._nonpersistent_fact_cache.get(host.name, dict()))

        # next, we merge in role params and task include params
        if task:
            if task._role:
                all_vars = combine_vars(all_vars, task._role.get_role_params(task.get_dep_chain()))

            # special case for include tasks, where the include params
            # may be specified in the vars field for the task, which should
            # have higher precedence than the vars/np facts above
            all_vars = combine_vars(all_vars, task.get_include_params())

        # extra vars
        all_vars = combine_vars(all_vars, self._extra_vars)

        # magic variables
        all_vars = combine_vars(all_vars, magic_variables)

        # special case for the 'environment' magic variable, as someone
        # may have set it as a variable and we don't want to stomp on it
        if task:
            all_vars['environment'] = task.environment

        # if we have a task and we're delegating to another host, figure out the
        # variables for that host now so we don't have to rely on hostvars later
        if task and task.delegate_to is not None and include_delegate_to:
            all_vars['ansible_delegated_vars'] = self._get_delegated_vars(play, task, all_vars)

        # 'vars' magic var
        if task or play:
            # has to be copy, otherwise recursive ref
            all_vars['vars'] = all_vars.copy()

        display.debug(""done with get_vars()"")
        return all_vars
","    def get_vars(self, play=None, host=None, task=None, include_hostvars=True, include_delegate_to=True, use_cache=True):
        '''
        Returns the variables, with optional ""context"" given via the parameters
        for the play, host, and task (which could possibly result in different
        sets of variables being returned due to the additional context).

        The order of precedence is:
        - play->roles->get_default_vars (if there is a play context)
        - group_vars_files[host] (if there is a host context)
        - host_vars_files[host] (if there is a host context)
        - host->get_vars (if there is a host context)
        - fact_cache[host] (if there is a host context)
        - play vars (if there is a play context)
        - play vars_files (if there's no host context, ignore
          file names that cannot be templated)
        - task->get_vars (if there is a task context)
        - vars_cache[host] (if there is a host context)
        - extra vars
        '''

        display.debug(""in VariableManager get_vars()"")

        all_vars = dict()
        magic_variables = self._get_magic_variables(
            play=play,
            host=host,
            task=task,
            include_hostvars=include_hostvars,
            include_delegate_to=include_delegate_to,
        )

        # default for all cases
        basedirs = [self._loader.get_basedir()]

        if play:
            # first we compile any vars specified in defaults/main.yml
            # for all roles within the specified play
            for role in play.get_roles():
                all_vars = combine_vars(all_vars, role.get_default_vars())

        if task:
            # set basedirs
            if C.PLAYBOOK_VARS_ROOT == 'all':  # should be default
                basedirs = task.get_search_path()
            elif C.PLAYBOOK_VARS_ROOT in ('bottom', 'playbook_dir'):  # only option in 2.4.0
                basedirs = [task.get_search_path()[0]]
            elif C.PLAYBOOK_VARS_ROOT != 'top':
                # preserves default basedirs, only option pre 2.3
                raise AnsibleError('Unkown playbook vars logic: %s' % C.PLAYBOOK_VARS_ROOT)

            # if we have a task in this context, and that task has a role, make
            # sure it sees its defaults above any other roles, as we previously
            # (v1) made sure each task had a copy of its roles default vars
            if task._role is not None and (play or task.action == 'include_role'):
                all_vars = combine_vars(all_vars, task._role.get_default_vars(dep_chain=task.get_dep_chain()))

        if host:
            # THE 'all' group and the rest of groups for a host, used below
            all_group = self._inventory.groups.get('all')
            host_groups = sort_groups([g for g in host.get_groups() if g.name not in ['all']])

            def _get_plugin_vars(plugin, path, entities):
                data = {}
                try:
                    data = plugin.get_vars(self._loader, path, entities)
                except AttributeError:
                    try:
                        for entity in entities:
                            if isinstance(entity, Host):
                                data.update(plugin.get_host_vars(entity.name))
                            else:
                                data.update(plugin.get_group_vars(entity.name))
                    except AttributeError:
                        if hasattr(plugin, 'run'):
                            raise AnsibleError(""Cannot use v1 type vars plugin %s from %s"" % (plugin._load_name, plugin._original_path))
                        else:
                            raise AnsibleError(""Invalid vars plugin %s from %s"" % (plugin._load_name, plugin._original_path))
                return data

            # internal fuctions that actually do the work
            def _plugins_inventory(entities):
                ''' merges all entities by inventory source '''
                data = {}
                for inventory_dir in self._inventory._sources:
                    if ',' in inventory_dir and not os.path.exists(inventory_dir):  # skip host lists
                        continue
                    elif not os.path.isdir(inventory_dir):  # always pass 'inventory directory'
                        inventory_dir = os.path.dirname(inventory_dir)

                    for plugin in vars_loader.all():

                        data = combine_vars(data, _get_plugin_vars(plugin, inventory_dir, entities))
                return data

            def _plugins_play(entities):
                ''' merges all entities adjacent to play '''
                data = {}
                for plugin in vars_loader.all():

                    for path in basedirs:
                        data = combine_vars(data, _get_plugin_vars(plugin, path, entities))
                return data

            # configurable functions that are sortable via config, rememer to add to _ALLOWED if expanding this list
            def all_inventory():
                return all_group.get_vars()

            def all_plugins_inventory():
                return _plugins_inventory([all_group])

            def all_plugins_play():
                return _plugins_play([all_group])

            def groups_inventory():
                ''' gets group vars from inventory '''
                return get_group_vars(host_groups)

            def groups_plugins_inventory():
                ''' gets plugin sources from inventory for groups '''
                return _plugins_inventory(host_groups)

            def groups_plugins_play():
                ''' gets plugin sources from play for groups '''
                return _plugins_play(host_groups)

            def plugins_by_groups():
                '''
                    merges all plugin sources by group,
                    This should be used instead, NOT in combination with the other groups_plugins* functions
                '''
                data = {}
                for group in host_groups:
                    data[group] = combine_vars(data[group], _plugins_inventory(group))
                    data[group] = combine_vars(data[group], _plugins_play(group))
                return data

            # Merge groups as per precedence config
            # only allow to call the functions we want exposed
            for entry in C.VARIABLE_PRECEDENCE:
                if entry in self._ALLOWED:
                    display.debug('Calling %s to load vars for %s' % (entry, host.name))
                    all_vars = combine_vars(all_vars, locals()[entry]())
                else:
                    display.warning('Ignoring unknown variable precedence entry: %s' % (entry))

            # host vars, from inventory, inventory adjacent and play adjacent via plugins
            all_vars = combine_vars(all_vars, host.get_vars())
            all_vars = combine_vars(all_vars, _plugins_inventory([host]))
            all_vars = combine_vars(all_vars, _plugins_play([host]))

            # finally, the facts caches for this host, if it exists
            try:
                facts = self._fact_cache.get(host.name, {})
                all_vars.update(namespace_facts(facts))

                # push facts to main namespace
                if C.INJECT_FACTS_AS_VARS:
                    all_vars = combine_vars(all_vars, wrap_var(facts))
                else:
                    # always 'promote' ansible_local
                    all_vars = combine_vars(all_vars, wrap_var({'ansible_local': facts.get('ansible_local', {})}))
            except KeyError:
                pass

        if play:
            all_vars = combine_vars(all_vars, play.get_vars())

            vars_files = play.get_vars_files()
            try:
                for vars_file_item in vars_files:
                    # create a set of temporary vars here, which incorporate the extra
                    # and magic vars so we can properly template the vars_files entries
                    temp_vars = combine_vars(all_vars, self._extra_vars)
                    temp_vars = combine_vars(temp_vars, magic_variables)
                    templar = Templar(loader=self._loader, variables=temp_vars)

                    # we assume each item in the list is itself a list, as we
                    # support ""conditional includes"" for vars_files, which mimics
                    # the with_first_found mechanism.
                    vars_file_list = vars_file_item
                    if not isinstance(vars_file_list, list):
                        vars_file_list = [vars_file_list]

                    # now we iterate through the (potential) files, and break out
                    # as soon as we read one from the list. If none are found, we
                    # raise an error, which is silently ignored at this point.
                    try:
                        for vars_file in vars_file_list:
                            vars_file = templar.template(vars_file)
                            if not (isinstance(vars_file, Sequence)):
                                raise AnsibleError(
                                    ""Invalid vars_files entry found: %r\n""
                                    ""vars_files entries should be either a string type or ""
                                    ""a list of string types after template expansion"" % vars_file
                                )
                            try:
                                data = preprocess_vars(self._loader.load_from_file(vars_file, unsafe=True))
                                if data is not None:
                                    for item in data:
                                        all_vars = combine_vars(all_vars, item)
                                break
                            except AnsibleFileNotFound:
                                # we continue on loader failures
                                continue
                            except AnsibleParserError:
                                raise
                        else:
                            # if include_delegate_to is set to False, we ignore the missing
                            # vars file here because we're working on a delegated host
                            if include_delegate_to:
                                raise AnsibleFileNotFound(""vars file %s was not found"" % vars_file_item)
                    except (UndefinedError, AnsibleUndefinedVariable):
                        if host is not None and self._fact_cache.get(host.name, dict()).get('module_setup') and task is not None:
                            raise AnsibleUndefinedVariable(""an undefined variable was found when attempting to template the vars_files item '%s'""
                                                           % vars_file_item, obj=vars_file_item)
                        else:
                            # we do not have a full context here, and the missing variable could be because of that
                            # so just show a warning and continue
                            display.vvv(""skipping vars_file '%s' due to an undefined variable"" % vars_file_item)
                            continue

                    display.vvv(""Read vars_file '%s'"" % vars_file_item)
            except TypeError:
                raise AnsibleParserError(""Error while reading vars files - please supply a list of file names. ""
                                         ""Got '%s' of type %s"" % (vars_files, type(vars_files)))

            # By default, we now merge in all vars from all roles in the play,
            # unless the user has disabled this via a config option
            if not C.DEFAULT_PRIVATE_ROLE_VARS:
                for role in play.get_roles():
                    all_vars = combine_vars(all_vars, role.get_vars(include_params=False))

        # next, we merge in the vars from the role, which will specifically
        # follow the role dependency chain, and then we merge in the tasks
        # vars (which will look at parent blocks/task includes)
        if task:
            if task._role:
                all_vars = combine_vars(all_vars, task._role.get_vars(task.get_dep_chain(), include_params=False))
            all_vars = combine_vars(all_vars, task.get_vars())

        # next, we merge in the vars cache (include vars) and nonpersistent
        # facts cache (set_fact/register), in that order
        if host:
            # include_vars non-persistent cache
            all_vars = combine_vars(all_vars, self._vars_cache.get(host.get_name(), dict()))
            # fact non-persistent cache
            all_vars = combine_vars(all_vars, self._nonpersistent_fact_cache.get(host.name, dict()))

        # next, we merge in role params and task include params
        if task:
            if task._role:
                all_vars = combine_vars(all_vars, task._role.get_role_params(task.get_dep_chain()))

            # special case for include tasks, where the include params
            # may be specified in the vars field for the task, which should
            # have higher precedence than the vars/np facts above
            all_vars = combine_vars(all_vars, task.get_include_params())

        # extra vars
        all_vars = combine_vars(all_vars, self._extra_vars)

        # magic variables
        all_vars = combine_vars(all_vars, magic_variables)

        # special case for the 'environment' magic variable, as someone
        # may have set it as a variable and we don't want to stomp on it
        if task:
            all_vars['environment'] = task.environment

        # if we have a task and we're delegating to another host, figure out the
        # variables for that host now so we don't have to rely on hostvars later
        if task and task.delegate_to is not None and include_delegate_to:
            all_vars['ansible_delegated_vars'] = self._get_delegated_vars(play, task, all_vars)

        # 'vars' magic var
        if task or play:
            # has to be copy, otherwise recursive ref
            all_vars['vars'] = all_vars.copy()

        display.debug(""done with get_vars()"")
        return all_vars
",https://github.com/ansible/ansible/issues/17594,CWE-754: Improper Check for Unusual or Exceptional Conditions,No check if vars_file has a proper type,lib/ansible/vars/manager.py,VariableManager.get_vars,"[189, 190]","ansible-playbook -vvv --syntax-check jenkins_ssh.yml
Using /home/lmadsen/src/github/leifmadsen/ansible-cira/ansible.cfg as config file
1 plays in jenkins_ssh.yml
ERROR! Unexpected Exception: 0
the full traceback was:

Traceback (most recent call last):
File ""/usr/bin/ansible-playbook"", line 86, in <module>
sys.exit(cli.run())
File ""/home/lmadsen/.local/lib/python2.7/site-packages/ansible/cli/playbook.py"", line 154, in run
results = pbex.run()
File ""/home/lmadsen/.local/lib/python2.7/site-packages/ansible/executor/playbook_executor.py"", line 117, in run
all_vars = self._variable_manager.get_vars(loader=self._loader, play=play)
File ""/home/lmadsen/.local/lib/python2.7/site-packages/ansible/vars/__init__.py"", line 305, in get_vars
data = preprocess_vars(loader.load_from_file(vars_file))
File ""/home/lmadsen/.local/lib/python2.7/site-packages/ansible/parsing/dataloader.py"", line 105, in load_from_file
file_name = self.path_dwim(file_name)
File ""/home/lmadsen/.local/lib/python2.7/site-packages/ansible/parsing/dataloader.py"", line 212, in path_dwim
given = unquote(given)
File ""/home/lmadsen/.local/lib/python2.7/site-packages/ansible/parsing/quoting.py"", line 28, in unquote
if is_quoted(data):
File ""/home/lmadsen/.local/lib/python2.7/site-packages/ansible/parsing/quoting.py"", line 24, in is_quoted
return len(data) > 1 and data[0] == data[-1] and data[0] in ('""', ""'"") and data[-2] != '\\'
KeyError: 0",KeyError,buggy_snippets_files/59f64056f98ad35319de61d9ad7de24b9290add6708f6eacee2c5ac7a018ed4e_before_merge.py,buggy_snippets_files/59f64056f98ad35319de61d9ad7de24b9290add6708f6eacee2c5ac7a018ed4e_after_merge.py
217999,"    def _get_elb_info(self, elb):
        elb_info = {
            'name': elb.name,
            'zones': elb.availability_zones,
            'dns_name': elb.dns_name,
            'canonical_hosted_zone_name': elb.canonical_hosted_zone_name,
            'canonical_hosted_zone_name_id': elb.canonical_hosted_zone_name_id,
            'hosted_zone_name': elb.canonical_hosted_zone_name,
            'hosted_zone_id': elb.canonical_hosted_zone_name_id,
            'instances': [instance.id for instance in elb.instances],
            'listeners': self._get_elb_listeners(elb.listeners),
            'scheme': elb.scheme,
            'security_groups': elb.security_groups,
            'health_check': self._get_health_check(elb.health_check),
            'subnets': elb.subnets,
            'instances_inservice': [],
            'instances_inservice_count': 0,
            'instances_outofservice': [],
            'instances_outofservice_count': 0,
            'instances_inservice_percent': 0.0,
            'tags': self._get_tags(elb.name)
        }

        if elb.vpc_id:
            elb_info['vpc_id'] = elb.vpc_id

        if elb.instances:
            try:
                instance_health = self.connection.describe_instance_health(elb.name)
            except BotoServerError as err:
                self.module.fail_json(msg=err.message)
            elb_info['instances_inservice'] = [inst.instance_id for inst in instance_health if inst.state == 'InService']
            elb_info['instances_inservice_count'] = len(elb_info['instances_inservice'])
            elb_info['instances_outofservice'] = [inst.instance_id for inst in instance_health if inst.state == 'OutOfService']
            elb_info['instances_outofservice_count'] = len(elb_info['instances_outofservice'])
            elb_info['instances_inservice_percent'] = float(elb_info['instances_inservice_count'])/(
                float(elb_info['instances_inservice_count']) +
                float(elb_info['instances_outofservice_count']))*100
        return elb_info
","    def _get_elb_info(self, elb):
        elb_info = {
            'name': elb.name,
            'zones': elb.availability_zones,
            'dns_name': elb.dns_name,
            'canonical_hosted_zone_name': elb.canonical_hosted_zone_name,
            'canonical_hosted_zone_name_id': elb.canonical_hosted_zone_name_id,
            'hosted_zone_name': elb.canonical_hosted_zone_name,
            'hosted_zone_id': elb.canonical_hosted_zone_name_id,
            'instances': [instance.id for instance in elb.instances],
            'listeners': self._get_elb_listeners(elb.listeners),
            'scheme': elb.scheme,
            'security_groups': elb.security_groups,
            'health_check': self._get_health_check(elb.health_check),
            'subnets': elb.subnets,
            'instances_inservice': [],
            'instances_inservice_count': 0,
            'instances_outofservice': [],
            'instances_outofservice_count': 0,
            'instances_inservice_percent': 0.0,
            'tags': self._get_tags(elb.name)
        }

        if elb.vpc_id:
            elb_info['vpc_id'] = elb.vpc_id

        if elb.instances:
            try:
                instance_health = self.connection.describe_instance_health(elb.name)
            except BotoServerError as err:
                self.module.fail_json(msg=err.message)
            elb_info['instances_inservice'] = [inst.instance_id for inst in instance_health if inst.state == 'InService']
            elb_info['instances_inservice_count'] = len(elb_info['instances_inservice'])
            elb_info['instances_outofservice'] = [inst.instance_id for inst in instance_health if inst.state == 'OutOfService']
            elb_info['instances_outofservice_count'] = len(elb_info['instances_outofservice'])
            try:
                elb_info['instances_inservice_percent'] = (
                    float(elb_info['instances_inservice_count']) /
                    float(elb_info['instances_inservice_count'] + elb_info['instances_outofservice_count'])
                ) * 100.
            except ZeroDivisionError:
                elb_info['instances_inservice_percent'] = 0.
        return elb_info
",https://github.com/ansible/ansible/issues/22014,"CWE-369: Divide By Zero, CWE-754: Improper Check for Unusual or Exceptional Conditions",No zero dividing check,lib/ansible/modules/cloud/amazon/ec2_elb_facts.py,ElbInformation._get_elb_info,"[36, 38]","Traceback (most recent call last):
File ""/tmp/ansible_mX7IN8/ansible_module_ec2_elb_facts.py"", line 248, in <module>
main()
File ""/tmp/ansible_mX7IN8/ansible_module_ec2_elb_facts.py"", line 240, in main
elbs=elb_information.list_elbs())
File ""/tmp/ansible_mX7IN8/ansible_module_ec2_elb_facts.py"", line 215, in list_elbs
return list(map(self._get_elb_info, elb_array))
File ""/tmp/ansible_mX7IN8/ansible_module_ec2_elb_facts.py"", line 195, in _get_elb_info
float(elb_info['instances_outofservice_count']))*100
ZeroDivisionError: float division by zero",ZeroDivisionError,buggy_snippets_files/cdc43509435ba2a47e8d59487185d645cb7d58db6c4a5aa00b11c7c46608f14a_before_merge.py,buggy_snippets_files/cdc43509435ba2a47e8d59487185d645cb7d58db6c4a5aa00b11c7c46608f14a_after_merge.py
216874,"    def disconnect_all_containers(self):
        containers = self.client.inspect_network(self.parameters.network_name)['Containers']
        for cont in containers.values():
            self.disconnect_container(cont['Name'])
","    def disconnect_all_containers(self):
        containers = self.client.inspect_network(self.parameters.network_name)['Containers']
        if not containers:
            return
        for cont in containers.values():
            self.disconnect_container(cont['Name'])
",https://github.com/ansible/ansible/issues/22530,CWE-754: Improper Check for Unusual or Exceptional Conditions,No checking if object `containers` is None,lib/ansible/modules/cloud/docker/docker_network.py,DockerNetworkManager.disconnect_all_containers,[3],"The full traceback is:
Traceback (most recent call last):
File ""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py"", line 392, in <module>
main()
File ""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py"", line 385, in main
cm = DockerNetworkManager(client)
File ""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py"", line 218, in __init__
self.present()
File ""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py"", line 352, in present
self.disconnect_missing()
File ""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py"", line 323, in disconnect_missing
for c in self.existing_network['Containers'].values():
AttributeError: 'NoneType' object has no attribute 'values'

failed: [192.168.1.120 -> 127.0.0.1] (item={u'driver': u'overlay', u'name': u'networks_default_1'}) => {
""failed"": true,
""item"": {
""driver"": ""overlay"",
""name"": ""networks_default_1""
},
""module_stderr"": ""Traceback (most recent call last):\n  File \""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py\"", line 392, in <module>\n    main()\n  File \""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py\"", line 385, in main\n    cm = DockerNetworkManager(client)\n  File \""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py\"", line 218, in __init__\n    self.present()\n  File \""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py\"", line 352, in present\n    self.disconnect_missing()\n  File \""/tmp/ansible_9Ntaqw/ansible_module_docker_network.py\"", line 323, in disconnect_missing\n    for c in self.existing_network['Containers'].values():\nAttributeError: 'NoneType' object has no attribute 'values'\n"",
""module_stdout"": """",
""msg"": ""MODULE FAILURE"",
""rc"": 0",AttributeError,buggy_snippets_files/92d3f1527e623471fac40d7dff88891498f15bdd08bd1b5a8c5d30bfd17bd82d_before_merge.py,buggy_snippets_files/92d3f1527e623471fac40d7dff88891498f15bdd08bd1b5a8c5d30bfd17bd82d_after_merge.py
217191,"def main():
    """""" main entry point for module execution
    """"""
    argument_spec = dict(
        lines=dict(type='list'),

        src=dict(type='path'),
        src_format=dict(choices=['xml', 'text', 'set', 'json']),

        # update operations
        update=dict(default='merge', choices=['merge', 'override', 'replace', 'update']),

        # deprecated replace in Ansible 2.3
        replace=dict(type='bool'),

        confirm=dict(default=0, type='int'),
        comment=dict(default=DEFAULT_COMMENT),

        # config operations
        backup=dict(type='bool', default=False),
        rollback=dict(type='int'),

        zeroize=dict(default=False, type='bool'),
    )

    argument_spec.update(junos_argument_spec)

    mutually_exclusive = [('lines', 'src', 'rollback', 'zeroize')]

    module = AnsibleModule(argument_spec=argument_spec,
                           mutually_exclusive=mutually_exclusive,
                           supports_check_mode=True)

    warnings = list()
    check_args(module, warnings)

    result = {'changed': False, 'warnings': warnings}

    if module.params['backup']:
        for conf_format in ['set', 'text']:
            reply = get_configuration(module, format=conf_format)
            match = reply.find('.//configuration-%s' % conf_format)
            if match is not None:
                break
        else:
            module.fail_json(msg='unable to retrieve device configuration')

        result['__backup__'] = str(match.text).strip()

    if module.params['rollback']:
        if not module.check_mode:
            diff = rollback(module)
            if module._diff:
                result['diff'] = {'prepared': diff}
        result['changed'] = True

    elif module.params['zeroize']:
        if not module.check_mode:
            zeroize(module)
        result['changed'] = True

    else:
        diff = configure_device(module, warnings)
        if diff is not None:
            if module._diff:
                result['diff'] = {'prepared': diff}
            result['changed'] = True

    module.exit_json(**result)
","def main():
    """""" main entry point for module execution
    """"""
    argument_spec = dict(
        lines=dict(type='list'),

        src=dict(type='path'),
        src_format=dict(choices=['xml', 'text', 'set', 'json']),

        # update operations
        update=dict(default='merge', choices=['merge', 'override', 'replace', 'update']),

        # deprecated replace in Ansible 2.3
        replace=dict(type='bool'),

        confirm=dict(default=0, type='int'),
        comment=dict(default=DEFAULT_COMMENT),

        # config operations
        backup=dict(type='bool', default=False),
        rollback=dict(type='int'),

        zeroize=dict(default=False, type='bool'),
    )

    argument_spec.update(junos_argument_spec)

    mutually_exclusive = [('lines', 'src', 'rollback', 'zeroize')]

    module = AnsibleModule(argument_spec=argument_spec,
                           mutually_exclusive=mutually_exclusive,
                           supports_check_mode=True)

    warnings = list()
    check_args(module, warnings)

    result = {'changed': False, 'warnings': warnings}

    if module.params['backup']:
        for conf_format in ['set', 'text']:
            reply = get_configuration(module, format=conf_format)
            match = reply.find('.//configuration-%s' % conf_format)
            if match is not None:
                break
        else:
            module.fail_json(msg='unable to retrieve device configuration')

        result['__backup__'] = match.text.strip()

    if module.params['rollback']:
        if not module.check_mode:
            diff = rollback(module)
            if module._diff:
                result['diff'] = {'prepared': diff}
        result['changed'] = True

    elif module.params['zeroize']:
        if not module.check_mode:
            zeroize(module)
        result['changed'] = True

    else:
        diff = configure_device(module, warnings)
        if diff is not None:
            if module._diff:
                result['diff'] = {'prepared': diff}
            result['changed'] = True

    module.exit_json(**result)
",https://github.com/ansible/ansible/issues/23316,CWE-176: Improper Handling of Unicode Encoding,No need to cast to str. It casuses error when there is problem with encoding (changes in function `filter_delete_statements` fix it),lib/ansible/modules/network/junos/junos_config.py,main,[48],"(ansible-git)[cns04.sea3:/home/users/cody.john/virtualenv/ansible-git/playbook/deploy]$ ansible-playbook junos_config_test.yml -i inventory --limit cor-001a.sea0.service-now.com --check --diff -vvvv
Using /home/users/cody.john/virtualenv/ansible-git/playbook/deploy/ansible.cfg as config file
Loading callback plugin default of type stdout, v2.0 from /home/users/cody.john/virtualenv/ansible-git/ansible/lib/ansible/plugins/callback/__init__.pyc
Loading callback plugin slack of type notification, v2.0 from /home/users/cody.john/virtualenv/ansible-git/ansible/lib/ansible/plugins/callback/__init__.pyc
[WARNING]: The `prettytable` python module is not installed. Disabling the Slack callback plugin.

[WARNING]: Slack Webhook URL was not provided. The Slack Webhook URL can be provided using the `SLACK_WEBHOOK_URL` environment variable.


PLAYBOOK: junos_config_test.yml *************************************************************************************************************
1 plays in junos_config_test.yml

PLAY [Test junos_config module] *************************************************************************************************************
META: ran handlers

TASK [junos_config ACL test] ****************************************************************************************************************
task path: /home/users/cody.john/virtualenv/ansible-git/playbook/deploy/junos_config_test.yml:13
<cor-001a.sea0.service-now.com> using connection plugin netconf
<cor-001a.sea0.service-now.com> socket_path: /home/users/cody.john/.ansible/pc/a647017206
Using module file /home/users/cody.john/virtualenv/ansible-git/ansible/lib/ansible/modules/network/junos/junos_config.py
<cor-001a.sea0.service-now.com> ESTABLISH LOCAL CONNECTION FOR USER: cody.john
<cor-001a.sea0.service-now.com> EXEC /bin/sh -c 'echo ~ &amp;&amp; sleep 0'
<cor-001a.sea0.service-now.com> EXEC /bin/sh -c '( umask 77 &amp;&amp; mkdir -p ""` echo /home/users/cody.john/.ansible/tmp/ansible-tmp-1491414351.75-223916974745349 `"" &amp;&amp; echo ansible-tmp-1491414351.75-223916974745349=""` echo /home/users/cody.john/.ansible/tmp/ansible-tmp-1491414351.75-223916974745349 `"" ) &amp;&amp; sleep 0'
<cor-001a.sea0.service-now.com> PUT /tmp/tmpUR5ngx TO /home/users/cody.john/.ansible/tmp/ansible-tmp-1491414351.75-223916974745349/junos_config.py
<cor-001a.sea0.service-now.com> EXEC /bin/sh -c 'chmod u+x /home/users/cody.john/.ansible/tmp/ansible-tmp-1491414351.75-223916974745349/ /home/users/cody.john/.ansible/tmp/ansible-tmp-1491414351.75-223916974745349/junos_config.py &amp;&amp; sleep 0'
<cor-001a.sea0.service-now.com> EXEC /bin/sh -c '/glide/bin/python27 /home/users/cody.john/.ansible/tmp/ansible-tmp-1491414351.75-223916974745349/junos_config.py; rm -rf ""/home/users/cody.john/.ansible/tmp/ansible-tmp-1491414351.75-223916974745349/"" > /dev/null 2>&amp;1 &amp;&amp; sleep 0'
The full traceback is:
Traceback (most recent call last):
File ""/tmp/ansible_R2QaH_/ansible_module_junos_config.py"", line 341, in <module>
main()
File ""/tmp/ansible_R2QaH_/ansible_module_junos_config.py"", line 316, in main
result['__backup__'] = str(match.text).strip()
UnicodeEncodeError: 'ascii' codec can't encode characters in position 76583-76585: ordinal not in range(128)

fatal: [cor-001a.sea0.service-now.com]: FAILED! => {
""changed"": false,
""failed"": true,
""module_stderr"": ""Traceback (most recent call last):\n  File \""/tmp/ansible_R2QaH_/ansible_module_junos_config.py\"", line 341, in <module>\n    main()\n  File \""/tmp/ansible_R2QaH_/ansible_module_junos_config.py\"", line 316, in main\n    result['__backup__'] = str(match.text).strip()\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 76583-76585: ordinal not in range(128)\n"",
""module_stdout"": """",
""msg"": ""MODULE FAILURE"",
""rc"": 0
}
to retry, use: --limit @/home/users/cody.john/virtualenv/ansible-git/playbook/deploy/junos_config_test.retry

PLAY RECAP **********************************************************************************************************************************
cor-001a.sea0.service-now.com : ok=0    changed=0    unreachable=0    failed=1

(ansible-git)[cns04.sea3:/home/users/cody.john/virtualenv/ansible-git/playbook/deploy]$",UnicodeEncodeError,buggy_snippets_files/6d3959a46f1de015ef99a947a6eca3b8f1d19ed1d09e351e7bee6c01c9ecafa0_before_merge.py,buggy_snippets_files/6d3959a46f1de015ef99a947a6eca3b8f1d19ed1d09e351e7bee6c01c9ecafa0_after_merge.py
336892,"    def list_databases(self):
        """"""
        List the databases available from the WFAU archive.
        """"""
        self.databases = set(self.all_databases + self._get_databases())
        return self.databases
","    def list_databases(self):
        """"""
        List the databases available from the WFAU archive.
        """"""
        self.databases = set(self.all_databases + tuple(self._get_databases()))
        return self.databases
",https://github.com/astropy/astroquery/issues/1763,CWE-704: Incorrect Type Conversion or Cast,"`self.all_databases` is a tuple and `self._get_databases()` is a list, type conversion is needed",astroquery/wfau/core.py,BaseWFAUClass.list_databases,[5],"In [1]: from astroquery.ukidss import Ukidss
dbus[49296]: Dynamic session lookup supported but failed: launchd did not provide a socket path, verify that org.freedesktop.dbus-session.plist is loaded!
dbus[49296]: Dynamic session lookup supported but failed: launchd did not provide a socket path, verify that org.freedesktop.dbus-session.plist is loaded!

In [2]: Ukidss.list_databases()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-5e67702382f5> in <module>
----> 1 Ukidss.list_databases()

/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/astroquery/wfau/core.py in list_databases(self)
674         List the databases available from the WFAU archive.
675         """"""
--> 676         self.databases = set(self.all_databases + self._get_databases())
677         return self.databases
678

TypeError: can only concatenate tuple (not ""list"") to tuple",TypeError,buggy_snippets_files/144a1c28c8e7704cfb6ca854c9af60ffdd66ea41383d3440e52ac27b4f788cce_before_merge.py,buggy_snippets_files/144a1c28c8e7704cfb6ca854c9af60ffdd66ea41383d3440e52ac27b4f788cce_after_merge.py
258588,"    def distribution(self, distr_args, loc=None, scale=None) -> Distribution:
        assert loc is None and scale is None
        distr = Dirichlet(distr_args)
        return distr
","    def distribution(self, distr_args, loc=None, scale=None) -> Distribution:
        distr = Dirichlet(distr_args)
        return distr
",https://github.com/awslabs/gluon-ts/issues/641,CWE-703: Improper Check or Handling of Exceptional Conditions,No need to assert because not every ditribution must request loc and scale,src/gluonts/distribution/dirichlet.py,DirichletOutput.distribution,[2],"---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-8-920902ee83c0> in <module>
22 )
23
---> 24 predictor = estimator.train(train_dataset)

/opt/amazon/lib/python3.6/site-packages/gluonts/model/estimator.py in train(self, training_data, validation_data)
221         self, training_data: Dataset, validation_data: Optional[Dataset] = None
222     ) -> Predictor:
--> 223         return self.train_model(training_data, validation_data).predictor

/opt/amazon/lib/python3.6/site-packages/gluonts/model/estimator.py in train_model(self, training_data, validation_data)
206             input_names=get_hybrid_forward_input_names(trained_net),
207             train_iter=training_data_loader,
--> 208             validation_iter=validation_data_loader,
209         )
210

/opt/amazon/lib/python3.6/site-packages/gluonts/trainer/_base.py in __call__(self, net, input_names, train_iter, validation_iter)
295                     )
296
--> 297                     epoch_loss = loop(epoch_no, train_iter)
298                     if is_validation_available:
299                         epoch_loss = loop(

/opt/amazon/lib/python3.6/site-packages/gluonts/trainer/_base.py in loop(epoch_no, batch_iter, is_training)
235
236                             with mx.autograd.record():
--> 237                                 output = net(*inputs)
238
239                                 # network can returns several outputs, the first being always the loss

/opt/amazon/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)
546             hook(self, args)
547
--> 548         out = self.forward(*args)
549
550         for hook in self._forward_hooks.values():

/opt/amazon/lib/python3.6/site-packages/mxnet/gluon/block.py in forward(self, x, *args)
923                     params = {i: j.data(ctx) for i, j in self._reg_params.items()}
924
--> 925                 return self.hybrid_forward(ndarray, x, *args, **params)
926
927         assert isinstance(x, Symbol), \

/opt/amazon/lib/python3.6/site-packages/gluonts/model/deepar/_network.py in hybrid_forward(self, F, feat_static_cat, feat_static_real, past_time_feat, past_target, past_observed_values, future_time_feat, future_target, future_observed_values)
380             future_time_feat=future_time_feat,
381             future_target=future_target,
--> 382             future_observed_values=future_observed_values,
383         )
384

/opt/amazon/lib/python3.6/site-packages/gluonts/model/deepar/_network.py in distribution(self, feat_static_cat, feat_static_real, past_time_feat, past_target, past_observed_values, future_time_feat, future_target, future_observed_values)
336         distr_args = self.proj_distr_args(rnn_outputs)
337
--> 338         return self.distr_output.distribution(distr_args, scale=scale)
339
340     # noinspection PyMethodOverriding,PyPep8Naming

/opt/amazon/lib/python3.6/site-packages/gluonts/distribution/dirichlet_multinomial.py in distribution(self, distr_args, loc, scale)
182
183     def distribution(self, distr_args, loc=None, scale=None) -> Distribution:
--> 184         assert loc is None and scale is None
185         distr = DirichletMultinomial(self.dim, self.n_trials, distr_args)
186         return distr

AssertionError:",AssertionError,buggy_snippets_files/059fc180e556e1e979a609c106517c26fc6e46f0f51fddd8a4be34f8360fb76a_before_merge.py,buggy_snippets_files/059fc180e556e1e979a609c106517c26fc6e46f0f51fddd8a4be34f8360fb76a_after_merge.py
385064,"def qimage_indexed_from_array(
        arr: np.ndarray, colortable: Sequence[Sequence[int]]
) -> QImage:
    arr = np.asarray(arr, dtype=np.uint8)
    h, w = arr.shape
    colortable = np.asarray(colortable, dtype=np.uint8)
    ncolors, nchannels = colortable.shape
    img = QImage(w, h, QImage.Format_Indexed8)
    img.setColorCount(ncolors)
    if nchannels == 4:
        qrgb_ = qrgba
    elif nchannels == 3:
        qrgb_ = qrgb
    else:
        raise ValueError

    for i, c in enumerate(colortable):
        img.setColor(i, qrgb_(*c))

    buffer = img.bits().asarray(w * h)
    view = np.frombuffer(buffer, np.uint8).reshape((h, w))
    view[:, :] = arr
    return img
","def qimage_indexed_from_array(
        arr: np.ndarray, colortable: Sequence[Sequence[int]]
) -> QImage:
    arr = np.asarray(arr, dtype=np.uint8)
    h, w = arr.shape
    colortable = np.asarray(colortable, dtype=np.uint8)
    ncolors, nchannels = colortable.shape
    img = QImage(w, h, QImage.Format_Indexed8)
    img.setColorCount(ncolors)
    if nchannels == 4:
        qrgb_ = qrgba
    elif nchannels == 3:
        qrgb_ = qrgb
    else:
        raise ValueError

    for i, c in enumerate(colortable):
        img.setColor(i, qrgb_(*c))
    if img.size().isEmpty():
        return img
    buffer = img.bits().asarray(w * h)
    view = np.frombuffer(buffer, np.uint8).reshape((h, w))
    view[:, :] = arr
    return img
",https://github.com/biolab/orange3/issues/4518,CWE-703: Improper Check or Handling of Exceptional Conditions,Need to check if `img` is empty,Orange/widgets/utils/image.py,qimage_indexed_from_array,"[17, 20]","libpng warning: iCCP: known incorrect sRGB profile
QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 0, 0, 0, 100'
/home/karin/Documents/git/orange-widget-base/orangewidget/widget.py:286: RuntimeWarning: subclassing of widget classes is deprecated and will be disabled in the future.
Extract code from OWGeneSets or explicitly open it.
RuntimeWarning)
/home/karin/Documents/git/orange-widget-base/orangewidget/widget.py:286: RuntimeWarning: subclassing of widget classes is deprecated and will be disabled in the future.
Extract code from OWPreprocess or explicitly open it.
RuntimeWarning)
/home/karin/Documents/git/orange3-bioinformatics/orangecontrib/bioinformatics/ncbi/gene/__init__.py:177: OrangeDeprecationWarning: Direct calls to Table's constructor are deprecated and will be removed. Replace this call with Table.from_list
table = Table(domain, data_x)
/home/karin/Documents/git/orange3/Orange/widgets/settings.py:82: UserWarning: Storing variables as strings in settings is deprecated.
Support for this will be dropped in Orange 3.26.
Change domain_role_hints to store an instance of `Variable`.
""Storing variables as strings in settings is deprecated.\n""
/home/karin/Documents/git/orange3/Orange/widgets/settings.py:82: UserWarning: Storing variables as strings in settings is deprecated.
Support for this will be dropped in Orange 3.26.
Change domain_role_hints to store an instance of `Variable`.
""Storing variables as strings in settings is deprecated.\n""
/home/karin/Documents/git/orange3-bioinformatics/orangecontrib/bioinformatics/ncbi/gene/__init__.py:177: OrangeDeprecationWarning: Direct calls to Table's constructor are deprecated and will be removed. Replace this call with Table.from_list
table = Table(domain, data_x)
/home/karin/Documents/git/orange3venv/lib/python3.6/site-packages/orangecontrib/prototypes/widgets/owunique.py:87: OrangeDeprecationWarning: Call to deprecated <function Domain.__iter__ at 0x7f581ae761e0>; Instead, use Domain.variables
self.model_attrs = (list(data.domain) + list(data.domain.metas), [])
ERROR:orangewidget.workflow.widgetsscheme:Error calling 'set_dataset' of 'Heat Map (1)'
Traceback (most recent call last):
File ""/home/karin/Documents/git/orange-widget-base/orangewidget/workflow/widgetsscheme.py"", line 826, in process_signals_for_widget
handler(*args)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 557, in set_dataset
self.update_heatmaps()
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 589, in update_heatmaps
self.construct_heatmaps_scene(parts, self.effective_data)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 773, in construct_heatmaps_scene
self.setup_scene(parts, data)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 796, in setup_scene
widget.setHeatmaps(parts)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 450, in setHeatmaps
data=X_part, span=parts.span, colormap=colormap,
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 953, in __init__
self.setHeatmapData(data)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 987, in setHeatmapData
self.__updatePixmap()
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 1020, in __updatePixmap
qimage = qimage_from_array(rgb)
File ""/home/karin/Documents/git/orange3/Orange/widgets/utils/image.py"", line 33, in qimage_from_array
buffer = img.bits().asarray(w * h * 4)
AttributeError: 'NoneType' object has no attribute 'asarray'
-------------------------- AttributeError Exception ---------------------------
Traceback (most recent call last):
File ""/home/karin/Documents/git/orange3venv/lib/python3.6/site-packages/orangecanvas/scheme/signalmanager.py"", line 936, in __process_next
if self.__process_next_helper(use_max_active=True):
File ""/home/karin/Documents/git/orange3venv/lib/python3.6/site-packages/orangecanvas/scheme/signalmanager.py"", line 974, in __process_next_helper
self.process_node(selected_node)
File ""/home/karin/Documents/git/orange3venv/lib/python3.6/site-packages/orangecanvas/scheme/signalmanager.py"", line 605, in process_node
self.send_to_node(node, signals_in)
File ""/home/karin/Documents/git/orange-widget-base/orangewidget/workflow/widgetsscheme.py"", line 792, in send_to_node
self.process_signals_for_widget(node, widget, signals)
File ""/home/karin/Documents/git/orange-widget-base/orangewidget/workflow/widgetsscheme.py"", line 826, in process_signals_for_widget
handler(*args)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 557, in set_dataset
self.update_heatmaps()
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 589, in update_heatmaps
self.construct_heatmaps_scene(parts, self.effective_data)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 773, in construct_heatmaps_scene
self.setup_scene(parts, data)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/owheatmap.py"", line 796, in setup_scene
widget.setHeatmaps(parts)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 450, in setHeatmaps
data=X_part, span=parts.span, colormap=colormap,
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 953, in __init__
self.setHeatmapData(data)
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 987, in setHeatmapData
self.__updatePixmap()
File ""/home/karin/Documents/git/orange3/Orange/widgets/visualize/utils/heatmap.py"", line 1020, in __updatePixmap
qimage = qimage_from_array(rgb)
File ""/home/karin/Documents/git/orange3/Orange/widgets/utils/image.py"", line 33, in qimage_from_array
buffer = img.bits().asarray(w * h * 4)
AttributeError: 'NoneType' object has no attribute 'asarray'
-------------------------------------------------------------------------------
QLayout: Attempting to add QLayout """" to ErrorReporting """", which already has a layout
Segmentation fault (core dumped)",AttributeError,buggy_snippets_files/1d65320bf9cb30cda8ef37159e5e8aaf606e116167e6654da1030f2b2f6a6779_before_merge.py,buggy_snippets_files/1d65320bf9cb30cda8ef37159e5e8aaf606e116167e6654da1030f2b2f6a6779_after_merge.py
385013,"    @contextmanager
    def _emit_changed(self):
        """"""
        A context manager that calls `emit_selection_rows_changed after
        changing a selection.
        """"""
        def map_from_source(rows):
            from_src = self.proxy.mapFromSource
            index = self.proxy.sourceModel().index
            return {from_src(index(row, 0)).row() for row in rows}

        old_rows = self._rows.copy()
        try:
            yield
        finally:
            deselected = map_from_source(old_rows - self._rows)
            selected = map_from_source(self._rows - old_rows)
            if selected or deselected:
                for model in self._selection_models:
                    model.emit_selection_rows_changed(selected, deselected)
","    @contextmanager
    def _emit_changed(self):
        """"""
        A context manager that calls `emit_selection_rows_changed after
        changing a selection.
        """"""
        def map_from_source(rows):
            from_src = self.proxy.mapFromSource
            index = self.proxy.sourceModel().index
            return {from_src(index(row, 0)).row() for row in rows}

        old_rows = self._rows.copy()
        try:
            yield
        finally:
            if self.proxy.sourceModel() is not None:
                deselected = map_from_source(old_rows - self._rows)
                selected = map_from_source(self._rows - old_rows)
                if selected or deselected:
                    for model in self._selection_models:
                        model.emit_selection_rows_changed(selected, deselected)
",https://github.com/biolab/orange3/issues/5074,CWE-703: Improper Check or Handling of Exceptional Conditions,Need to check if `self.proxy.sourceModel()` is not None,Orange/widgets/evaluate/owpredictions.py,SharedSelectionStore._emit_changed,[16],"-------------------------- AttributeError Exception ---------------------------
Traceback (most recent call last):
File ""/Users/ajda/orange/orange3/Orange/widgets/evaluate/owpredictions.py"", line 1130, in select
self.store.select(selection, flags)
File ""/Users/ajda/orange/orange3/Orange/widgets/evaluate/owpredictions.py"", line 1057, in select
rows = {selection.model().mapToSource(selection).row()}
AttributeError: 'NoneType' object has no attribute 'mapToSource'
-------------------------------------------------------------------------------",AttributeError,buggy_snippets_files/d19d6dfaee1612bc8984b900d72122fc1c4bf7344025813e1bc86bd1fe8a8a62_before_merge.py,buggy_snippets_files/d19d6dfaee1612bc8984b900d72122fc1c4bf7344025813e1bc86bd1fe8a8a62_after_merge.py
398301,"def latinify(string, default=""?"", pure_ascii=False):
    """"""
    Convert a unicode string to ""safe"" ascii/latin-1 characters.
    This is used as a last resort when normal encoding does not work.

    Arguments:
        string (str): A string to convert to 'safe characters' convertable
            to an latin-1 bytestring later.
        default (str, optional): Characters resisting mapping will be replaced
            with this character or string. The intent is to apply an encode operation
            on the string soon after.

    Returns:
        string (str): A 'latinified' string where each unicode character has been
            replaced with a 'safe' equivalent available in the ascii/latin-1 charset.
    Notes:
        This is inspired by the gist by Ricardo Murri:
            https://gist.github.com/riccardomurri/3c3ccec30f037be174d3

    """"""

    from unicodedata import name

    converted = []
    for unich in iter(string):
        try:
            ch = unich.decode(""ascii"")
        except UnicodeDecodeError:
            # deduce a latin letter equivalent from the Unicode data
            # point name; e.g., since `name(u'') == 'LATIN SMALL
            # LETTER A WITH ACUTE'` translate `` to `a`.  However, in
            # some cases the unicode name is still ""LATIN LETTER""
            # although no direct equivalent in the Latin alphabet
            # exists (e.g., , ""LATIN CAPITAL LETTER THORN"") -- we can
            # avoid these cases by checking that the letter name is
            # composed of one letter only.
            # We also supply some direct-translations for some particular
            # common cases.
            what = name(unich)
            if what in _UNICODE_MAP:
                ch = _UNICODE_MAP[what]
            else:
                what = what.split()
                if what[0] == ""LATIN"" and what[2] == ""LETTER"" and len(what[3]) == 1:
                    ch = what[3].lower() if what[1] == ""SMALL"" else what[3].upper()
                else:
                    ch = default
        converted.append(chr(ord(ch)))
    return """".join(converted)
","def latinify(string, default=""?"", pure_ascii=False):
    """"""
    Convert a unicode string to ""safe"" ascii/latin-1 characters.
    This is used as a last resort when normal encoding does not work.

    Arguments:
        string (str): A string to convert to 'safe characters' convertable
            to an latin-1 bytestring later.
        default (str, optional): Characters resisting mapping will be replaced
            with this character or string. The intent is to apply an encode operation
            on the string soon after.

    Returns:
        string (str): A 'latinified' string where each unicode character has been
            replaced with a 'safe' equivalent available in the ascii/latin-1 charset.
    Notes:
        This is inspired by the gist by Ricardo Murri:
            https://gist.github.com/riccardomurri/3c3ccec30f037be174d3

    """"""

    from unicodedata import name

    if isinstance(string, bytes):
        string = string.decode(""utf8"")

    converted = []
    for unich in iter(string):
        try:
            ch = unich.encode(""utf8"").decode(""ascii"")
        except UnicodeDecodeError:
            # deduce a latin letter equivalent from the Unicode data
            # point name; e.g., since `name(u'') == 'LATIN SMALL
            # LETTER A WITH ACUTE'` translate `` to `a`.  However, in
            # some cases the unicode name is still ""LATIN LETTER""
            # although no direct equivalent in the Latin alphabet
            # exists (e.g., , ""LATIN CAPITAL LETTER THORN"") -- we can
            # avoid these cases by checking that the letter name is
            # composed of one letter only.
            # We also supply some direct-translations for some particular
            # common cases.
            what = name(unich)
            if what in _UNICODE_MAP:
                ch = _UNICODE_MAP[what]
            else:
                what = what.split()
                if what[0] == ""LATIN"" and what[2] == ""LETTER"" and len(what[3]) == 1:
                    ch = what[3].lower() if what[1] == ""SMALL"" else what[3].upper()
                else:
                    ch = default
        converted.append(chr(ord(ch)))
    return """".join(converted)
",https://github.com/evennia/evennia/issues/1952,CWE-704: Incorrect Type Conversion or Cast,"The function latinify tries to call decode method on ints if byte string is provided or on str if the argument is a string. Both of them don't have a decode method, which causes AttributeError exception",evennia/utils/utils.py,latinify,27,"======================================================================
ERROR: test_byte_string (typeclasses.tests.LatinifyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
File ""/home/ubuntu/workspace/demogame/typeclasses/tests.py"", line 43, in test_byte_string
result = latinify(to_bytes(self.example_str))
File ""/home/ubuntu/workspace/evennia/evennia/utils/utils.py"", line 781, in latinify
ch = unich.decode('ascii')
AttributeError: 'int' object has no attribute 'decode'

======================================================================
ERROR: test_encoded_string (typeclasses.tests.LatinifyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
File ""/home/ubuntu/workspace/demogame/typeclasses/tests.py"", line 39, in test_encoded_string
result = latinify(self.example_str.encode('utf8'))
File ""/home/ubuntu/workspace/evennia/evennia/utils/utils.py"", line 781, in latinify
ch = unich.decode('ascii')
AttributeError: 'int' object has no attribute 'decode'

======================================================================
ERROR: test_plain_string (typeclasses.tests.LatinifyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
File ""/home/ubuntu/workspace/demogame/typeclasses/tests.py"", line 31, in test_plain_string
result = latinify(self.example_str)
File ""/home/ubuntu/workspace/evennia/evennia/utils/utils.py"", line 781, in latinify
ch = unich.decode('ascii')
AttributeError: 'str' object has no attribute 'decode'

======================================================================
ERROR: test_unicode_string (typeclasses.tests.LatinifyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
File ""/home/ubuntu/workspace/demogame/typeclasses/tests.py"", line 35, in test_unicode_string
result = latinify(self.example_ustr)
File ""/home/ubuntu/workspace/evennia/evennia/utils/utils.py"", line 781, in latinify
ch = unich.decode('ascii')
AttributeError: 'str' object has no attribute 'decode'

----------------------------------------------------------------------
Ran 4 tests in 2.104s

FAILED (errors=4)",AttributeError,buggy_snippets_files/238217e3e5aef6a1e4fc86c8982b159e7d6f9c66b6366c7905affe157341ee33_before_merge.py,buggy_snippets_files/238217e3e5aef6a1e4fc86c8982b159e7d6f9c66b6366c7905affe157341ee33_after_merge.py
133143,"def perform_codegen():
    # Set root codegen output directory
    # ---------------------------------
    # (relative to project root)
    outdir = 'plotly'

    # Delete prior codegen output
    # ---------------------------
    validators_pkgdir = opath.join(outdir, 'validators')
    if opath.exists(validators_pkgdir):
        shutil.rmtree(validators_pkgdir)

    graph_objs_pkgdir = opath.join(outdir, 'graph_objs')
    if opath.exists(graph_objs_pkgdir):
        shutil.rmtree(graph_objs_pkgdir)

    # plotly/datatypes is not used anymore, but was at one point so we'll
    # still delete it if we find it in case a developer is upgrading from an
    # older version
    datatypes_pkgdir = opath.join(outdir, 'datatypes')
    if opath.exists(datatypes_pkgdir):
        shutil.rmtree(datatypes_pkgdir)

    # Load plotly schema
    # ------------------
    with open('plotly/package_data/plot-schema.json', 'r') as f:
        plotly_schema = json.load(f)

    # Preprocess Schema
    # -----------------
    preprocess_schema(plotly_schema)

    # Build node lists
    # ----------------
    # ### TraceNode ###
    base_traces_node = TraceNode(plotly_schema)
    compound_trace_nodes = PlotlyNode.get_all_compound_datatype_nodes(
        plotly_schema, TraceNode)
    all_trace_nodes = PlotlyNode.get_all_datatype_nodes(
        plotly_schema, TraceNode)

    # ### LayoutNode ###
    compound_layout_nodes = PlotlyNode.get_all_compound_datatype_nodes(
        plotly_schema, LayoutNode)
    layout_node = compound_layout_nodes[0]
    all_layout_nodes = PlotlyNode.get_all_datatype_nodes(
        plotly_schema, LayoutNode)

    # ### FrameNode ###
    compound_frame_nodes = PlotlyNode.get_all_compound_datatype_nodes(
        plotly_schema, FrameNode)
    frame_node = compound_frame_nodes[0]
    all_frame_nodes = PlotlyNode.get_all_datatype_nodes(
        plotly_schema, FrameNode)

    # ### All nodes ###
    all_datatype_nodes = (all_trace_nodes +
                          all_layout_nodes +
                          all_frame_nodes)

    all_compound_nodes = [node for node in all_datatype_nodes
                          if node.is_compound and
                          not isinstance(node, ElementDefaultsNode)]

    # Write out validators
    # --------------------
    # # ### Layout ###
    for node in all_layout_nodes:
        write_validator_py(outdir, node)

    # ### Trace ###
    for node in all_trace_nodes:
        write_validator_py(outdir, node)

    # ### Frames ###
    for node in all_frame_nodes:
        write_validator_py(outdir, node)

    # ### Data (traces) validator ###
    write_data_validator_py(outdir, base_traces_node)

    # Write out datatypes
    # -------------------
    # ### Layout ###
    for node in compound_layout_nodes:
        write_datatype_py(outdir, node)

    # ### Trace ###
    for node in compound_trace_nodes:
        write_datatype_py(outdir, node)

    # ### Frames ###
    for node in compound_frame_nodes:
        write_datatype_py(outdir, node)

    # ### Deprecated ###
    # These are deprecated legacy datatypes like graph_objs.Marker
    write_deprecated_datatypes(outdir)

    # Write figure class to graph_objs
    # --------------------------------
    data_validator = get_data_validator_instance(base_traces_node)
    layout_validator = layout_node.get_validator_instance()
    frame_validator = frame_node.get_validator_instance()

    write_figure_classes(outdir,
                         base_traces_node,
                         data_validator,
                         layout_validator,
                         frame_validator)

    # Write validator __init__.py files
    # ---------------------------------
    # ### Write __init__.py files for each validator package ###
    path_to_validator_import_info = {}
    for node in all_datatype_nodes:
        if node.is_mapped:
            continue
        key = node.parent_path_parts
        path_to_validator_import_info.setdefault(key, []).append(
            (f""._{node.name_property}"", node.name_validator_class)
        )

    # Add Data validator
    root_validator_pairs = path_to_validator_import_info[()]
    root_validator_pairs.append(('._data', 'DataValidator'))

    # Output validator __init__.py files
    validators_pkg = opath.join(outdir, 'validators')
    for path_parts, import_pairs in path_to_validator_import_info.items():
        write_init_py(validators_pkg, path_parts, import_pairs)

    # Write datatype __init__.py files
    # --------------------------------
    # ### Build mapping from parent package to datatype class ###
    path_to_datatype_import_info = {}
    for node in all_compound_nodes:
        key = node.parent_path_parts

        # class import
        path_to_datatype_import_info.setdefault(key, []).append(
            (f""._{node.name_undercase}"", node.name_datatype_class)
        )

        # submodule import
        if node.child_compound_datatypes:
            
            path_to_datatype_import_info.setdefault(key, []).append(
                (f""plotly.graph_objs{node.parent_dotpath_str}"",
                 node.name_undercase)
            )

    # ### Write plotly/graph_objs/graph_objs.py ###
    # This if for backward compatibility. It just imports everything from
    # graph_objs/__init__.py
    write_graph_objs_graph_objs(outdir)

    # ### Add Figure and FigureWidget ###
    root_datatype_imports = path_to_datatype_import_info[()]
    root_datatype_imports.append(('._figure', 'Figure'))

    optional_figure_widget_import = """"""
try:
    import ipywidgets
    from ._figurewidget import FigureWidget
except ImportError:
    pass

""""""
    root_datatype_imports.append(optional_figure_widget_import)

    # ### Add deprecations ###
    root_datatype_imports.append(('._deprecations', DEPRECATED_DATATYPES.keys()))

    # ### Output datatype __init__.py files ###
    graph_objs_pkg = opath.join(outdir, 'graph_objs')
    for path_parts, import_pairs in path_to_datatype_import_info.items():
        write_init_py(graph_objs_pkg, path_parts, import_pairs)
","def perform_codegen():
    # Set root codegen output directory
    # ---------------------------------
    # (relative to project root)
    outdir = 'plotly'

    # Delete prior codegen output
    # ---------------------------
    validators_pkgdir = opath.join(outdir, 'validators')
    if opath.exists(validators_pkgdir):
        shutil.rmtree(validators_pkgdir)

    graph_objs_pkgdir = opath.join(outdir, 'graph_objs')
    if opath.exists(graph_objs_pkgdir):
        shutil.rmtree(graph_objs_pkgdir)

    # plotly/datatypes is not used anymore, but was at one point so we'll
    # still delete it if we find it in case a developer is upgrading from an
    # older version
    datatypes_pkgdir = opath.join(outdir, 'datatypes')
    if opath.exists(datatypes_pkgdir):
        shutil.rmtree(datatypes_pkgdir)

    # Load plotly schema
    # ------------------
    with open('plotly/package_data/plot-schema.json', 'r') as f:
        plotly_schema = json.load(f)

    # Preprocess Schema
    # -----------------
    preprocess_schema(plotly_schema)

    # Build node lists
    # ----------------
    # ### TraceNode ###
    base_traces_node = TraceNode(plotly_schema)
    compound_trace_nodes = PlotlyNode.get_all_compound_datatype_nodes(
        plotly_schema, TraceNode)
    all_trace_nodes = PlotlyNode.get_all_datatype_nodes(
        plotly_schema, TraceNode)

    # ### LayoutNode ###
    compound_layout_nodes = PlotlyNode.get_all_compound_datatype_nodes(
        plotly_schema, LayoutNode)
    layout_node = compound_layout_nodes[0]
    all_layout_nodes = PlotlyNode.get_all_datatype_nodes(
        plotly_schema, LayoutNode)

    # ### FrameNode ###
    compound_frame_nodes = PlotlyNode.get_all_compound_datatype_nodes(
        plotly_schema, FrameNode)
    frame_node = compound_frame_nodes[0]
    all_frame_nodes = PlotlyNode.get_all_datatype_nodes(
        plotly_schema, FrameNode)

    # ### All nodes ###
    all_datatype_nodes = (all_trace_nodes +
                          all_layout_nodes +
                          all_frame_nodes)

    all_compound_nodes = [node for node in all_datatype_nodes
                          if node.is_compound and
                          not isinstance(node, ElementDefaultsNode)]

    # Write out validators
    # --------------------
    # # ### Layout ###
    for node in all_layout_nodes:
        write_validator_py(outdir, node)

    # ### Trace ###
    for node in all_trace_nodes:
        write_validator_py(outdir, node)

    # ### Frames ###
    for node in all_frame_nodes:
        write_validator_py(outdir, node)

    # ### Data (traces) validator ###
    write_data_validator_py(outdir, base_traces_node)

    # Write out datatypes
    # -------------------
    # ### Layout ###
    for node in compound_layout_nodes:
        write_datatype_py(outdir, node)

    # ### Trace ###
    for node in compound_trace_nodes:
        write_datatype_py(outdir, node)

    # ### Frames ###
    for node in compound_frame_nodes:
        write_datatype_py(outdir, node)

    # ### Deprecated ###
    # These are deprecated legacy datatypes like graph_objs.Marker
    write_deprecated_datatypes(outdir)

    # Write figure class to graph_objs
    # --------------------------------
    data_validator = get_data_validator_instance(base_traces_node)
    layout_validator = layout_node.get_validator_instance()
    frame_validator = frame_node.get_validator_instance()

    write_figure_classes(outdir,
                         base_traces_node,
                         data_validator,
                         layout_validator,
                         frame_validator)

    # Write validator __init__.py files
    # ---------------------------------
    # ### Write __init__.py files for each validator package ###
    path_to_validator_import_info = {}
    for node in all_datatype_nodes:
        if node.is_mapped:
            continue
        key = node.parent_path_parts
        path_to_validator_import_info.setdefault(key, []).append(
            (f""._{node.name_property}"", node.name_validator_class)
        )

    # Add Data validator
    root_validator_pairs = path_to_validator_import_info[()]
    root_validator_pairs.append(('._data', 'DataValidator'))

    # Output validator __init__.py files
    validators_pkg = opath.join(outdir, 'validators')
    for path_parts, import_pairs in path_to_validator_import_info.items():
        write_init_py(validators_pkg, path_parts, import_pairs)

    # Write datatype __init__.py files
    # --------------------------------
    # ### Build mapping from parent package to datatype class ###
    path_to_datatype_import_info = {}
    for node in all_compound_nodes:
        key = node.parent_path_parts

        # class import
        path_to_datatype_import_info.setdefault(key, []).append(
            (f""._{node.name_undercase}"", node.name_datatype_class)
        )

        # submodule import
        if node.child_compound_datatypes:
            
            path_to_datatype_import_info.setdefault(key, []).append(
                (f""plotly.graph_objs{node.parent_dotpath_str}"",
                 node.name_undercase)
            )

    # ### Write plotly/graph_objs/graph_objs.py ###
    # This if for backward compatibility. It just imports everything from
    # graph_objs/__init__.py
    write_graph_objs_graph_objs(outdir)

    # ### Add Figure and FigureWidget ###
    root_datatype_imports = path_to_datatype_import_info[()]
    root_datatype_imports.append(('._figure', 'Figure'))

    optional_figure_widget_import = """"""
try:
    import ipywidgets
    from distutils.version import LooseVersion
    if LooseVersion(ipywidgets.__version__) >= LooseVersion('7.0.0'):
        from ._figurewidget import FigureWidget
    del LooseVersion
    del ipywidgets
except ImportError:
    pass
""""""
    root_datatype_imports.append(optional_figure_widget_import)

    # ### Add deprecations ###
    root_datatype_imports.append(('._deprecations', DEPRECATED_DATATYPES.keys()))

    # ### Output datatype __init__.py files ###
    graph_objs_pkg = opath.join(outdir, 'graph_objs')
    for path_parts, import_pairs in path_to_datatype_import_info.items():
        write_init_py(graph_objs_pkg, path_parts, import_pairs)
",https://github.com/plotly/plotly.py/issues/1421,"CWE-589: Call to Non-ubiquitous API, CWE-704: Incorrect Type Conversion or Cast, CWE-440: Expected Behavior Violation","The code was importing FigureWidget automatically, however the old version of ipywidgets module had different sematrics of FigureWidget, which caused the exception",codegen/__init__.py,perform_codegen,165,"Traceback (most recent call last):

File ""<ipython-input-13-5fb02038c35f>"", line 1, in <module>
import plotly

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\__init__.py"", line 31, in <module>
from plotly import (plotly, dashboard_objs, graph_objs, grid_objs, tools,

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\plotly\__init__.py"", line 10, in <module>
from . plotly import (

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\plotly\plotly.py"", line 35, in <module>
from plotly.graph_objs import Scatter

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\graph_objs\__init__.py"", line 82, in <module>
from ._figurewidget import FigureWidget

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\graph_objs\_figurewidget.py"", line 12, in <module>
class FigureWidget(BaseFigureWidget):

TypeError: function() argument 1 must be code, not str

Traceback (most recent call last):

File ""<ipython-input-13-5fb02038c35f>"", line 1, in <module>
import plotly

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\__init__.py"", line 31, in <module>
from plotly import (plotly, dashboard_objs, graph_objs, grid_objs, tools,

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\plotly\__init__.py"", line 10, in <module>
from . plotly import (

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\plotly\plotly.py"", line 35, in <module>
from plotly.graph_objs import Scatter

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\graph_objs\__init__.py"", line 82, in <module>
from ._figurewidget import FigureWidget

File ""C:\Users\Artemis\Anaconda3\lib\site-packages\plotly\graph_objs\_figurewidget.py"", line 12, in <module>
class FigureWidget(BaseFigureWidget):

TypeError: function() argument 1 must be code, not str",TypeError,buggy_snippets_files/37b698b28905db00e3e37ad440c550082b92762b21cf5e984c43a7df62a3b30c_before_merge.py,buggy_snippets_files/37b698b28905db00e3e37ad440c550082b92762b21cf5e984c43a7df62a3b30c_after_merge.py
204188,"    def compare_subnets(self):
        """"""
        Compare user subnets with current ELB subnets

        :return: bool True if they match otherwise False
        """"""

        subnet_mapping_id_list = []
        subnet_mappings = []

        # Check if we're dealing with subnets or subnet_mappings
        if self.subnets is not None:
            # Convert subnets to subnet_mappings format for comparison
            for subnet in self.subnets:
                subnet_mappings.append({'SubnetId': subnet})

        if self.subnet_mappings is not None:
            # Use this directly since we're comparing as a mapping
            subnet_mappings = self.subnet_mappings

        # Build a subnet_mapping style struture of what's currently
        # on the load balancer
        for subnet in self.elb['AvailabilityZones']:
            this_mapping = {'SubnetId': subnet['SubnetId']}
            for address in subnet['LoadBalancerAddresses']:
                if 'AllocationId' in address:
                    this_mapping['AllocationId'] = address['AllocationId']
                    break

            subnet_mapping_id_list.append(this_mapping)

        return set(frozenset(mapping.items()) for mapping in subnet_mapping_id_list) == set(frozenset(mapping.items()) for mapping in subnet_mappings)
","    def compare_subnets(self):
        """"""
        Compare user subnets with current ELB subnets

        :return: bool True if they match otherwise False
        """"""

        subnet_mapping_id_list = []
        subnet_mappings = []

        # Check if we're dealing with subnets or subnet_mappings
        if self.subnets is not None:
            # Convert subnets to subnet_mappings format for comparison
            for subnet in self.subnets:
                subnet_mappings.append({'SubnetId': subnet})

        if self.subnet_mappings is not None:
            # Use this directly since we're comparing as a mapping
            subnet_mappings = self.subnet_mappings

        # Build a subnet_mapping style struture of what's currently
        # on the load balancer
        for subnet in self.elb['AvailabilityZones']:
            this_mapping = {'SubnetId': subnet['SubnetId']}
            for address in subnet.get('LoadBalancerAddresses', []):
                if 'AllocationId' in address:
                    this_mapping['AllocationId'] = address['AllocationId']
                    break

            subnet_mapping_id_list.append(this_mapping)

        return set(frozenset(mapping.items()) for mapping in subnet_mapping_id_list) == set(frozenset(mapping.items()) for mapping in subnet_mappings)
",https://github.com/ansible/ansible/issues/49558,"CWE-166: Improper Handling of Missing Special Element, CWE-248: Uncaught Exception","The application load balancer sometimes fails to initialize because the subnet variable does not always have LoadBalancerAddresses key. The situation where this key is absent is a normal because it contains the list of network load balancers, not the application ones",lib/ansible/module_utils/aws/elbv2.py,ElasticLoadBalancerV2.compare_subnets,25,"TASK [ELB (Application)] ***********************************************************************************************
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: KeyError: 'LoadBalancerAddresses'
fatal: [localhost -> localhost]: FAILED! => {""changed"": false, ""module_stderr"": ""Traceback (most recent call last):\n  File \""/home/cho/.ansible/tmp/ansible-tmp-1544029326.15-8855540780494/AnsiballZ_elb_application_lb.py\"", line 113, in <module>\n    _ansiballz_main()\n  File \""/home/cho/.ansible/tmp/ansible-tmp-1544029326.15-8855540780494/AnsiballZ_elb_application_lb.py\"", line 105, in _ansiballz_main\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\n  File \""/home/cho/.ansible/tmp/ansible-tmp-1544029326.15-8855540780494/AnsiballZ_elb_application_lb.py\"", line 48, in invoke_module\n    imp.load_module('__main__', mod, module, MOD_DESC)\n  File \""/tmp/ansible_elb_application_lb_payload_9PFpmx/__main__.py\"", line 574, in <module>\n  File \""/tmp/ansible_elb_application_lb_payload_9PFpmx/__main__.py\"", line 568, in main\n  File \""/tmp/ansible_elb_application_lb_payload_9PFpmx/__main__.py\"", line 390, in create_or_update_elb\n  File \""/tmp/ansible_elb_application_lb_payload_9PFpmx/ansible_elb_application_lb_payload.zip/ansible/module_utils/aws/elbv2.py\"", line 172, in compare_subnets\nKeyError: 'LoadBalancerAddresses'\n"", ""module_stdout"": """", ""msg"": ""MODULE FAILURE\nSee stdout/stderr for the exact error"", ""rc"": 1}",KeyError,buggy_snippets_files/0beabf783aedb4496b9f792d0693195abd651d4701f66a11aff029e46a00d64c_before_merge.py,buggy_snippets_files/0beabf783aedb4496b9f792d0693195abd651d4701f66a11aff029e46a00d64c_after_merge.py
33472,"    def get_client(self, keystore, force_pair=True, *,
                   devices=None, allow_user_interaction=True) -> Optional['KeepKeyClient']:
        client = super().get_client(keystore, force_pair,
                                    devices=devices,
                                    allow_user_interaction=allow_user_interaction)
        # returns the client for a given keystore. can use xpub
        if client:
            client.used()
        return client
","    @runs_in_hwd_thread
    def get_client(self, keystore, force_pair=True, *,
                   devices=None, allow_user_interaction=True) -> Optional['KeepKeyClient']:
        client = super().get_client(keystore, force_pair,
                                    devices=devices,
                                    allow_user_interaction=allow_user_interaction)
        # returns the client for a given keystore. can use xpub
        if client:
            client.used()
        return client
",https://github.com/spesmilo/electrum/issues/6554,CWE-362: Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition'),Sometimes the program crashed with different errors if the user run several programs at once. The problem was fixed by running all device communications on a dedicated thread,electrum/plugins/keepkey/keepkey.py,KeepKeyPlugin.get_client,1,"$ ~/Downloads/apps/electrum-4.0.2-x86_64.AppImage --testnet
Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.
E | plugin.DeviceMgr | failed to create client for bitbox02 at b'0001:0009:01': Exception('No BitBox02 detected')
E | plugins.coldcard.coldcard.Coldcard_KeyStore |
Traceback (most recent call last):
File ""/tmp/.mount_electrcFBIsQ/usr/lib/python3.7/site-packages/electrum/plugins/coldcard/coldcard.py"", line 383, in sign_transaction
resp = client.sign_transaction_poll()
File ""/tmp/.mount_electrcFBIsQ/usr/lib/python3.7/site-packages/electrum/plugins/coldcard/coldcard.py"", line 233, in sign_transaction_poll
return self.dev.send_recv(CCProtocolPacker.get_signed_txn(), timeout=None)
File ""/tmp/.mount_electrcFBIsQ/usr/lib/python3.7/site-packages/ckcc/client.py"", line 139, in send_recv
buf = self.dev.read(64, timeout_ms=(timeout or 0))
File ""hid.pyx"", line 123, in hid.device.read
OSError: read error
double free or corruption (fasttop)
Aborted",OSError,buggy_snippets_files/7218387687b9b4325836de2385e1ee2d59e1e534779dd30d1e56f26a8f4c13cf_before_merge.py,buggy_snippets_files/7218387687b9b4325836de2385e1ee2d59e1e534779dd30d1e56f26a8f4c13cf_after_merge.py
269161,"def mod_hostname(hostname):
    '''
    Modify hostname

    .. versionchanged:: 2015.8.0
        Added support for SunOS (Solaris 10, Illumos, SmartOS)

    CLI Example:

    .. code-block:: bash

        salt '*' network.mod_hostname master.saltstack.com
    '''
    #
    # SunOS tested on SmartOS and OmniOS (Solaris 10 compatible)
    # Oracle Solaris 11 uses smf, currently not supported
    #
    # /etc/nodename is the hostname only, not fqdn
    # /etc/defaultdomain is the domain
    # /etc/hosts should have both fqdn and hostname entries
    #

    if hostname is None:
        return False

    hostname_cmd = salt.utils.which('hostnamectl') or salt.utils.which('hostname')
    if salt.utils.is_sunos():
        uname_cmd = '/usr/bin/uname' if salt.utils.is_smartos() else salt.utils.which('uname')
        check_hostname_cmd = salt.utils.which('check-hostname')

    # Grab the old hostname so we know which hostname to change and then
    # change the hostname using the hostname command
    if hostname_cmd.endswith('hostnamectl'):
        out = __salt__['cmd.run']('{0} status'.format(hostname_cmd))
        for line in out.splitlines():
            line = line.split(':')
            if 'Static hostname' in line[0]:
                o_hostname = line[1].strip()
    elif not salt.utils.is_sunos():
        # don't run hostname -f because -f is not supported on all platforms
        o_hostname = socket.getfqdn()
    else:
        # output: Hostname core OK: fully qualified as core.acheron.be
        o_hostname = __salt__['cmd.run'](check_hostname_cmd).split(' ')[-1]

    if hostname_cmd.endswith('hostnamectl'):
        __salt__['cmd.run']('{0} set-hostname {1}'.format(hostname_cmd, hostname))
    elif not salt.utils.is_sunos():
        __salt__['cmd.run']('{0} {1}'.format(hostname_cmd, hostname))
    else:
        __salt__['cmd.run']('{0} -S {1}'.format(uname_cmd, hostname.split('.')[0]))

    # Modify the /etc/hosts file to replace the old hostname with the
    # new hostname
    with salt.utils.fopen('/etc/hosts', 'r') as fp_:
        host_c = fp_.readlines()

    with salt.utils.fopen('/etc/hosts', 'w') as fh_:
        for host in host_c:
            host = host.split()

            try:
                host[host.index(o_hostname)] = hostname
                if salt.utils.is_sunos():
                    # also set a copy of the hostname
                    host[host.index(o_hostname.split('.')[0])] = hostname.split('.')[0]
            except ValueError:
                pass

            fh_.write('\t'.join(host) + '\n')

    # Modify the /etc/sysconfig/network configuration file to set the
    # new hostname
    if __grains__['os_family'] == 'RedHat':
        with salt.utils.fopen('/etc/sysconfig/network', 'r') as fp_:
            network_c = fp_.readlines()

        with salt.utils.fopen('/etc/sysconfig/network', 'w') as fh_:
            for net in network_c:
                if net.startswith('HOSTNAME'):
                    fh_.write('HOSTNAME={0}\n'.format(hostname))
                else:
                    fh_.write(net)
    elif __grains__['os_family'] in ('Debian', 'NILinuxRT'):
        with salt.utils.fopen('/etc/hostname', 'w') as fh_:
            fh_.write(hostname + '\n')
    elif __grains__['os_family'] == 'OpenBSD':
        with salt.utils.fopen('/etc/myname', 'w') as fh_:
            fh_.write(hostname + '\n')

    # Update /etc/nodename and /etc/defaultdomain on SunOS
    if salt.utils.is_sunos():
        with salt.utils.fopen('/etc/nodename', 'w') as fh_:
            fh_.write(hostname.split('.')[0] + '\n')
        with salt.utils.fopen('/etc/defaultdomain', 'w') as fh_:
            fh_.write(""."".join(hostname.split('.')[1:]) + '\n')

    return True
","def mod_hostname(hostname):
    '''
    Modify hostname

    .. versionchanged:: 2015.8.0
        Added support for SunOS (Solaris 10, Illumos, SmartOS)

    CLI Example:

    .. code-block:: bash

        salt '*' network.mod_hostname master.saltstack.com
    '''
    #
    # SunOS tested on SmartOS and OmniOS (Solaris 10 compatible)
    # Oracle Solaris 11 uses smf, currently not supported
    #
    # /etc/nodename is the hostname only, not fqdn
    # /etc/defaultdomain is the domain
    # /etc/hosts should have both fqdn and hostname entries
    #

    if hostname is None:
        return False

    hostname_cmd = salt.utils.which('hostnamectl') or salt.utils.which('hostname')
    if salt.utils.is_sunos():
        uname_cmd = '/usr/bin/uname' if salt.utils.is_smartos() else salt.utils.which('uname')
        check_hostname_cmd = salt.utils.which('check-hostname')

    # Grab the old hostname so we know which hostname to change and then
    # change the hostname using the hostname command
    if hostname_cmd.endswith('hostnamectl'):
        out = __salt__['cmd.run']('{0} status'.format(hostname_cmd))
        for line in out.splitlines():
            line = line.split(':')
            if 'Static hostname' in line[0]:
                o_hostname = line[1].strip()
    elif not salt.utils.is_sunos():
        # don't run hostname -f because -f is not supported on all platforms
        o_hostname = socket.getfqdn()
    else:
        # output: Hostname core OK: fully qualified as core.acheron.be
        o_hostname = __salt__['cmd.run'](check_hostname_cmd).split(' ')[-1]

    if hostname_cmd.endswith('hostnamectl'):
        __salt__['cmd.run']('{0} set-hostname {1}'.format(hostname_cmd, hostname))
    elif not salt.utils.is_sunos():
        __salt__['cmd.run']('{0} {1}'.format(hostname_cmd, hostname))
    else:
        __salt__['cmd.run']('{0} -S {1}'.format(uname_cmd, hostname.split('.')[0]))

    # Modify the /etc/hosts file to replace the old hostname with the
    # new hostname
    with salt.utils.fopen('/etc/hosts', 'r') as fp_:
        host_c = fp_.readlines()

    with salt.utils.fopen('/etc/hosts', 'w') as fh_:
        for host in host_c:
            host = host.split()

            try:
                host[host.index(o_hostname)] = hostname
                if salt.utils.is_sunos():
                    # also set a copy of the hostname
                    host[host.index(o_hostname.split('.')[0])] = hostname.split('.')[0]
            except ValueError:
                pass

            fh_.write('\t'.join(host) + '\n')

    # Modify the /etc/sysconfig/network configuration file to set the
    # new hostname
    if __grains__['os_family'] == 'RedHat':
        with salt.utils.fopen('/etc/sysconfig/network', 'r') as fp_:
            network_c = fp_.readlines()

        with salt.utils.fopen('/etc/sysconfig/network', 'w') as fh_:
            for net in network_c:
                if net.startswith('HOSTNAME'):
                    old_hostname = net.split('=', 1)[1].rstrip()
                    quote_type = salt.utils.is_quoted(old_hostname)
                    fh_.write('HOSTNAME={1}{0}{1}\n'.format(
                        salt.utils.dequote(hostname), quote_type))
                else:
                    fh_.write(net)
    elif __grains__['os_family'] in ('Debian', 'NILinuxRT'):
        with salt.utils.fopen('/etc/hostname', 'w') as fh_:
            fh_.write(hostname + '\n')
    elif __grains__['os_family'] == 'OpenBSD':
        with salt.utils.fopen('/etc/myname', 'w') as fh_:
            fh_.write(hostname + '\n')

    # Update /etc/nodename and /etc/defaultdomain on SunOS
    if salt.utils.is_sunos():
        with salt.utils.fopen('/etc/nodename', 'w') as fh_:
            fh_.write(hostname.split('.')[0] + '\n')
        with salt.utils.fopen('/etc/defaultdomain', 'w') as fh_:
            fh_.write(""."".join(hostname.split('.')[1:]) + '\n')

    return True
",https://github.com/saltstack/salt/issues/41341,CWE-149: Improper Neutralization of Quoting Syntax,The program had an TypeError exception if the HOSTNAME attribute in /etc/sysconfig/network was surrounded by quotes due to wrong filtering. The authors added correct quotes handling to the parser.,salt/modules/network.py,mod_hostname,81,"# ... <elided> ...
[INFO    ] Loading fresh modules for state activity
[DEBUG   ] LazyLoaded jinja.render
[DEBUG   ] LazyLoaded yaml.render
[DEBUG   ] In saltenv 'base', looking at rel_path 'test.sls' to resolve 'salt://test.sls'
[DEBUG   ] In saltenv 'base', ** considering ** path '/var/cache/salt/minion/files/base/test.sls' to resolve 'salt://test.sls'
[DEBUG   ] compile template: /var/cache/salt/minion/files/base/test.sls
[DEBUG   ] Jinja search path: ['/var/cache/salt/minion/files/base']
[PROFILE ] Time (in seconds) to render '/var/cache/salt/minion/files/base/test.sls' using 'jinja' renderer: 0.000791072845459
[DEBUG   ] Rendered data from file: /var/cache/salt/minion/files/base/test.sls:
set hostname:
network.system:
- hostname: foo
- apply_hostname: True
- retain_settings: True

[DEBUG   ] LazyLoaded config.get
[DEBUG   ] Results of YAML rendering:
OrderedDict([('set hostname', OrderedDict([('network.system', [OrderedDict([('hostname', 'foo')]), OrderedDict([('apply_hostname', True)]), OrderedDict([('retain_settings', True)])])]))])
[PROFILE ] Time (in seconds) to render '/var/cache/salt/minion/files/base/test.sls' using 'yaml' renderer: 0.0252919197083
[DEBUG   ] LazyLoaded ip.get_interface
[DEBUG   ] LazyLoaded network.system
[INFO    ] Running state [set hostname] at time 12:20:55.473338
[INFO    ] Executing state network.system for set hostname
[INFO    ] Using existing setting -- Setting: networking Value: ""yes""
[ERROR   ] An exception occurred in this state: Traceback (most recent call last):
File ""/usr/lib/python2.6/site-packages/salt/state.py"", line 1746, in call
**cdata['kwargs'])
File ""/usr/lib/python2.6/site-packages/salt/loader.py"", line 1704, in wrapper
return f(*args, **kwargs)
File ""/usr/lib/python2.6/site-packages/salt/states/network.py"", line 528, in system
new = __salt__['ip.build_network_settings'](**kwargs)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 1209, in build_network_settings
opts = _parse_network_settings(settings, current_network_settings)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 806, in _parse_network_settings
_raise_error_network('networking', valid)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 848, in _raise_error_network
msg = _error_msg_network(option, expected)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 105, in _error_msg_network
return msg.format(option, '|'.join(expected))
TypeError: sequence item 4: expected string, bool found

[INFO    ] Completed state [set hostname] at time 12:20:55.475410 duration_in_ms=2.073
[DEBUG   ] File /var/cache/salt/minion/accumulator/52678352 does not exist, no need to cleanup.
[DEBUG   ] LazyLoaded config.option
[DEBUG   ] LazyLoaded highstate.output
local:
----------
ID: set hostname
Function: network.system
Result: False
Comment: An exception occurred in this state: Traceback (most recent call last):
File ""/usr/lib/python2.6/site-packages/salt/state.py"", line 1746, in call
**cdata['kwargs'])
File ""/usr/lib/python2.6/site-packages/salt/loader.py"", line 1704, in wrapper
return f(*args, **kwargs)
File ""/usr/lib/python2.6/site-packages/salt/states/network.py"", line 528, in system
new = __salt__['ip.build_network_settings'](**kwargs)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 1209, in build_network_settings
opts = _parse_network_settings(settings, current_network_settings)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 806, in _parse_network_settings
_raise_error_network('networking', valid)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 848, in _raise_error_network
msg = _error_msg_network(option, expected)
File ""/usr/lib/python2.6/site-packages/salt/modules/rh_ip.py"", line 105, in _error_msg_network
return msg.format(option, '|'.join(expected))
TypeError: sequence item 4: expected string, bool found
Started: 12:20:55.473337
Duration: 2.073 ms
Changes:

Summary for local
------------
Succeeded: 0
Failed:    1
------------
Total states run:     1
Total run time:   2.073 ms",TypeError,buggy_snippets_files/3ebeae07ddccf85cbb9a7b1132aa143595c3d61b83be1320f3c657a2583ffdc7_before_merge.py,buggy_snippets_files/3ebeae07ddccf85cbb9a7b1132aa143595c3d61b83be1320f3c657a2583ffdc7_after_merge.py
219080,"def hash_params(params):
    if not isinstance(params, dict):
        if isinstance(params, list):
            return frozenset(params)
        else:
            return params
    else:
        s = set()
        for k,v in iteritems(params):
            if isinstance(v, dict):
                s.update((k, hash_params(v)))
            elif isinstance(v, list):
                things = []
                for item in v:
                    things.append(hash_params(item))
                s.update((k, tuple(things)))
            else:
                s.update((k, v))
        return frozenset(s)
","def hash_params(params):
    """"""
    Construct a data structure of parameters that is hashable.

    This requires changing any mutable data structures into immutable ones.
    We chose a frozenset because role parameters have to be unique.

    .. warning::  this does not handle unhashable scalars.  Two things
        mitigate that limitation:

        1) There shouldn't be any unhashable scalars specified in the yaml
        2) Our only choice would be to return an error anyway.
    """"""
    # Any container is unhashable if it contains unhashable items (for
    # instance, tuple() is a Hashable subclass but if it contains a dict, it
    # cannot be hashed)
    if isinstance(params, collections.Container) and not isinstance(params, (text_type, binary_type)):
        if isinstance(params, collections.Mapping):
            try:
                # Optimistically hope the contents are all hashable
                new_params = frozenset(params.items())
            except TypeError:
                new_params = set()
                for k, v in params.items():
                    # Hash each entry individually
                    new_params.update((k, hash_params(v)))
                new_params = frozenset(new_params)

        elif isinstance(params, (collections.Set, collections.Sequence)):
            try:
                # Optimistically hope the contents are all hashable
                new_params = frozenset(params)
            except TypeError:
                new_params = set()
                for v in params:
                    # Hash each entry individually
                    new_params.update(hash_params(v))
                new_params = frozenset(new_params)
        else:
            # This is just a guess.
            new_params = frozenset(params)
        return new_params

    # Note: We do not handle unhashable scalars but our only choice would be
    # to raise an error there anyway.
    return frozenset((params,))
",https://github.com/ansible/ansible/issues/18680,"CWE-704: Incorrect Type Conversion or Cast, CWE-241: Improper Handling of Unexpected Data Type","Parameter with_subelements in the config file can contain dicts. When creating the role the parameters are hashed. The hashing algorithm can deal with dicts and lists, but in this case, the parameter type is tuple with dict inside it, so the TypeError: unhashable type: 'dict' is thrown. ",lib/ansible/playbook/role/__init__.py,hash_params,1-20,"TASK [include_role] ************************************************************
task path: /Users/ahaines/work/iot/infrastructure/test.yml:3
ERROR! Unexpected Exception: unhashable type: 'dict'
the full traceback was:

Traceback (most recent call last):
File ""/usr/local/bin/ansible-playbook"", line 103, in <module>
exit_code = cli.run()
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/cli/playbook.py"", line 159, in run
results = pbex.run()
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/executor/playbook_executor.py"", line 154, in run
result = self._tqm.run(play=play)
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/executor/task_queue_manager.py"", line 282, in run
play_return = strategy.run(iterator, play_context)
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/plugins/strategy/linear.py"", line 304, in run
all_role_blocks.extend(new_ir.get_block_list(play=iterator._play, variable_manager=self._variable_manager, loader=self._loader))
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/playbook/role_include.py"", line 76, in get_block_list
actual_role = Role.load(ri, myplay, parent_role=self._parent_role, from_files=self._from_files)
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/playbook/role/__init__.py"", line 118, in load
hashed_params = hash_params(params)
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/playbook/role/__init__.py"", line 54, in hash_params
s.update((k, hash_params(v)))
File ""/usr/local/Cellar/ansible/2.2.0.0_1/libexec/lib/python2.7/site-packages/ansible/playbook/role/__init__.py"", line 61, in hash_params
s.update((k, v))
TypeError: unhashable type: 'dict'",TypeError,buggy_snippets_files/e1edf7f61cddcab0a27742a24dc1e61c3f0b321b0d3a4ebd0b119341e7d16b6a_before_merge.py,buggy_snippets_files/e1edf7f61cddcab0a27742a24dc1e61c3f0b321b0d3a4ebd0b119341e7d16b6a_after_merge.py
160413,"    async def is_online(self):
        if not self.id:
            self.id = await self.fetch_id()
        url = YOUTUBE_SEARCH_ENDPOINT
        params = {
            ""key"": self._token,
            ""part"": ""snippet"",
            ""channelId"": self.id,
            ""type"": ""video"",
            ""eventType"": ""live"",
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as r:
                data = await r.json()
        if ""items"" in data and len(data[""items""]) == 0:
            raise OfflineStream()
        elif ""items"" in data:
            vid_id = data[""items""][0][""id""][""videoId""]
            params = {""key"": self._token, ""id"": vid_id, ""part"": ""snippet""}
            async with aiohttp.ClientSession() as session:
                async with session.get(YOUTUBE_VIDEOS_ENDPOINT, params=params) as r:
                    data = await r.json()
            return self.make_embed(data)
","    async def is_online(self):
        if not self._token:
            raise InvalidYoutubeCredentials(""YouTube API key is not set."")

        if not self.id:
            self.id = await self.fetch_id()
        elif not self.name:
            self.name = await self.fetch_name()

        url = YOUTUBE_SEARCH_ENDPOINT
        params = {
            ""key"": self._token,
            ""part"": ""snippet"",
            ""channelId"": self.id,
            ""type"": ""video"",
            ""eventType"": ""live"",
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as r:
                data = await r.json()
        if ""items"" in data and len(data[""items""]) == 0:
            raise OfflineStream()
        elif ""items"" in data:
            vid_id = data[""items""][0][""id""][""videoId""]
            params = {""key"": self._token, ""id"": vid_id, ""part"": ""snippet""}
            async with aiohttp.ClientSession() as session:
                async with session.get(YOUTUBE_VIDEOS_ENDPOINT, params=params) as r:
                    data = await r.json()
            return self.make_embed(data)
",https://github.com/Cog-Creators/Red-DiscordBot/issues/1999,"CWE-456: Missing Initialization of a Variable, CWE-166: Improper Handling of Missing Special Element","If the youtube alert was added by id, it has no name. The authors forget to copy the name from the channel. In addition the further logic uses this name and not expect it to be None, so the Attribute Error is raised.",redbot/cogs/streams/streamtypes.py,YoutubeStream.is_online,1-24,"Exception during loading of cog
Traceback (most recent call last):
File ""C:\Users\jason\AppData\Local\Programs\Python\Python36\lib\site-packages\redbot\core\core_commands.py"", line 87, in _load
await bot.load_extension(spec)
File ""C:\Users\jason\AppData\Local\Programs\Python\Python36\lib\site-packages\redbot\core\bot.py"", line 222, in load_extension
await lib.setup(self)
File ""C:\Users\jason\AppData\Local\Programs\Python\Python36\lib\site-packages\redbot\cogs\streams\__init__.py"", line 6, in setup
await cog.initialize()
File ""C:\Users\jason\AppData\Local\Programs\Python\Python36\lib\site-packages\redbot\cogs\streams\streams.py"", line 71, in initialize
self.streams = await self.load_streams()
File ""C:\Users\jason\AppData\Local\Programs\Python\Python36\lib\site-packages\redbot\cogs\streams\streams.py"", line 618, in load_streams
return [x for x in streams if not (x.name.lower() in seen or seen_add(x.name.lower()))]
File ""C:\Users\jason\AppData\Local\Programs\Python\Python36\lib\site-packages\redbot\cogs\streams\streams.py"", line 618, in <listcomp>
return [x for x in streams if not (x.name.lower() in seen or seen_add(x.name.lower()))]
AttributeError: 'NoneType' object has no attribute 'lower'",AttributeError,buggy_snippets_files/f49608f4d0247306a44a14edba3c81ff3f5cb4b9205b49dc4e624af54ec23d8b_before_merge.py,buggy_snippets_files/f49608f4d0247306a44a14edba3c81ff3f5cb4b9205b49dc4e624af54ec23d8b_after_merge.py
21636,"def skip_200_and_304(record: logging.LogRecord) -> bool:
    # Apparently, `status_code` is added by Django and is not an actual
    # attribute of LogRecord; as a result, mypy throws an error if we
    # access the `status_code` attribute directly.
    if getattr(record, 'status_code') in [200, 304]:
        return False

    return True
","def skip_200_and_304(record: logging.LogRecord) -> bool:
    # Apparently, `status_code` is added by Django and is not an actual
    # attribute of LogRecord; as a result, mypy throws an error if we
    # access the `status_code` attribute directly.
    if getattr(record, 'status_code', None) in [200, 304]:
        return False

    return True
",https://github.com/zulip/zulip/issues/14595,CWE-166: Improper Handling of Missing Special Element,The function skip_200_and_304 gets status_code attribute but it does not always exist which causes an AttributeError,zerver/lib/logging_util.py,skip_200_and_304,5,"----------------------------------------
Exception happened during processing of request from ('127.0.0.1', 56444)
2020-04-16 11:35:49.159 INFO [zr] 127.0.0.1       POST    429  65ms (mem: 57ms/4) (+start: 24ms) /json/messages (10@zulip via website)
2020-04-16 11:35:49.160 INFO [zr] status=429, data=b'{""result"":""error"",""msg"":""API usage exceeded rate limit"",""retry-after"":2.6131470203}\n', uid=10@zulip
2020-04-16 11:35:49.162 INFO [zr] 127.0.0.1       POST    429  11ms (mem: 7ms/2) /json/messages (10@zulip via website)
2020-04-16 11:35:49.162 WARN [django.server] ""POST /json/messages HTTP/1.1"" 429 84
2020-04-16 11:35:49.173 INFO [zr] status=429, data=b'{""result"":""error"",""msg"":""API usage exceeded rate limit"",""retry-after"":2.6109778881}\n', uid=10@zulip
2020-04-16 11:35:49.179 INFO [zr] 127.0.0.1       POST    429  20ms (+start: 51ms) /json/messages (10@zulip via website)
2020-04-16 11:35:49.182 WARN [django.server] ""POST /json/messages HTTP/1.1"" 429 84
2020-04-16 11:35:49.195 INFO [zr] status=429, data=b'{""result"":""error"",""msg"":""API usage exceeded rate limit"",""retry-after"":2.5940015316}\n', uid=10@zulip
Traceback (most recent call last):
File ""/usr/lib/python3.7/socketserver.py"", line 650, in process_request_thread
self.finish_request(request, client_address)
File ""/usr/lib/python3.7/socketserver.py"", line 360, in finish_request
self.RequestHandlerClass(request, client_address, self)
File ""/usr/lib/python3.7/socketserver.py"", line 720, in __init__
self.handle()
File ""/srv/zulip-py3-venv/lib/python3.7/site-packages/django/core/servers/basehttp.py"", line 171, in handle
self.handle_one_request()
File ""/srv/zulip-py3-venv/lib/python3.7/site-packages/django/core/servers/basehttp.py"", line 187, in handle_one_request
if not self.parse_request():  # An error code has been sent, just exit
File ""/usr/lib/python3.7/http/server.py"", line 322, in parse_request
""Bad request syntax (%r)"" % requestline)
File ""/usr/lib/python3.7/http/server.py"", line 456, in send_error
self.log_error(""code %d, message %s"", code, message)
File ""/usr/lib/python3.7/http/server.py"", line 558, in log_error
self.log_message(format, *args)
File ""/srv/zulip-py3-venv/lib/python3.7/site-packages/django/core/servers/basehttp.py"", line 154, in log_message
level(format, *args, extra=extra)
File ""/usr/lib/python3.7/logging/__init__.py"", line 1383, in info
self._log(INFO, msg, args, **kwargs)
File ""/usr/lib/python3.7/logging/__init__.py"", line 1519, in _log
self.handle(record)
File ""/usr/lib/python3.7/logging/__init__.py"", line 1528, in handle
if (not self.disabled) and self.filter(record):
File ""/usr/lib/python3.7/logging/__init__.py"", line 762, in filter
result = f.filter(record)
File ""/srv/zulip-py3-venv/lib/python3.7/site-packages/django/utils/log.py"", line 147, in filter
if self.callback(record):
File ""/home/sjoerd/zulip/zerver/lib/logging_util.py"", line 122, in skip_200_and_304
if getattr(record, 'status_code') in [200, 304]:
AttributeError: 'LogRecord' object has no attribute 'status_code'
----------------------------------------",AttributeError,buggy_snippets_files/441207e2c6e35204b390d9d1cdca98bbce067a6f140d12470a991f4cd1c25fe1_before_merge.py,buggy_snippets_files/441207e2c6e35204b390d9d1cdca98bbce067a6f140d12470a991f4cd1c25fe1_after_merge.py
132223,"@hydra.main(config_name=""config"")
def my_app(cfg: DictConfig):
    log.info(f""Process ID {os.getpid()} executing task {cfg.task} ..."")

    time.sleep(1)
","@hydra.main(config_name=""config"")
def my_app(cfg: DictConfig):
    env = submitit.JobEnvironment()
    log.info(f""Process ID {os.getpid()} executing task {cfg.task}, with {env}"")
    time.sleep(1)
",https://github.com/facebookresearch/hydra/issues/849,CWE-909: Missing Initialization of Resource,"The program logic requires that job environment was set, however, sometimes it wasn't set, which caused the RuntimeError",plugins/hydra_submitit_launcher/example/my_app.py,my_app,3,"$ HYDRA_FULL_ERROR=1 python foo.py
Traceback (most recent call last):
File ""/private/home/calebh/miniconda3/envs/bench-detectron2/lib/python3.7/site-packages/hydra/_internal/utils.py"", line 198, in run_and_report
return func()
File ""/private/home/calebh/miniconda3/envs/bench-detectron2/lib/python3.7/site-packages/hydra/_internal/utils.py"", line 336, in <lambda>
overrides=args.overrides,
File ""/private/home/calebh/miniconda3/envs/bench-detectron2/lib/python3.7/site-packages/hydra/_internal/hydra.py"", line 109, in run
job_subdir_key=None,
File ""/private/home/calebh/miniconda3/envs/bench-detectron2/lib/python3.7/site-packages/hydra/core/utils.py"", line 123, in run_job
ret.return_value = task_function(task_cfg)
File ""foo.py"", line 7, in main
submitit.JobEnvironment()
File ""/private/home/calebh/miniconda3/envs/bench-detectron2/lib/python3.7/site-packages/submitit/core/job_environment.py"", line 37, in __new__
return plugins.get_job_environment()
File ""/private/home/calebh/miniconda3/envs/bench-detectron2/lib/python3.7/site-packages/submitit/core/plugins.py"", line 82, in get_job_environment
f""Could not figure out which environment the job is runnning in. Known environments: {', '.join(envs.keys())}.""
RuntimeError: Could not figure out which environment the job is runnning in. Known environments: slurm, local, debug.",RuntimeError,buggy_snippets_files/9f790923df8681c97dfffa6ba762c85f708641935928dd6c57769a5d51ebd535_before_merge.py,buggy_snippets_files/9f790923df8681c97dfffa6ba762c85f708641935928dd6c57769a5d51ebd535_after_merge.py
216903,"    def populate(self):
        self.get_cpu_facts()
        self.get_memory_facts()
        self.get_dmi_facts()
        self.get_device_facts()
        self.get_uptime_facts()
        try:
            self.get_mount_facts()
        except TimeoutError:
            pass
        return self.facts
","    def populate(self):
        self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}
        self.get_cpu_facts()
        self.get_memory_facts()
        self.get_dmi_facts()
        self.get_device_facts()
        self.get_uptime_facts()
        try:
            self.get_mount_facts()
        except TimeoutError:
            pass
        return self.facts
",https://github.com/ansible/ansible/issues/24542,"CWE-439: Behavioral Change in New Version or Environment,  CWE-628: Function Call with Incorrectly Specified Arguments","The float function is used in the program. The code behavior depends on system locale in which causes the float number in the wrong format (e.g ""1,1"" instead of ""1.1"")  to be passed in the float function which causes the ValueError",lib/ansible/module_utils/facts.py,SunOSHardware.populate,1,"$ LANG=de_DE.UTF-8
$ ansible puppet-d01 -m setup
puppet-d01 | FAILED! => {
""changed"": false,
""failed"": true,
""module_stderr"": ""######################################################################\nThis is a private computer facility.  Access for any reason must be  |\nspecifically authorized by the owner.  Unless you are so authorized, |\nyour continued access and any other use may expose you to criminal   |\nand/or civil proceedings.  Usage may be monitored.                   |\n---------------------------------------------------------------------|\n# Usage:                 puppet-d01                                  |\n######################################################################\nConnection to puppet-d01 closed.\r\n"",
""module_stdout"": ""Traceback (most recent call last):\r\n  File \""/tmp/ansible_c31TXu/ansible_module_setup.py\"", line 139, in <module>\r\n    main()\r\n  File \""/tmp/ansible_c31TXu/ansible_module_setup.py\"", line 131, in main\r\n    data = get_all_facts(module)\r\n  File \""/tmp/ansible_c31TXu/ansible_modlib.zip/ansible/module_utils/facts.py\"", line 3932, in get_all_facts\r\n  File \""/tmp/ansible_c31TXu/ansible_modlib.zip/ansible/module_utils/facts.py\"", line 3875, in ansible_facts\r\n  File \""/tmp/ansible_c31TXu/ansible_modlib.zip/ansible/module_utils/facts.py\"", line 1569, in populate\r\n  File \""/tmp/ansible_c31TXu/ansible_modlib.zip/ansible/module_utils/facts.py\"", line 1728, in get_uptime_facts\r\nValueError: invalid literal for float(): 26761555,805674\r\n\r\n"",
""msg"": ""MODULE FAILURE"",
""rc"": 0
}",nValueError,buggy_snippets_files/3a3199e96620d48fbf95d3e12658c2b3dcc463aeba5015e79b2128332128797c_before_merge.py,buggy_snippets_files/3a3199e96620d48fbf95d3e12658c2b3dcc463aeba5015e79b2128332128797c_after_merge.py
311463,"def isintlike(x):
    """"""Is x appropriate as an index into a sparse matrix? Returns True
    if it can be cast safely to a machine int.
    """"""
    if issequence(x):
        return False
    else:
        try:
            if int(x) == x:
                return True
            else:
                return False
        except TypeError:
            return False
","def isintlike(x):
    """"""Is x appropriate as an index into a sparse matrix? Returns True
    if it can be cast safely to a machine int.
    """"""
    if issequence(x):
        return False
    try:
        return bool(int(x) == x)
    except (TypeError, ValueError):
        return False
",https://github.com/scipy/scipy/issues/5123,"CWE-248: Uncaught Exception, CWE-440: Expected Behavior Violation, CWE-439: Behavioral Change in New Version or Environment","The problem is in isintlike function. When the object of memoryview type is passed in it, the exception ValueError is raised when trying to cast the argument to int.  In the Python 2 calling int to memoryview gives a TypeError, which is handled correctly, however in Python 3 it gives ValueError",scipy/sparse/sputils.py,isintlike,9,"Traceback (most recent call last):
File ""/home/jbw/data/src/kwant/python3-env/lib/python3.4/site-packages/nose/case.py"", line 198, in runTest
self.test(*self.arg)
File ""/home/jbw/data/src/kwant/python3-env/lib/python3.4/site-packages/kwant/tests/test_system.py"", line 33, in test_hamiltonian_submatrix
mat = sys2.hamiltonian_submatrix(sparse=True)
File ""kwant/_system.pyx"", line 313, in kwant._system.hamiltonian_submatrix (kwant/_system.c:8029)
File ""kwant/_system.pyx"", line 151, in kwant._system.make_sparse_full (kwant/_system.c:4482)
File ""/home/jbw/data/src/kwant/python3-env/lib/python3.4/site-packages/scipy/sparse/coo.py"", line 120, in __init__
if isshape(arg1):
File ""/home/jbw/data/src/kwant/python3-env/lib/python3.4/site-packages/scipy/sparse/sputils.py"", line 212, in isshape
if isintlike(M) and isintlike(N):
File ""/home/jbw/data/src/kwant/python3-env/lib/python3.4/site-packages/scipy/sparse/sputils.py"", line 195, in isintlike
if int(x) == x:
ValueError: invalid literal for int() with base 10: b'\x00\x00\x00\x00\x00\x00\xf0?\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xc0\x00\x00\x00",ValueError,buggy_snippets_files/1548c1299a882daea0fcd90093fb3dc62640c7f10b5c2b7ddfe5296f6f90148d_before_merge.py,buggy_snippets_files/1548c1299a882daea0fcd90093fb3dc62640c7f10b5c2b7ddfe5296f6f90148d_after_merge.py
264236,"def cachedir_index_del(minion_id, base=None):
    '''
    Delete an entry from the cachedir index. This generally only needs to happen
    when an instance is deleted.
    '''
    base = init_cachedir(base)
    index_file = os.path.join(base, 'index.p')
    lock_file(index_file)

    if os.path.exists(index_file):
        with salt.utils.fopen(index_file, 'r') as fh_:
            index = msgpack.load(fh_)
    else:
        return

    if minion_id in index:
        del index[minion_id]

    with salt.utils.fopen(index_file, 'w') as fh_:
        msgpack.dump(index, fh_)

    unlock_file(index_file)
","def cachedir_index_del(minion_id, base=None):
    '''
    Delete an entry from the cachedir index. This generally only needs to happen
    when an instance is deleted.
    '''
    base = init_cachedir(base)
    index_file = os.path.join(base, 'index.p')
    lock_file(index_file)

    if os.path.exists(index_file):
        with salt.utils.fopen(index_file, 'r') as fh_:
            index = msgpack.load(fh_, encoding=MSGPACK_ENCODING)
    else:
        return

    if minion_id in index:
        del index[minion_id]

    with salt.utils.fopen(index_file, 'w') as fh_:
        msgpack.dump(index, fh_, encoding=MSGPACK_ENCODING)

    unlock_file(index_file)
",https://github.com/saltstack/salt/issues/20823,"CWE-116: Improper Encoding or Escaping of Output, CWE-439: Behavioral Change in New Version or Environment","The msgpack.load function uses the system encoding to unserialize object, which gives UricodeEncodeError in the environments where ascii system encoding is used or the encoding is not specified",salt/utils/cloud.py,cachedir_index_del,"12,20","$ salt-call -l trace cloud.full_query
.....
[DEBUG   ] Failed to execute 'ec2.list_nodes_min()' while querying for running nodes: 'ascii' codec can't encode characters in position 70860-70863: ordinal not in range(128)
Traceback (most recent call last):
File ""/usr/lib/python2.7/dist-packages/salt/cloud/__init__.py"", line 2198, in run_parallel_map_providers_query
cloud.clouds[data['fun']]()
File ""/usr/lib/python2.7/dist-packages/salt/cloud/clouds/ec2.py"", line 2720, in list_nodes_min
instances = query(params)
File ""/usr/lib/python2.7/dist-packages/salt/cloud/clouds/ec2.py"", line 345, in query
result.text
UnicodeEncodeError: 'ascii' codec can't encode characters in position 70860-70863: ordinal not in range(128)
......",UnicodeEncodeError,buggy_snippets_files/b0a7c76a1a544565b0e2faa5aecdac8ed9aeaa7c2f23a77a172992d250bd7e15_before_merge.py,buggy_snippets_files/b0a7c76a1a544565b0e2faa5aecdac8ed9aeaa7c2f23a77a172992d250bd7e15_after_merge.py
56802,"    def execute(self,
                idempotent=False,
                create_instances=True,
                create_inventory=True,
                exit=True,
                hide_errors=True):
        """"""
        :param idempotent: Optionally provision servers quietly so output can be parsed for idempotence
        :param create_inventory: Toggle inventory creation
        :param create_instances: Toggle instance creation
        :return: Provisioning output
        """"""
        if self.molecule._state.created:
            create_instances = False

        if self.molecule._state.converged:
            create_inventory = False

        if self.molecule._state.multiple_platforms:
            self.args['--platform'] = 'all'
        else:
            if self.args[
                    '--platform'] == 'all' and self.molecule._state.created:
                create_instances = True
                create_inventory = True

        if create_instances and not idempotent:
            command_args, args = util.remove_args(self.command_args, self.args,
                                                  ['--tags'])
            c = create.Create(command_args, args, self.molecule)
            c.execute()

        if create_inventory:
            self.molecule._create_inventory_file()

        # install role dependencies only during `molecule converge`
        if not idempotent and 'requirements_file' in self.molecule.config.config[
                'ansible'] and not self.molecule._state.installed_deps:
            galaxy = ansible_galaxy.AnsibleGalaxy(self.molecule.config.config)
            galaxy.install()
            self.molecule._state.change_state('installed_deps', True)

        ansible = ansible_playbook.AnsiblePlaybook(self.molecule.config.config[
            'ansible'])

        # params to work with driver
        for k, v in self.molecule.driver.ansible_connection_params.items():
            ansible.add_cli_arg(k, v)

        # target tags passed in via CLI
        if self.molecule._args.get('--tags'):
            ansible.add_cli_arg('tags', self.molecule._args['--tags'].pop(0))

        if idempotent:
            ansible.remove_cli_arg('_out')
            ansible.remove_cli_arg('_err')
            ansible.add_env_arg('ANSIBLE_NOCOLOR', 'true')
            ansible.add_env_arg('ANSIBLE_FORCE_COLOR', 'false')

            # Save the previous callback plugin if any.
            callback_plugin = ansible.env.get('ANSIBLE_CALLBACK_PLUGINS', '')

            # Set the idempotence plugin.
            if callback_plugin:
                ansible.add_env_arg(
                    'ANSIBLE_CALLBACK_PLUGINS',
                    callback_plugin + ':' + os.path.join(
                        sys.prefix,
                        'share/molecule/ansible/plugins/callback/idempotence'))
            else:
                ansible.add_env_arg('ANSIBLE_CALLBACK_PLUGINS', os.path.join(
                    sys.prefix,
                    'share/molecule/ansible/plugins/callback/idempotence'))

        ansible.bake()
        if self.molecule._args.get('--debug'):
            ansible_env = {k: v
                           for (k, v) in ansible.env.items() if 'ANSIBLE' in k}
            other_env = {k: v
                         for (k, v) in ansible.env.items()
                         if 'ANSIBLE' not in k}
            util.debug('OTHER ENVIRONMENT',
                       yaml.dump(other_env,
                                 default_flow_style=False,
                                 indent=2))
            util.debug('ANSIBLE ENVIRONMENT',
                       yaml.dump(ansible_env,
                                 default_flow_style=False,
                                 indent=2))
            util.debug('ANSIBLE PLAYBOOK', str(ansible.ansible))

        util.print_info(""Starting Ansible Run ..."")
        status, output = ansible.execute(hide_errors=hide_errors)
        if status is not None:
            if exit:
                util.sysexit(status)
            return status, None

        if not self.molecule._state.converged:
            self.molecule._state.change_state('converged', True)

        return None, output
","    def execute(self,
                idempotent=False,
                create_instances=True,
                create_inventory=True,
                exit=True,
                hide_errors=True):
        """"""
        :param idempotent: Optionally provision servers quietly so output can be parsed for idempotence
        :param create_inventory: Toggle inventory creation
        :param create_instances: Toggle instance creation
        :return: Provisioning output
        """"""
        if self.molecule._state.created:
            create_instances = False

        if self.molecule._state.converged:
            create_inventory = False

        if self.molecule._state.multiple_platforms:
            self.args['--platform'] = 'all'
        else:
            if self.args[
                    '--platform'] == 'all' and self.molecule._state.created:
                create_instances = True
                create_inventory = True

        if create_instances and not idempotent:
            command_args, args = util.remove_args(self.command_args, self.args,
                                                  ['--tags'])
            c = create.Create(command_args, args, self.molecule)
            c.execute()

        if create_inventory:
            self.molecule._create_inventory_file()

        # install role dependencies only during `molecule converge`
        if not idempotent and 'requirements_file' in self.molecule.config.config[
                'ansible'] and not self.molecule._state.installed_deps:
            galaxy = ansible_galaxy.AnsibleGalaxy(self.molecule.config.config)
            galaxy.install()
            self.molecule._state.change_state('installed_deps', True)

        ansible = ansible_playbook.AnsiblePlaybook(self.molecule.config.config[
            'ansible'])

        # params to work with driver
        for k, v in self.molecule.driver.ansible_connection_params.items():
            ansible.add_cli_arg(k, v)

        # target tags passed in via CLI
        if self.molecule._args.get('--tags'):
            ansible.add_cli_arg('tags', self.molecule._args['--tags'].pop(0))

        if idempotent:
            ansible.remove_cli_arg('_out')
            ansible.remove_cli_arg('_err')
            ansible.add_env_arg('ANSIBLE_NOCOLOR', 'true')
            ansible.add_env_arg('ANSIBLE_FORCE_COLOR', 'false')

            # Save the previous callback plugin if any.
            callback_plugin = ansible.env.get('ANSIBLE_CALLBACK_PLUGINS', '')

            # Set the idempotence plugin.
            if callback_plugin:
                ansible.add_env_arg(
                    'ANSIBLE_CALLBACK_PLUGINS',
                    callback_plugin + ':' + os.path.join(
                        sys.prefix,
                        'share/molecule/ansible/plugins/callback/idempotence'))
            else:
                ansible.add_env_arg('ANSIBLE_CALLBACK_PLUGINS', os.path.join(
                    sys.prefix,
                    'share/molecule/ansible/plugins/callback/idempotence'))

        ansible.bake()
        if self.molecule._args.get('--debug'):
            ansible_env = {k: v
                           for (k, v) in ansible.env.items() if 'ANSIBLE' in k}
            other_env = {k: v
                         for (k, v) in ansible.env.items()
                         if 'ANSIBLE' not in k}
            util.debug('OTHER ENVIRONMENT',
                       yaml.dump(other_env,
                                 default_flow_style=False,
                                 indent=2))
            util.debug('ANSIBLE ENVIRONMENT',
                       yaml.dump(ansible_env,
                                 default_flow_style=False,
                                 indent=2))
            util.debug('ANSIBLE PLAYBOOK', str(ansible._ansible))

        util.print_info(""Starting Ansible Run ..."")
        status, output = ansible.execute(hide_errors=hide_errors)
        if status is not None:
            if exit:
                util.sysexit(status)
            return status, None

        if not self.molecule._state.converged:
            self.molecule._state.change_state('converged', True)

        return None, output
",https://github.com/ansible-community/molecule/issues/395,CWE-573: Improper Following of Specification by Caller,When debugging mode is enabled the information about ansible playbook is printed. But to print this information wrong attribute name is used: the ansible.ansible instead of ansible._ansible. This causes the AttributeError,molecule/command/converge.py,Converge.execute,90,"Traceback (most recent call last):
File ""/Users/jodewey/.homebrew/bin/molecule"", line 10, in <module>
sys.exit(main())
File ""/Users/jodewey/git/molecule/molecule/cli.py"", line 68, in main
CLI().main()
File ""/Users/jodewey/git/molecule/molecule/cli.py"", line 64, in main
util.sysexit(c.execute()[0])
File ""/Users/jodewey/git/molecule/molecule/command/converge.py"", line 136, in execute
util.debug('ANSIBLE PLAYBOOK', str(ansible.ansible))
AttributeError: 'AnsiblePlaybook' object has no attribute 'ansible'",AttributeError,buggy_snippets_files/e938d12c3ece6d2ffc01aa96bad3a4434423a1fc1fc84f3fc79ff15cbcf45c6b_before_merge.py,buggy_snippets_files/e938d12c3ece6d2ffc01aa96bad3a4434423a1fc1fc84f3fc79ff15cbcf45c6b_after_merge.py
136572,"    @attach_runtime_statistics(u""{0.__class__.__name__}.{function_name}"")
    @call_on_reactor_thread
    def data_came_in(self, addr, data):
        """""" The callback function that the thread pool will call when there is incoming data.
        :param addr: The (IP, port) address tuple of the sender.
        :param data: The data received.
        """"""
        if not self._is_running:
            return

        ip, port = addr

        # decode the packet
        try:
            packet = decode_packet(data)
        except InvalidPacketException as e:
            self._logger.error(u""Invalid packet from [%s:%s], packet=[%s], error=%s"", ip, port, hexlify(data), e)
            return

        if packet['opcode'] == OPCODE_WRQ:
            self._logger.error(u""WRQ is not supported from [%s:%s], packet=[%s]"", ip, port, repr(packet))
            return

        self._logger.debug(u""GOT packet opcode[%s] from %s:%s"", packet['opcode'], ip, port)
        # a new request
        if packet['opcode'] == OPCODE_RRQ:
            self._logger.debug(u""start handling new request: %s"", packet)
            self._handle_new_request(ip, port, packet)
            return

        if (ip, port, packet['session_id']) not in self._session_dict:
            self._logger.warn(u""got non-existing session from %s:%s, id = %s"", ip, port, packet['session_id'])
            return

        # handle the response
        session = self._session_dict[(ip, port, packet['session_id'])]
        self._process_packet(session, packet)

        if not session.is_done and not session.is_failed:
            return

        self._cleanup_session((ip, port, packet['session_id']))

        # schedule callback
        if session.is_failed:
            self._logger.info(u""%s failed"", session)
            if session.failure_callback:
                callback = lambda cb = session.failure_callback, a = session.address, fn = session.file_name,\
                    msg = ""download failed"", ei = session.extra_info: cb(a, fn, msg, ei)
                self._callbacks.append(callback)
        elif session.is_done:
            self._logger.info(u""%s finished"", session)
            if session.success_callback:
                callback = lambda cb = session.success_callback, a = session.address, fn = session.file_name,\
                    fd = session.file_data, ei = session.extra_info: cb(a, fn, fd, ei)
                self._callbacks.append(callback)

        self._schedule_callback_processing()
","    @attach_runtime_statistics(u""{0.__class__.__name__}.{function_name}"")
    @call_on_reactor_thread
    def data_came_in(self, addr, data):
        """""" The callback function that the thread pool will call when there is incoming data.
        :param addr: The (IP, port) address tuple of the sender.
        :param data: The data received.
        """"""
        if not self._is_running or not is_valid_address(addr):
            return

        ip, port = addr

        # decode the packet
        try:
            packet = decode_packet(data)
        except InvalidPacketException as e:
            self._logger.error(u""Invalid packet from [%s:%s], packet=[%s], error=%s"", ip, port, hexlify(data), e)
            return

        if packet['opcode'] == OPCODE_WRQ:
            self._logger.error(u""WRQ is not supported from [%s:%s], packet=[%s]"", ip, port, repr(packet))
            return

        self._logger.debug(u""GOT packet opcode[%s] from %s:%s"", packet['opcode'], ip, port)
        # a new request
        if packet['opcode'] == OPCODE_RRQ:
            self._logger.debug(u""start handling new request: %s"", packet)
            self._handle_new_request(ip, port, packet)
            return

        if (ip, port, packet['session_id']) not in self._session_dict:
            self._logger.warn(u""got non-existing session from %s:%s, id = %s"", ip, port, packet['session_id'])
            return

        # handle the response
        session = self._session_dict[(ip, port, packet['session_id'])]
        self._process_packet(session, packet)

        if not session.is_done and not session.is_failed:
            return

        self._cleanup_session((ip, port, packet['session_id']))

        # schedule callback
        if session.is_failed:
            self._logger.info(u""%s failed"", session)
            if session.failure_callback:
                callback = lambda cb = session.failure_callback, a = session.address, fn = session.file_name,\
                    msg = ""download failed"", ei = session.extra_info: cb(a, fn, msg, ei)
                self._callbacks.append(callback)
        elif session.is_done:
            self._logger.info(u""%s finished"", session)
            if session.success_callback:
                callback = lambda cb = session.success_callback, a = session.address, fn = session.file_name,\
                    fd = session.file_data, ei = session.extra_info: cb(a, fn, fd, ei)
                self._callbacks.append(callback)

        self._schedule_callback_processing()
",https://github.com/Tribler/tribler/issues/3268,CWE-703: Improper Check or Handling of Exceptional Conditions,The port number in the data_came_in function can be 0 which causes the AssertionError when trying to send a packet to this port,Tribler/Core/TFTP/handler.py,TftpHandler.data_came_in,8,"Traceback (most recent call last):
File ""TriblerGUI\event_request_manager.py"", line 99, in on_read_data
RuntimeError: Unhandled Error
Traceback (most recent call last):
File ""lib\multiprocessing\process.py"", line 114, in run

File ""TriblerGUI\core_manager.py"", line 63, in start_tribler_core

File ""lib\site-packages\twisted\internet\base.py"", line 1195, in run

File ""lib\site-packages\twisted\internet\base.py"", line 1204, in mainLoop

--- <exception caught here> ---
File ""lib\site-packages\twisted\internet\base.py"", line 798, in runUntilCurrent

File ""Tribler\Core\TFTP\handler.py"", line 233, in data_came_in

File ""Tribler\Core\TFTP\handler.py"", line 325, in _handle_new_request

File ""Tribler\Core\TFTP\handler.py"", line 563, in _send_oack_packet

File ""Tribler\Core\TFTP\handler.py"", line 516, in _send_packet

File ""Tribler\dispersy\candidate.py"", line 30, in __init__

exceptions.AssertionError: ('182.30.65.219', 0)",RuntimeError,buggy_snippets_files/2e788d2c2481aaf24d22d9e9629e57739da5488e8a5855a7a03ab03e1ca27884_before_merge.py,buggy_snippets_files/2e788d2c2481aaf24d22d9e9629e57739da5488e8a5855a7a03ab03e1ca27884_after_merge.py
20807,"def setup_readline():
    """"""Sets up the readline module and completion suppression, if available.""""""
    global RL_COMPLETION_SUPPRESS_APPEND, RL_LIB, RL_CAN_RESIZE, RL_STATE, \
        readline, RL_COMPLETION_QUERY_ITEMS
    if RL_COMPLETION_SUPPRESS_APPEND is not None:
        return
    for _rlmod_name in ('gnureadline', 'readline'):
        try:
            readline = importlib.import_module(_rlmod_name)
            sys.modules['readline'] = readline
        except ImportError:
            pass
        else:
            break

    if readline is None:
        print(""""""Skipping setup. Because no `readline` implementation available.
            Please install a backend (`readline`, `prompt-toolkit`, etc) to use
            `xonsh` interactively.
            See https://github.com/xonsh/xonsh/issues/1170"""""")
        return

    import ctypes
    import ctypes.util
    uses_libedit = readline.__doc__ and 'libedit' in readline.__doc__
    readline.set_completer_delims(' \t\n')
    # Cygwin seems to hang indefinitely when querying the readline lib
    if (not ON_CYGWIN) and (not readline.__file__.endswith('.py')):
        RL_LIB = lib = ctypes.cdll.LoadLibrary(readline.__file__)
        try:
            RL_COMPLETION_SUPPRESS_APPEND = ctypes.c_int.in_dll(
                lib, 'rl_completion_suppress_append')
        except ValueError:
            # not all versions of readline have this symbol, ie Macs sometimes
            RL_COMPLETION_SUPPRESS_APPEND = None
        try:
            RL_COMPLETION_QUERY_ITEMS = ctypes.c_int.in_dll(
                lib, 'rl_completion_query_items')
        except ValueError:
            # not all versions of readline have this symbol, ie Macs sometimes
            RL_COMPLETION_QUERY_ITEMS = None
        try:
            RL_STATE = ctypes.c_int.in_dll(lib, 'rl_readline_state')
        except Exception:
            pass
        RL_CAN_RESIZE = hasattr(lib, 'rl_reset_screen_size')
    env = builtins.__xonsh_env__
    # reads in history
    readline.set_history_length(-1)
    ReadlineHistoryAdder()
    # sets up IPython-like history matching with up and down
    readline.parse_and_bind('""\e[B"": history-search-forward')
    readline.parse_and_bind('""\e[A"": history-search-backward')
    # Setup Shift-Tab to indent
    readline.parse_and_bind('""\e[Z"": ""{0}""'.format(env.get('INDENT')))

    # handle tab completion differences found in libedit readline compatibility
    # as discussed at http://stackoverflow.com/a/7116997
    if uses_libedit and ON_DARWIN:
        readline.parse_and_bind(""bind ^I rl_complete"")
        print('\n'.join(['', ""*"" * 78,
                         ""libedit detected - readline will not be well behaved, including but not limited to:"",
                         ""   * crashes on tab completion"",
                         ""   * incorrect history navigation"",
                         ""   * corrupting long-lines"",
                         ""   * failure to wrap or indent lines properly"",
                         """",
                         ""It is highly recommended that you install gnureadline, which is installable with:"",
                         ""     pip install gnureadline"",
                         ""*"" * 78]), file=sys.stderr)
    else:
        readline.parse_and_bind(""tab: complete"")
    # try to load custom user settings
    inputrc_name = os.environ.get('INPUTRC')
    if inputrc_name is None:
        if uses_libedit:
            inputrc_name = '.editrc'
        else:
            inputrc_name = '.inputrc'
        inputrc_name = os.path.join(os.path.expanduser('~'), inputrc_name)
    if (not ON_WINDOWS) and (not os.path.isfile(inputrc_name)):
        inputrc_name = '/etc/inputrc'
    if os.path.isfile(inputrc_name):
        try:
            readline.read_init_file(inputrc_name)
        except Exception:
            # this seems to fail with libedit
            print_exception('xonsh: could not load readline default init file.')
    # properly reset intput typed before the first prompt
    readline.set_startup_hook(carriage_return)
","def setup_readline():
    """"""Sets up the readline module and completion suppression, if available.""""""
    global RL_COMPLETION_SUPPRESS_APPEND, RL_LIB, RL_CAN_RESIZE, RL_STATE, \
        readline, RL_COMPLETION_QUERY_ITEMS
    if RL_COMPLETION_SUPPRESS_APPEND is not None:
        return
    for _rlmod_name in ('gnureadline', 'readline'):
        try:
            readline = importlib.import_module(_rlmod_name)
            sys.modules['readline'] = readline
        except ImportError:
            pass
        else:
            break

    if readline is None:
        print(""""""Skipping setup. Because no `readline` implementation available.
            Please install a backend (`readline`, `prompt-toolkit`, etc) to use
            `xonsh` interactively.
            See https://github.com/xonsh/xonsh/issues/1170"""""")
        return

    import ctypes
    import ctypes.util
    uses_libedit = readline.__doc__ and 'libedit' in readline.__doc__
    readline.set_completer_delims(' \t\n')
    # Cygwin seems to hang indefinitely when querying the readline lib
    if (not ON_CYGWIN) and (not readline.__file__.endswith('.py')):
        RL_LIB = lib = ctypes.cdll.LoadLibrary(readline.__file__)
        try:
            RL_COMPLETION_SUPPRESS_APPEND = ctypes.c_int.in_dll(
                lib, 'rl_completion_suppress_append')
        except ValueError:
            # not all versions of readline have this symbol, ie Macs sometimes
            RL_COMPLETION_SUPPRESS_APPEND = None
        try:
            RL_COMPLETION_QUERY_ITEMS = ctypes.c_int.in_dll(
                lib, 'rl_completion_query_items')
        except ValueError:
            # not all versions of readline have this symbol, ie Macs sometimes
            RL_COMPLETION_QUERY_ITEMS = None
        try:
            RL_STATE = ctypes.c_int.in_dll(lib, 'rl_readline_state')
        except Exception:
            pass
        RL_CAN_RESIZE = hasattr(lib, 'rl_reset_screen_size')
    env = builtins.__xonsh_env__
    # reads in history
    readline.set_history_length(-1)
    ReadlineHistoryAdder()
    # sets up IPython-like history matching with up and down
    readline.parse_and_bind('""\e[B"": history-search-forward')
    readline.parse_and_bind('""\e[A"": history-search-backward')
    # Setup Shift-Tab to indent
    readline.parse_and_bind('""\e[Z"": ""{0}""'.format(env.get('INDENT')))

    # handle tab completion differences found in libedit readline compatibility
    # as discussed at http://stackoverflow.com/a/7116997
    if uses_libedit and ON_DARWIN:
        readline.parse_and_bind(""bind ^I rl_complete"")
        print('\n'.join(['', ""*"" * 78,
                         ""libedit detected - readline will not be well behaved, including but not limited to:"",
                         ""   * crashes on tab completion"",
                         ""   * incorrect history navigation"",
                         ""   * corrupting long-lines"",
                         ""   * failure to wrap or indent lines properly"",
                         """",
                         ""It is highly recommended that you install gnureadline, which is installable with:"",
                         ""     pip install gnureadline"",
                         ""*"" * 78]), file=sys.stderr)
    else:
        readline.parse_and_bind(""tab: complete"")
    # try to load custom user settings
    inputrc_name = os.environ.get('INPUTRC')
    if inputrc_name is None:
        if uses_libedit:
            inputrc_name = '.editrc'
        else:
            inputrc_name = '.inputrc'
        inputrc_name = os.path.join(os.path.expanduser('~'), inputrc_name)
    if (not ON_WINDOWS) and (not os.path.isfile(inputrc_name)):
        inputrc_name = '/etc/inputrc'
    if ON_WINDOWS:
        winutils.enable_virtual_terminal_processing()
    if os.path.isfile(inputrc_name):
        try:
            readline.read_init_file(inputrc_name)
        except Exception:
            # this seems to fail with libedit
            print_exception('xonsh: could not load readline default init file.')
    # properly reset intput typed before the first prompt
    readline.set_startup_hook(carriage_return)
",https://github.com/xonsh/xonsh/issues/2130,CWE-909: Missing Initialization of Resource,When created the console on windows os the console should be switched in special mode which provides extended console features. The switch construction was missing,xonsh/readline_shell.py,setup_readline,83,"Readline internal error
Traceback (most recent call last):
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\console\console.py"", line 768, in hook_wrapper_23
res = ensure_str(readline_hook(prompt))
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\rlmain.py"", line 571, in readline
self._readline_from_keyboard()
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\rlmain.py"", line 536, in _readline_from_keyboard
if self._readline_from_keyboard_poll():
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\rlmain.py"", line 556, in _readline_from_keyboard_poll
result = self.mode.process_keyevent(event.keyinfo)
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\modes\emacs.py"", line 243, in process_keyevent
r = self.process_keyevent_queue[-1](keyinfo)
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\modes\emacs.py"", line 286, in _process_keyevent
r = dispatch_func(keyinfo)
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\modes\basemode.py"", line 257, in complete
completions = self._get_completions()
File ""C:\Users\mel\Anaconda3\lib\site-packages\pyreadline\modes\basemode.py"", line 200, in _get_completions
r = self.completer(ensure_unicode(text), i)
File ""C:\Users\mel\Anaconda3\lib\cmd.py"", line 275, in complete
self.completion_matches = compfunc(text, line, begidx, endidx)
File ""C:\Users\mel\Anaconda3\lib\site-packages\xonsh\__amalgam__.py"", line 18735, in completedefault
show_completions = self._querycompletions(completions, endidx - begidx)
File ""C:\Users\mel\Anaconda3\lib\site-packages\xonsh\__amalgam__.py"", line 18695, in _querycompletions
rl_on_new_line()
TypeError: 'NoneType' object is not callable",TypeError,buggy_snippets_files/5c59ed3159849b172b016e9f5e0029f37a38aa50023bd44ceea6563ed9014fa1_before_merge.py,buggy_snippets_files/5c59ed3159849b172b016e9f5e0029f37a38aa50023bd44ceea6563ed9014fa1_after_merge.py
117985,"    def visualize(
        self, flow_state: ""prefect.engine.state.State"" = None, filename: str = None
    ) -> object:
        """"""
        Creates graphviz object for representing the current flow; this graphviz
        object will be rendered inline if called from an IPython notebook, otherwise
        it will be rendered in a new window.  If a `filename` is provided, the object
        will not be rendered and instead saved to the location specified.

        Args:
            - flow_state (State, optional): flow state object used to optionally color the nodes
            - filename (str, optional): a filename specifying a location to save this visualization to; if provided,
                the visualization will not be rendered automatically

        Raises:
            - ImportError: if `graphviz` is not installed
        """"""

        try:
            import graphviz
        except ImportError:
            msg = (
                ""This feature requires graphviz.\n""
                ""Try re-installing prefect with `pip install 'prefect[viz]'`""
            )
            raise ImportError(msg)

        def get_color(task: Task, map_index: int = None) -> str:
            assert flow_state
            assert isinstance(flow_state.result, dict)

            if map_index is not None:
                state = flow_state.result[task].map_states[map_index]
            else:
                state = flow_state.result.get(task)
            if state is not None:
                assert state is not None  # mypy assert
                return state.color + ""80""
            return ""#00000080""

        graph = graphviz.Digraph()

        for t in self.tasks:
            is_mapped = any(edge.mapped for edge in self.edges_to(t))
            shape = ""box"" if is_mapped else ""ellipse""
            name = ""{} <map>"".format(t.name) if is_mapped else t.name
            if is_mapped and flow_state:
                assert isinstance(flow_state.result, dict)
                for map_index, _ in enumerate(flow_state.result[t].map_states):
                    kwargs = dict(
                        color=get_color(t, map_index=map_index),
                        style=""filled"",
                        colorscheme=""svg"",
                    )
                    graph.node(str(id(t)) + str(map_index), name, shape=shape, **kwargs)
            else:
                kwargs = (
                    {}
                    if not flow_state
                    else dict(color=get_color(t), style=""filled"", colorscheme=""svg"")
                )
                graph.node(str(id(t)), name, shape=shape, **kwargs)

        for e in self.edges:
            style = ""dashed"" if e.mapped else None
            if (
                e.mapped
                or any(edge.mapped for edge in self.edges_to(e.downstream_task))
            ) and flow_state:
                assert isinstance(flow_state.result, dict)
                for map_index, _ in enumerate(
                    flow_state.result[e.downstream_task].map_states
                ):
                    upstream_id = str(id(e.upstream_task))
                    if any(edge.mapped for edge in self.edges_to(e.upstream_task)):
                        upstream_id += str(map_index)
                    graph.edge(
                        upstream_id,
                        str(id(e.downstream_task)) + str(map_index),
                        e.key,
                        style=style,
                    )
            else:
                graph.edge(
                    str(id(e.upstream_task)),
                    str(id(e.downstream_task)),
                    e.key,
                    style=style,
                )

        if filename:
            graph.render(filename, view=False)
        else:
            try:
                from IPython import get_ipython

                assert get_ipython().config.get(""IPKernelApp"") is not None
            except Exception:
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp.close()
                    try:
                        graph.render(tmp.name, view=True)
                    except graphviz.backend.ExecutableNotFound:
                        msg = ""It appears you do not have Graphviz installed, or it is not on your PATH.\n""
                        msg += ""Please install Graphviz from http://www.graphviz.org/download/\n""
                        msg += ""And note: just installing the `graphviz` python package is not sufficient!""
                        raise graphviz.backend.ExecutableNotFound(msg)
                    finally:
                        os.unlink(tmp.name)

        return graph
","    def visualize(
        self, flow_state: ""prefect.engine.state.State"" = None, filename: str = None
    ) -> object:
        """"""
        Creates graphviz object for representing the current flow; this graphviz
        object will be rendered inline if called from an IPython notebook, otherwise
        it will be rendered in a new window.  If a `filename` is provided, the object
        will not be rendered and instead saved to the location specified.

        Args:
            - flow_state (State, optional): flow state object used to optionally color the nodes
            - filename (str, optional): a filename specifying a location to save this visualization to; if provided,
                the visualization will not be rendered automatically

        Raises:
            - ImportError: if `graphviz` is not installed
        """"""

        try:
            import graphviz
        except ImportError:
            msg = (
                ""This feature requires graphviz.\n""
                ""Try re-installing prefect with `pip install 'prefect[viz]'`""
            )
            raise ImportError(msg)

        def get_color(task: Task, map_index: int = None) -> str:
            assert flow_state
            assert isinstance(flow_state.result, dict)

            if map_index is not None:
                state = flow_state.result[task].map_states[map_index]
            else:
                state = flow_state.result.get(task)
            if state is not None:
                assert state is not None  # mypy assert
                return state.color + ""80""
            return ""#00000080""

        graph = graphviz.Digraph()

        for t in self.tasks:
            is_mapped = any(edge.mapped for edge in self.edges_to(t))
            shape = ""box"" if is_mapped else ""ellipse""
            name = ""{} <map>"".format(t.name) if is_mapped else t.name
            if is_mapped and flow_state:
                assert isinstance(flow_state.result, dict)
                if flow_state.result[t].is_mapped():
                    for map_index, _ in enumerate(flow_state.result[t].map_states):
                        kwargs = dict(
                            color=get_color(t, map_index=map_index),
                            style=""filled"",
                            colorscheme=""svg"",
                        )
                        graph.node(
                            str(id(t)) + str(map_index), name, shape=shape, **kwargs
                        )
                else:
                    kwargs = dict(
                        color=get_color(t), style=""filled"", colorscheme=""svg"",
                    )
                    graph.node(str(id(t)), name, shape=shape, **kwargs)
            else:
                kwargs = (
                    {}
                    if not flow_state
                    else dict(color=get_color(t), style=""filled"", colorscheme=""svg"")
                )
                graph.node(str(id(t)), name, shape=shape, **kwargs)

        for e in self.edges:
            style = ""dashed"" if e.mapped else None
            if (
                e.mapped
                or any(edge.mapped for edge in self.edges_to(e.downstream_task))
            ) and flow_state:
                assert isinstance(flow_state.result, dict)
                down_state = flow_state.result[e.downstream_task]
                if down_state.is_mapped():
                    for map_index, _ in enumerate(down_state.map_states):
                        upstream_id = str(id(e.upstream_task))
                        if any(edge.mapped for edge in self.edges_to(e.upstream_task)):
                            upstream_id += str(map_index)
                        graph.edge(
                            upstream_id,
                            str(id(e.downstream_task)) + str(map_index),
                            e.key,
                            style=style,
                        )
                else:
                    graph.edge(
                        str(id(e.upstream_task)),
                        str(id(e.downstream_task)),
                        e.key,
                        style=style,
                    )
            else:
                graph.edge(
                    str(id(e.upstream_task)),
                    str(id(e.downstream_task)),
                    e.key,
                    style=style,
                )

        if filename:
            graph.render(filename, view=False)
        else:
            try:
                from IPython import get_ipython

                assert get_ipython().config.get(""IPKernelApp"") is not None
            except Exception:
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp.close()
                    try:
                        graph.render(tmp.name, view=True)
                    except graphviz.backend.ExecutableNotFound:
                        msg = ""It appears you do not have Graphviz installed, or it is not on your PATH.\n""
                        msg += ""Please install Graphviz from http://www.graphviz.org/download/\n""
                        msg += ""And note: just installing the `graphviz` python package is not sufficient!""
                        raise graphviz.backend.ExecutableNotFound(msg)
                    finally:
                        os.unlink(tmp.name)

        return graph
",https://github.com/PrefectHQ/prefect/issues/1765,CWE-703: Improper Check or Handling of Exceptional Conditions,"If the flow contains the task which maps over collection, it visualized improperly and the AttrubuteError exception was raised. The code was missing check on this special case",src/prefect/core/flow.py,Flow.visualize,49,"[2019-11-21 09:41:33,824] INFO - prefect.FlowRunner | Beginning Flow run for 'ETL'
[2019-11-21 09:41:33,826] INFO - prefect.FlowRunner | Starting flow run.
[2019-11-21 09:41:33,831] INFO - prefect.TaskRunner | Task 'extract': Starting task run...
[2019-11-21 09:41:33,834] INFO - prefect.TaskRunner | Task 'extract': finished task run for task with final state: 'Success'
[2019-11-21 09:41:33,839] INFO - prefect.TaskRunner | Task 'do_load': Starting task run...
[2019-11-21 09:41:33,841] INFO - prefect.TaskRunner | Task 'do_load': finished task run for task with final state: 'Success'
[2019-11-21 09:41:33,848] INFO - prefect.TaskRunner | Task 'CompareValue: ""False""': Starting task run...
[2019-11-21 09:41:33,850] INFO - prefect.TaskRunner | Task 'CompareValue: ""False""': finished task run for task with final state: 'Success'
[2019-11-21 09:41:33,856] INFO - prefect.TaskRunner | Task 'CompareValue: ""True""': Starting task run...
[2019-11-21 09:41:33,858] INFO - prefect.TaskRunner | Task 'CompareValue: ""True""': finished task run for task with final state: 'Skipped'
[2019-11-21 09:41:33,863] INFO - prefect.TaskRunner | Task 'transform': Starting task run...
[2019-11-21 09:41:33,865] INFO - prefect.TaskRunner | Task 'transform': finished task run for task with final state: 'Success'
[2019-11-21 09:41:33,870] INFO - prefect.TaskRunner | Task 'load': Starting task run...
[2019-11-21 09:41:33,872] INFO - prefect.TaskRunner | Task 'load': finished task run for task with final state: 'Skipped'
[2019-11-21 09:41:33,874] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded
Traceback (most recent call last):
File ""min_example.py"", line 28, in <module>
flow.visualize(flow_state=state)
File ""/Users/jtherhaa/miniconda3/envs/prefect/lib/python3.6/site-packages/prefect/core/flow.py"", line 1089, in visualize
for map_index, _ in enumerate(flow_state.result[t].map_states):
AttributeError: 'Skipped' object has no attribute 'map_states'",AttributeError,buggy_snippets_files/8886d79145528301073d727cf23d14f330ab9847b21e9ef302f22da501bca655_before_merge.py,buggy_snippets_files/8886d79145528301073d727cf23d14f330ab9847b21e9ef302f22da501bca655_after_merge.py
204579,"    def _update_magp_data(self, magp_data):
        for magp_item in magp_data:
            magp_id = self.get_magp_id(magp_item)
            inst_data = self._create_magp_instance_data(magp_id, magp_item)
            self._current_config[magp_id] = inst_data
","    def _update_magp_data(self, magp_data):
        if self._os_version >= self.ONYX_API_VERSION:
            for magp_config in magp_data:
                for magp_name, data in iteritems(magp_config):
                    magp_id = int(magp_name.replace('MAGP ', ''))
                    self._current_config[magp_id] = \
                        self._create_magp_instance_data(magp_id, data[0])
        else:
            for magp_item in magp_data:
                magp_id = self.get_magp_id(magp_item)
                inst_data = self._create_magp_instance_data(magp_id, magp_item)
                self._current_config[magp_id] = inst_data
",https://github.com/ansible/ansible/issues/46475,Use of Function with Inconsistent Implementations,"The onyx library updated the API version which is incompatible with the older one. The program tried to work with API version like with an older one, which caused AttributeError exception",lib/ansible/modules/network/onyx/onyx_magp.py,OnyxMagpModule._update_magp_data,2,"fatal: [sn2410_leaf1_1]: FAILED! => {
""changed"": false,
""module_stderr"": ""Traceback (most recent call last):\n  File \""/tmp/ansible_2cIQBk/ansible_module_onyx_magp.py\"", line 227, in <module>\n    main()\n  File \""/tmp/ansible_2cIQBk/ansible_module_onyx_magp.py\"", line 223, in main\n    OnyxMagpModule.main()\n  File \""/tmp/ansible_2cIQBk/ansible_modlib.zip/ansible/module_utils/network/onyx/onyx.py\"", line 240, in main\n  File \""/tmp/ansible_2cIQBk/ansible_modlib.zip/ansible/module_utils/network/onyx/onyx.py\"", line 218, in run\n  File \""/tmp/ansible_2cIQBk/ansible_module_onyx_magp.py\"", line 150, in load_current_config\n    self._update_magp_data(magp_data)\n  File \""/tmp/ansible_2cIQBk/ansible_module_onyx_magp.py\"", line 137, in _update_magp_data\n    magp_id = self.get_magp_id(magp_item)\n  File \""/tmp/ansible_2cIQBk/ansible_module_onyx_magp.py\"", line 123, in get_magp_id\n    return int(header.split()[1])\nAttributeError: 'NoneType' object has no attribute 'split'\n"",
""module_stdout"": """",
""msg"": ""MODULE FAILURE"",
""rc"": 1
}",nAttributeError,buggy_snippets_files/24f6971d64f3d46b9a197b242235e0cab767dfc3378f4b64ed87bff5aed83a51_before_merge.py,buggy_snippets_files/24f6971d64f3d46b9a197b242235e0cab767dfc3378f4b64ed87bff5aed83a51_after_merge.py
83453,"def make_main(argv=sys.argv):
    """"""Sphinx build ""make mode"" entry.""""""
    warnings.warn(
        '`sphinx.build_main()` has moved to `sphinx.cmd.build.make_main()`.',
        RemovedInSphinx20Warning,
        stacklevel=2,
    )
    return build.make_main(argv[1:])  # skip first argument to adjust arguments (refs: #4615)
","def make_main(argv=sys.argv):
    """"""Sphinx build ""make mode"" entry.""""""
    from .cmd import build
    warnings.warn(
        '`sphinx.build_main()` has moved to `sphinx.cmd.build.make_main()`.',
        RemovedInSphinx20Warning,
        stacklevel=2,
    )
    return build.make_main(argv[1:])  # skip first argument to adjust arguments (refs: #4615)
",https://github.com/sphinx-doc/sphinx/issues/4669,CWE-456: Missing Initialization of a Variable,The deprecated call make_main caused a NameError: name build is not defined. The import build into namespace was missing so there is not build object.,sphinx/__init__.py,make_main,8,"Traceback (most recent call last):
File ""./build-deploy.py"", line 141, in <module>
errno = build_html(args.sourcedir, args.outputdir, args.branch, args.nitpicky, args.force)
File ""./build-deploy.py"", line 65, in build_html
errno = _sphinx_build(arglist)
File ""./build-deploy.py"", line 44, in _sphinx_build
return sphinx.make_main(arglist)
File ""/usr/local/lib/python3.6/site-packages/sphinx/__init__.py"", line 91, in make_main
return build.make_main(argv[1:])  # skip first argument to adjust arguments (refs: #4615)
NameError: name 'build' is not defined",NameError,buggy_snippets_files/8fe1a274b51fcb81fd2e9c031a123c2d636477c09cfed76acf439ad991735706_before_merge.py,buggy_snippets_files/8fe1a274b51fcb81fd2e9c031a123c2d636477c09cfed76acf439ad991735706_after_merge.py
287589,"    def write_data_to_db(self, workspace, post_group=False, image_number=None):
        """"""Write the data in the measurements out to the database
        workspace - contains the measurements
        mappings  - map a feature name to a column name
        image_number - image number for primary database key. Defaults to current.
        """"""
        if self.show_window:
            disp_header = [""Table"", ""Statement""]
            disp_columns = []
        try:
            zeros_for_nan = False
            measurements = workspace.measurements
            assert isinstance(measurements, Measurements)
            pipeline = workspace.pipeline
            image_set_list = workspace.image_set_list
            measurement_cols = self.get_pipeline_measurement_columns(
                pipeline, image_set_list
            )
            mapping = self.get_column_name_mappings(pipeline, image_set_list)

            ###########################################
            #
            # The image table
            #
            ###########################################
            if image_number is None:
                image_number = measurements.image_set_number

            image_row = []
            if not post_group:
                image_row += [(image_number, ""integer"", C_IMAGE_NUMBER,)]
            feature_names = set(measurements.get_feature_names(""Image""))
            for m_col in measurement_cols:
                if m_col[0] != ""Image"":
                    continue
                if not self.should_write(m_col, post_group):
                    continue
                #
                # Skip if feature name not in measurements. This
                # can happen if image set gets aborted or for some legacy
                # measurement files.
                #
                if m_col[1] not in feature_names:
                    continue
                feature_name = ""%s_%s"" % (""Image"", m_col[1])
                value = measurements.get_measurement(""Image"", m_col[1], image_number)
                if isinstance(value, numpy.ndarray):
                    value = value[0]
                if (
                    isinstance(value, float)
                    and not numpy.isfinite(value)
                    and zeros_for_nan
                ):
                    value = 0
                image_row.append((value, m_col[2], feature_name))
            #
            # Aggregates for the image table
            #
            agg_dict = measurements.compute_aggregate_measurements(
                image_number, self.agg_names
            )
            agg_columns = self.get_aggregate_columns(
                pipeline, image_set_list, post_group
            )
            image_row += [
                (agg_dict[agg[3]], COLTYPE_FLOAT, agg[3]) for agg in agg_columns
            ]

            #
            # Delete any prior data for this image
            #
            # Useful if you rerun a partially-complete batch
            #
            if not post_group:
                stmt = ""DELETE FROM %s WHERE %s=%d"" % (
                    self.get_table_name(""Image""),
                    C_IMAGE_NUMBER,
                    image_number,
                )
                execute(self.cursor, stmt, return_result=False)
                #
                # Delete relationships as well.
                #
                if self.wants_relationship_table:
                    for col in (COL_IMAGE_NUMBER1, COL_IMAGE_NUMBER2):
                        stmt = ""DELETE FROM %s WHERE %s=%d"" % (
                            self.get_table_name(T_RELATIONSHIPS),
                            col,
                            image_number,
                        )
                        execute(self.cursor, stmt, return_result=False)

            ########################################
            #
            # Object tables
            #
            ########################################
            object_names = self.get_object_names(pipeline, image_set_list)
            if len(object_names) > 0:
                if self.separate_object_tables == OT_COMBINE:
                    data = [(OBJECT, object_names)]
                else:
                    data = [
                        (object_name, [object_name]) for object_name in object_names
                    ]
                for table_object_name, object_list in data:
                    table_name = self.get_table_name(table_object_name)
                    columns = [
                        column
                        for column in measurement_cols
                        if column[0] in object_list
                        and self.should_write(column, post_group)
                    ]
                    if post_group and len(columns) == 0:
                        continue
                    max_count = 0
                    for object_name in object_list:
                        ftr_count = ""Count_%s"" % object_name
                        count = measurements.get_measurement(
                            ""Image"", ftr_count, image_number
                        )
                        max_count = max(max_count, int(count))
                    column_values = []
                    for column in columns:
                        object_name, feature, coltype = column[:3]
                        values = measurements.get_measurement(
                            object_name, feature, image_number
                        )

                        if len(values) < max_count:
                            values = list(values) + [None] * (max_count - len(values))
                        values = [
                            None
                            if v is None or numpy.isnan(v) or numpy.isinf(v)
                            else str(v)
                            for v in values
                        ]
                        column_values.append(values)
                    object_cols = []
                    if not post_group:
                        object_cols += [C_IMAGE_NUMBER]
                    if table_object_name == OBJECT:
                        object_number_column = C_OBJECT_NUMBER
                        if not post_group:
                            object_cols += [object_number_column]
                        object_numbers = numpy.arange(1, max_count + 1)
                    else:
                        object_number_column = ""_"".join(
                            (object_name, M_NUMBER_OBJECT_NUMBER)
                        )
                        object_numbers = measurements.get_measurement(
                            object_name, M_NUMBER_OBJECT_NUMBER, image_number
                        )

                    object_cols += [
                        mapping[""%s_%s"" % (column[0], column[1])] for column in columns
                    ]
                    object_rows = []
                    for j in range(max_count):
                        if not post_group:
                            object_row = [image_number]
                            if table_object_name == OBJECT:
                                # the object number
                                object_row.append(object_numbers[j])
                        else:
                            object_row = []

                        for column, values in zip(columns, column_values):
                            object_name, feature, coltype = column[:3]
                            object_row.append(values[j])
                        if post_group:
                            object_row.append(object_numbers[j])
                        object_rows.append(object_row)
                    #
                    # Delete any prior data for this image
                    #
                    if not post_group:
                        stmt = ""DELETE FROM %s WHERE %s=%d"" % (
                            table_name,
                            C_IMAGE_NUMBER,
                            image_number,
                        )

                        execute(self.cursor, stmt, return_result=False)
                        #
                        # Write the object table data
                        #
                        stmt = ""INSERT INTO %s (%s) VALUES (%s)"" % (
                            table_name,
                            "","".join(object_cols),
                            "","".join([""%s""] * len(object_cols)),
                        )
                    else:
                        stmt = (
                            (""UPDATE %s SET\n"" % table_name)
                            + ("",\n"".join([""  %s=%%s"" % c for c in object_cols]))
                            + (""\nWHERE %s = %d"" % (C_IMAGE_NUMBER, image_number))
                            + (""\nAND %s = %%s"" % object_number_column)
                        )

                    if self.db_type == DB_MYSQL:
                        # Write 25 rows at a time (to get under the max_allowed_packet limit)
                        for i in range(0, len(object_rows), 25):
                            my_rows = object_rows[i : min(i + 25, len(object_rows))]
                            self.cursor.executemany(stmt, my_rows)
                        if self.show_window and len(object_rows) > 0:
                            disp_columns.append(
                                (
                                    table_name,
                                    self.truncate_string_for_display(
                                        stmt % tuple(my_rows[0])
                                    ),
                                )
                            )
                    else:
                        for row in object_rows:
                            row = [""NULL"" if x is None else x for x in row]
                            row_stmt = stmt % tuple(row)
                            execute(self.cursor, row_stmt, return_result=False)
                        if self.show_window and len(object_rows) > 0:
                            disp_columns.append(
                                (table_name, self.truncate_string_for_display(row_stmt))
                            )

            image_table = self.get_table_name(""Image"")
            replacement = ""%s"" if self.db_type == DB_MYSQL else ""?""
            image_row_values = [
                None
                if field[0] is None
                else None
                if (
                    (field[1] == COLTYPE_FLOAT)
                    and (numpy.isnan(field[0]) or numpy.isinf(field[0]))
                )
                else float(field[0])
                if (field[1] == COLTYPE_FLOAT)
                else int(field[0])
                if (field[1] == ""integer"")
                else field[0].encode()
                if field[1] in (COLTYPE_BLOB, COLTYPE_LONGBLOB, COLTYPE_MEDIUMBLOB,)
                else field[0]
                for field in image_row
            ]
            if len(image_row) > 0:
                if not post_group:
                    stmt = ""INSERT INTO %s (%s) VALUES (%s)"" % (
                        image_table,
                        "","".join(
                            [mapping[colname] for val, dtype, colname in image_row]
                        ),
                        "","".join([replacement] * len(image_row)),
                    )
                else:
                    stmt = (
                        (""UPDATE %s SET\n"" % image_table)
                        + "",\n"".join(
                            [
                                ""  %s = %s"" % (mapping[colname], replacement)
                                for val, dtype, colname in image_row
                            ]
                        )
                        + (""\nWHERE %s = %d"" % (C_IMAGE_NUMBER, image_number))
                    )
                execute(self.cursor, stmt, image_row_values, return_result=False)

            if self.show_window:
                disp_columns.append(
                    (
                        image_table,
                        self.truncate_string_for_display(
                            stmt + "" VALUES(%s)"" % "","".join(map(str, image_row_values))
                        )
                        if len(image_row) > 0
                        else """",
                    )
                )

            if self.wants_relationship_table:
                #
                # Relationships table - for SQLite, check for previous existence
                # but for MySQL use REPLACE INTO to do the same
                #
                rtbl_name = self.get_table_name(T_RELATIONSHIPS)
                columns = [
                    COL_RELATIONSHIP_TYPE_ID,
                    COL_IMAGE_NUMBER1,
                    COL_OBJECT_NUMBER1,
                    COL_IMAGE_NUMBER2,
                    COL_OBJECT_NUMBER2,
                ]
                if self.db_type == DB_SQLITE:
                    stmt = ""INSERT INTO %s (%s, %s, %s, %s, %s) "" % tuple(
                        [rtbl_name] + columns
                    )
                    stmt += "" SELECT %d, %d, %d, %d, %d WHERE NOT EXISTS ""
                    stmt += ""(SELECT 'x' FROM %s WHERE "" % rtbl_name
                    stmt += "" AND "".join([""%s = %%d"" % col for col in columns]) + "")""
                else:
                    stmt = ""REPLACE INTO %s (%s, %s, %s, %s, %s) "" % tuple(
                        [rtbl_name] + columns
                    )
                    stmt += ""VALUES (%s, %s, %s, %s, %s)""
                for (
                    module_num,
                    relationship,
                    object_name1,
                    object_name2,
                    when,
                ) in pipeline.get_object_relationships():
                    if post_group != (when == MCA_AVAILABLE_POST_GROUP):
                        continue
                    r = measurements.get_relationships(
                        module_num,
                        relationship,
                        object_name1,
                        object_name2,
                        image_numbers=[image_number],
                    )
                    rt_id = self.get_relationship_type_id(
                        workspace, module_num, relationship, object_name1, object_name2
                    )
                    if self.db_type == DB_MYSQL:
                        # max_allowed_packet is 16 MB by default
                        # 8 x 10 = 80/row -> 200K rows
                        row_values = [(rt_id, i1, o1, i2, o2) for i1, o1, i2, o2 in r]
                        self.cursor.executemany(stmt, row_values)
                        if self.show_window and len(r) > 0:
                            disp_columns.append(
                                (
                                    rtbl_name,
                                    self.truncate_string_for_display(
                                        stmt % tuple(row_values[0])
                                    ),
                                )
                            )
                    else:
                        for i1, o1, i2, o2 in r:
                            row = (rt_id, i1, o1, i2, o2, rt_id, i1, o1, i2, o2)
                            row_stmt = stmt % tuple(row)
                            execute(self.cursor, row_stmt, return_result=False)
                        if self.show_window and len(r) > 0:
                            disp_columns.append(
                                (rtbl_name, self.truncate_string_for_display(row_stmt))
                            )

            if self.show_window:
                workspace.display_data.header = disp_header
                workspace.display_data.columns = disp_columns

            ###########################################
            #
            # The experiment table
            #
            ###########################################
            stmt = ""UPDATE %s SET %s='%s'"" % (
                self.get_table_name(EXPERIMENT),
                M_MODIFICATION_TIMESTAMP,
                datetime.datetime.now().isoformat(),
            )
            execute(self.cursor, stmt, return_result=False)

            self.connection.commit()
        except:
            logging.error(""Failed to write measurements to database"", exc_info=True)
            self.connection.rollback()
            raise
","    def write_data_to_db(self, workspace, post_group=False, image_number=None):
        """"""Write the data in the measurements out to the database
        workspace - contains the measurements
        mappings  - map a feature name to a column name
        image_number - image number for primary database key. Defaults to current.
        """"""
        if self.show_window:
            disp_header = [""Table"", ""Statement""]
            disp_columns = []
        try:
            zeros_for_nan = False
            measurements = workspace.measurements
            assert isinstance(measurements, Measurements)
            pipeline = workspace.pipeline
            image_set_list = workspace.image_set_list
            measurement_cols = self.get_pipeline_measurement_columns(
                pipeline, image_set_list
            )
            mapping = self.get_column_name_mappings(pipeline, image_set_list)

            ###########################################
            #
            # The image table
            #
            ###########################################
            if image_number is None:
                image_number = measurements.image_set_number

            image_row = []
            if not post_group:
                image_row += [(image_number, ""integer"", C_IMAGE_NUMBER,)]
            feature_names = set(measurements.get_feature_names(""Image""))
            for m_col in measurement_cols:
                if m_col[0] != ""Image"":
                    continue
                if not self.should_write(m_col, post_group):
                    continue
                #
                # Skip if feature name not in measurements. This
                # can happen if image set gets aborted or for some legacy
                # measurement files.
                #
                if m_col[1] not in feature_names:
                    continue
                feature_name = ""%s_%s"" % (""Image"", m_col[1])
                value = measurements.get_measurement(""Image"", m_col[1], image_number)
                if isinstance(value, numpy.ndarray):
                    value = value[0]
                if (
                    isinstance(value, float)
                    and not numpy.isfinite(value)
                    and zeros_for_nan
                ):
                    value = 0
                image_row.append((value, m_col[2], feature_name))
            #
            # Aggregates for the image table
            #
            agg_dict = measurements.compute_aggregate_measurements(
                image_number, self.agg_names
            )
            agg_columns = self.get_aggregate_columns(
                pipeline, image_set_list, post_group
            )
            image_row += [
                (agg_dict[agg[3]], COLTYPE_FLOAT, agg[3]) for agg in agg_columns
            ]

            #
            # Delete any prior data for this image
            #
            # Useful if you rerun a partially-complete batch
            #
            if not post_group:
                stmt = ""DELETE FROM %s WHERE %s=%d"" % (
                    self.get_table_name(""Image""),
                    C_IMAGE_NUMBER,
                    image_number,
                )
                execute(self.cursor, stmt, return_result=False)
                #
                # Delete relationships as well.
                #
                if self.wants_relationship_table:
                    for col in (COL_IMAGE_NUMBER1, COL_IMAGE_NUMBER2):
                        stmt = ""DELETE FROM %s WHERE %s=%d"" % (
                            self.get_table_name(T_RELATIONSHIPS),
                            col,
                            image_number,
                        )
                        execute(self.cursor, stmt, return_result=False)

            ########################################
            #
            # Object tables
            #
            ########################################
            object_names = self.get_object_names(pipeline, image_set_list)
            if len(object_names) > 0:
                if self.separate_object_tables == OT_COMBINE:
                    data = [(OBJECT, object_names)]
                else:
                    data = [
                        (object_name, [object_name]) for object_name in object_names
                    ]
                for table_object_name, object_list in data:
                    table_name = self.get_table_name(table_object_name)
                    columns = [
                        column
                        for column in measurement_cols
                        if column[0] in object_list
                        and self.should_write(column, post_group)
                    ]
                    if post_group and len(columns) == 0:
                        continue
                    max_count = 0
                    for object_name in object_list:
                        ftr_count = ""Count_%s"" % object_name
                        count = measurements.get_measurement(
                            ""Image"", ftr_count, image_number
                        )
                        max_count = max(max_count, int(count))
                    column_values = []
                    for column in columns:
                        object_name, feature, coltype = column[:3]
                        values = measurements.get_measurement(
                            object_name, feature, image_number
                        )

                        if len(values) < max_count:
                            values = list(values) + [None] * (max_count - len(values))
                        values = [
                            None
                            if v is None or numpy.isnan(v) or numpy.isinf(v)
                            else str(v)
                            for v in values
                        ]
                        column_values.append(values)
                    object_cols = []
                    if not post_group:
                        object_cols += [C_IMAGE_NUMBER]
                    if table_object_name == OBJECT:
                        object_number_column = C_OBJECT_NUMBER
                        if not post_group:
                            object_cols += [object_number_column]
                        object_numbers = numpy.arange(1, max_count + 1)
                    else:
                        object_number_column = ""_"".join(
                            (object_name, M_NUMBER_OBJECT_NUMBER)
                        )
                        object_numbers = measurements.get_measurement(
                            object_name, M_NUMBER_OBJECT_NUMBER, image_number
                        )

                    object_cols += [
                        mapping[""%s_%s"" % (column[0], column[1])] for column in columns
                    ]
                    object_rows = []
                    for j in range(max_count):
                        if not post_group:
                            object_row = [image_number]
                            if table_object_name == OBJECT:
                                # the object number
                                object_row.append(object_numbers[j])
                        else:
                            object_row = []

                        for column, values in zip(columns, column_values):
                            object_name, feature, coltype = column[:3]
                            object_row.append(values[j])
                        if post_group:
                            object_row.append(object_numbers[j])
                        object_rows.append(object_row)
                    #
                    # Delete any prior data for this image
                    #
                    if not post_group:
                        stmt = ""DELETE FROM %s WHERE %s=%d"" % (
                            table_name,
                            C_IMAGE_NUMBER,
                            image_number,
                        )

                        execute(self.cursor, stmt, return_result=False)
                        #
                        # Write the object table data
                        #
                        stmt = ""INSERT INTO %s (%s) VALUES (%s)"" % (
                            table_name,
                            "","".join(object_cols),
                            "","".join([""%s""] * len(object_cols)),
                        )
                    else:
                        stmt = (
                            (""UPDATE %s SET\n"" % table_name)
                            + ("",\n"".join([""  %s=%%s"" % c for c in object_cols]))
                            + (""\nWHERE %s = %d"" % (C_IMAGE_NUMBER, image_number))
                            + (""\nAND %s = %%s"" % object_number_column)
                        )

                    if self.db_type == DB_MYSQL:
                        # Write 25 rows at a time (to get under the max_allowed_packet limit)
                        for i in range(0, len(object_rows), 25):
                            my_rows = object_rows[i : min(i + 25, len(object_rows))]
                            self.cursor.executemany(stmt, my_rows)
                        if self.show_window and len(object_rows) > 0:
                            disp_columns.append(
                                (
                                    table_name,
                                    self.truncate_string_for_display(
                                        stmt % tuple(my_rows[0])
                                    ),
                                )
                            )
                    else:
                        for row in object_rows:
                            row = [""NULL"" if x is None else x for x in row]
                            row_stmt = stmt % tuple(row)
                            execute(self.cursor, row_stmt, return_result=False)
                        if self.show_window and len(object_rows) > 0:
                            disp_columns.append(
                                (table_name, self.truncate_string_for_display(row_stmt))
                            )

            image_table = self.get_table_name(""Image"")
            replacement = ""%s"" if self.db_type == DB_MYSQL else ""?""
            image_row_values = [
                None
                if field[0] is None
                else None
                if (
                    (field[1] == COLTYPE_FLOAT)
                    and (numpy.isnan(field[0]) or numpy.isinf(field[0]))
                )
                else float(field[0])
                if (field[1] == COLTYPE_FLOAT)
                else int(field[0])
                if (field[1] == ""integer"")
                else field[0]
                for field in image_row
            ]
            if len(image_row) > 0:
                if not post_group:
                    stmt = ""INSERT INTO %s (%s) VALUES (%s)"" % (
                        image_table,
                        "","".join(
                            [mapping[colname] for val, dtype, colname in image_row]
                        ),
                        "","".join([replacement] * len(image_row)),
                    )
                else:
                    stmt = (
                        (""UPDATE %s SET\n"" % image_table)
                        + "",\n"".join(
                            [
                                ""  %s = %s"" % (mapping[colname], replacement)
                                for val, dtype, colname in image_row
                            ]
                        )
                        + (""\nWHERE %s = %d"" % (C_IMAGE_NUMBER, image_number))
                    )
                execute(self.cursor, stmt, image_row_values, return_result=False)

            if self.show_window:
                disp_columns.append(
                    (
                        image_table,
                        self.truncate_string_for_display(
                            stmt + "" VALUES(%s)"" % "","".join(map(str, image_row_values))
                        )
                        if len(image_row) > 0
                        else """",
                    )
                )

            if self.wants_relationship_table:
                #
                # Relationships table - for SQLite, check for previous existence
                # but for MySQL use REPLACE INTO to do the same
                #
                rtbl_name = self.get_table_name(T_RELATIONSHIPS)
                columns = [
                    COL_RELATIONSHIP_TYPE_ID,
                    COL_IMAGE_NUMBER1,
                    COL_OBJECT_NUMBER1,
                    COL_IMAGE_NUMBER2,
                    COL_OBJECT_NUMBER2,
                ]
                if self.db_type == DB_SQLITE:
                    stmt = ""INSERT INTO %s (%s, %s, %s, %s, %s) "" % tuple(
                        [rtbl_name] + columns
                    )
                    stmt += "" SELECT %d, %d, %d, %d, %d WHERE NOT EXISTS ""
                    stmt += ""(SELECT 'x' FROM %s WHERE "" % rtbl_name
                    stmt += "" AND "".join([""%s = %%d"" % col for col in columns]) + "")""
                else:
                    stmt = ""REPLACE INTO %s (%s, %s, %s, %s, %s) "" % tuple(
                        [rtbl_name] + columns
                    )
                    stmt += ""VALUES (%s, %s, %s, %s, %s)""
                for (
                    module_num,
                    relationship,
                    object_name1,
                    object_name2,
                    when,
                ) in pipeline.get_object_relationships():
                    if post_group != (when == MCA_AVAILABLE_POST_GROUP):
                        continue
                    r = measurements.get_relationships(
                        module_num,
                        relationship,
                        object_name1,
                        object_name2,
                        image_numbers=[image_number],
                    )
                    rt_id = self.get_relationship_type_id(
                        workspace, module_num, relationship, object_name1, object_name2
                    )
                    if self.db_type == DB_MYSQL:
                        # max_allowed_packet is 16 MB by default
                        # 8 x 10 = 80/row -> 200K rows
                        row_values = [(rt_id, i1, o1, i2, o2) for i1, o1, i2, o2 in r]
                        self.cursor.executemany(stmt, row_values)
                        if self.show_window and len(r) > 0:
                            disp_columns.append(
                                (
                                    rtbl_name,
                                    self.truncate_string_for_display(
                                        stmt % tuple(row_values[0])
                                    ),
                                )
                            )
                    else:
                        for i1, o1, i2, o2 in r:
                            row = (rt_id, i1, o1, i2, o2, rt_id, i1, o1, i2, o2)
                            row_stmt = stmt % tuple(row)
                            execute(self.cursor, row_stmt, return_result=False)
                        if self.show_window and len(r) > 0:
                            disp_columns.append(
                                (rtbl_name, self.truncate_string_for_display(row_stmt))
                            )

            if self.show_window:
                workspace.display_data.header = disp_header
                workspace.display_data.columns = disp_columns

            ###########################################
            #
            # The experiment table
            #
            ###########################################
            stmt = ""UPDATE %s SET %s='%s'"" % (
                self.get_table_name(EXPERIMENT),
                M_MODIFICATION_TIMESTAMP,
                datetime.datetime.now().isoformat(),
            )
            execute(self.cursor, stmt, return_result=False)

            self.connection.commit()
        except:
            logging.error(""Failed to write measurements to database"", exc_info=True)
            self.connection.rollback()
            raise
",https://github.com/CellProfiler/CellProfiler/issues/4254,CWE-1070: Serializable Data Element Containing non-Serializable Item Elements,The write_data_to_db serializes object to json. Sometimes object contains another object of type bytes which is not JSON-serializable. This causes the TypeError.,cellprofiler/modules/exporttodatabase.py,ExportToDatabase.write_data_to_db,239,"Worker 1: Traceback (most recent call last):
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/pipeline/_pipeline.py"", line 1142, in run_image_set
Worker 1:     self.run_module(module, workspace)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/pipeline/_pipeline.py"", line 1275, in run_module
Worker 1:     module.run(workspace)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/CellProfiler/cellprofiler/modules/exporttodatabase.py"", line 2556, in run
Worker 1:     workspace.interaction_request(
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/workspace/_workspace.py"", line 263, in interaction_request
Worker 1:     return self.interaction_handler(module, *args, **kwargs)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/worker/_worker.py"", line 356, in interaction_handler
Worker 1:     rep = self.send(req)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/worker/_worker.py"", line 410, in send
Worker 1:     req.send_only(work_socket)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/utilities/zmq/communicable/request/_request.py"", line 38, in send_only
Worker 1:     Communicable.send(self, socket)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/utilities/zmq/communicable/_communicable.py"", line 18, in send
Worker 1:     json_str, buffers = cellprofiler_core.utilities.zmq.json_encode(self.__dict__)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/utilities/zmq/__init__.py"", line 174, in json_encode
Worker 1:     json_str = json.dumps(sendable_dict, default=encoder)
Worker 1:   File ""/usr/local/Cellar/python@3.8/3.8.2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/__init__.py"", line 234, in dumps
Worker 1:     return cls(
Worker 1:   File ""/usr/local/Cellar/python@3.8/3.8.2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py"", line 199, in encode
Worker 1:     chunks = self.iterencode(o, _one_shot=True)
Worker 1:   File ""/usr/local/Cellar/python@3.8/3.8.2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py"", line 257, in iterencode
Worker 1:     return _iterencode(o, 0)
Worker 1:   File ""/Users/bcimini/Documents/GitHub/CellProfiler/core/cellprofiler_core/utilities/zmq/__init__.py"", line 70, in encoder
Worker 1:     raise TypeError(""%r of type %r is not JSON serializable"" % (data, type(data)))
Worker 1: TypeError: b'iVBORw0KGgoAAAANSUhEUgA[...]I=' of type <class 'bytes'> is not JSON serializable",TypeError,buggy_snippets_files/edb855ef2e6629e8f8e113bd76dce40dd35362a45bb2b90927f02acf00a18633_before_merge.py,buggy_snippets_files/edb855ef2e6629e8f8e113bd76dce40dd35362a45bb2b90927f02acf00a18633_after_merge.py
168751,"def get_available_translations():
    """"""
    List available translations for spyder based on the folders found in the
    locale folder. This function checks if LANGUAGE_CODES contain the same
    information that is found in the 'locale' folder to ensure that when a new
    language is added, LANGUAGE_CODES is updated.
    """"""
    locale_path = get_module_data_path(""spyder"", relpath=""locale"",
                                       attr_name='LOCALEPATH')
    listdir = os.listdir(locale_path)
    langs = [d for d in listdir if osp.isdir(osp.join(locale_path, d))]
    langs = [DEFAULT_LANGUAGE] + langs

    # Remove disabled languages
    langs = list( set(langs) - set(DISABLED_LANGUAGES) )

    # Check that there is a language code available in case a new translation
    # is added, to ensure LANGUAGE_CODES is updated.
    for lang in langs:
        if lang not in LANGUAGE_CODES:
            error = _('Update LANGUAGE_CODES (inside config/base.py) if a new '
                      'translation has been added to Spyder')
            raise Exception(error)
    return langs
","def get_available_translations():
    """"""
    List available translations for spyder based on the folders found in the
    locale folder. This function checks if LANGUAGE_CODES contain the same
    information that is found in the 'locale' folder to ensure that when a new
    language is added, LANGUAGE_CODES is updated.
    """"""
    locale_path = get_module_data_path(""spyder"", relpath=""locale"",
                                       attr_name='LOCALEPATH')
    listdir = os.listdir(locale_path)
    langs = [d for d in listdir if osp.isdir(osp.join(locale_path, d))]
    langs = [DEFAULT_LANGUAGE] + langs

    # Remove disabled languages
    langs = list( set(langs) - set(DISABLED_LANGUAGES) )

    # Check that there is a language code available in case a new translation
    # is added, to ensure LANGUAGE_CODES is updated.
    for lang in langs:
        if lang not in LANGUAGE_CODES:
            error = ('Update LANGUAGE_CODES (inside config/base.py) if a new '
                     'translation has been added to Spyder')
            print(error)  # spyder: test-skip
            return ['en']
    return langs
",https://github.com/spyder-ide/spyder/issues/7489,CWE-457: Use of Uninitialized Variable,"The get_available_translations tries to call the ""_"" function which does not exist at the time of the call.",spyder/config/base.py,get_available_translations,21,"Traceback (most recent call last):
File ""/home/yang/anaconda3/bin/spyder"", line 7, in <module>
from spyder.app.start import main
File ""/home/yang/anaconda3/lib/python3.6/site-packages/spyder/app/start.py"", line 37, in <module>
from spyder.config.base import (get_conf_path, running_in_mac_app,
File ""/home/yang/anaconda3/lib/python3.6/site-packages/spyder/config/base.py"", line 415, in <module>
_ = get_translation(""spyder"")
File ""/home/yang/anaconda3/lib/python3.6/site-packages/spyder/config/base.py"", line 386, in get_translation
language = load_lang_conf()
File ""/home/yang/anaconda3/lib/python3.6/site-packages/spyder/config/base.py"", line 359, in load_lang_conf
lang = get_interface_language()
File ""/home/yang/anaconda3/lib/python3.6/site-packages/spyder/config/base.py"", line 330, in get_interface_language
spyder_languages = get_available_translations()
File ""/home/yang/anaconda3/lib/python3.6/site-packages/spyder/config/base.py"", line 297, in get_available_translations
error = _('Update LANGUAGE_CODES (inside config/base.py) if a new '
NameError: name '_' is not defined",NameError,buggy_snippets_files/f180abba124d6fec96afcee3d9db7d9bd1bcdc4ef48582925607177ab53e0168_before_merge.py,buggy_snippets_files/f180abba124d6fec96afcee3d9db7d9bd1bcdc4ef48582925607177ab53e0168_after_merge.py
200197,"    def render_config(self, spec, conf, vlan_info):
        """"""
        Render config as dictionary structure and delete keys
          from spec for null values

        :param spec: The facts tree, generated from the argspec
        :param conf: The configuration
        :rtype: dictionary
        :returns: The generated config
        """"""
        config = deepcopy(spec)

        if vlan_info == 'Name' and 'Name' not in conf:
            conf = filter(None, conf.split(' '))
            config['vlan_id'] = int(conf[0])
            config['name'] = conf[1]
            if len(conf[2].split('/')) > 1:
                if conf[2].split('/')[0] == 'sus':
                    config['state'] = 'suspend'
                elif conf[2].split('/')[0] == 'act':
                    config['state'] = 'active'
                config['shutdown'] = 'enabled'
            else:
                if conf[2] == 'suspended':
                    config['state'] = 'suspend'
                elif conf[2] == 'active':
                    config['state'] = 'active'
                config['shutdown'] = 'disabled'
        elif vlan_info == 'Type' and 'Type' not in conf:
            conf = filter(None, conf.split(' '))
            config['mtu'] = int(conf[3])
        elif vlan_info == 'Remote':
            if len(conf.split(',')) > 1 or conf.isdigit():
                remote_span_vlan = []
                if len(conf.split(',')) > 1:
                    remote_span_vlan = conf.split(',')
                else:
                    remote_span_vlan.append(conf)
                remote_span = []
                for each in remote_span_vlan:
                    remote_span.append(int(each))
                config['remote_span'] = remote_span

        return utils.remove_empties(config)
","    def render_config(self, spec, conf, vlan_info):
        """"""
        Render config as dictionary structure and delete keys
          from spec for null values

        :param spec: The facts tree, generated from the argspec
        :param conf: The configuration
        :rtype: dictionary
        :returns: The generated config
        """"""
        config = deepcopy(spec)

        if vlan_info == 'Name' and 'Name' not in conf:
            conf = list(filter(None, conf.split(' ')))
            config['vlan_id'] = int(conf[0])
            config['name'] = conf[1]
            if len(conf[2].split('/')) > 1:
                if conf[2].split('/')[0] == 'sus':
                    config['state'] = 'suspend'
                elif conf[2].split('/')[0] == 'act':
                    config['state'] = 'active'
                config['shutdown'] = 'enabled'
            else:
                if conf[2] == 'suspended':
                    config['state'] = 'suspend'
                elif conf[2] == 'active':
                    config['state'] = 'active'
                config['shutdown'] = 'disabled'
        elif vlan_info == 'Type' and 'Type' not in conf:
            conf = list(filter(None, conf.split(' ')))
            config['mtu'] = int(conf[3])
        elif vlan_info == 'Remote':
            if len(conf.split(',')) > 1 or conf.isdigit():
                remote_span_vlan = []
                if len(conf.split(',')) > 1:
                    remote_span_vlan = conf.split(',')
                else:
                    remote_span_vlan.append(conf)
                remote_span = []
                for each in remote_span_vlan:
                    remote_span.append(int(each))
                config['remote_span'] = remote_span

        return utils.remove_empties(config)
",https://github.com/ansible/ansible/issues/64515,CWE-229: Improper Handling of Values,filter() returned list in Python 2. It returns iterator object in Python 3.,lib/ansible/module_utils/network/ios/facts/vlans/vlans.py,VlansFacts.render_config,"[14], [30]","$ ansible-playbook ios_vlans.yml

PLAY [sccs-2ug-07] ******************************************************************************************************

TASK [Gathering Facts] **************************************************************************************************
[WARNING]: Ignoring timeout(10) for ios_facts

ok: [sccs-2ug-07]

TASK [Delete all VLANs] *************************************************************************************************
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: TypeError: 'filter' object is not subscriptable
fatal: [sccs-2ug-07]: FAILED! => {""changed"": false, ""module_stderr"": ""Traceback (most recent call last):\n  File \""/home/misch/.ansible/tmp/ansible-local-112400ljdojbw/ansible-tmp-1573048754.5352461-196207205443014/AnsiballZ_ios_vlans.py\"", line 102, in <module>\n    _ansiballz_main()\n  File \""/home/misch/.ansible/tmp/ansible-local-112400ljdojbw/ansible-tmp-1573048754.5352461-196207205443014/AnsiballZ_ios_vlans.py\"", line 94, in _ansiballz_main\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\n  File \""/home/misch/.ansible/tmp/ansible-local-112400ljdojbw/ansible-tmp-1573048754.5352461-196207205443014/AnsiballZ_ios_vlans.py\"", line 40, in invoke_module\n    runpy.run_module(mod_name='ansible.modules.network.ios.ios_vlans', init_globals=None, run_name='__main__', alter_sys=False)\n  File \""/usr/lib64/python3.6/runpy.py\"", line 208, in run_module\n    return _run_code(code, {}, init_globals, run_name, mod_spec)\n  File \""/usr/lib64/python3.6/runpy.py\"", line 85, in _run_code\n    exec(code, run_globals)\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/modules/network/ios/ios_vlans.py\"", line 464, in <module>\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/modules/network/ios/ios_vlans.py\"", line 459, in main\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/module_utils/network/ios/config/vlans/vlans.py\"", line 63, in execute_module\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/module_utils/network/ios/config/vlans/vlans.py\"", line 47, in get_interfaces_facts\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/module_utils/network/ios/facts/facts.py\"", line 68, in get_facts\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/module_utils/network/common/facts/facts.py\"", line 105, in get_network_resources_facts\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/module_utils/network/ios/facts/vlans/vlans.py\"", line 69, in populate_facts\n  File \""/tmp/ansible_ios_vlans_payload_8jji30h2/ansible_ios_vlans_payload.zip/ansible/module_utils/network/ios/facts/vlans/vlans.py\"", line 115, in render_config\nTypeError: 'filter' object is not subscriptable\n"", ""module_stdout"": """", ""msg"": ""MODULE FAILURE\nSee stdout/stderr for the exact error"", ""rc"": 1}

PLAY RECAP **************************************************************************************************************
sccs-2ug-07                : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0",TypeError,buggy_snippets_files/547c4ed37e9feb2c874d7b59752b5fbf90a19a8cb1261bd7c985da924f5517b6_before_merge.py,buggy_snippets_files/547c4ed37e9feb2c874d7b59752b5fbf90a19a8cb1261bd7c985da924f5517b6_after_merge.py
26967,"def check_handle_timedout(timeoutvalue: int) -> None:
    """"""
    Check if any orders are timed out and cancel if neccessary
    :param timeoutvalue: Number of minutes until order is considered timed out
    :return: None
    """"""
    timeoutthreashold = arrow.utcnow().shift(minutes=-timeoutvalue).datetime

    for trade in Trade.query.filter(Trade.open_order_id.isnot(None)).all():
        order = exchange.get_order(trade.open_order_id)
        ordertime = arrow.get(order['opened'])

        if order['type'] == ""LIMIT_BUY"" and ordertime < timeoutthreashold:
            # Buy timeout - cancel order
            exchange.cancel_order(trade.open_order_id)
            if order['remaining'] == order['amount']:
                # if trade is not partially completed, just delete the trade
                Trade.session.delete(trade)
                Trade.session.flush()
                logger.info('Buy order timeout for %s.', trade)
            else:
                # if trade is partially complete, edit the stake details for the trade
                # and close the order
                trade.amount = order['amount'] - order['remaining']
                trade.stake_amount = trade.amount * trade.open_rate
                trade.open_order_id = None
                logger.info('Partial buy order timeout for %s.', trade)
        elif order['type'] == ""LIMIT_SELL"" and ordertime < timeoutthreashold:
            # Sell timeout - cancel order and update trade
            if order['remaining'] == order['amount']:
                # if trade is not partially completed, just cancel the trade
                exchange.cancel_order(trade.open_order_id)
                trade.close_rate = None
                trade.close_profit = None
                trade.close_date = None
                trade.is_open = True
                trade.open_order_id = None
                logger.info('Sell order timeout for %s.', trade)
                return True
            else:
                # TODO: figure out how to handle partially complete sell orders
                pass
","def check_handle_timedout(timeoutvalue: int) -> None:
    """"""
    Check if any orders are timed out and cancel if neccessary
    :param timeoutvalue: Number of minutes until order is considered timed out
    :return: None
    """"""
    timeoutthreashold = arrow.utcnow().shift(minutes=-timeoutvalue).datetime

    for trade in Trade.query.filter(Trade.open_order_id.isnot(None)).all():
        try:
            order = exchange.get_order(trade.open_order_id)
        except requests.exceptions.RequestException:
            logger.info('Cannot query order for %s due to %s', trade, traceback.format_exc())
            continue
        ordertime = arrow.get(order['opened'])

        # Check if trade is still actually open
        if int(order['remaining']) == 0:
            continue

        if order['type'] == ""LIMIT_BUY"" and ordertime < timeoutthreashold:
            # Buy timeout - cancel order
            exchange.cancel_order(trade.open_order_id)
            if order['remaining'] == order['amount']:
                # if trade is not partially completed, just delete the trade
                Trade.session.delete(trade)
                Trade.session.flush()
                logger.info('Buy order timeout for %s.', trade)
                rpc.send_msg('*Timeout:* Unfilled buy order for {} cancelled'.format(
                             trade.pair.replace('_', '/')))
            else:
                # if trade is partially complete, edit the stake details for the trade
                # and close the order
                trade.amount = order['amount'] - order['remaining']
                trade.stake_amount = trade.amount * trade.open_rate
                trade.open_order_id = None
                logger.info('Partial buy order timeout for %s.', trade)
                rpc.send_msg('*Timeout:* Remaining buy order for {} cancelled'.format(
                             trade.pair.replace('_', '/')))
        elif order['type'] == ""LIMIT_SELL"" and ordertime < timeoutthreashold:
            # Sell timeout - cancel order and update trade
            if order['remaining'] == order['amount']:
                # if trade is not partially completed, just cancel the trade
                exchange.cancel_order(trade.open_order_id)
                trade.close_rate = None
                trade.close_profit = None
                trade.close_date = None
                trade.is_open = True
                trade.open_order_id = None
                rpc.send_msg('*Timeout:* Unfilled sell order for {} cancelled'.format(
                             trade.pair.replace('_', '/')))
                logger.info('Sell order timeout for %s.', trade)
                return True
            else:
                # TODO: figure out how to handle partially complete sell orders
                pass
",https://github.com/freqtrade/freqtrade/issues/385,CWE-248: Uncaught Exception,exchange.get_order() may throw an unhandled exception (if order is cancelled externally) ,freqtrade/main.py,check_handle_timedout,[10],"Traceback (most recent call last):
File ""/home/freqtrade/freqtrade/freqtrade/main.py"", line 104, in _process
check_handle_timedout(_CONF['unfilledtimeout'])
File ""/home/freqtrade/freqtrade/freqtrade/main.py"", line 154, in check_handle_timedout
exchange.cancel_order(trade.open_order_id)
File ""/home/freqtrade/freqtrade/freqtrade/exchange/__init__.py"", line 150, in cancel_order
return _API.cancel_order(order_id)
File ""/home/freqtrade/freqtrade/freqtrade/exchange/bittrex.py"", line 197, in cancel_order
order_id=order_id))
freqtrade.OperationalException: ORDER_NOT_OPEN params=(03b02exx-adxx-4xx9-xxfc-xx258XXX26xx)",freqtrade.OperationalException,buggy_snippets_files/a8f82ec91d2617fa23abc0c9968ce08112268181af0de60c71310f79c8b4bd3c_before_merge.py,buggy_snippets_files/a8f82ec91d2617fa23abc0c9968ce08112268181af0de60c71310f79c8b4bd3c_after_merge.py
136584,"    def render_GET(self, request):
        """"""
        .. http:get:: /torrents/(string: torrent infohash)/health

        Fetch the swarm health of a specific torrent. You can optionally specify the timeout to be used in the
        connections to the trackers. This is by default 20 seconds.
        By default, we will not check the health of a torrent again if it was recently checked. You can force a health
        recheck by passing the refresh parameter.

            **Example request**:

            .. sourcecode:: none

                curl http://localhost:8085/torrents/97d2d8f5d37e56cfaeaae151d55f05b077074779/health?timeout=15&refresh=1

            **Example response**:

            .. sourcecode:: javascript

                {
                    ""http://mytracker.com:80/announce"": [{
                        ""seeders"": 43,
                        ""leechers"": 20,
                        ""infohash"": ""97d2d8f5d37e56cfaeaae151d55f05b077074779""
                    }],
                    ""http://nonexistingtracker.com:80/announce"": {
                        ""error"": ""timeout""
                    }
                }

            :statuscode 404: if the torrent is not found in the database
        """"""
        timeout = 20
        if 'timeout' in request.args:
            timeout = int(request.args['timeout'][0])

        refresh = False
        if 'refresh' in request.args and len(request.args['refresh']) > 0 and request.args['refresh'][0] == ""1"":
            refresh = True

        torrent_db_columns = ['C.torrent_id', 'num_seeders', 'num_leechers', 'next_tracker_check']
        torrent_info = self.torrent_db.getTorrent(self.infohash.decode('hex'), torrent_db_columns)

        if torrent_info is None:
            request.setResponseCode(http.NOT_FOUND)
            return json.dumps({""error"": ""torrent not found in database""})

        def on_health_result(result):
            request.write(json.dumps({'health': result}))
            self.finish_request(request)

        def on_request_error(failure):
            request.setResponseCode(http.BAD_REQUEST)
            request.write(json.dumps({""error"": failure.getErrorMessage()}))
            self.finish_request(request)

        self.session.check_torrent_health(self.infohash.decode('hex'), timeout=timeout, scrape_now=refresh)\
            .addCallback(on_health_result).addErrback(on_request_error)

        return NOT_DONE_YET
","    def render_GET(self, request):
        """"""
        .. http:get:: /torrents/(string: torrent infohash)/health

        Fetch the swarm health of a specific torrent. You can optionally specify the timeout to be used in the
        connections to the trackers. This is by default 20 seconds.
        By default, we will not check the health of a torrent again if it was recently checked. You can force a health
        recheck by passing the refresh parameter.

            **Example request**:

            .. sourcecode:: none

                curl http://localhost:8085/torrents/97d2d8f5d37e56cfaeaae151d55f05b077074779/health?timeout=15&refresh=1

            **Example response**:

            .. sourcecode:: javascript

                {
                    ""http://mytracker.com:80/announce"": [{
                        ""seeders"": 43,
                        ""leechers"": 20,
                        ""infohash"": ""97d2d8f5d37e56cfaeaae151d55f05b077074779""
                    }],
                    ""http://nonexistingtracker.com:80/announce"": {
                        ""error"": ""timeout""
                    }
                }

            :statuscode 404: if the torrent is not found in the database
        """"""
        timeout = 20
        if 'timeout' in request.args:
            timeout = int(request.args['timeout'][0])

        refresh = False
        if 'refresh' in request.args and len(request.args['refresh']) > 0 and request.args['refresh'][0] == ""1"":
            refresh = True

        torrent_db_columns = ['C.torrent_id', 'num_seeders', 'num_leechers', 'next_tracker_check']
        torrent_info = self.torrent_db.getTorrent(self.infohash.decode('hex'), torrent_db_columns)

        if torrent_info is None:
            request.setResponseCode(http.NOT_FOUND)
            return json.dumps({""error"": ""torrent not found in database""})

        def on_health_result(result):
            request.write(json.dumps({'health': result}))
            self.finish_request(request)

        def on_request_error(failure):
            if not request.finished:
                request.setResponseCode(http.BAD_REQUEST)
                request.write(json.dumps({""error"": failure.getErrorMessage()}))
            # If the above request.write failed, the request will have already been finished
            if not request.finished:
                self.finish_request(request)

        self.session.check_torrent_health(self.infohash.decode('hex'), timeout=timeout, scrape_now=refresh)\
            .addCallback(on_health_result).addErrback(on_request_error)

        return NOT_DONE_YET
",https://github.com/Tribler/tribler/issues/3203,CWE-666: Operation on Resource in Wrong Phase of Lifetime,request may already be finished before attempting to work with it,Tribler/Core/Modules/restapi/torrents_endpoint.py,TorrentHealthEndpoint.render_GET,[53],"Traceback (most recent call last):
File ""/usr/share/tribler/TriblerGUI/event_request_manager.py"", line 99, in on_read_data
raise RuntimeError(json_dict[""event""][""text""])
RuntimeError: [Failure instance: Traceback: <type 'exceptions.AttributeError'>: 'NoneType' object has no attribute 'writeSequence'
/usr/lib/python2.7/site-packages/twisted/internet/defer.py:653:_runCallbacks
/usr/lib/python2.7/site-packages/twisted/internet/defer.py:1117:_cbDeferred
/usr/lib/python2.7/site-packages/twisted/internet/defer.py:459:callback
/usr/lib/python2.7/site-packages/twisted/internet/defer.py:567:_startRunCallbacks
--- <exception caught here> ---
/usr/lib/python2.7/site-packages/twisted/internet/defer.py:653:_runCallbacks
/usr/share/tribler/Tribler/Core/Modules/restapi/torrents_endpoint.py:218:on_request_error
/usr/lib/python2.7/site-packages/twisted/web/server.py:234:write
/usr/lib/python2.7/site-packages/twisted/web/http.py:1084:write
]",RuntimeError,buggy_snippets_files/42c5891994f3ac217859f630eb029961ecbd52d5e7d4eb27c663d851bf39fb41_before_merge.py,buggy_snippets_files/42c5891994f3ac217859f630eb029961ecbd52d5e7d4eb27c663d851bf39fb41_after_merge.py
252227,"    def transform(self, y):
        """"""Transform labels to normalized encoding.

        Parameters
        ----------
        y : array-like of shape [n_samples]
            Target values.

        Returns
        -------
        y : array-like of shape [n_samples]
        """"""
        check_is_fitted(self, 'classes_')
        y = column_or_1d(y, warn=True)

        classes = np.unique(y)
        if len(np.intersect1d(classes, self.classes_)) < len(classes):
            diff = np.setdiff1d(classes, self.classes_)
            raise ValueError(
                    ""y contains previously unseen labels: %s"" % str(diff))
        return np.searchsorted(self.classes_, y)
","    def transform(self, y):
        """"""Transform labels to normalized encoding.

        Parameters
        ----------
        y : array-like of shape [n_samples]
            Target values.

        Returns
        -------
        y : array-like of shape [n_samples]
        """"""
        check_is_fitted(self, 'classes_')
        y = column_or_1d(y, warn=True)
        # transform of empty array is empty array
        if _num_samples(y) == 0:
            return np.array([])

        classes = np.unique(y)
        if len(np.intersect1d(classes, self.classes_)) < len(classes):
            diff = np.setdiff1d(classes, self.classes_)
            raise ValueError(
                    ""y contains previously unseen labels: %s"" % str(diff))
        return np.searchsorted(self.classes_, y)
",https://github.com/scikit-learn/scikit-learn/issues/10458,CWE-703: Improper Check or Handling of Exceptional Conditions,Unprocessed case if y is an empty list.,sklearn/preprocessing/label.py,LabelEncoder.transform,[21],"from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit([1,2])
LabelEncoder()
le.transform([])
array([], dtype=int64)
le.fit([""a"",""b""])
LabelEncoder()
le.transform([])
Traceback (most recent call last):
File ""[...]\Python36\lib\site-packages\numpy\core\fromnumeric.py"", line 57, in _wrapfunc
return getattr(obj, method)(*args, **kwds)
TypeError: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""[...]\Python36\lib\site-packages\sklearn\preprocessing\label.py"", line 134, in transform
return np.searchsorted(self.classes_, y)
File ""[...]\Python36\lib\site-packages\numpy\core\fromnumeric.py"", line 1075, in searchsorted
return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter)
File ""[...]\Python36\lib\site-packages\numpy\core\fromnumeric.py"", line 67, in _wrapfunc
return _wrapit(obj, method, *args, **kwds)
File ""[...]\Python36\lib\site-packages\numpy\core\fromnumeric.py"", line 47, in _wrapit
result = getattr(asarray(obj), method)(*args, **kwds)
TypeError: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'",TypeError,buggy_snippets_files/6bb10de6dd2bceb3f5393d739b5c275895d4d81728f4c46ccb856a1181afca16_before_merge.py,buggy_snippets_files/6bb10de6dd2bceb3f5393d739b5c275895d4d81728f4c46ccb856a1181afca16_after_merge.py
309273,"def solve_collocation_system(fun, t, y, h, Z0, scale, tol,
                             LU_real, LU_complex, solve_lu):
    """"""Solve the collocation system.

    Parameters
    ----------
    fun : callable
        Right-hand side of the system.
    t : float
        Current time.
    y : ndarray, shape (n,)
        Current state.
    h : float
        Step to try.
    Z0 : ndarray, shape (3, n)
        Initial guess for the solution. It determines new values of `y` at
        ``t + h * C`` as ``y + Z0``, where ``C`` is the Radau method constants.
    scale : float
        Problem tolerance scale, i.e. ``rtol * abs(y) + atol``.
    tol : float
        Tolerance to which solve the system. This value is compared with
        the normalized by `scale` error.
    LU_real, LU_complex
        LU decompositions of the system Jacobians.
    solve_lu : callable
        Callable which solves a linear system given a LU decomposition. The
        signature is ``solve_lu(LU, b)``.

    Returns
    -------
    converged : bool
        Whether iterations converged.
    n_iter : int
        Number of completed iterations.
    Z : ndarray, shape (3, n)
        Found solution.
    rate : float
        The rate of convergence.
    """"""
    n = y.shape[0]
    M_real = MU_REAL / h
    M_complex = MU_COMPLEX / h

    W = TI.dot(Z0)
    Z = Z0

    F = np.empty((3, n))
    ch = h * C

    dW_norm_old = None
    dW = np.empty_like(W)
    converged = False
    for k in range(NEWTON_MAXITER):
        for i in range(3):
            F[i] = fun(t + ch[i], y + Z[i])

        if not np.all(np.isfinite(F)):
            break

        f_real = F.T.dot(TI_REAL) - M_real * W[0]
        f_complex = F.T.dot(TI_COMPLEX) - M_complex * (W[1] + 1j * W[2])

        dW_real = solve_lu(LU_real, f_real)
        dW_complex = solve_lu(LU_complex, f_complex)

        dW[0] = dW_real
        dW[1] = dW_complex.real
        dW[2] = dW_complex.imag

        dW_norm = norm(dW / scale)
        if dW_norm_old is not None:
            rate = dW_norm / dW_norm_old
        else:
            rate = None

        if (rate is not None and (rate >= 1 or
                rate ** (NEWTON_MAXITER - k) / (1 - rate) * dW_norm > tol)):
            break

        W += dW
        Z = T.dot(W)

        if (dW_norm == 0 or
                rate is not None and rate / (1 - rate) * dW_norm < tol):
            converged = True
            break

        dW_norm_old = dW_norm

    return converged, k + 1, Z, rate
","def solve_collocation_system(fun, t, y, h, Z0, scale, tol,
                             LU_real, LU_complex, solve_lu):
    """"""Solve the collocation system.

    Parameters
    ----------
    fun : callable
        Right-hand side of the system.
    t : float
        Current time.
    y : ndarray, shape (n,)
        Current state.
    h : float
        Step to try.
    Z0 : ndarray, shape (3, n)
        Initial guess for the solution. It determines new values of `y` at
        ``t + h * C`` as ``y + Z0``, where ``C`` is the Radau method constants.
    scale : float
        Problem tolerance scale, i.e. ``rtol * abs(y) + atol``.
    tol : float
        Tolerance to which solve the system. This value is compared with
        the normalized by `scale` error.
    LU_real, LU_complex
        LU decompositions of the system Jacobians.
    solve_lu : callable
        Callable which solves a linear system given a LU decomposition. The
        signature is ``solve_lu(LU, b)``.

    Returns
    -------
    converged : bool
        Whether iterations converged.
    n_iter : int
        Number of completed iterations.
    Z : ndarray, shape (3, n)
        Found solution.
    rate : float
        The rate of convergence.
    """"""
    n = y.shape[0]
    M_real = MU_REAL / h
    M_complex = MU_COMPLEX / h

    W = TI.dot(Z0)
    Z = Z0

    F = np.empty((3, n))
    ch = h * C

    dW_norm_old = None
    dW = np.empty_like(W)
    converged = False
    rate = None
    for k in range(NEWTON_MAXITER):
        for i in range(3):
            F[i] = fun(t + ch[i], y + Z[i])

        if not np.all(np.isfinite(F)):
            break

        f_real = F.T.dot(TI_REAL) - M_real * W[0]
        f_complex = F.T.dot(TI_COMPLEX) - M_complex * (W[1] + 1j * W[2])

        dW_real = solve_lu(LU_real, f_real)
        dW_complex = solve_lu(LU_complex, f_complex)

        dW[0] = dW_real
        dW[1] = dW_complex.real
        dW[2] = dW_complex.imag

        dW_norm = norm(dW / scale)
        if dW_norm_old is not None:
            rate = dW_norm / dW_norm_old

        if (rate is not None and (rate >= 1 or
                rate ** (NEWTON_MAXITER - k) / (1 - rate) * dW_norm > tol)):
            break

        W += dW
        Z = T.dot(W)

        if (dW_norm == 0 or
                rate is not None and rate / (1 - rate) * dW_norm < tol):
            converged = True
            break

        dW_norm_old = dW_norm

    return converged, k + 1, Z, rate
",https://github.com/scipy/scipy/issues/10775,CWE-232: Improper Handling of Undefined Values,Function returns variable 'rate' which is unassigned in some control paths,scipy/integrate/_ivp/radau.py,solve_collocation_system,[90],"Traceback (most recent call last):
File ""/Users/gabriel/radau_issue.py"", line 8, in <module>
radau.step()  # UnboundLocalError
File ""/Users/gabriel/Library/Python/3.7/lib/python/site-packages/scipy/integrate/_ivp/base.py"", line 182, in step
success, message = self._step_impl()
File ""/Users/gabriel/Library/Python/3.7/lib/python/site-packages/scipy/integrate/_ivp/radau.py"", line 451, in _step_impl
LU_real, LU_complex, self.solve_lu)
File ""/Users/gabriel/Library/Python/3.7/lib/python/site-packages/scipy/integrate/_ivp/radau.py"", line 138, in solve_collocation_system
return converged, k + 1, Z, rate
UnboundLocalError: local variable 'rate' referenced before assignment",UnboundLocalError,buggy_snippets_files/a0f28a5969b1baa15cb02250a1738e9a8f712de261bc6734c144f1c82a9d2c13_before_merge.py,buggy_snippets_files/a0f28a5969b1baa15cb02250a1738e9a8f712de261bc6734c144f1c82a9d2c13_after_merge.py
308218,"def parse_txt_dict(data, msg):
    """"""Parse DNS TXT record containing a dict.""""""
    output = {}
    txt, _ = qname_decode(data, msg, raw=True)
    for prop in txt:
        key, value = prop.split(b'=')
        output[key] = value
    return output
","def parse_txt_dict(data, msg):
    """"""Parse DNS TXT record containing a dict.""""""
    output = {}
    txt, _ = qname_decode(data, msg, raw=True)
    for prop in txt:
        key, value = prop.split(b'=', 1)
        output[key] = value
    return output
",https://github.com/postlund/pyatv/issues/405,CWE-159: Improper Handling of Invalid Use of Special Elements,String 'prop' contains data in the form 'key=value'. The case of value containing the symbol '=' is not properly handled.,pyatv/udns.py,parse_txt_dict,[6],"$ atvremote scan -s 192.168.0.6
ERROR: Exception in callback _SelectorDatagramTransport._read_ready()
handle: <Handle _SelectorDatagramTransport._read_ready()>
Traceback (most recent call last):
File ""/usr/lib/python3.6/asyncio/events.py"", line 145, in _run
self._callback(*self._args)
File ""/usr/lib/python3.6/asyncio/selector_events.py"", line 1068, in _read_ready
self._protocol.datagram_received(data, addr)
File ""/home/jacob/.local/lib/python3.6/site-packages/pyatv/udns.py"", line 229, in datagram_received
self.result = DnsMessage().unpack(data)
File ""/home/jacob/.local/lib/python3.6/site-packages/pyatv/udns.py"", line 147, in unpack
ptr, data = unpack_rr(ptr, msg)
File ""/home/jacob/.local/lib/python3.6/site-packages/pyatv/udns.py"", line 98, in unpack_rr
rd_data = parse_txt_dict(rd_data, msg)
File ""/home/jacob/.local/lib/python3.6/site-packages/pyatv/udns.py"", line 58, in parse_txt_dict
key, value = prop.split(b'=')
ValueError: too many values to unpack (expected 2)
Scan Results
========================================",ValueError,buggy_snippets_files/38720e3f7780cd93afe4aff66822d19b70f78427054d16145b24f6e1daf7420e_before_merge.py,buggy_snippets_files/38720e3f7780cd93afe4aff66822d19b70f78427054d16145b24f6e1daf7420e_after_merge.py
215357,"def main():
    argument_spec = ec2_argument_spec()
    argument_spec.update(dict(
        name=dict(),
        group_id=dict(),
        description=dict(),
        vpc_id=dict(),
        rules=dict(type='list'),
        rules_egress=dict(type='list'),
        state=dict(default='present', type='str', choices=['present', 'absent']),
        purge_rules=dict(default=True, required=False, type='bool'),
        purge_rules_egress=dict(default=True, required=False, type='bool'),
        tags=dict(required=False, type='dict', aliases=['resource_tags']),
        purge_tags=dict(default=True, required=False, type='bool')
    )
    )
    module = AnsibleModule(
        argument_spec=argument_spec,
        supports_check_mode=True,
        required_one_of=[['name', 'group_id']],
        required_if=[['state', 'present', ['name']]],
    )

    if not HAS_BOTO3:
        module.fail_json(msg='boto3 required for this module')

    name = module.params['name']
    group_id = module.params['group_id']
    description = module.params['description']
    vpc_id = module.params['vpc_id']
    rules = deduplicate_rules_args(rules_expand_sources(rules_expand_ports(module.params['rules'])))
    rules_egress = deduplicate_rules_args(rules_expand_sources(rules_expand_ports(module.params['rules_egress'])))
    state = module.params.get('state')
    purge_rules = module.params['purge_rules']
    purge_rules_egress = module.params['purge_rules_egress']
    tags = module.params['tags']
    purge_tags = module.params['purge_tags']

    if state == 'present' and not description:
        module.fail_json(msg='Must provide description when state is present.')

    changed = False
    region, ec2_url, aws_connect_params = get_aws_connection_info(module, boto3=True)
    if not region:
        module.fail_json(msg=""The AWS region must be specified as an ""
                             ""environment variable or in the AWS credentials ""
                             ""profile."")
    client = boto3_conn(module, conn_type='client', resource='ec2', endpoint=ec2_url, region=region, **aws_connect_params)
    group = None
    groups = dict()
    security_groups = []
    # do get all security groups
    # find if the group is present
    try:
        response = get_security_groups_with_backoff(client)
        security_groups = response.get('SecurityGroups', [])
    except botocore.exceptions.NoCredentialsError as e:
        module.fail_json(msg=""Error in describe_security_groups: %s"" % ""Unable to locate credentials"", exception=traceback.format_exc())
    except botocore.exceptions.ClientError as e:
        module.fail_json(msg=""Error in describe_security_groups: %s"" % e, exception=traceback.format_exc(),
                         **camel_dict_to_snake_dict(e.response))

    for sg in security_groups:
        groups[sg['GroupId']] = sg
        groupName = sg['GroupName']
        if groupName in groups:
            # Prioritise groups from the current VPC
            if vpc_id is None or sg['VpcId'] == vpc_id:
                groups[groupName] = sg
        else:
            groups[groupName] = sg

        if group_id:
            if sg['GroupId'] == group_id:
                group = sg
        else:
            if groupName == name and (vpc_id is None or sg['VpcId'] == vpc_id):
                group = sg

    # Ensure requested group is absent
    if state == 'absent':
        if group:
            # found a match, delete it
            try:
                if not module.check_mode:
                    client.delete_security_group(GroupId=group['GroupId'])
            except botocore.exceptions.ClientError as e:
                module.fail_json(msg=""Unable to delete security group '%s' - %s"" % (group, e),
                                 exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
            else:
                group = None
                changed = True
        else:
            # no match found, no changes required
            pass

    # Ensure requested group is present
    elif state == 'present':
        if group:
            # existing group
            if group['Description'] != description:
                module.fail_json(
                    msg=""Group description does not match existing group. ec2_group does not support this case."")

        # if the group doesn't exist, create it now
        else:
            # no match found, create it
            if not module.check_mode:
                params = dict(GroupName=name, Description=description)
                if vpc_id:
                    params['VpcId'] = vpc_id
                group = client.create_security_group(**params)
                # When a group is created, an egress_rule ALLOW ALL
                # to 0.0.0.0/0 is added automatically but it's not
                # reflected in the object returned by the AWS API
                # call. We re-read the group for getting an updated object
                # amazon sometimes takes a couple seconds to update the security group so wait till it exists
                while True:
                    group = get_security_groups_with_backoff(client, GroupIds=[group['GroupId']])['SecurityGroups'][0]
                    if not group['IpPermissionsEgress']:
                        pass
                    else:
                        break

            changed = True

        if tags is not None:
            current_tags = boto3_tag_list_to_ansible_dict(group.get('Tags', []))
            tags_need_modify, tags_to_delete = compare_aws_tags(current_tags, tags, purge_tags)
            if tags_to_delete:
                try:
                    client.delete_tags(Resources=[group['GroupId']], Tags=[{'Key': tag} for tag in tags_to_delete])
                except botocore.exceptions.ClientError as e:
                    module.fail_json(msg=e.message, exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True

            # Add/update tags
            if tags_need_modify:
                try:
                    client.create_tags(Resources=[group['GroupId']], Tags=ansible_dict_to_boto3_tag_list(tags_need_modify))
                except botocore.exceptions.ClientError as e:
                    module.fail_json(msg=e.message, exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True

    else:
        module.fail_json(msg=""Unsupported state requested: %s"" % state)

    # create a lookup for all existing rules on the group
    if group:
        # Manage ingress rules
        groupRules = {}
        add_rules_to_lookup(group['IpPermissions'], group['GroupId'], 'in', groupRules)
        # Now, go through all provided rules and ensure they are there.
        if rules is not None:
            ip_permission = []
            for rule in rules:
                validate_rule(module, rule)
                group_id, ip, ipv6, target_group_created = get_target_from_rule(module, client, rule, name,
                                                                                group, groups, vpc_id)
                if target_group_created:
                    changed = True

                if rule['proto'] in ('all', '-1', -1):
                    rule['proto'] = -1
                    rule['from_port'] = None
                    rule['to_port'] = None

                if group_id:
                    rule_id = make_rule_key('in', rule, group['GroupId'], group_id)
                    if rule_id in groupRules:
                        del groupRules[rule_id]
                    else:
                        if not module.check_mode:
                            ip_permission = serialize_group_grant(group_id, rule)
                            if ip_permission:
                                ips = ip_permission
                                if vpc_id:
                                    [useridpair.update({'VpcId': vpc_id}) for useridpair in
                                     ip_permission.get('UserIdGroupPairs')]
                                try:
                                    client.authorize_security_group_ingress(GroupId=group['GroupId'], IpPermissions=[ips])
                                except botocore.exceptions.ClientError as e:
                                    module.fail_json(
                                        msg=""Unable to authorize ingress for group %s security group '%s' - %s"" %
                                            (group_id, group['GroupName'], e),
                                        exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                        changed = True
                elif ip:
                    # Convert ip to list we can iterate over
                    if ip and not isinstance(ip, list):
                        ip = [ip]

                    changed, ip_permission = authorize_ip(""in"", changed, client, group, groupRules, ip, ip_permission,
                                                          module, rule, ""ipv4"")
                elif ipv6:
                    # Convert ip to list we can iterate over
                    if not isinstance(ipv6, list):
                        ipv6 = [ipv6]
                    # If rule already exists, don't later delete it
                    changed, ip_permission = authorize_ip(""in"", changed, client, group, groupRules, ipv6, ip_permission,
                                                          module, rule, ""ipv6"")
        # Finally, remove anything left in the groupRules -- these will be defunct rules
        if purge_rules:
            for (rule, grant) in groupRules.values():
                ip_permission = serialize_revoke(grant, rule)
                if not module.check_mode:
                    try:
                        client.revoke_security_group_ingress(GroupId=group['GroupId'], IpPermissions=[ip_permission])
                    except botocore.exceptions.ClientError as e:
                        module.fail_json(
                            msg=""Unable to revoke ingress for security group '%s' - %s"" %
                                (group['GroupName'], e),
                            exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True

        # Manage egress rules
        groupRules = {}
        add_rules_to_lookup(group['IpPermissionsEgress'], group['GroupId'], 'out', groupRules)
        # Now, go through all provided rules and ensure they are there.
        if rules_egress is not None:
            for rule in rules_egress:
                validate_rule(module, rule)
                group_id, ip, ipv6, target_group_created = get_target_from_rule(module, client, rule, name,
                                                                                group, groups, vpc_id)
                if target_group_created:
                    changed = True

                if rule['proto'] in ('all', '-1', -1):
                    rule['proto'] = -1
                    rule['from_port'] = None
                    rule['to_port'] = None

                if group_id:
                    rule_id = make_rule_key('out', rule, group['GroupId'], group_id)
                    if rule_id in groupRules:
                        del groupRules[rule_id]
                    else:
                        if not module.check_mode:
                            ip_permission = serialize_group_grant(group_id, rule)
                            if ip_permission:
                                ips = ip_permission
                                if vpc_id:
                                    [useridpair.update({'VpcId': vpc_id}) for useridpair in
                                     ip_permission.get('UserIdGroupPairs')]
                                try:
                                    client.authorize_security_group_egress(GroupId=group['GroupId'], IpPermissions=[ips])
                                except botocore.exceptions.ClientError as e:
                                    module.fail_json(
                                        msg=""Unable to authorize egress for group %s security group '%s' - %s"" %
                                            (group_id, group['GroupName'], e),
                                        exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                        changed = True
                elif ip:
                    # Convert ip to list we can iterate over
                    if not isinstance(ip, list):
                        ip = [ip]
                    changed, ip_permission = authorize_ip(""out"", changed, client, group, groupRules, ip,
                                                          ip_permission, module, rule, ""ipv4"")
                elif ipv6:
                    # Convert ip to list we can iterate over
                    if not isinstance(ipv6, list):
                        ipv6 = [ipv6]
                    # If rule already exists, don't later delete it
                    changed, ip_permission = authorize_ip(""out"", changed, client, group, groupRules, ipv6,
                                                          ip_permission, module, rule, ""ipv6"")
        else:
            # when no egress rules are specified,
            # we add in a default allow all out rule, which was the
            # default behavior before egress rules were added
            default_egress_rule = 'out--1-None-None-' + group['GroupId'] + '-0.0.0.0/0'
            if default_egress_rule not in groupRules:
                if not module.check_mode:
                    ip_permission = [{'IpProtocol': '-1',
                                      'IpRanges': [{'CidrIp': '0.0.0.0/0'}]
                                      }
                                     ]
                    try:
                        client.authorize_security_group_egress(GroupId=group['GroupId'], IpPermissions=ip_permission)
                    except botocore.exceptions.ClientError as e:
                        module.fail_json(msg=""Unable to authorize egress for ip %s security group '%s' - %s"" %
                                             ('0.0.0.0/0',
                                              group['GroupName'],
                                              e),
                                         exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True
            else:
                # make sure the default egress rule is not removed
                del groupRules[default_egress_rule]

        # Finally, remove anything left in the groupRules -- these will be defunct rules
        if purge_rules_egress:
            for (rule, grant) in groupRules.values():
                # we shouldn't be revoking 0.0.0.0 egress
                if grant != '0.0.0.0/0':
                    ip_permission = serialize_revoke(grant, rule)
                    if not module.check_mode:
                        try:
                            client.revoke_security_group_egress(GroupId=group['GroupId'], IpPermissions=[ip_permission])
                        except botocore.exceptions.ClientError as e:
                            module.fail_json(msg=""Unable to revoke egress for ip %s security group '%s' - %s"" %
                                                 (grant, group['GroupName'], e),
                                             exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                    changed = True

    if group:
        security_group = get_security_groups_with_backoff(client, GroupIds=[group['GroupId']])['SecurityGroups'][0]
        security_group = camel_dict_to_snake_dict(security_group)
        security_group['tags'] = boto3_tag_list_to_ansible_dict(security_group.get('tags', []),
                                                                tag_name_key_name='key', tag_value_key_name='value')
        module.exit_json(changed=changed, **security_group)
    else:
        module.exit_json(changed=changed, group_id=None)
","def main():
    argument_spec = ec2_argument_spec()
    argument_spec.update(dict(
        name=dict(),
        group_id=dict(),
        description=dict(),
        vpc_id=dict(),
        rules=dict(type='list'),
        rules_egress=dict(type='list'),
        state=dict(default='present', type='str', choices=['present', 'absent']),
        purge_rules=dict(default=True, required=False, type='bool'),
        purge_rules_egress=dict(default=True, required=False, type='bool'),
        tags=dict(required=False, type='dict', aliases=['resource_tags']),
        purge_tags=dict(default=True, required=False, type='bool')
    )
    )
    module = AnsibleModule(
        argument_spec=argument_spec,
        supports_check_mode=True,
        required_one_of=[['name', 'group_id']],
        required_if=[['state', 'present', ['name']]],
    )

    if not HAS_BOTO3:
        module.fail_json(msg='boto3 required for this module')

    name = module.params['name']
    group_id = module.params['group_id']
    description = module.params['description']
    vpc_id = module.params['vpc_id']
    rules = deduplicate_rules_args(rules_expand_sources(rules_expand_ports(module.params['rules'])))
    rules_egress = deduplicate_rules_args(rules_expand_sources(rules_expand_ports(module.params['rules_egress'])))
    state = module.params.get('state')
    purge_rules = module.params['purge_rules']
    purge_rules_egress = module.params['purge_rules_egress']
    tags = module.params['tags']
    purge_tags = module.params['purge_tags']

    if state == 'present' and not description:
        module.fail_json(msg='Must provide description when state is present.')

    changed = False
    region, ec2_url, aws_connect_params = get_aws_connection_info(module, boto3=True)
    if not region:
        module.fail_json(msg=""The AWS region must be specified as an ""
                             ""environment variable or in the AWS credentials ""
                             ""profile."")
    client = boto3_conn(module, conn_type='client', resource='ec2', endpoint=ec2_url, region=region, **aws_connect_params)
    group = None
    groups = dict()
    security_groups = []
    # do get all security groups
    # find if the group is present
    try:
        response = get_security_groups_with_backoff(client)
        security_groups = response.get('SecurityGroups', [])
    except botocore.exceptions.NoCredentialsError as e:
        module.fail_json(msg=""Error in describe_security_groups: %s"" % ""Unable to locate credentials"", exception=traceback.format_exc())
    except botocore.exceptions.ClientError as e:
        module.fail_json(msg=""Error in describe_security_groups: %s"" % e, exception=traceback.format_exc(),
                         **camel_dict_to_snake_dict(e.response))

    for sg in security_groups:
        groups[sg['GroupId']] = sg
        groupName = sg['GroupName']
        if groupName in groups:
            # Prioritise groups from the current VPC
            if vpc_id is None or sg['VpcId'] == vpc_id:
                groups[groupName] = sg
        else:
            groups[groupName] = sg

        if group_id:
            if sg['GroupId'] == group_id:
                group = sg
        else:
            if groupName == name and (vpc_id is None or sg['VpcId'] == vpc_id):
                group = sg

    # Ensure requested group is absent
    if state == 'absent':
        if group:
            # found a match, delete it
            try:
                if not module.check_mode:
                    client.delete_security_group(GroupId=group['GroupId'])
            except botocore.exceptions.ClientError as e:
                module.fail_json(msg=""Unable to delete security group '%s' - %s"" % (group, e),
                                 exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
            else:
                group = None
                changed = True
        else:
            # no match found, no changes required
            pass

    # Ensure requested group is present
    elif state == 'present':
        if group:
            # existing group
            if group['Description'] != description:
                module.fail_json(
                    msg=""Group description does not match existing group. ec2_group does not support this case."")

        # if the group doesn't exist, create it now
        else:
            # no match found, create it
            if not module.check_mode:
                params = dict(GroupName=name, Description=description)
                if vpc_id:
                    params['VpcId'] = vpc_id
                group = client.create_security_group(**params)
                # When a group is created, an egress_rule ALLOW ALL
                # to 0.0.0.0/0 is added automatically but it's not
                # reflected in the object returned by the AWS API
                # call. We re-read the group for getting an updated object
                # amazon sometimes takes a couple seconds to update the security group so wait till it exists
                while True:
                    group = get_security_groups_with_backoff(client, GroupIds=[group['GroupId']])['SecurityGroups'][0]
                    if not group['IpPermissionsEgress']:
                        pass
                    else:
                        break

            changed = True

        if tags is not None:
            current_tags = boto3_tag_list_to_ansible_dict(group.get('Tags', []))
            tags_need_modify, tags_to_delete = compare_aws_tags(current_tags, tags, purge_tags)
            if tags_to_delete:
                try:
                    client.delete_tags(Resources=[group['GroupId']], Tags=[{'Key': tag} for tag in tags_to_delete])
                except botocore.exceptions.ClientError as e:
                    module.fail_json(msg=e.message, exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True

            # Add/update tags
            if tags_need_modify:
                try:
                    client.create_tags(Resources=[group['GroupId']], Tags=ansible_dict_to_boto3_tag_list(tags_need_modify))
                except botocore.exceptions.ClientError as e:
                    module.fail_json(msg=e.message, exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True

    else:
        module.fail_json(msg=""Unsupported state requested: %s"" % state)

    # create a lookup for all existing rules on the group
    ip_permission = []
    if group:
        # Manage ingress rules
        groupRules = {}
        add_rules_to_lookup(group['IpPermissions'], group['GroupId'], 'in', groupRules)
        # Now, go through all provided rules and ensure they are there.
        if rules is not None:
            for rule in rules:
                validate_rule(module, rule)
                group_id, ip, ipv6, target_group_created = get_target_from_rule(module, client, rule, name,
                                                                                group, groups, vpc_id)
                if target_group_created:
                    changed = True

                if rule['proto'] in ('all', '-1', -1):
                    rule['proto'] = -1
                    rule['from_port'] = None
                    rule['to_port'] = None

                if group_id:
                    rule_id = make_rule_key('in', rule, group['GroupId'], group_id)
                    if rule_id in groupRules:
                        del groupRules[rule_id]
                    else:
                        if not module.check_mode:
                            ip_permission = serialize_group_grant(group_id, rule)
                            if ip_permission:
                                ips = ip_permission
                                if vpc_id:
                                    [useridpair.update({'VpcId': vpc_id}) for useridpair in
                                     ip_permission.get('UserIdGroupPairs')]
                                try:
                                    client.authorize_security_group_ingress(GroupId=group['GroupId'], IpPermissions=[ips])
                                except botocore.exceptions.ClientError as e:
                                    module.fail_json(
                                        msg=""Unable to authorize ingress for group %s security group '%s' - %s"" %
                                            (group_id, group['GroupName'], e),
                                        exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                        changed = True
                elif ip:
                    # Convert ip to list we can iterate over
                    if ip and not isinstance(ip, list):
                        ip = [ip]

                    changed, ip_permission = authorize_ip(""in"", changed, client, group, groupRules, ip, ip_permission,
                                                          module, rule, ""ipv4"")
                elif ipv6:
                    # Convert ip to list we can iterate over
                    if not isinstance(ipv6, list):
                        ipv6 = [ipv6]
                    # If rule already exists, don't later delete it
                    changed, ip_permission = authorize_ip(""in"", changed, client, group, groupRules, ipv6, ip_permission,
                                                          module, rule, ""ipv6"")
        # Finally, remove anything left in the groupRules -- these will be defunct rules
        if purge_rules:
            for (rule, grant) in groupRules.values():
                ip_permission = serialize_revoke(grant, rule)
                if not module.check_mode:
                    try:
                        client.revoke_security_group_ingress(GroupId=group['GroupId'], IpPermissions=[ip_permission])
                    except botocore.exceptions.ClientError as e:
                        module.fail_json(
                            msg=""Unable to revoke ingress for security group '%s' - %s"" %
                                (group['GroupName'], e),
                            exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True

        # Manage egress rules
        groupRules = {}
        add_rules_to_lookup(group['IpPermissionsEgress'], group['GroupId'], 'out', groupRules)
        # Now, go through all provided rules and ensure they are there.
        if rules_egress is not None:
            for rule in rules_egress:
                validate_rule(module, rule)
                group_id, ip, ipv6, target_group_created = get_target_from_rule(module, client, rule, name,
                                                                                group, groups, vpc_id)
                if target_group_created:
                    changed = True

                if rule['proto'] in ('all', '-1', -1):
                    rule['proto'] = -1
                    rule['from_port'] = None
                    rule['to_port'] = None

                if group_id:
                    rule_id = make_rule_key('out', rule, group['GroupId'], group_id)
                    if rule_id in groupRules:
                        del groupRules[rule_id]
                    else:
                        if not module.check_mode:
                            ip_permission = serialize_group_grant(group_id, rule)
                            if ip_permission:
                                ips = ip_permission
                                if vpc_id:
                                    [useridpair.update({'VpcId': vpc_id}) for useridpair in
                                     ip_permission.get('UserIdGroupPairs')]
                                try:
                                    client.authorize_security_group_egress(GroupId=group['GroupId'], IpPermissions=[ips])
                                except botocore.exceptions.ClientError as e:
                                    module.fail_json(
                                        msg=""Unable to authorize egress for group %s security group '%s' - %s"" %
                                            (group_id, group['GroupName'], e),
                                        exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                        changed = True
                elif ip:
                    # Convert ip to list we can iterate over
                    if not isinstance(ip, list):
                        ip = [ip]
                    changed, ip_permission = authorize_ip(""out"", changed, client, group, groupRules, ip,
                                                          ip_permission, module, rule, ""ipv4"")
                elif ipv6:
                    # Convert ip to list we can iterate over
                    if not isinstance(ipv6, list):
                        ipv6 = [ipv6]
                    # If rule already exists, don't later delete it
                    changed, ip_permission = authorize_ip(""out"", changed, client, group, groupRules, ipv6,
                                                          ip_permission, module, rule, ""ipv6"")
        else:
            # when no egress rules are specified,
            # we add in a default allow all out rule, which was the
            # default behavior before egress rules were added
            default_egress_rule = 'out--1-None-None-' + group['GroupId'] + '-0.0.0.0/0'
            if default_egress_rule not in groupRules:
                if not module.check_mode:
                    ip_permission = [{'IpProtocol': '-1',
                                      'IpRanges': [{'CidrIp': '0.0.0.0/0'}]
                                      }
                                     ]
                    try:
                        client.authorize_security_group_egress(GroupId=group['GroupId'], IpPermissions=ip_permission)
                    except botocore.exceptions.ClientError as e:
                        module.fail_json(msg=""Unable to authorize egress for ip %s security group '%s' - %s"" %
                                             ('0.0.0.0/0',
                                              group['GroupName'],
                                              e),
                                         exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                changed = True
            else:
                # make sure the default egress rule is not removed
                del groupRules[default_egress_rule]

        # Finally, remove anything left in the groupRules -- these will be defunct rules
        if purge_rules_egress:
            for (rule, grant) in groupRules.values():
                # we shouldn't be revoking 0.0.0.0 egress
                if grant != '0.0.0.0/0':
                    ip_permission = serialize_revoke(grant, rule)
                    if not module.check_mode:
                        try:
                            client.revoke_security_group_egress(GroupId=group['GroupId'], IpPermissions=[ip_permission])
                        except botocore.exceptions.ClientError as e:
                            module.fail_json(msg=""Unable to revoke egress for ip %s security group '%s' - %s"" %
                                                 (grant, group['GroupName'], e),
                                             exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))
                    changed = True

    if group:
        security_group = get_security_groups_with_backoff(client, GroupIds=[group['GroupId']])['SecurityGroups'][0]
        security_group = camel_dict_to_snake_dict(security_group)
        security_group['tags'] = boto3_tag_list_to_ansible_dict(security_group.get('tags', []),
                                                                tag_name_key_name='key', tag_value_key_name='value')
        module.exit_json(changed=changed, **security_group)
    else:
        module.exit_json(changed=changed, group_id=None)
",https://github.com/ansible/ansible/issues/29113,CWE-232: Improper Handling of Undefined Values,local variable 'ip_permission' referenced before assignment in some control paths,lib/ansible/modules/cloud/amazon/ec2_group.py,main,[193],"The full traceback is:
Traceback (most recent call last):
File ""/var/folders/y2/l4g6_6zn03s1mh34lkz1czjm0000gn/T/ansible_OwHGhF/ansible_module_ec2_group.py"", line 884, in <module>
main()
File ""/var/folders/y2/l4g6_6zn03s1mh34lkz1czjm0000gn/T/ansible_OwHGhF/ansible_module_ec2_group.py"", line 826, in main
ip_permission, module, rule, ""ipv4"")
UnboundLocalError: local variable 'ip_permission' referenced before assignment

fatal: [localhost]: FAILED! => {
""changed"": false,
""failed"": true,
""module_stderr"": ""Traceback (most recent call last):\n  File \""/var/folders/y2/l4g6_6zn03s1mh34lkz1czjm0000gn/T/ansible_OwHGhF/ansible_module_ec2_group.py\"", line 884, in <module>\n    main()\n  File \""/var/folders/y2/l4g6_6zn03s1mh34lkz1czjm0000gn/T/ansible_OwHGhF/ansible_module_ec2_group.py\"", line 826, in main\n    ip_permission, module, rule, \""ipv4\"")\nUnboundLocalError: local variable 'ip_permission' referenced before assignment\n"",
""module_stdout"": """",
""msg"": ""MODULE FAILURE"",
""rc"": 0
}",UnboundLocalError,buggy_snippets_files/bbc689ab92ad44070c0f9f77b046a4bd5e2dfd568cb854fa0242669c7e671ff5_before_merge.py,buggy_snippets_files/bbc689ab92ad44070c0f9f77b046a4bd5e2dfd568cb854fa0242669c7e671ff5_after_merge.py
20418,"    def append(self, cmd):
        if not self.remember_history:
            return
        envs = builtins.__xonsh__.env
        inp = cmd[""inp""].rstrip()
        self.inps.append(inp)
        self.outs.append(cmd.get(""out""))
        self.rtns.append(cmd[""rtn""])
        self.tss.append(cmd.get(""ts"", (None, None)))

        opts = envs.get(""HISTCONTROL"")
        if ""ignoredups"" in opts and inp == self._last_hist_inp:
            # Skipping dup cmd
            return
        if ""ignoreerr"" in opts and cmd[""rtn""] != 0:
            # Skipping failed cmd
            return
        if ""ignorespace"" in opts and cmd.get(""spc""):
            # Skipping cmd starting with space
            return

        try:
            del cmd[""spc""]
        except KeyError:
            pass
        self._last_hist_inp = inp
        xh_sqlite_append_history(
            cmd,
            str(self.sessionid),
            store_stdout=envs.get(""XONSH_STORE_STDOUT"", False),
            filename=self.filename,
            remove_duplicates=(""erasedups"" in opts),
        )","    def append(self, cmd):
        if not self.remember_history:
            return
        envs = builtins.__xonsh__.env
        inp = cmd[""inp""].rstrip()
        self.inps.append(inp)
        self.outs.append(cmd.get(""out""))
        self.rtns.append(cmd[""rtn""])
        self.tss.append(cmd.get(""ts"", (None, None)))

        opts = envs.get(""HISTCONTROL"")
        if ""ignoredups"" in opts and inp == self._last_hist_inp:
            # Skipping dup cmd
            return
        if ""ignoreerr"" in opts and cmd[""rtn""] != 0:
            # Skipping failed cmd
            return
        if ""ignorespace"" in opts and cmd.get(""spc""):
            # Skipping cmd starting with space
            return

        try:
            del cmd[""spc""]
        except KeyError:
            pass
        self._last_hist_inp = inp
        try:
            xh_sqlite_append_history(
                cmd,
                str(self.sessionid),
                store_stdout=envs.get(""XONSH_STORE_STDOUT"", False),
                filename=self.filename,
                remove_duplicates=(""erasedups"" in opts),
            )
        except sqlite3.OperationalError as err:
            print(f""SQLite History Backend Error: {err}"")
",https://github.com/xonsh/xonsh/issues/4084,CWE-248: Uncaught Exception,"xh_sqlite_append_history() may throw exception (for example, if disk if full) which is uncaught.",xonsh/history/sqlite.py,SqliteHistory.append,[27],"Traceback (most recent call last):
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/main.py"", line 421, in main
sys.exit(main_xonsh(args))
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/main.py"", line 459, in main_xonsh
shell.shell.cmdloop()
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/ptk_shell/shell.py"", line 344, in cmdloop
self.default(line, raw_line)
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/base_shell.py"", line 389, in default
self._append_history(
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/base_shell.py"", line 429, in _append_history
hist.append(info)
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/history/sqlite.py"", line 275, in append
xh_sqlite_append_history(
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/history/sqlite.py"", line 172, in xh_sqlite_append_history
_xh_sqlite_insert_command(c, cmd, sessionid, store_stdout, remove_duplicates)
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/history/sqlite.py"", line 123, in _xh_sqlite_insert_command
values[""frequency""] = _xh_sqlite_erase_dups(cursor, values[""inp""]) + 1
File ""/opt/miniconda/lib/python3.8/site-packages/xonsh/history/sqlite.py"", line 91, in _xh_sqlite_erase_dups
cursor.execute(sql, (input,))
sqlite3.OperationalError: database or disk is full
Xonsh encountered an issue during launch
Failback to /bin/bash
(base) pc@pc:~$",sqlite3.OperationalError,buggy_snippets_files/384fcd1d9338e737b6a041363c1c057179d9677ab547a1a3ab5f999a690c6734_before_merge.py,buggy_snippets_files/384fcd1d9338e737b6a041363c1c057179d9677ab547a1a3ab5f999a690c6734_after_merge.py
402826,"    def batch_to(data):
        # try to move torchtext data first
        if TORCHTEXT_AVAILABLE and isinstance(data, Batch):

            # Shallow copy because each Batch has a reference to Dataset which contains all examples
            device_data = copy(data)
            for field in data.fields:
                device_field = move_data_to_device(getattr(data, field), device)
                setattr(device_data, field, device_field)
            return device_data

        kwargs = dict(non_blocking=True) if isinstance(data, torch.Tensor) else {}
        return data.to(device, **kwargs)
","    def batch_to(data):
        # try to move torchtext data first
        if TORCHTEXT_AVAILABLE and isinstance(data, Batch):

            # Shallow copy because each Batch has a reference to Dataset which contains all examples
            device_data = copy(data)
            for field, field_value in data.dataset.fields.items():
                if field_value is None:
                    continue
                device_field = move_data_to_device(getattr(data, field), device)
                setattr(device_data, field, device_field)
            return device_data

        kwargs = dict(non_blocking=True) if isinstance(data, torch.Tensor) else {}
        return data.to(device, **kwargs)
",https://github.com/PyTorchLightning/pytorch-lightning/issues/4782,CWE-159: Improper Handling of Invalid Use of Special Elements,Unhandled case when 'field' value is 'None',pytorch_lightning/utilities/apply_func.py,move_data_to_device.batch_to,[8],"---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-11-bd904d609559> in <module>()
----> 1 test_x(tmpdir)  # works when using no gpu

15 frames
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/apply_func.py in batch_to(data)
115             device_data = copy(data)
116             for field in data.fields:
--> 117                 device_field = move_data_to_device(getattr(data, field), device)
118                 setattr(device_data, field, device_field)
119             return device_data

AttributeError: 'Batch' object has no attribute 'TITLE'",AttributeError,buggy_snippets_files/7647195fc58fdbab0b5b946c50e0df7f4710ac4ec320f0201094eec5e45f2ad4_before_merge.py,buggy_snippets_files/7647195fc58fdbab0b5b946c50e0df7f4710ac4ec320f0201094eec5e45f2ad4_after_merge.py
378541,"    def pageChanged(self, event):
        """"""Event called when the user swtiches between editor tabs.""""""
        old = event.GetOldSelection()
        # close any auto-complete or calltips when swtiching pages
        if old != wx.NOT_FOUND:
            oldPage = self.notebook.GetPage(old)
            if hasattr(oldPage, 'CallTipActive'):
                if oldPage.CallTipActive():
                    oldPage.CallTipCancel()
                    oldPage.openBrackets = 0
            if hasattr(oldPage, 'AutoCompActive'):
                if oldPage.AutoCompActive():
                    oldPage.AutoCompCancel()

        new = event.GetSelection()
        self.currentDoc = self.notebook.GetPage(new)
        self.app.updateWindowMenu()
        self.setFileModified(self.currentDoc.UNSAVED)
        self.SetLabel('%s - PsychoPy Coder' % self.currentDoc.filename)

        self.currentDoc.analyseScript()

        fileType = self.currentDoc.getFileType()
        # enable run buttons if current file is a Python script
        if hasattr(self, 'cdrBtnRunner'):
            isExp = fileType == 'Python'
            self.toolbar.EnableTool(self.cdrBtnRunner.Id, isExp)
            self.toolbar.EnableTool(self.cdrBtnRun.Id, isExp)

        self.statusBar.SetStatusText(fileType, 2)

        # todo: reduce redundancy w.r.t OnIdle()
        if not self.expectedModTime(self.currentDoc):
            filename = os.path.basename(self.currentDoc.filename)
            msg = _translate(""'%s' was modified outside of PsychoPy:\n\n""
                             ""Reload (without saving)?"") % filename
            dlg = dialogs.MessageDialog(self, message=msg, type='Warning')
            if dlg.ShowModal() == wx.ID_YES:
                self.statusBar.SetStatusText(_translate('Reloading file'))
                self.fileReload(event,
                                filename=self.currentDoc.filename,
                                checkSave=False)
                self.setFileModified(False)
            self.statusBar.SetStatusText('')
            dlg.Destroy()
","    def pageChanged(self, event):
        """"""Event called when the user switches between editor tabs.""""""
        old = event.GetOldSelection()
        # close any auto-complete or calltips when switching pages
        if old != wx.NOT_FOUND:
            oldPage = None

            try:  # last page was closed, this will raise and error
                oldPage = self.notebook.GetPage(old)
            except Exception:
                pass

            if oldPage is not None:
                if hasattr(oldPage, 'CallTipActive'):
                    if oldPage.CallTipActive():
                        oldPage.CallTipCancel()
                        oldPage.openBrackets = 0
                if hasattr(oldPage, 'AutoCompActive'):
                    if oldPage.AutoCompActive():
                        oldPage.AutoCompCancel()

        new = event.GetSelection()
        self.currentDoc = self.notebook.GetPage(new)
        self.app.updateWindowMenu()
        self.setFileModified(self.currentDoc.UNSAVED)
        self.SetLabel('%s - PsychoPy Coder' % self.currentDoc.filename)

        self.currentDoc.analyseScript()

        fileType = self.currentDoc.getFileType()
        # enable run buttons if current file is a Python script
        if hasattr(self, 'cdrBtnRunner'):
            isExp = fileType == 'Python'
            self.toolbar.EnableTool(self.cdrBtnRunner.Id, isExp)
            self.toolbar.EnableTool(self.cdrBtnRun.Id, isExp)

        self.statusBar.SetStatusText(fileType, 2)

        # todo: reduce redundancy w.r.t OnIdle()
        if not self.expectedModTime(self.currentDoc):
            filename = os.path.basename(self.currentDoc.filename)
            msg = _translate(""'%s' was modified outside of PsychoPy:\n\n""
                             ""Reload (without saving)?"") % filename
            dlg = dialogs.MessageDialog(self, message=msg, type='Warning')
            if dlg.ShowModal() == wx.ID_YES:
                self.statusBar.SetStatusText(_translate('Reloading file'))
                self.fileReload(event,
                                filename=self.currentDoc.filename,
                                checkSave=False)
                self.setFileModified(False)
            self.statusBar.SetStatusText('')
            dlg.Destroy()
",https://github.com/psychopy/psychopy/issues/3446,CWE-248: Uncaught Exception,"self.notebook.GetPage(old) tries to access previously selected page object which may be already closed, raising an exception.",psychopy/app/coder/coder.py,CoderFrame.pageChanged,[3],"Traceback (most recent call last):
File ""C:\Program Files\PsychoPy3\lib\site-packages\psychopy\app\coder\coder.py"", line 2004, in pageChanged
oldPage = self.notebook.GetPage(old)
File ""C:\Program Files\PsychoPy3\lib\site-packages\wx\lib\agw\aui\auibook.py"", line 4440, in GetPage
raise Exception(""invalid notebook page"")
Exception: invalid notebook page",Exception,buggy_snippets_files/829f7c4c7d70b2de5fab7a33115d2c5552702c9d8096caec9f3d134700fe19f4_before_merge.py,buggy_snippets_files/829f7c4c7d70b2de5fab7a33115d2c5552702c9d8096caec9f3d134700fe19f4_after_merge.py
210963,"    def _add_group(self, host, result_item):
        '''
        Helper function to add a group (if it does not exist), and to assign the
        specified host to that group.
        '''

        changed = False

        # the host here is from the executor side, which means it was a
        # serialized/cloned copy and we'll need to look up the proper
        # host object from the master inventory
        real_host = self._inventory.hosts[host.name]
        group_name = result_item.get('add_group')
        parent_group_names = result_item.get('parent_groups', [])

        for name in [group_name] + parent_group_names:
            if name not in self._inventory.groups:
                # create the new group and add it to inventory
                self._inventory.add_group(name)
                changed = True
        group = self._inventory.groups[group_name]
        for parent_group_name in parent_group_names:
            parent_group = self._inventory.groups[parent_group_name]
            parent_group.add_child_group(group)

        if real_host.name not in group.get_hosts():
            group.add_host(real_host)
            changed = True

        if group_name not in host.get_groups():
            real_host.add_group(group)
            changed = True

        if changed:
            self._inventory.reconcile_inventory()

        return changed
","    def _add_group(self, host, result_item):
        '''
        Helper function to add a group (if it does not exist), and to assign the
        specified host to that group.
        '''

        changed = False

        # the host here is from the executor side, which means it was a
        # serialized/cloned copy and we'll need to look up the proper
        # host object from the master inventory
        real_host = self._inventory.hosts.get(host.name)
        if real_host is None:
            if host.name == self._inventory.localhost.name:
                real_host = self._inventory.localhost
            else:
                raise AnsibleError('%s cannot be matched in inventory' % host.name)
        group_name = result_item.get('add_group')
        parent_group_names = result_item.get('parent_groups', [])

        for name in [group_name] + parent_group_names:
            if name not in self._inventory.groups:
                # create the new group and add it to inventory
                self._inventory.add_group(name)
                changed = True
        group = self._inventory.groups[group_name]
        for parent_group_name in parent_group_names:
            parent_group = self._inventory.groups[parent_group_name]
            parent_group.add_child_group(group)

        if real_host.name not in group.get_hosts():
            group.add_host(real_host)
            changed = True

        if group_name not in host.get_groups():
            real_host.add_group(group)
            changed = True

        if changed:
            self._inventory.reconcile_inventory()

        return changed
",https://github.com/ansible/ansible/issues/32152,"CWE-248: Uncaught Exception, CWE-703: Improper Check or Handling of Exceptional Conditions",The case is not handled when self._inventory.hosts may not contain 'localhost' when program is running from localhost and host.name is 'localhost'.,lib/ansible/plugins/strategy/__init__.py,StrategyBase._add_group,[12],"PLAYBOOK: rex.yml ******************************************************************************************************
1 plays in rex.yml

PLAY [localhost] *******************************************************************************************************

TASK [Gathering Facts] *************************************************************************************************
Using module file /usr/local/lib/python3.5/dist-packages/ansible/modules/system/setup.py
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: root
<127.0.0.1> EXEC /bin/sh -c 'echo ~ &amp;&amp; sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 &amp;&amp; mkdir -p ""` echo /root/.ansible/tmp/ansible-tmp-1508876288.0254786-168044730198300 `"" &amp;&amp; echo ansible-tmp-1508876288.0254786-168044730198300=""` echo /root/.ansible/tmp/ansible-tmp-1508876288.0254786-168044730198300 `"" ) &amp;&amp; sleep 0'
<127.0.0.1> PUT /tmp/tmpd750uvbw TO /root/.ansible/tmp/ansible-tmp-1508876288.0254786-168044730198300/setup.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /root/.ansible/tmp/ansible-tmp-1508876288.0254786-168044730198300/ /root/.ansible/tmp/ansible-tmp-1508876288.0254786-168044730198300/setup.py &amp;&amp; sleep 0'
<127.0.0.1> EXEC /bin/sh -c '/usr/bin/python3 /root/.ansible/tmp/ansible-tmp-1508876288.0254786-168044730198300/setup.py; rm -rf ""/root/.ansible/tmp/ansible-tmp-1508876288.0254786-168044730198300/"" > /dev/null 2>&amp;1 &amp;&amp; sleep 0'
ok: [localhost]

TASK [group_by] ********************************************************************************************************
task path: /opt/app/rex.yml:18
ERROR! Unexpected Exception, this is probably a bug: 'localhost'
the full traceback was:

Traceback (most recent call last):
File ""/usr/local/bin/ansible-playbook"", line 106, in <module>
exit_code = cli.run()
File ""/usr/local/lib/python3.5/dist-packages/ansible/cli/playbook.py"", line 130, in run
results = pbex.run()
File ""/usr/local/lib/python3.5/dist-packages/ansible/executor/playbook_executor.py"", line 154, in run
result = self._tqm.run(play=play)
File ""/usr/local/lib/python3.5/dist-packages/ansible/executor/task_queue_manager.py"", line 302, in run
play_return = strategy.run(iterator, play_context)
File ""/usr/local/lib/python3.5/dist-packages/ansible/plugins/strategy/linear.py"", line 292, in run
results += self._wait_on_pending_results(iterator)
File ""/usr/local/lib/python3.5/dist-packages/ansible/plugins/strategy/__init__.py"", line 586, in _wait_on_pending_results
results = self._process_pending_results(iterator)
File ""/usr/local/lib/python3.5/dist-packages/ansible/plugins/strategy/__init__.py"", line 497, in _process_pending_results
self._add_group(original_host, result_item)
File ""/usr/local/lib/python3.5/dist-packages/ansible/plugins/strategy/__init__.py"", line 632, in _add_group
real_host = self._inventory.hosts[host.name]
KeyError: 'localhost'",KeyError,buggy_snippets_files/9236cd13752acd7d3394c4c3a71ae6004b9440a3079a08ff54d193f770f91ffa_before_merge.py,buggy_snippets_files/9236cd13752acd7d3394c4c3a71ae6004b9440a3079a08ff54d193f770f91ffa_after_merge.py
398604,"    def msg(self, text=None, from_obj=None, session=None, options=None, **kwargs):
        """"""
        Evennia -> User
        This is the main route for sending data back to the user from the
        server.

        Args:
            text (str, optional): text data to send
            from_obj (Object or Player, optional): Object sending. If given,
                its at_msg_send() hook will be called.
            session (Session or list, optional): Session object or a list of
                Sessions to receive this send. If given, overrules the
                default send behavior for the current
                MULTISESSION_MODE.
            options (list): Protocol-specific options. Passed on to the protocol.
        Kwargs:
            any (dict): All other keywords are passed on to the protocol.

        """"""
        if from_obj:
            # call hook
            try:
                from_obj.at_msg_send(text=text, to_obj=self, **kwargs)
            except Exception:
                pass
        try:
            if not self.at_msg_receive(text=text, **kwargs):
                # abort message to this player
                return
        except Exception:
            pass

        kwargs[""options""] = options

        # session relay
        sessions = make_iter(session) if session else self.sessions.all()
        for session in sessions:
            session.data_out(text=text, **kwargs)
","    def msg(self, text=None, from_obj=None, session=None, options=None, **kwargs):
        """"""
        Evennia -> User
        This is the main route for sending data back to the user from the
        server.

        Args:
            text (str, optional): text data to send
            from_obj (Object or Player, optional): Object sending. If given,
                its at_msg_send() hook will be called.
            session (Session or list, optional): Session object or a list of
                Sessions to receive this send. If given, overrules the
                default send behavior for the current
                MULTISESSION_MODE.
            options (list): Protocol-specific options. Passed on to the protocol.
        Kwargs:
            any (dict): All other keywords are passed on to the protocol.

        """"""
        if from_obj:
            # call hook
            try:
                from_obj.at_msg_send(text=text, to_obj=self, **kwargs)
            except Exception:
                pass
        try:
            if not self.at_msg_receive(text=text, **kwargs):
                # abort message to this player
                return
        except Exception:
            pass

        text = None if text is None else str(text)
        kwargs[""options""] = options

        # session relay
        sessions = make_iter(session) if session else self.sessions.all()
        for session in sessions:
            session.data_out(text=text, **kwargs)
",https://github.com/evennia/evennia/issues/965,CWE-241: Improper Handling of Unexpected Data Type,"In function msg(), argument text may have non-text value, which is unhandled.",evennia/players/players.py,DefaultPlayer.msg,[38],"Traceback (most recent call last):
File ""/evennia/evennia/commands/cmdhandler.py"", line 464, in _run_command
ret = yield cmd.func()
File ""/evennia/evennia/commands/default/building.py"", line 1807, in func
caller.msg(obj.locks)
File ""/evennia/evennia/objects/objects.py"", line 509, in msg
session.data_out(text=text, **kwargs)
File ""/evennia/evennia/server/serversession.py"", line 362, in data_out
self.sessionhandler.data_out(self, **kwargs)
File ""/evennia/evennia/server/sessionhandler.py"", line 621, in data_out
**kwargs)
File ""/evennia/evennia/server/amp.py"", line 467, in send_MsgServer2Portal
return self.send_data(MsgServer2Portal, session.sessid, **kwargs)
File ""/evennia/evennia/server/amp.py"", line 406, in send_data
packed_data=dumps((sessid, kwargs))
File ""/evennia/evennia/server/amp.py"", line 319, in <lambda>
dumps = lambda data: to_str(pickle.dumps(to_str(data), pickle.HIGHEST_PROTOCOL))
File ""/pyenv/local/lib/python2.7/site-packages/twisted/persisted/styles.py"", line 119, in _pickleFunction
tuple([""."".join([f.__module__, f.__qualname__])]))
AttributeError: 'function' object has no attribute '__qualname__'",AttributeError,buggy_snippets_files/9ffb3a6ea31c1fcc6552dfd6df380d84bdac95abb03ea887ce74bfa19f19528f_before_merge.py,buggy_snippets_files/9ffb3a6ea31c1fcc6552dfd6df380d84bdac95abb03ea887ce74bfa19f19528f_after_merge.py
157594,"    def html(self, obj, fmt=None, css=None, comm=True, **kwargs):
        """"""
        Renders plot or data structure and wraps the output in HTML.
        The comm argument defines whether the HTML output includes
        code to initialize a Comm, if the plot supplies one.
        """"""
        plot, fmt =  self._validate(obj, fmt)
        figdata, _ = self(plot, fmt, **kwargs)
        if css is None: css = self.css

        if fmt in ['html', 'json']:
            return figdata
        else:
            if fmt == 'svg':
                figdata = figdata.encode(""utf-8"")
            elif fmt == 'pdf' and 'height' not in css:
                _, h = self.get_size(plot)
                css['height'] = '%dpx' % (h*self.dpi*1.15)

        if isinstance(css, dict):
            css = '; '.join(""%s: %s"" % (k, v) for k, v in css.items())
        else:
            raise ValueError(""CSS must be supplied as Python dictionary"")

        b64 = base64.b64encode(figdata).decode(""utf-8"")
        (mime_type, tag) = MIME_TYPES[fmt], HTML_TAGS[fmt]
        src = HTML_TAGS['base64'].format(mime_type=mime_type, b64=b64)
        html = tag.format(src=src, mime_type=mime_type, css=css)
        if comm and plot.comm is not None:
            comm, msg_handler = self.comms[self.mode]
            msg_handler = msg_handler.format(comm_id=plot.comm.id)
            return comm.template.format(init_frame=html,
                                        msg_handler=msg_handler,
                                        comm_id=plot.comm.id)
        else:
            return html
","    def html(self, obj, fmt=None, css=None, comm=True, **kwargs):
        """"""
        Renders plot or data structure and wraps the output in HTML.
        The comm argument defines whether the HTML output includes
        code to initialize a Comm, if the plot supplies one.
        """"""
        plot, fmt =  self._validate(obj, fmt)
        figdata, _ = self(plot, fmt, **kwargs)
        if css is None: css = self.css

        if fmt in ['html', 'json']:
            return figdata
        else:
            if fmt == 'svg':
                figdata = figdata.encode(""utf-8"")
            elif fmt == 'pdf' and 'height' not in css:
                _, h = self.get_size(plot)
                css['height'] = '%dpx' % (h*self.dpi*1.15)

        if isinstance(css, dict):
            css = '; '.join(""%s: %s"" % (k, v) for k, v in css.items())
        else:
            raise ValueError(""CSS must be supplied as Python dictionary"")

        b64 = base64.b64encode(figdata).decode(""utf-8"")
        (mime_type, tag) = MIME_TYPES[fmt], HTML_TAGS[fmt]
        src = HTML_TAGS['base64'].format(mime_type=mime_type, b64=b64)
        html = tag.format(src=src, mime_type=mime_type, css=css)
        if comm and plot.comm is not None:
            comm, msg_handler = self.comms[self.mode]
            if msg_handler is None:
                return html
            msg_handler = msg_handler.format(comm_id=plot.comm.id)
            return comm.template.format(init_frame=html,
                                        msg_handler=msg_handler,
                                        comm_id=plot.comm.id)
        else:
            return html
",https://github.com/holoviz/holoviews/issues/1735,CWE-229: Improper Handling of Values,Unhandled case when msg_handler is None,holoviews/plotting/renderer.py,Renderer.html,[31],"AttributeError                            Traceback (most recent call last)
~/miniconda3/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj)
330                 pass
331             else:
--> 332                 return printer(obj)
333             # Finally look for special method names
334             method = get_real_method(obj, self.print_method)

~/miniconda3/lib/python3.6/site-packages/holoviews/ipython/display_hooks.py in pprint_display(obj)
257     if not ip.display_formatter.formatters['text/plain'].pprint:
258         return None
--> 259     return display(obj, raw=True)
260
261

~/miniconda3/lib/python3.6/site-packages/holoviews/ipython/display_hooks.py in display(obj, raw, **kwargs)
243     elif isinstance(obj, (HoloMap, DynamicMap)):
244         with option_state(obj):
--> 245             html = map_display(obj)
246     else:
247         return repr(obj) if raw else IPython.display.display(obj, **kwargs)

~/miniconda3/lib/python3.6/site-packages/holoviews/ipython/display_hooks.py in wrapped(element)
129         try:
130             html = fn(element,
--> 131                       max_frames=OutputSettings.options['max_frames'])
132
133             # Only want to add to the archive for one display hook...

~/miniconda3/lib/python3.6/site-packages/holoviews/ipython/display_hooks.py in map_display(vmap, max_frames)
198         return None
199
--> 200     return render(vmap)
201
202

~/miniconda3/lib/python3.6/site-packages/holoviews/ipython/display_hooks.py in render(obj, **kwargs)
59     if renderer.fig == 'pdf':
60         renderer = renderer.instance(fig='png')
---> 61     return renderer.html(obj, **kwargs)
62
63

~/miniconda3/lib/python3.6/site-packages/holoviews/plotting/renderer.py in html(self, obj, fmt, css, comm, **kwargs)
277         if comm and plot.comm is not None:
278             comm, msg_handler = self.comms[self.mode]
--> 279             msg_handler = msg_handler.format(comm_id=plot.comm.id)
280             return comm.template.format(init_frame=html,
281                                         msg_handler=msg_handler,

AttributeError: 'NoneType' object has no attribute 'format'

Out[13]:
:DynamicMap   []",AttributeError,buggy_snippets_files/f739c90def7505fb3be75db02e4dc3fc04622e93e2b703406422a3656e9615d8_before_merge.py,buggy_snippets_files/f739c90def7505fb3be75db02e4dc3fc04622e93e2b703406422a3656e9615d8_after_merge.py
274512,"def _wait_for_task(task, vm_name, task_type, sleep_seconds=1, log_level='debug'):
    time_counter = 0
    starttime = time.time()
    while task.info.state == 'running':
        if time_counter % sleep_seconds == 0:
            message = ""[ {0} ] Waiting for {1} task to finish [{2} s]"".format(vm_name, task_type, time_counter)
            if log_level == 'info':
                log.info(message)
            else:
                log.debug(message)
        time.sleep(1.0 - ((time.time() - starttime) % 1.0))
        time_counter += 1
    if task.info.state == 'success':
        message = ""[ {0} ] Successfully completed {1} task in {2} seconds"".format(vm_name, task_type, time_counter)
        if log_level == 'info':
            log.info(message)
        else:
            log.debug(message)
    else:
        raise Exception(task.info.error)
","def _wait_for_task(task, vm_name, task_type, sleep_seconds=1, log_level='debug'):
    time_counter = 0
    starttime = time.time()
    while task.info.state == 'running' or task.info.state == 'queued':
        if time_counter % sleep_seconds == 0:
            message = ""[ {0} ] Waiting for {1} task to finish [{2} s]"".format(vm_name, task_type, time_counter)
            if log_level == 'info':
                log.info(message)
            else:
                log.debug(message)
        time.sleep(1.0 - ((time.time() - starttime) % 1.0))
        time_counter += 1
    if task.info.state == 'success':
        message = ""[ {0} ] Successfully completed {1} task in {2} seconds"".format(vm_name, task_type, time_counter)
        if log_level == 'info':
            log.info(message)
        else:
            log.debug(message)
    else:
        raise Exception(task.info.error)
",https://github.com/saltstack/salt/issues/28511,CWE-372: Incomplete Internal State Distinction,The case when task.info.state is 'queued' is not handled properly. The program throws an exception instead of waiting for the queued task to start.,salt/cloud/clouds/vmware.py,_wait_for_task,[4],"[ERROR   ] Error creating <machine>: None
Traceback (most recent call last):
File ""/usr/lib/python2.7/dist-packages/salt/cloud/clouds/vmware.py"", line 2446, in create
_wait_for_task(task, vm_name, ""clone"", 5, 'info')
File ""/usr/lib/python2.7/dist-packages/salt/cloud/clouds/vmware.py"", line 823, in _wait_for_task
raise Exception(task.info.error)
Exception: None
[DEBUG   ] LazyLoaded nested.output
<machine>:
----------
Error:
Error creating <machine>: None",Exception,buggy_snippets_files/cfefe172033818341b30ec323d992d489a8d5efda3baaac0301a1045e663e7e2_before_merge.py,buggy_snippets_files/cfefe172033818341b30ec323d992d489a8d5efda3baaac0301a1045e663e7e2_after_merge.py
142206,"    def close(self):
        if self.conn and not self._closed:
            self.conn.shutdown(socket.SHUT_RDWR)
        self._closed = True
","    def close(self):
        if self.conn and not self._closed:
            try:
                # May already be closed despite self._closed == False if a network error occurred and `close` is being
                # called as part of cleanup.
                self.conn.shutdown(socket.SHUT_RDWR)
            except socket.error:
                pass
        self._closed = True
",https://github.com/DMOJ/judge-server/issues/685,"CWE-248: Uncaught Exception, CWE-664: Improper Control of a Resource Through its Lifetime",Attempting to self.conn.shutdown(socket.SHUT_RDWR) may throw an unhandled exception if socket is already closed due to a network error.,dmoj/packet.py,PacketManager.close,[3],"INFO 2020-05-31 22:42:05,322 56 packet Received abortion request for 2124570
ERROR: ld.so: object '/libjemalloc.so.' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ERROR: ld.so: object '/libjemalloc.so.' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ERROR 2020-05-31 22:42:10,327 56 judge Judge IPC process is still alive, sending SIGKILL!
Traceback (most recent call last):
File ""/judge/dmoj/packet.py"", line 151, in _read_forever
self._receive_packet(self._read_single())
File ""/judge/dmoj/packet.py"", line 240, in _receive_packet
self.ping_packet(packet['when'])
File ""/judge/dmoj/packet.py"", line 364, in ping_packet
self._send_packet(data)
File ""/judge/dmoj/packet.py"", line 235, in _send_packet
self.output.writelines((PacketManager.SIZE_PACK.pack(len(raw)), raw))
File ""/usr/lib/python3.8/socket.py"", line 687, in write
return self._sock.send(b)
File ""/usr/lib/python3.8/ssl.py"", line 1173, in send
return self._sslobj.write(data)
BrokenPipeError: [Errno 32] Broken pipe
Exception in thread Thread-936:
Traceback (most recent call last):
File ""/judge/dmoj/judge.py"", line 137, in _grading_thread_main
handler_func(report, *data)
File ""/judge/dmoj/judge.py"", line 161, in _ipc_compile_message
self.packet_manager.compile_message_packet(compile_message)
File ""/judge/dmoj/packet.py"", line 316, in compile_message_packet
self._send_packet(
File ""/judge/dmoj/packet.py"", line 235, in _send_packet
self.output.writelines((PacketManager.SIZE_PACK.pack(len(raw)), raw))
File ""/usr/lib/python3.8/socket.py"", line 687, in write
return self._sock.send(b)
File ""/usr/lib/python3.8/ssl.py"", line 1173, in send
return self._sslobj.write(data)
BrokenPipeError: [Errno 32] Broken pipe
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
self.run()
File ""/usr/lib/python3.8/threading.py"", line 870, in run
self._target(*self._args, **self._kwargs)
File ""/judge/dmoj/judge.py"", line 145, in _grading_thread_main
self.log_internal_error()
File ""/judge/dmoj/judge.py"", line 250, in log_internal_error
self.packet_manager.internal_error_packet(strip_ansi(message))
File ""/judge/dmoj/packet.py"", line 323, in internal_error_packet
self._send_packet(
File ""/judge/dmoj/packet.py"", line 235, in _send_packet
self.output.writelines((PacketManager.SIZE_PACK.pack(len(raw)), raw))
File ""/usr/lib/python3.8/socket.py"", line 687, in write
return self._sock.send(b)
File ""/usr/lib/python3.8/ssl.py"", line 1173, in send
return self._sslobj.write(data)
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
File ""/judge/dmoj/packet.py"", line 151, in _read_forever
self._receive_packet(self._read_single())
File ""/judge/dmoj/packet.py"", line 240, in _receive_packet
self.ping_packet(packet['when'])
File ""/judge/dmoj/packet.py"", line 364, in ping_packet
self._send_packet(data)
File ""/judge/dmoj/packet.py"", line 235, in _send_packet
self.output.writelines((PacketManager.SIZE_PACK.pack(len(raw)), raw))
File ""/usr/lib/python3.8/socket.py"", line 687, in write
return self._sock.send(b)
File ""/usr/lib/python3.8/ssl.py"", line 1173, in send
return self._sslobj.write(data)
BrokenPipeError: [Errno 32] Broken pipe
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""/judge/dmoj/judge.py"", line 564, in main
judge.listen()
File ""/judge/dmoj/judge.py"", line 219, in listen
self.packet_manager.run()
File ""/judge/dmoj/packet.py"", line 178, in run
self._read_forever()
File ""/judge/dmoj/packet.py"", line 156, in _read_forever
raise SystemExit(1)
SystemExit: 1
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""/usr/local/bin/dmoj"", line 11, in <module>
load_entry_point('dmoj', 'console_scripts', 'dmoj')()
File ""/judge/dmoj/judge.py"", line 570, in main
judge.murder()
File ""/judge/dmoj/judge.py"", line 229, in murder
self.packet_manager.close()
File ""/judge/dmoj/packet.py"", line 145, in close
self.conn.shutdown(socket.SHUT_RDWR)
File ""/usr/lib/python3.8/ssl.py"", line 1280, in shutdown
super().shutdown(how)
OSError: [Errno 107] Transport endpoint is not connected",BrokenPipeError,buggy_snippets_files/c783fb31c62af4ebc7d90cb6dccbb400cc258fe162ceb98bbe818f7b21a89ed6_before_merge.py,buggy_snippets_files/c783fb31c62af4ebc7d90cb6dccbb400cc258fe162ceb98bbe818f7b21a89ed6_after_merge.py
46853,"    @classmethod
    def write_content(cls, filename, content, rewrite_html=True):
        """"""Write content to file.""""""
        if rewrite_html:
            doc = html.document_fromstring(content)
            doc.rewrite_links(replacer)
            content = html.tostring(doc, encoding='utf8')
        else:
            content = content.encode('utf-8')

        utils.makedirs(os.path.dirname(filename))
        with open(filename, ""wb+"") as fd:
            fd.write(content)
","    @classmethod
    def write_content(cls, filename, content, rewrite_html=True):
        """"""Write content to file.""""""
        if rewrite_html:
            try:
                doc = html.document_fromstring(content)
                doc.rewrite_links(replacer)
                content = html.tostring(doc, encoding='utf8')
            except etree.ParserError:
                content = content.encode('utf-8')
        else:
            content = content.encode('utf-8')

        utils.makedirs(os.path.dirname(filename))
        with open(filename, ""wb+"") as fd:
            fd.write(content)
",https://github.com/getnikola/nikola/issues/2263,"CWE-241: Improper Handling of Unexpected Data Type, CWE-248: Uncaught Exception",Variable 'content' is fed to html parser html.document_fromstring() which will throw an unhandled exception if 'content' does not contain a proper html code.,nikola/plugins/basic_import.py,ImportMixin.write_content,[5],"$ nikola import_wordpress /Users/davidak/Downloads/davidakweblog.wordpress.2016-02-23.xml
[2016-02-23T17:06:37Z] NOTICE: Nikola: The folder new_site already exists - assuming that this is a already existing Nikola site.
Traceback (most recent call last):
File ""/usr/local/lib/python3.5/site-packages/doit/doit_cmd.py"", line 168, in run
return command.parse_execute(args)
File ""/usr/local/lib/python3.5/site-packages/doit/cmd_base.py"", line 122, in parse_execute
return self.execute(params, args)
File ""/usr/local/lib/python3.5/site-packages/nikola/plugin_categories.py"", line 124, in execute
return self._execute(options, args)
File ""/usr/local/lib/python3.5/site-packages/nikola/plugins/command/import_wordpress.py"", line 332, in _execute
self.import_posts(channel)
File ""/usr/local/lib/python3.5/site-packages/nikola/plugins/command/import_wordpress.py"", line 1015, in import_posts
self.process_item_if_post_or_page(item)
File ""/usr/local/lib/python3.5/site-packages/nikola/plugins/command/import_wordpress.py"", line 996, in process_item_if_post_or_page
out_folder_slug = self.import_postpage_item(item, wordpress_namespace, 'posts', attachments)
File ""/usr/local/lib/python3.5/site-packages/nikola/plugins/command/import_wordpress.py"", line 938, in import_postpage_item
rewrite_html)
File ""/usr/local/lib/python3.5/site-packages/nikola/plugins/basic_import.py"", line 130, in write_content
doc = html.document_fromstring(content)
File ""/usr/local/lib/python3.5/site-packages/lxml/html/__init__.py"", line 755, in document_fromstring
""Document is empty"")
lxml.etree.ParserError: Document is empty",lxml.etree.ParserError,buggy_snippets_files/4a5bcbfc9d51b6af3e5ac16a5e97ae6858002e27a42879e86f9a58d9d2c5e0a4_before_merge.py,buggy_snippets_files/4a5bcbfc9d51b6af3e5ac16a5e97ae6858002e27a42879e86f9a58d9d2c5e0a4_after_merge.py
50665,"    def _restore_or_init_optimizer(
        self,
        completed_trials: ""List[optuna.trial.FrozenTrial]"",
        search_space: Dict[str, BaseDistribution],
        ordered_keys: List[str],
    ) -> CMA:

        # Restore a previous CMA object.
        for trial in reversed(completed_trials):
            serialized_optimizer = trial.system_attrs.get(
                ""cma:optimizer"", None
            )  # type: Optional[str]
            if serialized_optimizer is None:
                continue
            return pickle.loads(bytes.fromhex(serialized_optimizer))

        # Init a CMA object.
        if self._x0 is None:
            self._x0 = _initialize_x0(search_space)

        if self._sigma0 is None:
            sigma0 = _initialize_sigma0(search_space)
        else:
            sigma0 = self._sigma0
        sigma0 = max(sigma0, _MIN_SIGMA0)
        mean = np.array([self._x0[k] for k in ordered_keys])
        bounds = _get_search_space_bound(ordered_keys, search_space)
        n_dimension = len(ordered_keys)
        return CMA(
            mean=mean,
            sigma=sigma0,
            bounds=bounds,
            seed=self._cma_rng.randint(1, 2 ** 32),
            n_max_resampling=10 * n_dimension,
        )","    def _restore_or_init_optimizer(
        self,
        completed_trials: ""List[optuna.trial.FrozenTrial]"",
        search_space: Dict[str, BaseDistribution],
        ordered_keys: List[str],
    ) -> CMA:

        # Restore a previous CMA object.
        for trial in reversed(completed_trials):
            serialized_optimizer = trial.system_attrs.get(
                ""cma:optimizer"", None
            )  # type: Optional[str]
            if serialized_optimizer is None:
                continue
            return pickle.loads(bytes.fromhex(serialized_optimizer))

        # Init a CMA object.
        if self._x0 is None:
            self._x0 = _initialize_x0(search_space)

        if self._sigma0 is None:
            sigma0 = _initialize_sigma0(search_space)
        else:
            sigma0 = self._sigma0
        sigma0 = max(sigma0, _MIN_SIGMA0)
        mean = np.array([self._x0[k] for k in ordered_keys])
        bounds = _get_search_space_bound(ordered_keys, search_space)
        n_dimension = len(ordered_keys)
        return CMA(
            mean=mean,
            sigma=sigma0,
            bounds=bounds,
            seed=self._cma_rng.randint(1, 2 ** 31 - 2),
            n_max_resampling=10 * n_dimension,
        )",https://github.com/optuna/optuna/issues/1230,"CWE-190: Integer Overflow or Wraparound, CWE-196: Unsigned to Signed Conversion Error","randint() takes only int32 as arguments, but the code attempts to process 2**32 which is larger than maximum value of 2**31-1",optuna/samplers/cmaes.py,CmaEsSampler._restore_or_init_optimizer,[33],"Traceback (most recent call last):

File ""C:\Users\User\work\untitled0.py"", line 10, in <module>
study.optimize(objective, n_trials=20)

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\study.py"", line 334, in optimize
func, n_trials, timeout, catch, callbacks, gc_after_trial, None

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\study.py"", line 648, in _optimize_sequential
self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\study.py"", line 678, in _run_trial_and_callbacks
trial = self._run_trial(func, catch, gc_after_trial)

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\study.py"", line 695, in _run_trial
trial = trial_module.Trial(self, trial_id)

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\trial.py"", line 409, in __init__
self._init_relative_params()

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\trial.py"", line 420, in _init_relative_params
self.study, trial, self.relative_search_space

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\samplers\cmaes.py"", line 175, in sample_relative
optimizer = self._restore_or_init_optimizer(completed_trials, search_space, ordered_keys)

File ""C:\Users\User\Anaconda3\envs\base_clone\lib\site-packages\optuna\samplers\cmaes.py"", line 251, in _restore_or_init_optimizer
seed=self._cma_rng.randint(1, 2 ** 32),

File ""mtrand.pyx"", line 745, in numpy.random.mtrand.RandomState.randint

File ""_bounded_integers.pyx"", line 1360, in numpy.random._bounded_integers._rand_int32

ValueError: high is out of bounds for int32",ValueError,buggy_snippets_files/76f6060283adc3ab5cceb6584abe7f2c6295fa75739397fbf55995ec9017c373_before_merge.py,buggy_snippets_files/76f6060283adc3ab5cceb6584abe7f2c6295fa75739397fbf55995ec9017c373_after_merge.py
401713,"def setup_vpn(runner: Runner, args):
    runner.require([""sshuttle-telepresence""],
                   ""Part of the Telepresence package. Try reinstalling."")
    if runner.platform == ""linux"":
        # Need conntrack for sshuttle on Linux:
        runner.require([""conntrack"", ""iptables""],
                       ""Required for the vpn-tcp method"")
    if runner.platform == ""darwin"":
        runner.require([""pfctl""], ""Required for the vpn-tcp method"")
    runner.require_sudo()
    if runner.chatty:
        runner.show(
            ""Starting proxy with method 'vpn-tcp', which has the following ""
            ""limitations: All processes are affected, only one telepresence ""
            ""can run per machine, and you can't use other VPNs. You may need ""
            ""to add cloud hosts and headless services with --also-proxy. For ""
            ""a full list of method limitations see ""
            ""https://telepresence.io/reference/methods.html""
        )
    command = args.run or [""bash"" ""--norc""]

    def launch(runner_, remote_info, env, _socks_port, ssh, _mount_dir):
        return launch_vpn(
            runner_, remote_info, command, args.also_proxy, env, ssh
        )

    return launch
","def setup_vpn(runner: Runner, args):
    runner.require([""sshuttle-telepresence""],
                   ""Part of the Telepresence package. Try reinstalling."")
    if runner.platform == ""linux"":
        # Need conntrack for sshuttle on Linux:
        runner.require([""conntrack"", ""iptables""],
                       ""Required for the vpn-tcp method"")
    if runner.platform == ""darwin"":
        runner.require([""pfctl""], ""Required for the vpn-tcp method"")
    runner.require_sudo()
    if runner.chatty:
        runner.show(
            ""Starting proxy with method 'vpn-tcp', which has the following ""
            ""limitations: All processes are affected, only one telepresence ""
            ""can run per machine, and you can't use other VPNs. You may need ""
            ""to add cloud hosts and headless services with --also-proxy. For ""
            ""a full list of method limitations see ""
            ""https://telepresence.io/reference/methods.html""
        )
    command = args.run or [""bash"", ""--norc""]

    def launch(runner_, remote_info, env, _socks_port, ssh, _mount_dir):
        return launch_vpn(
            runner_, remote_info, command, args.also_proxy, env, ssh
        )

    return launch
",https://github.com/telepresenceio/telepresence/issues/738,CWE-133: String Errors,Missing comma gives the output 'bash--norc' instead of 'bash --norc',telepresence/outbound/__init__.py,setup_vpn,[20],"Traceback (most recent call last):
File ""/usr/bin/telepresence/telepresence/cli.py"", line 129, in crash_reporting
yield
File ""/usr/bin/telepresence/telepresence/main.py"", line 78, in main
runner, remote_info, env, socks_port, ssh, mount_dir
File ""/usr/bin/telepresence/telepresence/outbound/__init__.py"", line 60, in launch
runner_, remote_info, command, args.also_proxy, env, ssh
File ""/usr/bin/telepresence/telepresence/outbound/local.py"", line 123, in launch_vpn
process = Popen(command, env=env)
File ""/usr/lib64/python3.6/subprocess.py"", line 709, in __init__
restore_signals, start_new_session)
File ""/usr/lib64/python3.6/subprocess.py"", line 1344, in _execute_child
raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'bash--norc': 'bash--norc'",FileNotFoundError,buggy_snippets_files/bbda0d33102373f15f7aa55c725a57c4a3da9b1808ac6b855f8cd60a33eb7081_before_merge.py,buggy_snippets_files/bbda0d33102373f15f7aa55c725a57c4a3da9b1808ac6b855f8cd60a33eb7081_after_merge.py
375505,"    @classmethod
    def from_line(cls, line):
        """"""Create a Log Line from a string line.

        :param line:
        :type line: str
        :return:
        :rtype: LogLine or None
        """"""
        lines = line.split('\n')
        match = LogLine.log_re.match(lines[0])
        if not match:
            return

        g = match.groupdict()
        return LogLine(line=lines[0], message=g['message'], level_name=g['level_name'], extra=g.get('extra'), curhash=g['curhash'],
                       thread_name=g['thread_name'], thread_id=int(g['thread_id']) if g['thread_id'] else None, traceback_lines=lines[1:],
                       timestamp=datetime.datetime(year=int(g['year']), month=int(g['month']), day=int(g['day']),
                                                   hour=int(g['hour']), minute=int(g['minute']), second=int(g['second'])))
","    @classmethod
    def from_line(cls, line):
        """"""Create a Log Line from a string line.

        :param line:
        :type line: str
        :return:
        :rtype: LogLine or None
        """"""
        lines = line.split(b'\n')
        match = LogLine.log_re.match(lines[0])
        if not match:
            return

        g = match.groupdict()
        return LogLine(line=lines[0], message=g['message'], level_name=g['level_name'], extra=g.get('extra'), curhash=g['curhash'],
                       thread_name=g['thread_name'], thread_id=int(g['thread_id']) if g['thread_id'] else None, traceback_lines=lines[1:],
                       timestamp=datetime.datetime(year=int(g['year']), month=int(g['month']), day=int(g['day']),
                                                   hour=int(g['hour']), minute=int(g['minute']), second=int(g['second'])))
",https://github.com/pymedusa/Medusa/issues/1236,CWE-351: Insufficient Type Distinction,"Variable 'line' is a byte literal, therefore, split() should also use byte literal as an argument",medusa/logger.py,LogLine.from_line,[10],"2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Five set to: [ the five uk, the five ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Strong set to: [ strong ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Tomorrow When the War Began set to: [ tomorrow when the war began ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for BrainDead set to: [ braindead ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Southern Justice set to: [ southern justice ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Returned (US) set to: [ the returned us ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Path set to: [ the path ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Submission set to: [ submission ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Coupled set to: [ coupled ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Seven Year Switch (AU) set to: [ seven year switch au ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Person of Interest set to: [ person of interest ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Animal Kingdom (2016) set to: [ animal kingdom us, animal kingdom 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Lady Dynamite set to: [ lady dynamite ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Still the King set to: [ still the king ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Durrells set to: [ the durrells ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Wrecked (2016) set to: [ wrecked, wrecked 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Dead of Summer set to: [ dead of summer ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Guilt set to: [ guilt ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Secret City set to: [ secret city ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Barbarians Rising set to: [ barbarians rising ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Shooter set to: [ shooter ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Face Off set to: [ face off ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Separation Anxiety set to: [ separation anxiety ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Top Gear (US) set to: [ top gear us ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Las Vegas Law (2016) set to: [ las vegas law 2016, las vegas law ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Private Eyes set to: [ private eyes ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Spartan: Ultimate Team Challenge set to: [ spartan ultimate team challenge ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Sex Factor set to: [ the sex factor 2016, the sex factor, sex factor ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Timeless (2016) set to: [ timeless 2016, timeless ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for MacGyver (2016) set to: [ macgyver 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Frequency set to: [ frequency ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Good Place set to: [ the good place ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for This Is Us set to: [ this is us ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for American Gods set to: [ american gods ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Conviction (2016) set to: [ conviction 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Quarry set to: [ quarry ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Amazing Race Canada set to: [ the amazing race canada ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for The Night Of set to: [ the night of ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Aftermath (2016) set to: [ aftermath 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Better Things set to: [ better things ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Goliath set to: [ goliath ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for People of Earth set to: [ people of earth ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for StartUp (2016) set to: [ startup 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Chance set to: [ chance ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Chesapeake Shores set to: [ chesapeake shores ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Lethal Weapon set to: [ lethal weapon ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Pure Genius set to: [ pure genius ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for No Tomorrow set to: [ no tomorrow ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Kevin Can Wait set to: [ kevin can wait ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Falling Water set to: [ falling water ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Speechless set to: [ speechless ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Pitch set to: [ pitch ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for One Of Us set to: [ one of us ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Notorious (2016) set to: [ notorious 2016, notorious ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Man With A Plan set to: [ man with a plan ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Vice Principals set to: [ vice principals ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Fleabag set to: [ fleabag ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Mickey Mouse Clubhouse set to: [ mickey mouse clubhouse ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Divorce (2016) set to: [ divorce 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for American Housewife set to: [ american housewife ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Easy set to: [ easy ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Chicago Justice set to: [ chicago justice ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Designated Survivor set to: [ designated survivor ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Bull (2016) set to: [ bull 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Making History (2016) set to: [ making history 2016 ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Internal name cache for Big Brother: Over the Top set to: [ big brother over the top ]
2016-10-09 07:41:58 DEBUG    MAIN :: [61b6052] Finished updating network timezones
2016-10-09 07:41:59 DEBUG    CHECKVERSION :: [61b6052] ""C:\Users\rusty\AppData\Local\GitHub\PortableGit_284a859b0e6deba86edc624fef1e4db2aa8241a9\cmd\git.exe"" fetch origin : returned successful
2016-10-09 07:41:59 DEBUG    CHECKVERSION :: [61b6052] Executing ""C:\Users\rusty\AppData\Local\GitHub\PortableGit_284a859b0e6deba86edc624fef1e4db2aa8241a9\cmd\git.exe"" rev-parse --verify --quiet ""@{upstream}"" with your shell in C:\pymedusaSickRage
2016-10-09 07:41:59 DEBUG    CHECKVERSION :: [61b6052] ""C:\Users\rusty\AppData\Local\GitHub\PortableGit_284a859b0e6deba86edc624fef1e4db2aa8241a9\cmd\git.exe"" rev-parse --verify --quiet ""@{upstream}"" : returned successful
2016-10-09 07:41:59 DEBUG    CHECKVERSION :: [61b6052] Executing ""C:\Users\rusty\AppData\Local\GitHub\PortableGit_284a859b0e6deba86edc624fef1e4db2aa8241a9\cmd\git.exe"" rev-list --left-right ""@{upstream}""...HEAD with your shell in C:\pymedusaSickRage
2016-10-09 07:41:59 DEBUG    CHECKVERSION :: [61b6052] ""C:\Users\rusty\AppData\Local\GitHub\PortableGit_284a859b0e6deba86edc624fef1e4db2aa8241a9\cmd\git.exe"" rev-list --left-right ""@{upstream}""...HEAD : returned successful
2016-10-09 07:41:59 DEBUG    CHECKVERSION :: [61b6052] cur_commit = 61b60522b98450f541a0f9c4b57c8242338432ba, newest_commit = 61b60522b98450f541a0f9c4b57c8242338432ba, num_commits_behind = 0, num_commits_ahead = 0
2016-10-09 07:41:59 DEBUG    CHECKVERSION :: [61b6052] check_for_new_news: Checking GitHub for latest news.
2016-10-09 07:42:10 ERROR    Thread-18 :: [61b6052] Failed doing web ui callback: Traceback (most recent call last):
  File ""C:\pymedusaSickRage\medusa\server\web\core\base.py"", line 270, in async_call
    result = function(**kwargs)
  File ""C:\pymedusaSickRage\medusa\server\web\core\error_logs.py"", line 133, in viewlog
    predicate=lambda l: filter_logline(l, min_level=min_level,
  File ""C:\pymedusaSickRage\medusa\logger.py"", line 142, in read_loglines
    logline = LogLine.from_line(line)
  File ""C:\pymedusaSickRage\medusa\logger.py"", line 375, in from_line
    lines = line.split('\n')
  File ""C:\Python27\lib\encodings\cp1252.py"", line 15, in decode
    return codecs.charmap_decode(input,errors,decoding_table)
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 106: character maps to <undefined>
Traceback (most recent call last):
  File ""C:\pymedusaSickRage\medusa\server\web\core\base.py"", line 270, in async_call
    result = function(**kwargs)
  File ""C:\pymedusaSickRage\medusa\server\web\core\error_logs.py"", line 133, in viewlog
    predicate=lambda l: filter_logline(l, min_level=min_level,
  File ""C:\pymedusaSickRage\medusa\logger.py"", line 142, in read_loglines
    logline = LogLine.from_line(line)
  File ""C:\pymedusaSickRage\medusa\logger.py"", line 375, in from_line
    lines = line.split('\n')
  File ""C:\Python27\lib\encodings\cp1252.py"", line 15, in decode
    return codecs.charmap_decode(input,errors,decoding_table)
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 106: character maps to <undefined>",UnicodeDecodeError,buggy_snippets_files/2c8beeb9fc0f2e57a195007d106f4b05b4676c3e5aa0025054d0f9246f30f7fa_before_merge.py,buggy_snippets_files/2c8beeb9fc0f2e57a195007d106f4b05b4676c3e5aa0025054d0f9246f30f7fa_after_merge.py
27161,"    def get_ticker_history(self, pair: str, tick_interval: int) -> List[Dict]:
        if tick_interval == 1:
            interval = 'oneMin'
        elif tick_interval == 5:
            interval = 'fiveMin'
        else:
            raise ValueError('Cannot parse tick_interval: {}'.format(tick_interval))

        data = _API_V2.get_candles(pair.replace('_', '-'), interval)
        # This sanity check is necessary because bittrex returns nonsense sometimes
        for prop in ['C', 'V', 'O', 'H', 'L', 'T']:
            for tick in data['result']:
                if prop not in tick.keys():
                    logger.warning('Required property %s not present in response', prop)
                    return []

        if not data['success']:
            raise RuntimeError('{message} params=({pair})'.format(
                message=data['message'],
                pair=pair))

        return data['result']
","    def get_ticker_history(self, pair: str, tick_interval: int) -> List[Dict]:
        if tick_interval == 1:
            interval = 'oneMin'
        elif tick_interval == 5:
            interval = 'fiveMin'
        else:
            raise ValueError('Cannot parse tick_interval: {}'.format(tick_interval))

        data = _API_V2.get_candles(pair.replace('_', '-'), interval)

        # These sanity check are necessary because bittrex cannot keep their API stable.
        if not data.get('result'):
            return []

        for prop in ['C', 'V', 'O', 'H', 'L', 'T']:
            for tick in data['result']:
                if prop not in tick.keys():
                    logger.warning('Required property %s not present in response', prop)
                    return []

        if not data['success']:
            raise RuntimeError('{message} params=({pair})'.format(
                message=data['message'],
                pair=pair))

        return data['result']
",https://github.com/freqtrade/freqtrade/issues/103,"CWE-230: Improper Handling of Missing Values, CWE-248: Uncaught Exception",Unhandled case where the external API function _API_V2.get_candles()  may return an empty value.,freqtrade/exchange/bittrex.py,Bittrex.get_ticker_history,[12],"2017-11-15 00:48:38,788 - freqtrade - INFO - Checked all whitelisted currencies. Found no suitable entry positions for buying. Will keep looking ...
2017-11-15 00:48:48,267 - freqtrade - INFO - Checking buy signals to create a new trade with stake_amount: 0.050000 ...
Traceback (most recent call last):
File ""./freqtrade/main.py"", line 361, in <module>
main()
File ""./freqtrade/main.py"", line 355, in main
dynamic_whitelist=args.dynamic_whitelist,
File ""/Users/sgrimm/dev/cryptos/bots/freqtrade/freqtrade/misc.py"", line 52, in throttle
result = func(*args, **kwargs)
File ""./freqtrade/main.py"", line 70, in _process
trade = create_trade(float(_CONF['stake_amount']))
File ""./freqtrade/main.py"", line 223, in create_trade
if get_buy_signal(_pair):
File ""/Users/sgrimm/dev/cryptos/bots/freqtrade/freqtrade/analyze.py"", line 99, in get_buy_signal
dataframe = analyze_ticker(pair)
File ""/Users/sgrimm/dev/cryptos/bots/freqtrade/freqtrade/analyze.py"", line 82, in analyze_ticker
ticker_hist = get_ticker_history(pair)
File ""/Users/sgrimm/dev/cryptos/bots/freqtrade/.env/lib/python3.6/site-packages/cachetools/__init__.py"", line 46, in wrapper
v = func(*args, **kwargs)
File ""/Users/sgrimm/dev/cryptos/bots/freqtrade/freqtrade/exchange/__init__.py"", line 133, in get_ticker_history
return _API.get_ticker_history(pair, tick_interval)
File ""/Users/sgrimm/dev/cryptos/bots/freqtrade/freqtrade/exchange/bittrex.py"", line 106, in get_ticker_history
for tick in data['result']:
TypeError: 'NoneType' object is not iterable",TypeError,buggy_snippets_files/4b3e218fd50eef20cd3fcb5d6b38615eb48cec9eef4c2965744377f6855971b0_before_merge.py,buggy_snippets_files/4b3e218fd50eef20cd3fcb5d6b38615eb48cec9eef4c2965744377f6855971b0_after_merge.py
123028,"    def validateMasterArgument(self, master_arg):
        """"""
        Parse the <master> argument.

        @param master_arg: the <master> argument to parse

        @return: tuple of master's host and port
        @raise UsageError: on errors parsing the argument
        """"""
        if master_arg[:5] == ""http:"":
            raise usage.UsageError(""<master> is not a URL - do not use URL"")

        if "":"" not in master_arg:
            master = master_arg
            port = 9989
        else:
            master, port = master_arg.split("":"")

        if not master:
            raise usage.UsageError(""invalid <master> argument '{}'"".format(
                                   master_arg))
        try:
            port = int(port)
        except ValueError:
            raise usage.UsageError(""invalid master port '{}', ""
                                   ""needs to be a number"".format(port))

        return master, port
","    def validateMasterArgument(self, master_arg):
        """"""
        Parse the <master> argument.

        @param master_arg: the <master> argument to parse

        @return: tuple of master's host and port
        @raise UsageError: on errors parsing the argument
        """"""
        if master_arg[:5] == ""http:"":
            raise usage.UsageError(""<master> is not a URL - do not use URL"")

        if master_arg.startswith(""["") and ""]"" in master_arg:
            # detect ipv6 address with format [2001:1:2:3:4::1]:4321
            master, port_tmp = master_arg.split(""]"")
            master = master[1:]
            if "":"" not in port_tmp:
                port = 9989
            else:
                port = port_tmp.split("":"")[1]

        elif "":"" not in master_arg:
            master = master_arg
            port = 9989
        else:
            try:
                master, port = master_arg.split("":"")
            except ValueError:
                raise usage.UsageError(""invalid <master> argument '{}', ""
                                       ""if it is an ipv6 address, it must be enclosed by []"".format(master_arg))

        if not master:
            raise usage.UsageError(""invalid <master> argument '{}'"".format(
                                   master_arg))
        try:
            port = int(port)
        except ValueError:
            raise usage.UsageError(""invalid master port '{}', ""
                                   ""needs to be a number"".format(port))

        return master, port
",https://github.com/buildbot/buildbot/issues/5155,CWE-628: Function Call with Incorrectly Specified Arguments,the situation with the use of ipv6 addresses in the argument was not handled,worker/buildbot_worker/scripts/runner.py,CreateWorkerOptions.validateMasterArgument,[15],"Traceback (most recent call last):
File ""/usr/bin/buildbot-worker"", line 11, in <module>
load_entry_point('buildbot-worker==2.0.1.post2', 'console_scripts', 'buildbot-worker')()
File ""/usr/lib/python3/dist-packages/buildbot_worker/scripts/runner.py"", line 238, in run
config.parseOptions()
File ""/usr/lib/python3/dist-packages/twisted/python/usage.py"", line 267, in parseOptions
self.subOptions.parseOptions(rest)
File ""/usr/lib/python3/dist-packages/twisted/python/usage.py"", line 273, in parseOptions
self.parseArgs(*args)
File ""/usr/lib/python3/dist-packages/buildbot_worker/scripts/runner.py"", line 183, in parseArgs
self['host'], self['port'] = self.validateMasterArgument(master)
File ""/usr/lib/python3/dist-packages/buildbot_worker/scripts/runner.py"", line 161, in validateMasterArgument
master, port = master_arg.split("":"")
ValueError: too many values to unpack (expected 2)",ValueError,buggy_snippets_files/409f632953fc2ed5d7a14ba2ecc7985ee17730e29c5686907536bf86a31c9490_before_merge.py,buggy_snippets_files/409f632953fc2ed5d7a14ba2ecc7985ee17730e29c5686907536bf86a31c9490_after_merge.py
182807,"def highest_precedence_type(exprs):
    # Return the highest precedence type from the passed expressions. Also
    # verifies that there are valid implicit casts between any of the types and
    # the selected highest precedence type
    selector = _TypePrecedence(exprs)
    return selector.get_result()
","def highest_precedence_type(exprs):
    # Return the highest precedence type from the passed expressions. Also
    # verifies that there are valid implicit casts between any of the types and
    # the selected highest precedence type
    if not exprs:
        raise ValueError('Must pass at least one expression')

    type_counts = Counter(expr.type() for expr in exprs)
    scores = (
        (_TYPE_PRECEDENCE[k.name.lower()], k) for k, v in type_counts.items()
    )
    _, highest_type = max(scores, key=first)

    for expr in exprs:
        if not expr._can_cast_implicit(highest_type):
            raise TypeError(
                'Expression with type {0} cannot be implicitly casted to {1}'
                .format(expr.type(), highest_type)
            )

    return highest_type
",https://github.com/ibis-project/ibis/issues/894,CWE-116: Improper Encoding or Escaping of Output,Missing key in self._precedence variable,ibis/expr/rules.py,highest_precedence_type,[6],"ta.group_by('uuid').aggregate(min_date=ta.ts.min(where=ta.search_level==1)).compile()

KeyError                                  Traceback (most recent call last)
<ipython-input-70-ec191fa92eab> in <module>()
----> 1 ta.group_by('uuid').aggregate(min_date=ta.ts.min(where=ta.search_level==1)).compile()

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/types.py in compile(self, limit)
155         """"""
156         from ibis.client import compile
--> 157         return compile(self, limit=limit)
158
159     def verify(self):

/home/pteehan/.local/lib/python3.5/site-packages/ibis/client.py in compile(expr, limit)
317 def compile(expr, limit=None):
318     backend = find_backend(expr)
--> 319     return backend.compile(expr, limit=limit)
320
321

/home/pteehan/.local/lib/python3.5/site-packages/ibis/client.py in compile(self, expr, params, limit)
243         """"""
244         ast = self._build_ast_ensure_limit(expr, limit)
--> 245         queries = [query.compile() for query in ast.queries]
246         return queries[0] if len(queries) == 1 else queries
247

/home/pteehan/.local/lib/python3.5/site-packages/ibis/client.py in <listcomp>(.0)
243         """"""
244         ast = self._build_ast_ensure_limit(expr, limit)
--> 245         queries = [query.compile() for query in ast.queries]
246         return queries[0] if len(queries) == 1 else queries
247

/home/pteehan/.local/lib/python3.5/site-packages/ibis/impala/compiler.py in compile(self)
99
100         # SELECT
--> 101         select_frag = self.format_select_set()
102
103         # FROM, JOIN, UNION

/home/pteehan/.local/lib/python3.5/site-packages/ibis/impala/compiler.py in format_select_set(self)
143         for expr in self.select_set:
144             if isinstance(expr, ir.ValueExpr):
--> 145                 expr_str = self._translate(expr, named=True)
146             elif isinstance(expr, ir.TableExpr):
147                 # A * selection, possibly prefixed

/home/pteehan/.local/lib/python3.5/site-packages/ibis/sql/compiler.py in _translate(self, expr, context, named, permit_subquery)
1302                                      named=named,
1303                                      permit_subquery=permit_subquery)
-> 1304         return translator.get_result()
1305
1306     def equals(self, other):

/home/pteehan/.local/lib/python3.5/site-packages/ibis/sql/compiler.py in get_result(self)
1090         Build compiled SQL expression from the bottom up and return as a string
1091         """"""
-> 1092         translated = self.translate(self.expr)
1093         if self._needs_name(self.expr):
1094             # TODO: this could fail in various ways

/home/pteehan/.local/lib/python3.5/site-packages/ibis/sql/compiler.py in translate(self, expr)
1129         elif type(op) in self._registry:
1130             formatter = self._registry[type(op)]
-> 1131             return formatter(self, expr)
1132         else:
1133             raise com.TranslationError('No translator rule for {0}'

/home/pteehan/.local/lib/python3.5/site-packages/ibis/impala/compiler.py in formatter(translator, expr)
654         arg, where = op.args[:2]
655
--> 656         return _reduction_format(translator, func_name, arg, where)
657     return formatter
658

/home/pteehan/.local/lib/python3.5/site-packages/ibis/impala/compiler.py in _reduction_format(translator, func_name, arg, where)
639 def _reduction_format(translator, func_name, arg, where):
640     if where is not None:
--> 641         case = where.ifelse(arg, ibis.NA)
642         arg = translator.translate(case)
643     else:

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/api.py in ifelse(arg, true_expr, false_expr)
1166     # must be used.
1167     case = _ops.SearchedCaseBuilder()
-> 1168     return case.when(arg, true_expr).else_(false_expr).end()
1169
1170

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/operations.py in end(self)
1272
1273         op = SearchedCase(self.cases, self.results, default)
-> 1274         return op.to_expr()
1275
1276

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/types.py in to_expr(self)
272
273     def to_expr(self):
--> 274         klass = self.output_type()
275         return klass(self)
276

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/operations.py in output_type(self)
1320         cases, results, default = self.args
1321         out_exprs = results + [default]
-> 1322         typename = rules.highest_precedence_type(out_exprs)
1323         return rules.shape_like_args(cases, typename)
1324

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/rules.py in highest_precedence_type(exprs)
114     # the selected highest precedence type
115     selector = _TypePrecedence(exprs)
--> 116     return selector.get_result()
117
118

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/rules.py in get_result(self)
153
154     def get_result(self):
--> 155         highest_type = self._get_highest_type()
156         self._check_casts(highest_type)
157         return highest_type

/home/pteehan/.local/lib/python3.5/site-packages/ibis/expr/rules.py in _get_highest_type(self)
166             if not v:
167                 continue
--> 168             score = self._precedence[k.name()]
169
170             scores.append((score, k))

KeyError: 'timestamp'",KeyError,buggy_snippets_files/5ef74011245274fd3116852055757ad997eb32dac77223a43a7698396993e0cd_before_merge.py,buggy_snippets_files/5ef74011245274fd3116852055757ad997eb32dac77223a43a7698396993e0cd_after_merge.py
262744,"def list_nodes(full=False, call=None):
    '''
    list of nodes, keeping only a brief listing

    CLI Example:

    .. code-block:: bash

        salt-cloud -Q
    '''
    if call == 'action':
        raise SaltCloudSystemExit(
            'The list_nodes function must be called with -f or --function.'
        )

    ret = {}
    if POLL_ALL_LOCATIONS:
        for location in JOYENT_LOCATIONS:
            result = query(command='my/machines', location=location,
                           method='GET')
            if result[0] in VALID_RESPONSE_CODES:
                nodes = result[1]
                for node in nodes:
                    if 'name' in node:
                        node['location'] = location
                        ret[node['name']] = reformat_node(item=node, full=full)
            else:
                log.error('Invalid response when listing Joyent nodes: {0}'.format(result[1]))

    else:
        result = query(command='my/machines', location=DEFAULT_LOCATION,
                       method='GET')
        nodes = result[1]
        for node in nodes:
            if 'name' in node:
                node['location'] = DEFAULT_LOCATION
                ret[node['name']] = reformat_node(item=node, full=full)
    return ret
","def list_nodes(full=False, call=None):
    '''
    list of nodes, keeping only a brief listing

    CLI Example:

    .. code-block:: bash

        salt-cloud -Q
    '''
    if call == 'action':
        raise SaltCloudSystemExit(
            'The list_nodes function must be called with -f or --function.'
        )

    ret = {}
    if POLL_ALL_LOCATIONS:
        for location in JOYENT_LOCATIONS:
            result = query(command='my/machines', location=location,
                           method='GET')
            if result[0] in VALID_RESPONSE_CODES:
                nodes = result[1]
                for node in nodes:
                    if 'name' in node:
                        node['location'] = location
                        ret[node['name']] = reformat_node(item=node, full=full)
            else:
                log.error('Invalid response when listing Joyent nodes: {0}'.format(result[1]))

    else:
        location = get_location()
        result = query(command='my/machines', location=location,
                       method='GET')
        nodes = result[1]
        for node in nodes:
            if 'name' in node:
                node['location'] = location
                ret[node['name']] = reformat_node(item=node, full=full)
    return ret
",https://github.com/saltstack/salt/issues/37077,CWE-573: Improper Following of Specification by Caller,Location parameter defined in the cloud provider config was ignored in the list_nodes function. This caused an issue when querying a node after creation.,salt/cloud/clouds/joyent.py,list_nodes,[32],"sudo salt-cloud -p sdc-linuxkvm-2G triton-test11 -l debug
[DEBUG   ] Reading configuration from /etc/salt/cloud
[DEBUG   ] Reading configuration from /etc/salt/master
[DEBUG   ] Including configuration from '/etc/salt/master.d/reactor.conf'
[DEBUG   ] Reading configuration from /etc/salt/master.d/reactor.conf
[DEBUG   ] Missing configuration file: /etc/salt/cloud.providers
[DEBUG   ] Including configuration from '/etc/salt/cloud.providers.d/triton.conf'
[DEBUG   ] Reading configuration from /etc/salt/cloud.providers.d/triton.conf
[DEBUG   ] Missing configuration file: /etc/salt/cloud.profiles
[DEBUG   ] Including configuration from '/etc/salt/cloud.profiles.d/localcloud-triton.conf'
[DEBUG   ] Reading configuration from /etc/salt/cloud.profiles.d/localcloud-triton.conf
[DEBUG   ] Configuration file path: /etc/salt/cloud
[WARNING ] Insecure logging configuration detected! Sensitive data may be logged.
[INFO    ] salt-cloud starting
[DEBUG   ] Could not LazyLoad parallels.avail_sizes
[DEBUG   ] LazyLoaded parallels.avail_locations
[DEBUG   ] LazyLoaded proxmox.avail_sizes
[DEBUG   ] Could not LazyLoad saltify.destroy
[DEBUG   ] Could not LazyLoad saltify.avail_sizes
[DEBUG   ] Could not LazyLoad saltify.avail_images
[DEBUG   ] Could not LazyLoad saltify.avail_locations
[DEBUG   ] LazyLoaded rackspace.reboot
[DEBUG   ] LazyLoaded openstack.list_locations
[DEBUG   ] LazyLoaded rackspace.list_locations
[DEBUG   ] Could not LazyLoad vmware.optimize_providers
[DEBUG   ] The 'vmware' cloud driver is unable to be optimized.
[DEBUG   ] Could not LazyLoad joyent.optimize_providers
[DEBUG   ] The 'joyent' cloud driver is unable to be optimized.
[DEBUG   ] Could not LazyLoad parallels.avail_sizes
[DEBUG   ] LazyLoaded parallels.avail_locations
[DEBUG   ] LazyLoaded proxmox.avail_sizes
[DEBUG   ] Could not LazyLoad saltify.destroy
[DEBUG   ] Could not LazyLoad saltify.avail_sizes
[DEBUG   ] Could not LazyLoad saltify.avail_images
[DEBUG   ] Could not LazyLoad saltify.avail_locations
[DEBUG   ] LazyLoaded rackspace.reboot
[DEBUG   ] LazyLoaded openstack.list_locations
[DEBUG   ] LazyLoaded rackspace.list_locations
[DEBUG   ] User: 'user' on PATH: https://us-east-1.api.somehost.com/my/machines
[DEBUG   ] Could not LazyLoad parallels.avail_sizes
[DEBUG   ] LazyLoaded parallels.avail_locations
[DEBUG   ] LazyLoaded proxmox.avail_sizes
[DEBUG   ] Requesting URL https://us-east-1.api.somehost.com/my/machines using GET method
[DEBUG   ] Could not LazyLoad saltify.destroy
[DEBUG   ] Could not LazyLoad saltify.avail_sizes
[DEBUG   ] Could not LazyLoad saltify.avail_images
[DEBUG   ] Could not LazyLoad saltify.avail_locations
[DEBUG   ] LazyLoaded rackspace.reboot
[DEBUG   ] LazyLoaded openstack.list_locations
[DEBUG   ] LazyLoaded rackspace.list_locations
[DEBUG   ] Failed to execute 'joyent.list_nodes()' while querying for running nodes: [Errno -2] Name or service not known
Traceback (most recent call last):
File ""/usr/lib/python2.7/site-packages/salt/cloud/__init__.py"", line 2364, in run_parallel_map_providers_query
cloud.clouds[data['fun']]()
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 737, in list_nodes
method='GET')
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 1092, in query
opts=__opts__,
File ""/usr/lib/python2.7/site-packages/salt/utils/http.py"", line 484, in query
**req_kwargs
File ""/usr/lib64/python2.7/site-packages/tornado/httpclient.py"", line 102, in fetch
self._async_client.fetch, request, **kwargs))
File ""/usr/lib64/python2.7/site-packages/tornado/ioloop.py"", line 444, in run_sync
return future_cell[0].result()
File ""/usr/lib64/python2.7/site-packages/tornado/concurrent.py"", line 214, in result
raise_exc_info(self._exc_info)
File ""<string>"", line 3, in raise_exc_info
gaierror: [Errno -2] Name or service not known
[DEBUG   ] Could not LazyLoad parallels.avail_sizes
[DEBUG   ] LazyLoaded parallels.avail_locations
[DEBUG   ] LazyLoaded proxmox.avail_sizes
[DEBUG   ] Could not LazyLoad saltify.destroy
[DEBUG   ] Could not LazyLoad saltify.avail_sizes
[DEBUG   ] Could not LazyLoad saltify.avail_images
[WARNING ] /usr/lib/python2.7/site-packages/urllib3/connectionpool.py:769: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
InsecureRequestWarning)

[DEBUG   ] Could not LazyLoad saltify.avail_locations
[DEBUG   ] LazyLoaded rackspace.reboot
[DEBUG   ] LazyLoaded openstack.list_locations
[DEBUG   ] LazyLoaded rackspace.list_locations
[DEBUG   ] Generating minion keys for 'triton-test11'
[DEBUG   ] MasterEvent PUB socket URI: /var/run/salt/master/master_event_pub.ipc
[DEBUG   ] MasterEvent PULL socket URI: /var/run/salt/master/master_event_pull.ipc
[DEBUG   ] Initializing new IPCClient for path: /var/run/salt/master/master_event_pull.ipc
[DEBUG   ] Sending event - data = {'profile': 'sdc-linuxkvm-2G', 'event': 'starting create', '_stamp': '2016-10-18T15:46:26.606112', 'name': 'triton-test11', 'provider': 'triton:joyent'}
[INFO    ] Creating Cloud VM triton-test11 in localcloud
[DEBUG   ] User: 'user' on PATH: https://localcloud.api.somehost.com/my/images
[DEBUG   ] Requesting URL https://localcloud.api.somehost.com/my/images using GET method
[DEBUG   ] Response Status Code: 200
[DEBUG   ] Joyent Response Status Code: 200
[DEBUG   ] [{'files': [{'sha1': '6f986062b33b1682112ff6141e5f695bda43426a', 'compression': 'gzip', 'size': 388657371}], 'requirements': {}, 'name': 'debian-8', 'tags': {'role': 'os'}, 'homepage': 'https://docs.joyent.com/images/kvm/debian', 'published_at': '2015-07-02T15:37:02Z', 'id': '2f56d126-20d0-11e5-9e5b-5f3ef6688aba', 'state': 'active', 'version': '20150702', 'type': 'virtualmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'linux', 'public': True, 'description': 'Debian 8 (jessie) 64-bit image with just essential packages installed, built to run on KVM virtual machines.'}, {'files': [{'sha1': '8faaf0fea66fc2e6e90ed73048e511a4d82d424c', 'compression': 'gzip', 'size': 306773694}], 'requirements': {}, 'name': 'base-64-lts', 'tags': {'role': 'os', 'group': 'base-64-lts'}, 'homepage': 'https://docs.joyent.com/images/smartos/base', 'published_at': '2016-01-19T14:19:02Z', 'id': '96bcddda-beb7-11e5-af20-a3fb54c8ae29', 'state': 'active', 'version': '15.4.0', 'type': 'smartmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'smartos', 'public': True, 'description': 'A 64-bit SmartOS image with just essential packages installed. Ideal for users who are comfortable with setting up their own environment and tools.'}, {'files': [{'sha1': '8d28a887f0ace58c8c859156cc8805761a787995', 'compression': 'gzip', 'size': 73691842}], 'requirements': {}, 'name': 'minimal-64-lts', 'tags': {'role': 'os', 'group': 'minimal-64-lts'}, 'homepage': 'https://docs.joyent.com/images/smartos/minimal', 'published_at': '2016-03-03T23:58:53Z', 'id': 'e1faace4-e19b-11e5-928b-83849e2fd94a', 'state': 'active', 'version': '15.4.1', 'type': 'smartmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'smartos', 'public': True, 'description': 'A 64-bit SmartOS image with just bootstrap packages installed. Ideal for users who want the smallest possible image upon which to build.'}, {'files': [{'sha1': '63f4e7ad13453a4fac4ed3f1badc33ed41a0eb28', 'compression': 'gzip', 'size': 307766730}], 'requirements': {}, 'name': 'base-64-lts', 'tags': {'role': 'os', 'group': 'base-64-lts'}, 'homepage': 'https://docs.joyent.com/images/smartos/base', 'published_at': '2016-03-04T00:35:45Z', 'id': '088b97b0-e1a1-11e5-b895-9baa2086eb33', 'state': 'active', 'version': '15.4.1', 'type': 'smartmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'smartos', 'public': True, 'description': 'A 64-bit SmartOS image with just essential packages installed. Ideal for users who are comfortable with setting up their own environment and tools.'}, {'files': [{'sha1': 'cb889f89430252fc0791672662007a2655eab93a', 'compression': 'gzip', 'size': 655651895}], 'requirements': {}, 'name': 'centos-7', 'tags': {'role': 'os'}, 'homepage': 'https://docs.joyent.com/images/linux/centos', 'published_at': '2016-04-15T15:29:37Z', 'id': 'dd31507e-031e-11e6-be8a-8f2707b5b3ee', 'state': 'active', 'version': '20160415', 'type': 'virtualmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'linux', 'public': True, 'description': 'CentOS 7.2 64-bit image with just essential packages installed, built to run on KVM virtual machines.'}, {'files': [{'sha1': 'd342f137c5ccef0702ec479acb63c196cf81b38a', 'compression': 'gzip', 'size': 134969110}], 'requirements': {}, 'name': 'ubuntu-16.04', 'tags': {'kernel_version': '4.3.0', 'role': 'os'}, 'homepage': 'https://docs.joyent.com/images/container-native-linux', 'published_at': '2016-06-01T02:17:41Z', 'id': '05140a7e-279f-11e6-aedf-47d4f69d2887', 'state': 'active', 'version': '20160601', 'type': 'smartmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'linux', 'public': True, 'description': 'Container-native Ubuntu 16.04 64-bit image. Built to run on containers with bare metal speed, while offering all the services of a typical unix host.'}, {'files': [{'sha1': '520c617293601b5e6d5686bc725c0f804511c099', 'compression': 'gzip', 'size': 159414675}], 'requirements': {}, 'name': 'debian-8', 'tags': {'kernel_version': '3.16.0', 'role': 'os'}, 'homepage': 'https://docs.joyent.com/images/container-native-linux', 'published_at': '2016-06-22T15:23:15Z', 'id': '3da6330e-388d-11e6-b41b-d766707c6c3d', 'state': 'active', 'version': '20160622', 'type': 'smartmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'linux', 'public': True, 'description': 'Container-native Debian 8.5 (jessie) 64-bit image. Built to run on containers with bare metal speed, while offering all the services of a typical unix host.'}]
[DEBUG   ] User: 'user' on PATH: https://localcloud.api.somehost.com//my/packages
[DEBUG   ] Requesting URL https://localcloud.api.somehost.com//my/packages using GET method


[DEBUG   ] Response Status Code: 200
[DEBUG   ] Joyent Response Status Code: 200
[DEBUG   ] MasterEvent PUB socket URI: /var/run/salt/master/master_event_pub.ipc
[DEBUG   ] MasterEvent PULL socket URI: /var/run/salt/master/master_event_pull.ipc
[DEBUG   ] Initializing new IPCClient for path: /var/run/salt/master/master_event_pull.ipc
[DEBUG   ] Sending event - data = {'_stamp': '2016-10-18T15:46:27.888844', 'event': 'requesting instance', 'kwargs': {'image': {'files': [{'sha1': 'cb889f89430252fc0791672662007a2655eab93a', 'compression': 'gzip', 'size': 655651895}], 'requirements': {}, 'name': 'dd31507e-031e-11e6-be8a-8f2707b5b3ee', 'tags': {'role': 'os'}, 'homepage': 'https://docs.joyent.com/images/linux/centos', 'published_at': '2016-04-15T15:29:37Z', 'id': 'dd31507e-031e-11e6-be8a-8f2707b5b3ee', 'state': 'active', 'version': '20160415', 'type': 'virtualmachine', 'owner': '930896af-bf8c-48d4-885c-6573a94b1853', 'os': 'linux', 'public': True, 'description': 'CentOS 7.2 64-bit image with just essential packages installed, built to run on KVM virtual machines.'}, 'location': 'localcloud', 'name': 'triton-test11', 'size': {'group': 'triton', 'name': 'sample-2G', 'default': False, 'lwps': 4000, 'vcpus': 0, 'version': '1.0.0', 'swap': 8192, 'memory': 2048, 'disk': 51200, 'id': '6278263d-bf13-c982-e0f9-84c9c539edfa'}}}
[DEBUG   ] User: 'user' on PATH: https://localcloud.api.somehost.com//my/machines
[DEBUG   ] Requesting URL https://localcloud.api.somehost.com//my/machines using POST method
[DEBUG   ] Response Status Code: 201
[DEBUG   ] Joyent Response Status Code: 201
[DEBUG   ] MasterEvent PUB socket URI: /var/run/salt/master/master_event_pub.ipc
[DEBUG   ] MasterEvent PULL socket URI: /var/run/salt/master/master_event_pull.ipc
[DEBUG   ] Initializing new IPCClient for path: /var/run/salt/master/master_event_pull.ipc
[DEBUG   ] Sending event - data = {'_stamp': '2016-10-18T15:46:29.069164', 'event': 'querying instance'}
[DEBUG   ] Waiting for VM IP. Giving up in 00:10:00.
[DEBUG   ] User: 'user' on PATH: https://us-east-1.api.somehost.com/my/machines
[DEBUG   ] Requesting URL https://us-east-1.api.somehost.com/my/machines using GET method
[ERROR   ] There was a profile error: [Errno -2] Name or service not known
Traceback (most recent call last):
File ""/usr/lib/python2.7/site-packages/salt/cloud/cli.py"", line 284, in run
self.config.get('names')
File ""/usr/lib/python2.7/site-packages/salt/cloud/__init__.py"", line 1446, in run_profile
ret[name] = self.create(vm_)
File ""/usr/lib/python2.7/site-packages/salt/cloud/__init__.py"", line 1281, in create
output = self.clouds[func](vm_)
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 317, in create
query_instance(vm_)
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 219, in query_instance
'wait_for_ip_interval_multiplier', vm_, __opts__, default=1),
File ""/usr/lib/python2.7/site-packages/salt/utils/cloud.py"", line 2314, in wait_for_ip
data = update_callback(*update_args, **update_kwargs)
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 190, in _query_ip_address
data = show_instance(vm_['name'], call='action')
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 647, in show_instance
node = get_node(name)
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 628, in get_node
nodes = list_nodes()
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 737, in list_nodes
method='GET')
File ""/usr/lib/python2.7/site-packages/salt/cloud/clouds/joyent.py"", line 1092, in query
opts=__opts__,
File ""/usr/lib/python2.7/site-packages/salt/utils/http.py"", line 484, in query
**req_kwargs
File ""/usr/lib64/python2.7/site-packages/tornado/httpclient.py"", line 102, in fetch
self._async_client.fetch, request, **kwargs))
File ""/usr/lib64/python2.7/site-packages/tornado/ioloop.py"", line 444, in run_sync
return future_cell[0].result()
File ""/usr/lib64/python2.7/site-packages/tornado/concurrent.py"", line 214, in result
raise_exc_info(self._exc_info)
File ""<string>"", line 3, in raise_exc_info
gaierror: [Errno -2] Name or service not known",gaierror,buggy_snippets_files/e535814853e97f88b1b4312aa7bc8025bb016069112ea9b52b172e9c462130a8_before_merge.py,buggy_snippets_files/e535814853e97f88b1b4312aa7bc8025bb016069112ea9b52b172e9c462130a8_after_merge.py
260862,"    def run_job(self, name):
        '''
        Run a schedule job now
        '''
        data = self._get_schedule().get(name, {})

        if 'function' in data:
            func = data['function']
        elif 'func' in data:
            func = data['func']
        elif 'fun' in data:
            func = data['fun']
        else:
            func = None
        if func not in self.functions:
            log.info(
                'Invalid function: %s in scheduled job %s.',
                func, name
            )

        if 'name' not in data:
            data['name'] = name
        log.info('Running Job: %s', name)

        if not self.standalone:
            data = self._check_max_running(func,
                                           data,
                                           self.opts,
                                           datetime.datetime.now())

        # Grab run, assume True
        run = data.get('run', True)
        if run:
            self._run_job(func, data)
","    def run_job(self, name):
        '''
        Run a schedule job now
        '''
        data = self._get_schedule().get(name, {})

        if 'function' in data:
            func = data['function']
        elif 'func' in data:
            func = data['func']
        elif 'fun' in data:
            func = data['fun']
        else:
            func = None
        if func not in self.functions:
            log.info(
                'Invalid function: %s in scheduled job %s.',
                func, name
            )

        if 'name' not in data:
            data['name'] = name

        # Assume run should be True until we check max_running
        if 'run' not in data:
            data['run'] = True

        if not self.standalone:
            data = self._check_max_running(func,
                                           data,
                                           self.opts,
                                           datetime.datetime.now())

        # Grab run, assume True
        if data.get('run'):
            log.info('Running Job: %s', name)
            self._run_job(func, data)
",https://github.com/saltstack/salt/issues/54741,"CWE-166: Improper Handling of Missing Special Element, CWE-248: Uncaught Exception","If a scheduled job does not contains a time element parameter, running that job with schedule.run_job fails and causes a traceback because data['run'] does not exist.",salt/utils/schedule.py,Schedule.run_job,[13],"2019-09-25 14:06:41,391 [salt.utils.schedule:461 ][INFO    ][6910] Running Job: test-scheduler-present
2019-09-25 14:06:41,391 [tornado.application:611 ][ERROR   ][6910] Exception in callback <functools.partial object at 0x7f5bcc528520>
Traceback (most recent call last):
File ""/usr/lib64/python2.7/site-packages/tornado/ioloop.py"", line 591, in _run_callback
ret = callback()
File ""/usr/lib64/python2.7/site-packages/tornado/stack_context.py"", line 274, in null_wrapper
return fn(*args, **kwargs)
File ""/usr/lib64/python2.7/site-packages/tornado/ioloop.py"", line 597, in <lambda>
self.add_future(ret, lambda f: f.result())
File ""/usr/lib64/python2.7/site-packages/tornado/concurrent.py"", line 214, in result
raise_exc_info(self._exc_info)
File ""/usr/lib64/python2.7/site-packages/tornado/gen.py"", line 888, in run
yielded = self.gen.throw(*exc_info)
File ""/usr/lib/python2.7/site-packages/salt/minion.py"", line 959, in handle_event
yield [minion.handle_event(package) for minion in self.minions]
File ""/usr/lib64/python2.7/site-packages/tornado/gen.py"", line 882, in run
value = future.result()
File ""/usr/lib64/python2.7/site-packages/tornado/concurrent.py"", line 214, in result
raise_exc_info(self._exc_info)
File ""/usr/lib64/python2.7/site-packages/tornado/gen.py"", line 668, in callback
result_list.append(f.result())
File ""/usr/lib64/python2.7/site-packages/tornado/concurrent.py"", line 214, in result
raise_exc_info(self._exc_info)
File ""/usr/lib64/python2.7/site-packages/tornado/gen.py"", line 232, in wrapper
yielded = next(result)
File ""/usr/lib/python2.7/site-packages/salt/minion.py"", line 2371, in handle_event
self.manage_schedule(tag, data)
File ""/usr/lib/python2.7/site-packages/salt/minion.py"", line 2233, in manage_schedule
self.schedule.run_job(name)
File ""/usr/lib/python2.7/site-packages/salt/utils/schedule.py"", line 467, in run_job
datetime.datetime.now())
File ""/usr/lib/python2.7/site-packages/salt/utils/schedule.py"", line 212, in _check_max_running
if not data['run']:
KeyError: u'run'",KeyError,buggy_snippets_files/24d3c6e2a2fcf12b41eb9096744ab52beb1cfde2780b7e8bd1523e334c4d6ab3_before_merge.py,buggy_snippets_files/24d3c6e2a2fcf12b41eb9096744ab52beb1cfde2780b7e8bd1523e334c4d6ab3_after_merge.py
82966,"def object_description(object):
    # type: (Any) -> unicode
    """"""A repr() implementation that returns text safe to use in reST context.""""""
    if isinstance(object, dict):
        try:
            sorted_keys = sorted(object)
        except TypeError:
            pass  # Cannot sort dict keys, fall back to generic repr
        else:
            items = (""%s: %s"" %
                     (object_description(key), object_description(object[key]))
                     for key in sorted_keys)
            return ""{%s}"" % "", "".join(items)
    if isinstance(object, set):
        try:
            sorted_values = sorted(object)
        except TypeError:
            pass  # Cannot sort set values, fall back to generic repr
        else:
            template = ""{%s}"" if PY3 else ""set([%s])""
            return template % "", "".join(object_description(x)
                                        for x in sorted_values)
    try:
        s = repr(object)
    except Exception:
        raise ValueError
    if isinstance(s, binary_type):
        s = force_decode(s, None)  # type: ignore
    # Strip non-deterministic memory addresses such as
    # ``<__main__.A at 0x7f68cb685710>``
    s = memory_address_re.sub('', s)
    return s.replace('\n', ' ')
","def object_description(object):
    # type: (Any) -> unicode
    """"""A repr() implementation that returns text safe to use in reST context.""""""
    if isinstance(object, dict):
        try:
            sorted_keys = sorted(object)
        except Exception:
            pass  # Cannot sort dict keys, fall back to generic repr
        else:
            items = (""%s: %s"" %
                     (object_description(key), object_description(object[key]))
                     for key in sorted_keys)
            return ""{%s}"" % "", "".join(items)
    if isinstance(object, set):
        try:
            sorted_values = sorted(object)
        except TypeError:
            pass  # Cannot sort set values, fall back to generic repr
        else:
            template = ""{%s}"" if PY3 else ""set([%s])""
            return template % "", "".join(object_description(x)
                                        for x in sorted_values)
    try:
        s = repr(object)
    except Exception:
        raise ValueError
    if isinstance(s, binary_type):
        s = force_decode(s, None)  # type: ignore
    # Strip non-deterministic memory addresses such as
    # ``<__main__.A at 0x7f68cb685710>``
    s = memory_address_re.sub('', s)
    return s.replace('\n', ' ')
",https://github.com/sphinx-doc/sphinx/issues/5143,CWE-703: Improper Check or Handling of Exceptional Conditions,Not all exceptions are handled,sphinx/util/inspect.py,object_description,[7],"Traceback (most recent call last):
......
File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc/__init__.py"", line 1252, in add_directive_header
objrepr = object_description(self.object)
File ""/usr/lib/python2.7/site-packages/sphinx/util/inspect.py"", line 243, in object_description
sorted_keys = sorted(object)
File ""/opt/fabrix/fabrix/config.py"", line 59, in __iter__
return self.__super__getitem__(env.host_string).__iter__()
KeyError: None",KeyError,buggy_snippets_files/481663959a96e5fb20c2bd1f5bf95c5276f679a9fc8b6d81a11aaf4819c91a01_before_merge.py,buggy_snippets_files/481663959a96e5fb20c2bd1f5bf95c5276f679a9fc8b6d81a11aaf4819c91a01_after_merge.py
375713,"def get_current_subtitles(video_path):
    """"""Returns a list of current subtitles for the episode

    :param video_path: the video path
    :return: the current subtitles for the specified video
    """"""
    video = get_video(video_path)
    if not video:
        logger.log(u""Exception caught in subliminal.scan_video, subtitles couldn't be refreshed"", logger.DEBUG)
    else:
        return get_subtitles(video)
","def get_current_subtitles(video_path):
    """"""Returns a list of current subtitles for the episode

    :param video_path: the video path
    :type video_path: str
    :return: the current subtitles (3-letter opensubtitles codes) for the specified video
    :rtype: list of str
    """"""
    video = get_video(video_path)
    if not video:
        logger.log(u""Exception caught in subliminal.scan_video, subtitles couldn't be refreshed"", logger.DEBUG)
    else:
        return get_subtitles(video)
",https://github.com/pymedusa/Medusa/issues/274,CWE-704: Incorrect Type Conversion or Cast,Missing type conversion from int to str,sickbeard/subtitles.py,get_current_subtitles,[21],"2016-04-05 12:05:53 FINDSUBTITLES :: [a90e41d] Traceback (most recent call last):
File ""/home/dariox/.sickrage/sickbeard/subtitles.py"", line 325, in download_best_subs
get_subtitle_description(subtitle)), logger.DEBUG)
File ""/home/dariox/.sickrage/sickbeard/subtitles.py"", line 489, in get_subtitle_description
return subtitle.id + '-' + desc if desc not in subtitle.id else desc
TypeError: argument of type 'int' is not iterable",TypeError,buggy_snippets_files/65013c5edc59953f3a9c8c787ae50f3ea8f410e59634a057d1f5cddf34cf1c37_before_merge.py,buggy_snippets_files/65013c5edc59953f3a9c8c787ae50f3ea8f410e59634a057d1f5cddf34cf1c37_after_merge.py
229894,"    def get_additional_deps(self, file: MypyFile) -> List[Tuple[int, str, int]]:
        # for settings
        if file.fullname() == 'django.conf' and self.django_context.django_settings_module:
            return [self._new_dependency(self.django_context.django_settings_module)]

        # for values / values_list
        if file.fullname() == 'django.db.models':
            return [self._new_dependency('mypy_extensions'), self._new_dependency('typing')]

        # for `get_user_model()`
        if self.django_context.settings:
            if (file.fullname() == 'django.contrib.auth'
                    or file.fullname() in {'django.http', 'django.http.request'}):
                auth_user_model_name = self.django_context.settings.AUTH_USER_MODEL
                try:
                    auth_user_module = self.django_context.apps_registry.get_model(auth_user_model_name).__module__
                except LookupError:
                    # get_user_model() model app is not installed
                    return []
                return [self._new_dependency(auth_user_module)]

        # ensure that all mentioned to='someapp.SomeModel' are loaded with corresponding related Fields
        defined_model_classes = self.django_context.model_modules.get(file.fullname())
        if not defined_model_classes:
            return []
        deps = set()
        for model_class in defined_model_classes:
            # forward relations
            for field in self.django_context.get_model_fields(model_class):
                if isinstance(field, RelatedField):
                    related_model_cls = self.django_context.get_field_related_model_cls(field)
                    related_model_module = related_model_cls.__module__
                    if related_model_module != file.fullname():
                        deps.add(self._new_dependency(related_model_module))
            # reverse relations
            for relation in model_class._meta.related_objects:
                related_model_cls = self.django_context.get_field_related_model_cls(relation)
                related_model_module = related_model_cls.__module__
                if related_model_module != file.fullname():
                    deps.add(self._new_dependency(related_model_module))
        return list(deps)
","    def get_additional_deps(self, file: MypyFile) -> List[Tuple[int, str, int]]:
        # for settings
        if file.fullname() == 'django.conf' and self.django_context.django_settings_module:
            return [self._new_dependency(self.django_context.django_settings_module)]

        # for values / values_list
        if file.fullname() == 'django.db.models':
            return [self._new_dependency('mypy_extensions'), self._new_dependency('typing')]

        # for `get_user_model()`
        if self.django_context.settings:
            if (file.fullname() == 'django.contrib.auth'
                    or file.fullname() in {'django.http', 'django.http.request'}):
                auth_user_model_name = self.django_context.settings.AUTH_USER_MODEL
                try:
                    auth_user_module = self.django_context.apps_registry.get_model(auth_user_model_name).__module__
                except LookupError:
                    # get_user_model() model app is not installed
                    return []
                return [self._new_dependency(auth_user_module)]

        # ensure that all mentioned to='someapp.SomeModel' are loaded with corresponding related Fields
        defined_model_classes = self.django_context.model_modules.get(file.fullname())
        if not defined_model_classes:
            return []
        deps = set()
        for model_class in defined_model_classes:
            # forward relations
            for field in self.django_context.get_model_fields(model_class):
                if isinstance(field, RelatedField):
                    related_model_cls = self.django_context.get_field_related_model_cls(field)
                    if related_model_cls is None:
                        continue
                    related_model_module = related_model_cls.__module__
                    if related_model_module != file.fullname():
                        deps.add(self._new_dependency(related_model_module))
            # reverse relations
            for relation in model_class._meta.related_objects:
                related_model_cls = self.django_context.get_field_related_model_cls(relation)
                related_model_module = related_model_cls.__module__
                if related_model_module != file.fullname():
                    deps.add(self._new_dependency(related_model_module))
        return list(deps)
",https://github.com/typeddjango/django-stubs/issues/196,"CWE-166: Improper Handling of Missing Special Element, CWE-754: Improper Check for Unusual or Exceptional Conditions",No checking if related_model_cls is None,mypy_django_plugin/main.py,NewSemanalDjangoPlugin.get_additional_deps,[29],"Traceback (most recent call last):
File ""/usr/local/bin/mypy"", line 10, in <module>
sys.exit(console_entry())
File ""/usr/local/lib/python3.6/site-packages/mypy/__main__.py"", line 8, in console_entry
main(None, sys.stdout, sys.stderr)
File ""/usr/local/lib/python3.6/site-packages/mypy/main.py"", line 83, in main
res = build.build(sources, options, None, flush_errors, fscache, stdout, stderr)
File ""/usr/local/lib/python3.6/site-packages/mypy/build.py"", line 164, in build
result = _build(sources, options, alt_lib_path, flush_errors, fscache, stdout, stderr)
File ""/usr/local/lib/python3.6/site-packages/mypy/build.py"", line 224, in _build
graph = dispatch(sources, manager, stdout)
File ""/usr/local/lib/python3.6/site-packages/mypy/build.py"", line 2526, in dispatch
graph = load_graph(sources, manager)
File ""/usr/local/lib/python3.6/site-packages/mypy/build.py"", line 2748, in load_graph
caller_state=st, caller_line=st.dep_line_map.get(dep, 1))
File ""/usr/local/lib/python3.6/site-packages/mypy/build.py"", line 1772, in __init__
self.compute_dependencies()
File ""/usr/local/lib/python3.6/site-packages/mypy/build.py"", line 2059, in compute_dependencies
self.manager.plugin.get_additional_deps(self.tree))
File ""/usr/local/lib/python3.6/site-packages/mypy/plugin.py"", line 743, in get_additional_deps
deps.extend(plugin.get_additional_deps(file))
File ""/usr/local/lib/python3.6/site-packages/mypy_django_plugin/main.py"", line 152, in get_additional_deps
related_model_module = related_model_cls.__module__
AttributeError: 'NoneType' object has no attribute '__module__'",AttributeError,buggy_snippets_files/ece202e51681b307c385402fda13a883743d45db525230dfcd852ca7580af551_before_merge.py,buggy_snippets_files/ece202e51681b307c385402fda13a883743d45db525230dfcd852ca7580af551_after_merge.py
167581,"    def show_tree(self):
        """"""Populate the tree with profiler data and display it.""""""
        self.initialize_view() # Clear before re-populating
        self.setItemsExpandable(True)
        self.setSortingEnabled(False)
        rootkey = self.find_root()  # This root contains profiler overhead
        if rootkey:
            self.populate_tree(self, self.find_callees(rootkey))
            self.resizeColumnToContents(0)
            self.setSortingEnabled(True)
            self.sortItems(1, Qt.AscendingOrder) # FIXME: hardcoded index
            self.change_view(1)
","    def show_tree(self):
        """"""Populate the tree with profiler data and display it.""""""
        self.initialize_view() # Clear before re-populating
        self.setItemsExpandable(True)
        self.setSortingEnabled(False)
        rootkey = self.find_root()  # This root contains profiler overhead
        if rootkey is not None:
            self.populate_tree(self, self.find_callees(rootkey))
            self.resizeColumnToContents(0)
            self.setSortingEnabled(True)
            self.sortItems(1, Qt.AscendingOrder) # FIXME: hardcoded index
            self.change_view(1)
",https://github.com/spyder-ide/spyder/issues/8336,"CWE-166: Improper Handling of Missing Special Element, CWE-754: Improper Check for Unusual or Exceptional Conditions",No checking if self.profdata is None,spyder_profiler/widgets/profilergui.py,ProfilerDataTree.show_tree,[3],"File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_profiler\widgets\profilergui.py"", line 264, in <lambda>
self.finished(ec, es))
File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_profiler\widgets\profilergui.py"", line 335, in finished
self.show_data(justanalyzed=True)
File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_profiler\widgets\profilergui.py"", line 362, in show_data
self.datatree.show_tree()
File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_profiler\widgets\profilergui.py"", line 526, in show_tree
rootkey = self.find_root()  # This root contains profiler overhead
File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_profiler\widgets\profilergui.py"", line 507, in find_root
self.profdata.sort_stats(""cumulative"")
AttributeError: 'NoneType' object has no attribute 'sort_stats'",AttributeError,buggy_snippets_files/aa9e9ab49831c2c496658db767c9bb1c388144c9ade2b35a9566f3c2267c4a6f_before_merge.py,buggy_snippets_files/aa9e9ab49831c2c496658db767c9bb1c388144c9ade2b35a9566f3c2267c4a6f_after_merge.py
323616,"def _find_domains_or_certname(config, installer):
    """"""Retrieve domains and certname from config or user input.
    """"""
    domains = None
    if config.domains:
        domains = config.domains
    elif not config.certname:
        domains = display_ops.choose_names(installer)

    if not domains and not config.certname:
        raise errors.Error(""Please specify --domains, or --installer that ""
                           ""will help in domain names autodiscovery, or ""
                           ""--cert-name for an existing certificate name."")

    return domains, config.certname
","def _find_domains_or_certname(config, installer):
    """"""Retrieve domains and certname from config or user input.
    """"""
    domains = None
    certname = config.certname
    # first, try to get domains from the config
    if config.domains:
        domains = config.domains
    # if we can't do that but we have a certname, get the domains
    # with that certname
    elif certname:
        domains = cert_manager.domains_for_certname(config, certname)

    # that certname might not have existed, or there was a problem.
    # try to get domains from the user.
    if not domains:
        domains = display_ops.choose_names(installer)

    if not domains and not certname:
        raise errors.Error(""Please specify --domains, or --installer that ""
                           ""will help in domain names autodiscovery, or ""
                           ""--cert-name for an existing certificate name."")

    return domains, certname
",https://github.com/certbot/certbot/issues/3935,CWE-703: Improper Check or Handling of Exceptional Conditions,Wrong condition was used when checking,certbot/main.py,_find_domains_or_certname,[7],"certbot certificates
Saving debug log to /var/log/letsencrypt/letsencrypt.log

-------------------------------------------------------------------------------
Found the following certs:
Certificate Name: isnot.org
Domains: isnot.org goxogle.com is.isnot.org www.isnot.org
Expiry Date: 2017-03-20 17:57:00+00:00 (VALID: 89 days)
Certificate Path: /etc/letsencrypt/live/isnot.org/fullchain.pem
Private Key Path: /etc/letsencrypt/live/isnot.org/privkey.pem
...

certbot --cert-name isnot.org --debug
Saving debug log to /var/log/letsencrypt/letsencrypt.log
Starting new HTTPS connection (1): acme-v01.api.letsencrypt.org
Cert not yet due for renewal

You have an existing certificate that has exactly the same domains or certificate name you requested and isn't close to expiry.
(ref: /etc/letsencrypt/renewal/isnot.org.conf)

What would you like to do?
-------------------------------------------------------------------------------
1: Attempt to reinstall this existing certificate
2: Renew &amp; replace the cert (limit ~5 per 7 days)
-------------------------------------------------------------------------------
Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 1
Keeping the existing certificate
Traceback (most recent call last):
File ""venv/bin/certbot"", line 11, in <module>
load_entry_point('certbot', 'console_scripts', 'certbot')()
File ""/home/ubuntu/letsencrypt/certbot/main.py"", line 831, in main
return config.func(config, plugins)
File ""/home/ubuntu/letsencrypt/certbot/main.py"", line 570, in run
lineage.chain, lineage.fullchain)
File ""/home/ubuntu/letsencrypt/certbot/client.py"", line 380, in deploy_certificate
for dom in domains:
TypeError: 'NoneType' object is not iterable",TypeError,buggy_snippets_files/2c7fec605878680099d7d572b7aa3e7fbc278ba27e0f8ed00813f4187fb312d3_before_merge.py,buggy_snippets_files/2c7fec605878680099d7d572b7aa3e7fbc278ba27e0f8ed00813f4187fb312d3_after_merge.py
306585,"def _catch_login_errors(func) -> Callable:
    """"""Detect AlexapyLoginError and attempt relogin.""""""
    import functools
    @functools.wraps(func)
    async def wrapper(*args, **kwargs) -> Any:
        try:
            result = await func(*args, **kwargs)
        except AlexapyLoginError as ex:  # pylint: disable=broad-except
            template = (""An exception of type {0} occurred.""
                        "" Arguments:\n{1!r}"")
            message = template.format(type(ex).__name__, ex.args)
            _LOGGER.debug(""%s.%s: detected bad login: %s"",
                          func.__module__[func.__module__.find('.')+1:],
                          func.__name__,
                          message)
            instance = args[0]
            if hasattr(instance, '_login'):
                login = instance._login
                email = login.email
                hass = instance.hass if instance.hass else None
                if (hass and not
                    (hass.data[DATA_ALEXAMEDIA]['accounts'][email]
                     ['configurator'])):
                    config_entry = (
                        hass.data[DATA_ALEXAMEDIA]
                        ['accounts']
                        [email]
                        ['config_entry'])
                    callback = (
                        hass.data[DATA_ALEXAMEDIA]
                        ['accounts']
                        [email]
                        ['setup_platform_callback'])
                    test_login_status = (
                        hass.data[DATA_ALEXAMEDIA]
                        ['accounts']
                        [email]
                        ['test_login_status'])
                    _LOGGER.debug(
                        ""%s: Alexa API disconnected; attempting to relogin"",
                        hide_email(email))
                    await login.login_with_cookie()
                    await test_login_status(hass,
                                            config_entry, login,
                                            callback)
            return None
        return result
    return wrapper
","def _catch_login_errors(func) -> Callable:
    """"""Detect AlexapyLoginError and attempt relogin.""""""
    import functools
    @functools.wraps(func)
    async def wrapper(*args, **kwargs) -> Any:
        try:
            result = await func(*args, **kwargs)
        except AlexapyLoginError as ex:  # pylint: disable=broad-except
            template = (""An exception of type {0} occurred.""
                        "" Arguments:\n{1!r}"")
            message = template.format(type(ex).__name__, ex.args)
            _LOGGER.debug(""%s.%s: detected bad login: %s"",
                          func.__module__[func.__module__.find('.')+1:],
                          func.__name__,
                          message)
            instance = args[0]
            if hasattr(instance, '_login'):
                login = instance._login
                email = login.email
                hass = instance.hass if instance.hass else None
                if (hass and (
                    'configurator' not in (hass.data[DATA_ALEXAMEDIA]
                                           ['accounts'][email])
                    or not (hass.data[DATA_ALEXAMEDIA]['accounts'][email]
                        ['configurator']))):
                    config_entry = (
                        hass.data[DATA_ALEXAMEDIA]
                        ['accounts']
                        [email]
                        ['config_entry'])
                    callback = (
                        hass.data[DATA_ALEXAMEDIA]
                        ['accounts']
                        [email]
                        ['setup_platform_callback'])
                    test_login_status = (
                        hass.data[DATA_ALEXAMEDIA]
                        ['accounts']
                        [email]
                        ['test_login_status'])
                    _LOGGER.debug(
                        ""%s: Alexa API disconnected; attempting to relogin"",
                        hide_email(email))
                    await login.login_with_cookie()
                    await test_login_status(hass,
                                            config_entry, login,
                                            callback)
            return None
        return result
    return wrapper
",https://github.com/custom-components/alexa_media_player/issues/423,CWE-703: Improper Check or Handling of Exceptional Conditions,no check for the presence of configurator key,custom_components/alexa_media/helpers.py,_catch_login_errors,"[17,20]","2019-10-09 15:58:30 ERROR (MainThread) [homeassistant.components.websocket_api.http.connection.139875941900688] 'configurator'
Traceback (most recent call last):
File ""/config/custom_components/alexa_media/helpers.py"", line 143, in wrapper
result = await func(*args, **kwargs)
File ""/config/custom_components/alexa_media/media_player.py"", line 796, in async_play_media
customer_id=self._customer_id, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/alexapy/alexaapi.py"", line 291, in play_music
musicProviderId=provider_id)
File ""/usr/local/lib/python3.7/site-packages/alexapy/alexaapi.py"", line 208, in send_sequence
data=data)
File ""/usr/local/lib/python3.7/site-packages/alexapy/alexaapi.py"", line 90, in _post_request
return await self._request('post', uri, data)
File ""/usr/local/lib/python3.7/site-packages/alexapy/alexaapi.py"", line 82, in _request
raise AlexapyLoginError(response.reason)
alexapy.errors.AlexapyLoginError: Unauthorized

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""/usr/src/homeassistant/homeassistant/components/websocket_api/commands.py"", line 130, in handle_call_service
connection.context(msg),
File ""/usr/src/homeassistant/homeassistant/core.py"", line 1234, in async_call
await asyncio.shield(self._execute_service(handler, service_call))
File ""/usr/src/homeassistant/homeassistant/core.py"", line 1259, in _execute_service
await handler.func(service_call)
File ""/usr/src/homeassistant/homeassistant/helpers/entity_component.py"", line 213, in handle_service
self._platforms.values(), func, call, service_name, required_features
File ""/usr/src/homeassistant/homeassistant/helpers/service.py"", line 349, in entity_service_call
future.result()  # pop exception if have
File ""/usr/src/homeassistant/homeassistant/helpers/service.py"", line 373, in _handle_service_platform_call
await func(entity, data)
File ""/config/custom_components/alexa_media/helpers.py"", line 159, in wrapper
['configurator'])):
KeyError: 'configurator'",alexapy.errors.AlexapyLoginError,buggy_snippets_files/24c44c0a1e94f6cbdc2f0f1234d45fec9829c99566b69767d365b3afa548f65e_before_merge.py,buggy_snippets_files/24c44c0a1e94f6cbdc2f0f1234d45fec9829c99566b69767d365b3afa548f65e_after_merge.py
205467,"def get_volume_info(volume, region):

    attachment = volume[""attachments""]

    volume_info = {
        'create_time': volume[""create_time""],
        'id': volume[""volume_id""],
        'encrypted': volume[""encrypted""],
        'iops': volume[""iops""] if ""iops"" in volume else None,
        'size': volume[""size""],
        'snapshot_id': volume[""snapshot_id""],
        'status': volume[""state""],
        'type': volume[""volume_type""],
        'zone': volume[""availability_zone""],
        'region': region,
        'attachment_set': {
            'attach_time': attachment[0][""attach_time""] if len(attachment) > 0 else None,
            'device': attachment[0][""device""] if len(attachment) > 0 else None,
            'instance_id': attachment[0][""instance_id""] if len(attachment) > 0 else None,
            'status': attachment[0][""state""] if len(attachment) > 0 else None,
            'delete_on_termination': attachment[0][""delete_on_termination""] if len(attachment) > 0 else None
        },
        'tags': boto3_tag_list_to_ansible_dict(volume['tags'])
    }

    return volume_info
","def get_volume_info(volume, region):

    attachment = volume[""attachments""]

    volume_info = {
        'create_time': volume[""create_time""],
        'id': volume[""volume_id""],
        'encrypted': volume[""encrypted""],
        'iops': volume[""iops""] if ""iops"" in volume else None,
        'size': volume[""size""],
        'snapshot_id': volume[""snapshot_id""],
        'status': volume[""state""],
        'type': volume[""volume_type""],
        'zone': volume[""availability_zone""],
        'region': region,
        'attachment_set': {
            'attach_time': attachment[0][""attach_time""] if len(attachment) > 0 else None,
            'device': attachment[0][""device""] if len(attachment) > 0 else None,
            'instance_id': attachment[0][""instance_id""] if len(attachment) > 0 else None,
            'status': attachment[0][""state""] if len(attachment) > 0 else None,
            'delete_on_termination': attachment[0][""delete_on_termination""] if len(attachment) > 0 else None
        },
        'tags': boto3_tag_list_to_ansible_dict(volume['tags']) if ""tags"" in volume else None
    }

    return volume_info
",https://github.com/ansible/ansible/issues/46800,CWE-703: Improper Check or Handling of Exceptional Conditions,no check for the presence of tags key,lib/ansible/modules/cloud/amazon/ec2_vol_facts.py,get_volume_info,[21],"TASK [ec2_vol_facts] *******************************************************************************************************************************************************
task path: /home/ph/git/zaplox-software-deploy/ph.yml:8
Using module file /usr/lib/python2.7/dist-packages/ansible/modules/cloud/amazon/ec2_vol_facts.py
<localhost> ESTABLISH LOCAL CONNECTION FOR USER: ph
<localhost> EXEC /bin/sh -c 'AWS_REGION=eu-west-1 /usr/bin/python &amp;&amp; sleep 0'
The full traceback is:
Traceback (most recent call last):
File ""<stdin>"", line 113, in <module>
File ""<stdin>"", line 105, in _ansiballz_main
File ""<stdin>"", line 48, in invoke_module
File ""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py"", line 156, in <module>
File ""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py"", line 152, in main
File ""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py"", line 124, in list_ec2_volumes
File ""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py"", line 96, in get_volume_info
KeyError: 'tags'

fatal: [poc1-web -> localhost]: FAILED! => {
""changed"": false,
""module_stderr"": ""Traceback (most recent call last):\n  File \""<stdin>\"", line 113, in <module>\n  File \""<stdin>\"", line 105, in _ansiballz_main\n  File \""<stdin>\"", line 48, in invoke_module\n  File \""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py\"", line 156, in <module>\n  File \""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py\"", line 152, in main\n  File \""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py\"", line 124, in list_ec2_volumes\n  File \""/tmp/ansible_ec2_vol_facts_payload_F9x1Gs/__main__.py\"", line 96, in get_volume_info\nKeyError: 'tags'\n"",
""module_stdout"": """",
""msg"": ""MODULE FAILURE\nSee stdout/stderr for the exact error"",
""rc"": 1
}",KeyError,buggy_snippets_files/6a54eff640001935570d20347344c9079def6fee6ee95b989fe966906b447281_before_merge.py,buggy_snippets_files/6a54eff640001935570d20347344c9079def6fee6ee95b989fe966906b447281_after_merge.py
122807,"    def __init__(self, serial=''):
        self._serial = serial
        # logging.log_path only exists when this is used in an Mobly test run.
        self._log_path_base = getattr(logging, 'log_path', '/tmp/logs')
        self._log_path = os.path.join(self._log_path_base,
                                      'AndroidDevice%s' % self._serial)
        self._debug_tag = self._serial
        self.log = AndroidDeviceLoggerAdapter(logging.getLogger(), {
            'tag': self.debug_tag
        })
        self.sl4a = None
        self.ed = None
        self._adb_logcat_process = None
        self.adb_logcat_file_path = None
        self.adb = adb.AdbProxy(serial)
        self.fastboot = fastboot.FastbootProxy(serial)
        if not self.is_bootloader and self.is_rootable:
            self.root_adb()
        # A dict for tracking snippet clients. Keys are clients' attribute
        # names, values are the clients: {<attr name string>: <client object>}.
        self._snippet_clients = {}
        # Device info cache.
        self._user_added_device_info = {}
","    def __init__(self, serial=''):
        self._serial = str(serial)
        # logging.log_path only exists when this is used in an Mobly test run.
        self._log_path_base = getattr(logging, 'log_path', '/tmp/logs')
        self._log_path = os.path.join(self._log_path_base,
                                      'AndroidDevice%s' % self._serial)
        self._debug_tag = self._serial
        self.log = AndroidDeviceLoggerAdapter(logging.getLogger(), {
            'tag': self.debug_tag
        })
        self.sl4a = None
        self.ed = None
        self._adb_logcat_process = None
        self.adb_logcat_file_path = None
        self.adb = adb.AdbProxy(serial)
        self.fastboot = fastboot.FastbootProxy(serial)
        if not self.is_bootloader and self.is_rootable:
            self.root_adb()
        # A dict for tracking snippet clients. Keys are clients' attribute
        # names, values are the clients: {<attr name string>: <client object>}.
        self._snippet_clients = {}
        # Device info cache.
        self._user_added_device_info = {}
",https://github.com/google/mobly/issues/378,CWE-241: Improper Handling of Unexpected Data Type,No type conversion,mobly/controllers/android_device.py,AndroidDevice.__init__,[1],"Traceback (most recent call last):
File "".../mobly/test_runner.py"", line 420, in _register_controller
objects = create(controller_config)
File "".../mobly/controllers/android_device.py"", line 87, in create
ads = get_instances_with_configs(configs)
File "".../mobly/controllers/android_device.py"", line 257, in get_instances_with_configs
ad = AndroidDevice(serial)
File "".../mobly/controllers/android_device.py"", line 402, in __init__
if not self.is_bootloader and self.is_rootable:
File "".../mobly/controllers/android_device.py"", line 637, in is_rootable
build_type_output = self.adb.getprop('ro.build.type').lower()
File "".../mobly/controllers/android_device_lib/adb.py"", line 175, in getprop
return self.shell('getprop %s' % prop_name).decode('utf-8').strip()
File "".../mobly/controllers/android_device_lib/adb.py"", line 199, in adb_call
clean_name, args, shell=shell, timeout=timeout)
File "".../mobly/controllers/android_device_lib/adb.py"", line 161, in _exec_adb_cmd
return self._exec_cmd(adb_cmd, shell=shell, timeout=timeout)
File "".../mobly/controllers/android_device_lib/adb.py"", line 122, in _exec_cmd
args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=shell)
File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__
errread, errwrite)
File ""/usr/lib/python2.7/subprocess.py"", line 1343, in _execute_child
raise child_exception
TypeError: execv() arg 2 must contain only strings",TypeError,buggy_snippets_files/d582cf97e162eb286457e41034dc92af137980494356fb1732c5b654e1f9fa1f_before_merge.py,buggy_snippets_files/d582cf97e162eb286457e41034dc92af137980494356fb1732c5b654e1f9fa1f_after_merge.py
379304,"    def _get_series_episodes(self, task, config, series_name, series_url, series_info):
        log.info('Retrieving new episodes for %s', series_name)
        response = requests.get(series_url + '/search?media_type=broadcast')  # only shows full episodes
        page = get_soup(response.content)

        if page.find('div', class_='npo3-show-items'):
            log.debug('Parsing as npo3')
            entries = self._parse_episodes_npo3(task, config, series_name, page, series_info)
        else:
            log.debug('Parsing as std')
            entries = self._parse_episodes_std(task, config, series_name, page, series_info)

        if not entries:
            log.warning('No episodes found for %s', series_name)

        return entries
","    def _get_series_episodes(self, task, config, series_name, series_url):
        log.info('Retrieving new episodes for %s', series_name)
        try:
            response = requests.get(series_url)
            page = get_soup(response.content)

            series_info = self._parse_series_info(task, config, page, series_url)
            episodes = page.find('div', id='component-grid-episodes')
            entries = self._parse_tiles(task, config, episodes, series_info)

            if not entries:
                log.warning('No episodes found for %s', series_name)

            return entries
        except RequestException as e:
            raise plugin.PluginError('Request error: %s' % e.args[0])
",https://github.com/Flexget/Flexget/issues/1892,"CWE-166: Improper Handling of Missing Special Element, CWE-754: Improper Check for Unusual or Exceptional Conditions",No checking if tvseries is None,flexget/plugins/input/npo_watchlist.py,NPOWatchlist._get_series_episodes,[7],"2017-07-05 20:31 CRITICAL task tv-journaal BUG: Unhandled error in plugin npo_watchlist: 'NoneType' object has no attribute 'find'
Traceback (most recent call last):
File ""/usr/local/lib/python2.7/dist-packages/flexget/task.py"", line 486, in __run_plugin
return method(*args, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/flexget/event.py"", line 23, in call
return self.func(*args, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/flexget/plugins/input/npo_watchlist.py"", line 326, in on_task_input
entries += self._get_favorites_entries(task, config)
File ""/usr/local/lib/python2.7/dist-packages/flexget/plugins/input/npo_watchlist.py"", line 318, in _get_favorites_entries
series_info = self._get_series_info(task, config, series_name, url)
File ""/usr/local/lib/python2.7/dist-packages/flexget/plugins/input/npo_watchlist.py"", line 180, in _get_series_info
series_info['npo_url'] = tvseries.find('a', itemprop='url')['href']
AttributeError: 'NoneType' object has no attribute 'find'",AttributeError,buggy_snippets_files/4af7a6b46e03a362984c4031c822bbb1d611a7c3c2eb1c0fbe928ff03b5c3a4a_before_merge.py,buggy_snippets_files/4af7a6b46e03a362984c4031c822bbb1d611a7c3c2eb1c0fbe928ff03b5c3a4a_after_merge.py
188425,"    def execute(self, cmdstr: str):
        """"""
            Execute a command string. May raise CommandError.
        """"""
        parts = list(lexer(cmdstr))
        if not len(parts) >= 1:
            raise exceptions.CommandError(""Invalid command: %s"" % cmdstr)
        return self.call_strings(parts[0], parts[1:])
","    def execute(self, cmdstr: str):
        """"""
            Execute a command string. May raise CommandError.
        """"""
        try:
            parts = list(lexer(cmdstr))
        except ValueError as e:
            raise exceptions.CommandError(""Command error: %s"" % e)
        if not len(parts) >= 1:
            raise exceptions.CommandError(""Invalid command: %s"" % cmdstr)
        return self.call_strings(parts[0], parts[1:])
",https://github.com/mitmproxy/mitmproxy/issues/2810,"CWE-248: Uncaught Exception, CWE-440: Expected Behavior Violation",an exception was not handled,mitmproxy/command.py,CommandManager.execute,[5],"Traceback (most recent call last):
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/master.py"", line 216, in run
self.loop.run()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 278, in run
self._run()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 376, in _run
self.event_loop.run()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 682, in run
self._loop()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 719, in _loop
self._watch_files[fd]()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/raw_display.py"", line 393, in <lambda>
event_loop, callback, self.get_available_raw_input())
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/raw_display.py"", line 493, in parse_input
callback(processed, processed_codes)
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 403, in _update
self.process_input(keys)
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 503, in process_input
k = self._topmost_widget.keypress(self.screen_size, k)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/window.py"", line 308, in keypress
k = super().keypress(size, k)
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/container.py"", line 1116, in keypress
return self.footer.keypress((maxcol,),key)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/statusbar.py"", line 149, in keypress
return self.ab.keypress(*args, **kwargs)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/statusbar.py"", line 104, in keypress
self.prompt_execute(self._w.get_edit_text())
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/statusbar.py"", line 124, in prompt_execute
msg = p(txt)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/commandexecutor.py"", line 17, in __call__
ret = self.master.commands.call(cmd)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/command.py"", line 218, in call
parts = list(lexer(cmdstr))
File ""/usr/lib/python3.5/shlex.py"", line 263, in __next__
token = self.get_token()
File ""/usr/lib/python3.5/shlex.py"", line 90, in get_token
raw = self.read_token()
File ""/usr/lib/python3.5/shlex.py"", line 185, in read_token
raise ValueError(""No escaped character"")
ValueError: No escaped character",ValueError,buggy_snippets_files/a0d737e9dd619d9f97dae6af962e6db30a0452a6065b47512fb5c20de11e7b93_before_merge.py,buggy_snippets_files/a0d737e9dd619d9f97dae6af962e6db30a0452a6065b47512fb5c20de11e7b93_after_merge.py
321273,"    def update(self):
        """"""Update the stats.""""""
        if batinfo_tag:
            # Use the batinfo lib to grab the stats
            # Compatible with multiple batteries
            self.bat.update()
            self.bat_list = [{
                'label': 'Battery',
                'value': self.battery_percent,
                'unit': '%'}]
        elif psutil_tag and hasattr(self.bat.sensors_battery(), 'percent'):
            # Use the PSUtil 5.2.0 or higher lib to grab the stats
            # Give directly the battery percent
            self.bat_list = [{
                'label': 'Battery',
                'value': int(self.bat.sensors_battery().percent),
                'unit': '%'}]
        else:
            # No stats...
            self.bat_list = []
","    def update(self):
        """"""Update the stats.""""""
        if batinfo_tag:
            # Use the batinfo lib to grab the stats
            # Compatible with multiple batteries
            self.bat.update()
            self.bat_list = [{
                'label': 'Battery',
                'value': self.battery_percent,
                'unit': '%'}]
        elif psutil_tag and hasattr(self.bat.sensors_battery(), 'percent'):
            # Use psutil to grab the stats
            # Give directly the battery percent
            self.bat_list = [{
                'label': 'Battery',
                'value': int(self.bat.sensors_battery().percent),
                'unit': '%'}]
        else:
            # No stats...
            self.bat_list = []
",https://github.com/nicolargo/glances/issues/1055,"CWE-248: Uncaught Exception, CWE-440: Expected Behavior Violation",an exception was not handled if a file or directory does not exist,glances/plugins/glances_batpercent.py,GlancesGrabBat.update,[6],"Traceback (most recent call last):
File ""/usr/local/bin/glances"", line 11, in <module>
load_entry_point('Glances==2.8.8', 'console_scripts', 'glances')()
File ""/usr/local/lib/python2.7/site-packages/glances/__init__.py"", line 225, in main
start_standalone(config=config, args=args)
File ""/usr/local/lib/python2.7/site-packages/glances/__init__.py"", line 105, in start_standalone
standalone = GlancesStandalone(config=config, args=args)
File ""/usr/local/lib/python2.7/site-packages/glances/standalone.py"", line 43, in __init__
self.stats = GlancesStats(config=config, args=args)
File ""/usr/local/lib/python2.7/site-packages/glances/stats.py"", line 43, in __init__
self.load_modules(self.args)
File ""/usr/local/lib/python2.7/site-packages/glances/stats.py"", line 77, in load_modules
self.load_plugins(args=args)
File ""/usr/local/lib/python2.7/site-packages/glances/stats.py"", line 96, in load_plugins
plugin = __import__(os.path.basename(item)[:-3])
File ""/usr/local/lib/python2.7/site-packages/glances/plugins/glances_batpercent.py"", line 38, in <module>
psutil.sensors_battery()
File ""/usr/local/lib/python2.7/site-packages/psutil/__init__.py"", line 2263, in sensors_battery
return _psplatform.sensors_battery()
File ""/usr/local/lib/python2.7/site-packages/psutil/_psbsd.py"", line 411, in sensors_battery
percent, minsleft, power_plugged = cext.sensors_battery()
OSError: [Errno 2] No such file or directory",OSError,buggy_snippets_files/887dfb56f3b7226fd15af9a81f5698f35c2cdd470b10903079c9314bcac4442a_before_merge.py,buggy_snippets_files/887dfb56f3b7226fd15af9a81f5698f35c2cdd470b10903079c9314bcac4442a_after_merge.py
188554,"    @command.command(""export.file"")
    def file(self, fmt: str, f: flow.Flow, path: mitmproxy.types.Path) -> None:
        """"""
            Export a flow to path.
        """"""
        if fmt not in formats:
            raise exceptions.CommandError(""No such export format: %s"" % fmt)
        func = formats[fmt]  # type: typing.Any
        v = func(f)
        with open(path, ""wb"") as fp:
            if isinstance(v, bytes):
                fp.write(v)
            else:
                fp.write(v.encode(""utf-8""))
","    @command.command(""export.file"")
    def file(self, fmt: str, f: flow.Flow, path: mitmproxy.types.Path) -> None:
        """"""
            Export a flow to path.
        """"""
        if fmt not in formats:
            raise exceptions.CommandError(""No such export format: %s"" % fmt)
        func = formats[fmt]  # type: typing.Any
        v = func(f)
        try:
            with open(path, ""wb"") as fp:
                if isinstance(v, bytes):
                    fp.write(v)
                else:
                    fp.write(v.encode(""utf-8""))
        except IOError as e:
            ctx.log.error(str(e))
",https://github.com/mitmproxy/mitmproxy/issues/2750,"CWE-248: Uncaught Exception, CWE-440: Expected Behavior Violation",an exception was not handled,mitmproxy/addons/export.py,Export.file,"[5,12]","Traceback (most recent call last):
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/master.py"", line 216, in run
self.loop.run()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 278, in run
self._run()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 376, in _run
self.event_loop.run()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 682, in run
self._loop()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 719, in _loop
self._watch_files[fd]()
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/raw_display.py"", line 393, in <lambda>
event_loop, callback, self.get_available_raw_input())
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/raw_display.py"", line 493, in parse_input
callback(processed, processed_codes)
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 403, in _update
self.process_input(keys)
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/main_loop.py"", line 503, in process_input
k = self._topmost_widget.keypress(self.screen_size, k)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/window.py"", line 318, in keypress
k = super().keypress(size, k)
File ""/home/kajoj/Mitmproxy/mitmproxy/venv/lib/python3.5/site-packages/urwid/container.py"", line 1116, in keypress
return self.footer.keypress((maxcol,),key)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/statusbar.py"", line 149, in keypress
return self.ab.keypress(*args, **kwargs)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/statusbar.py"", line 104, in keypress
self.prompt_execute(self._w.get_value())
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/statusbar.py"", line 124, in prompt_execute
msg = p(txt)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/commandexecutor.py"", line 17, in __call__
ret = self.master.commands.call(cmd)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/command.py"", line 221, in call
return self.call_args(parts[0], parts[1:])
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/command.py"", line 212, in call_args
return self.commands[path].call(args)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/command.py"", line 101, in call
ret = self.func(*pargs)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/command.py"", line 251, in wrapper
return function(*args, **kwargs)
File ""/home/kajoj/Mitmproxy/mitmproxy/mitmproxy/tools/console/consoleaddons.py"", line 463, in grideditor_save
with open(path, ""w"", newline='', encoding=""utf8"") as fp:
PermissionError: [Errno 13] Permission denied: '/file.txt'",PermissionError,buggy_snippets_files/0853840b1a402db59b9f0c859947993205ff2ca759111c166fcbb8c445163a7f_before_merge.py,buggy_snippets_files/0853840b1a402db59b9f0c859947993205ff2ca759111c166fcbb8c445163a7f_after_merge.py
147565,"    def __m3u_export(self, file_path, files):
        try:
            fhandler = open(file_path, ""w"")
        except IOError:
            self.__file_error(file_path)
        else:
            text = ""#EXTM3U\n""

            for f in files:
                text += ""#EXTINF:%d,%s\n"" % (f['length'], f['title'])
                text += f['path'] + ""\n""

            fhandler.write(text.encode(""utf-8""))
            fhandler.close()
","    def __m3u_export(self, file_path, files):
        try:
            fhandler = open(file_path, ""wb"")
        except IOError:
            self.__file_error(file_path)
        else:
            text = ""#EXTM3U\n""

            for f in files:
                text += ""#EXTINF:%d,%s\n"" % (f['length'], f['title'])
                text += f['path'] + ""\n""

            fhandler.write(text.encode(""utf-8""))
            fhandler.close()
",https://github.com/quodlibet/quodlibet/issues/2425,CWE-628: Function Call with Incorrectly Specified Arguments,Reading binary data occurred without key b,quodlibet/quodlibet/ext/songsmenu/playlist.py,PlaylistExport.__m3u_export,[3],"Traceback (most recent call last):
File ""/home/nick/workspace/quodlibet/quodlibet/quodlibet/plugins/playlist.py"", line 179, in __handle
ret = plugin.plugin_single_playlist(pl)
File ""/home/nick/workspace/quodlibet/quodlibet/quodlibet/ext/songsmenu/playlist.py"", line 39, in plugin_single_playlist
return self.__save_playlist(playlist.songs, playlist.name)
File ""/home/nick/workspace/quodlibet/quodlibet/quodlibet/ext/songsmenu/playlist.py"", line 101, in __save_playlist
self.__m3u_export(file_path, files)
File ""/home/nick/workspace/quodlibet/quodlibet/quodlibet/ext/songsmenu/playlist.py"", line 147, in __m3u_export
fhandler.write(text.encode(""utf-8""))
TypeError: write() argument must be str, not bytes",TypeError,buggy_snippets_files/272e0ac84ac5d7d6e3ea70f1e3e34aaf8933b984b1b9871ce7e8c2d797f87440_before_merge.py,buggy_snippets_files/272e0ac84ac5d7d6e3ea70f1e3e34aaf8933b984b1b9871ce7e8c2d797f87440_after_merge.py
160290,"    @audioset.command()
    @checks.mod_or_permissions(administrator=True)
    async def thumbnail(self, ctx):
        """"""Toggle displaying a thumbnail on audio messages.""""""
        thumbnail = await self.config.guild(ctx.guild).thumbnail()
        await self.config.guild(ctx.guild).thumbnail.set(not thumbnail)
        await self._embed_msg(ctx, _(""Thumbnail display: {}."").format(not thumbnail))
","    @audioset.command()
    @checks.mod_or_permissions(administrator=True)
    async def thumbnail(self, ctx):
        """"""Toggle displaying a thumbnail on audio messages.""""""
        thumbnail = await self.config.guild(ctx.guild).thumbnail()
        await self.config.guild(ctx.guild).thumbnail.set(not thumbnail)
        await self._embed_msg(
            ctx, _(""Thumbnail display: {true_or_false}."").format(true_or_false=not thumbnail)
        )",https://github.com/Cog-Creators/Red-DiscordBot/issues/2190,CWE-628: Function Call with Incorrectly Specified Arguments,Passing an unnamed attribute,redbot/cogs/audio/audio.py,Audio.thumbnail,[18],"[06/10/2018 13:59] ERROR events on_command_error 208: Exception in command 'playlist list'
Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/discord/ext/commands/core.py"", line 61, in wrapped
ret = await coro(*args, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/redbot/cogs/audio/audio.py"", line 1100, in _playlist_list
_(""Author: {name}"").format(self.bot.get_user(author)),
KeyError: 'name'
[06/10/2018 13:59] ERROR events on_command_error 213: Exception in command 'playlist list'
Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/discord/ext/commands/core.py"", line 61, in wrapped
ret = await coro(*args, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/redbot/cogs/audio/audio.py"", line 1100, in _playlist_list
_(""Author: {name}"").format(self.bot.get_user(author)),
KeyError: 'name'",KeyError,buggy_snippets_files/f81ef120ed455add7140ad0f1cefc83e42e0edd472bb4dd994ca437193d08770_before_merge.py,buggy_snippets_files/f81ef120ed455add7140ad0f1cefc83e42e0edd472bb4dd994ca437193d08770_after_merge.py
135715,"    def on_play_file(self, file_info):
        self.window().left_menu_button_video_player.click()
        self.window().video_player_page.play_media_item(self.current_download[""infohash""], file_info[""index""])
","    def on_play_file(self, file_info):
        self.window().left_menu_button_video_player.click()
        self.window().video_player_page.play_media_item(self.current_download[""infohash""],
                                                        self.get_video_file_index(file_info[""index""]))
",https://github.com/Tribler/tribler/issues/3681,"CWE-166: Improper Handling of Missing Special Element, CWE-754: Improper Check for Unusual or Exceptional Conditions",No checking if file_info is None,TriblerGUI/widgets/downloadsdetailstabwidget.py,DownloadsDetailsTabWidget.on_play_file,[4],"Traceback (most recent call last):
File ""TriblerGUI/widgets/videoplayerpage.py"", line 155, in on_files_list_loaded
File ""TriblerGUI/widgets/videoplayerpage.py"", line 178, in play_active_item
TypeError: 'NoneType' object has no attribute '__getitem__'",TypeError,buggy_snippets_files/3d668c08fff89bcf4c85b8c35280688971e77adfde3349c19d0f06c9003ca24d_before_merge.py,buggy_snippets_files/3d668c08fff89bcf4c85b8c35280688971e77adfde3349c19d0f06c9003ca24d_after_merge.py
274985,"def delete_service_certificate(kwargs=None, conn=None, call=None):
    '''
    .. versionadded:: 2015.8.0

    Delete a specific certificate associated with the service

    CLI Examples:

    .. code-block:: bash

        salt-cloud -f delete_service_certificate my-azure name=my_service_certificate \
            thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
    '''
    if call != 'function':
        raise SaltCloudSystemExit(
            'The delete_service_certificate function must be called with -f or --function.'
        )

    if kwargs is None:
        kwargs = {}

    if 'name' not in kwargs:
        raise SaltCloudSystemExit('A name must be specified as ""name""')

    if 'thumbalgorithm' not in kwargs:
        raise SaltCloudSystemExit('A thumbalgorithm must be specified as ""thumbalgorithm""')

    if 'thumbprint' not in kwargs:
        raise SaltCloudSystemExit('A thumbprint must be specified as ""thumbprint""')

    if not conn:
        conn = get_conn()

    try:
        data = conn.delete_service_certificate(
            kwargs['name'],
            kwargs['thumbalgorithm'],
            kwargs['thumbprint'],
        )
        return {'Success': 'The service certificate was successfully deleted'}
    except WindowsAzureMissingResourceError as exc:
        return {'Error': exc.message}
","def delete_service_certificate(kwargs=None, conn=None, call=None):
    '''
    .. versionadded:: 2015.8.0

    Delete a specific certificate associated with the service

    CLI Examples:

    .. code-block:: bash

        salt-cloud -f delete_service_certificate my-azure name=my_service_certificate \\
            thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
    '''
    if call != 'function':
        raise SaltCloudSystemExit(
            'The delete_service_certificate function must be called with -f or --function.'
        )

    if kwargs is None:
        kwargs = {}

    if 'name' not in kwargs:
        raise SaltCloudSystemExit('A name must be specified as ""name""')

    if 'thumbalgorithm' not in kwargs:
        raise SaltCloudSystemExit('A thumbalgorithm must be specified as ""thumbalgorithm""')

    if 'thumbprint' not in kwargs:
        raise SaltCloudSystemExit('A thumbprint must be specified as ""thumbprint""')

    if not conn:
        conn = get_conn()

    try:
        data = conn.delete_service_certificate(
            kwargs['name'],
            kwargs['thumbalgorithm'],
            kwargs['thumbprint'],
        )
        return {'Success': 'The service certificate was successfully deleted'}
    except WindowsAzureMissingResourceError as exc:
        return {'Error': exc.message}
",https://github.com/saltstack/salt/issues/25463,"CWE-166: Improper Handling of Missing Special Element, CWE-754: Improper Check for Unusual or Exceptional Conditions",No checking if data is None,salt/cloud/clouds/msazure.py,delete_service_certificate,[26],"# salt-cloud -f add_input_endpoint az-mysetup name=http    port=80    protocol=tcp deployment=myproject service=myproject role=myproject

Traceback (most recent call last):
File ""/usr/local/lib/python2.7/site-packages/salt/cloud/cli.py"", line 245, in run
self.function_provider, self.function_name, kwargs
File ""/usr/local/lib/python2.7/site-packages/salt/cloud/__init__.py"", line 1525, in do_function
call='function', kwargs=kwargs
File ""/usr/local/lib/python2.7/site-packages/salt/cloud/clouds/msazure.py"", line 2190, in add_input_endpoint
activity='add',
File ""/usr/local/lib/python2.7/site-packages/salt/cloud/clouds/msazure.py"", line 2109, in update_input_endpoint
old_endpoints = list_input_endpoints(kwargs, call='function')
File ""/usr/local/lib/python2.7/site-packages/salt/cloud/clouds/msazure.py"", line 2013, in list_input_endpoints
for item in data:
TypeError: 'NoneType' object is not iterable",TypeError,buggy_snippets_files/cf45cf1b87c9099baaf7afd107ebffe5afc31022f971e8a82fe6e45bb10cb5d0_before_merge.py,buggy_snippets_files/cf45cf1b87c9099baaf7afd107ebffe5afc31022f971e8a82fe6e45bb10cb5d0_after_merge.py
209884,"def normalize_known_hosts_key(key):
    '''
    Transform a key, either taken from a known_host file or provided by the
    user, into a normalized form.
    The host part (which might include multiple hostnames or be hashed) gets
    replaced by the provided host. Also, any spurious information gets removed
    from the end (like the username@host tag usually present in hostkeys, but
    absent in known_hosts files)
    '''
    k = key.strip()  # trim trailing newline
    k = key.split()
    d = dict()
    # The optional ""marker"" field, used for @cert-authority or @revoked
    if k[0][0] == '@':
        d['options'] = k[0]
        d['host'] = k[1]
        d['type'] = k[2]
        d['key'] = k[3]
    else:
        d['host'] = k[0]
        d['type'] = k[1]
        d['key'] = k[2]
    return d
","def normalize_known_hosts_key(key):
    '''
    Transform a key, either taken from a known_host file or provided by the
    user, into a normalized form.
    The host part (which might include multiple hostnames or be hashed) gets
    replaced by the provided host. Also, any spurious information gets removed
    from the end (like the username@host tag usually present in hostkeys, but
    absent in known_hosts files)
    '''
    key = key.strip()  # trim trailing newline
    k = key.split()
    d = dict()
    # The optional ""marker"" field, used for @cert-authority or @revoked
    if k[0][0] == '@':
        d['options'] = k[0]
        d['host'] = k[1]
        d['type'] = k[2]
        d['key'] = k[3]
    else:
        d['host'] = k[0]
        d['type'] = k[1]
        d['key'] = k[2]
    return d
",https://github.com/ansible/ansible/issues/44284,Improper Interaction Between Multiple Correctly-Behaving Entities,Use variable k instead of key variable.,lib/ansible/modules/system/known_hosts.py,normalize_known_hosts_key,[10],"The full traceback is:
Traceback (most recent call last):
File ""/tmp/ansible_nz17ajoh/ansible_module_known_hosts.py"", line 332, in <module>
main()
File ""/tmp/ansible_nz17ajoh/ansible_module_known_hosts.py"", line 327, in main
results = enforce_state(module, module.params)
File ""/tmp/ansible_nz17ajoh/ansible_module_known_hosts.py"", line 106, in enforce_state
found, replace_or_add, found_line, key = search_for_host_key(module, host, key, hash_host, path, sshkeygen)
File ""/tmp/ansible_nz17ajoh/ansible_module_known_hosts.py"", line 254, in search_for_host_key
hashed_host = normalize_known_hosts_key(hashed_lines[lnum])
File ""/tmp/ansible_nz17ajoh/ansible_module_known_hosts.py"", line 278, in normalize_known_hosts_key
if k[0][0] == '@':
IndexError: list index out of range",IndexError,buggy_snippets_files/48bdc9ffc03fab0a520d9478f3b4579f7abe04b0f674c015ab770c6a5fec8c81_before_merge.py,buggy_snippets_files/48bdc9ffc03fab0a520d9478f3b4579f7abe04b0f674c015ab770c6a5fec8c81_after_merge.py
263772,"def shutdown_multiprocessing_logging_listener(daemonizing=False):
    global __MP_LOGGING_QUEUE
    global __MP_LOGGING_QUEUE_PROCESS
    global __MP_LOGGING_LISTENER_CONFIGURED

    if daemonizing is False and __MP_IN_MAINPROCESS is True:
        # We're in the MainProcess and we're not daemonizing, return!
        # No multiprocessing logging listener shutdown shall happen
        return
    if __MP_LOGGING_QUEUE_PROCESS is None:
        return
    if __MP_LOGGING_QUEUE_PROCESS.is_alive():
        logging.getLogger(__name__).debug('Stopping the multiprocessing logging queue listener')
        try:
            # Sent None sentinel to stop the logging processing queue
            __MP_LOGGING_QUEUE.put(None)
            # Let's join the multiprocessing logging handle thread
            time.sleep(0.5)
            logging.getLogger(__name__).debug('closing multiprocessing queue')
            __MP_LOGGING_QUEUE.close()
            logging.getLogger(__name__).debug('joining multiprocessing queue thread')
            __MP_LOGGING_QUEUE.join_thread()
            __MP_LOGGING_QUEUE = None
            __MP_LOGGING_QUEUE_PROCESS.join(1)
            __MP_LOGGING_QUEUE = None
        except IOError:
            # We were unable to deliver the sentinel to the queue
            # carry on...
            pass
        if __MP_LOGGING_QUEUE_PROCESS.is_alive():
            # Process is still alive!?
            __MP_LOGGING_QUEUE_PROCESS.terminate()
        __MP_LOGGING_QUEUE_PROCESS = None
        __MP_LOGGING_LISTENER_CONFIGURED = False
        logging.getLogger(__name__).debug('Stopped the multiprocessing logging queue listener')
","def shutdown_multiprocessing_logging_listener(daemonizing=False):
    global __MP_LOGGING_QUEUE
    global __MP_LOGGING_QUEUE_PROCESS
    global __MP_LOGGING_LISTENER_CONFIGURED

    if daemonizing is False and __MP_IN_MAINPROCESS is True:
        # We're in the MainProcess and we're not daemonizing, return!
        # No multiprocessing logging listener shutdown shall happen
        return

    if not daemonizing:
        # Need to remove the queue handler so that it doesn't try to send
        # data over a queue that was shut down on the listener end.
        shutdown_multiprocessing_logging()

    if __MP_LOGGING_QUEUE_PROCESS is None:
        return
    if __MP_LOGGING_QUEUE_PROCESS.is_alive():
        logging.getLogger(__name__).debug('Stopping the multiprocessing logging queue listener')
        try:
            # Sent None sentinel to stop the logging processing queue
            __MP_LOGGING_QUEUE.put(None)
            # Let's join the multiprocessing logging handle thread
            time.sleep(0.5)
            logging.getLogger(__name__).debug('closing multiprocessing queue')
            __MP_LOGGING_QUEUE.close()
            logging.getLogger(__name__).debug('joining multiprocessing queue thread')
            __MP_LOGGING_QUEUE.join_thread()
            __MP_LOGGING_QUEUE = None
            __MP_LOGGING_QUEUE_PROCESS.join(1)
            __MP_LOGGING_QUEUE = None
        except IOError:
            # We were unable to deliver the sentinel to the queue
            # carry on...
            pass
        if __MP_LOGGING_QUEUE_PROCESS.is_alive():
            # Process is still alive!?
            __MP_LOGGING_QUEUE_PROCESS.terminate()
        __MP_LOGGING_QUEUE_PROCESS = None
        __MP_LOGGING_LISTENER_CONFIGURED = False
        logging.getLogger(__name__).debug('Stopped the multiprocessing logging queue listener')
",https://github.com/saltstack/salt/issues/45301,Improper Interaction Between Multiple Correctly-Behaving Entities,Need to remove the queue handler so that it doesn't try to send data over a queue that was shut down on the listener end.,salt/log/setup.py,shutdown_multiprocessing_logging_listener,[10],"Traceback (most recent call last):
File ""c:\dev\salt\salt\log\handlers\__init__.py"", line 211, in emit
self.enqueue(self.prepare(record))
File ""c:\dev\salt\salt\log\handlers\__init__.py"", line 177, in enqueue
File ""c:\python27\lib\multiprocessing\queues.py"", line 264, in _feed
self.queue.put_nowait(record)
File ""c:\python27\lib\multiprocessing\queues.py"", line 155, in put_nowait
return self.put(obj, False)
File ""c:\python27\lib\multiprocessing\queues.py"", line 100, in put
assert not self._closed
AssertionError
Logged from file setup.py, line 973
send(obj)
IOError: [Errno 232] The pipe is being closed",IOError,buggy_snippets_files/6d17eb93e0b356a3bf7089d482e258417d52febe72f50d7e769081ab0ff14404_before_merge.py,buggy_snippets_files/6d17eb93e0b356a3bf7089d482e258417d52febe72f50d7e769081ab0ff14404_after_merge.py
144907,"    def run(self):
        """""" Start the primary Qt event loop for the interface """"""

        res = self.exec_()

        try:
            self.settings.save()
        except Exception as ex:
            log.error(""Couldn't save user settings on exit.\n{}"".format(ex))

        # return exit result
        return res
","    def run(self):
        """""" Start the primary Qt event loop for the interface """"""

        res = self.exec_()

        try:
            from classes.logger import log
            self.settings.save()
        except Exception as ex:
            log.error(""Couldn't save user settings on exit.\n{}"".format(ex))

        # return exit result
        return res
",https://github.com/OpenShot/openshot-qt/issues/2939,Improper Control of a Resource Through its Lifetime,Library (function) is used without import.,src/classes/app.py,OpenShotApp.run,[7],"...
timeline_webview:INFO UpdateLayerIndex
version:INFO Found current version: {""openshot_version"": ""2.4.4""}
main_window:INFO foundCurrentVersion: Found the latest version: 2.4.4
preview_thread:INFO onModeChanged
preview_thread:INFO exiting thread
json_data:ERROR Couldn't save user settings file:
C:\Users\***\.openshot_qt\openshot.settings
[Errno 13] Permission denied: 'C:\\Users\\***\\.openshot_qt\\openshot.settings'
exceptions:ERROR Unhandled Exception
Traceback (most recent call last):
File ""I:\Temp\OpenShot-dev\01\{app}\classes\json_data.py"", line 151, in write_to_file
with open(file_path, 'w') as f:
PermissionError: [Errno 13] Permission denied: 'C:\\Users\\***\\.openshot_qt\\openshot.settings'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""I:\Temp\OpenShot-dev\01\{app}\classes\app.py"", line 229, in run
self.settings.save()
File ""I:\Temp\OpenShot-dev\01\{app}\classes\settings.py"", line 118, in save
self.write_to_file(file_path, self._data)
File ""I:\Temp\OpenShot-dev\01\{app}\classes\json_data.py"", line 156, in write_to_file
raise Exception(msg)
Exception: Couldn't save user settings file:
C:\Users\***\.openshot_qt\openshot.settings
[Errno 13] Permission denied: 'C:\\Users\\***\\.openshot_qt\\openshot.settings'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\msys64\mingw64\lib\python3.7\site-packages\cx_Freeze\initscripts\__startup__.py"", line 14, in run
File ""C:\msys64\mingw64\lib\python3.7\site-packages\cx_Freeze\initscripts\Console.py"", line 26, in run
File ""openshot_qt/launch.py"", line 103, in <module>
File ""openshot_qt/launch.py"", line 99, in main
File ""I:\Temp\OpenShot-dev\01\{app}\classes\app.py"", line 231, in run
log.error(""Couldn't save user settings on exit.\n{}"".format(ex))
NameError: name 'log' is not defined",PermissionError,buggy_snippets_files/ff3e96b17ff0a3887eea0f6b6c86f465d7b766c0f00d7bcb95ee70fb63996826_before_merge.py,buggy_snippets_files/ff3e96b17ff0a3887eea0f6b6c86f465d7b766c0f00d7bcb95ee70fb63996826_after_merge.py
110038,"    def _link_package_versions(self, link, search_name):
        """"""
        Return an iterable of triples (pkg_resources_version_key,
        link, python_version) that can be extracted from the given
        link.

        Meant to be overridden by subclasses, not called by clients.
        """"""
        if link.egg_fragment:
            egg_info = link.egg_fragment
        else:
            egg_info, ext = link.splitext()
            if not ext:
                if link not in self.logged_links:
                    logger.debug('Skipping link %s; not a file' % link)
                    self.logged_links.add(link)
                return []
            if egg_info.endswith('.tar'):
                # Special double-extension case:
                egg_info = egg_info[:-4]
                ext = '.tar' + ext
            if ext not in ('.tar.gz', '.tar.bz2', '.tar', '.tgz', '.zip'):
                if link not in self.logged_links:
                    logger.debug('Skipping link %s; unknown archive format: %s' % (link, ext))
                    self.logged_links.add(link)
                return []
        version = self._egg_info_matches(egg_info, search_name, link)
        if version is None:
            logger.debug('Skipping link %s; wrong project name (not %s)' % (link, search_name))
            return []
        match = self._py_version_re.search(version)
        if match:
            version = version[:match.start()]
            py_version = match.group(1)
            if py_version != sys.version[:3]:
                logger.debug('Skipping %s because Python version is incorrect' % link)
                return []
        logger.debug('Found link %s, version: %s' % (link, version))
        return [(pkg_resources.parse_version(version),
               link,
               version)]
","    def _link_package_versions(self, link, search_name):
        """"""
        Return an iterable of triples (pkg_resources_version_key,
        link, python_version) that can be extracted from the given
        link.

        Meant to be overridden by subclasses, not called by clients.
        """"""
        if link.egg_fragment:
            egg_info = link.egg_fragment
        else:
            egg_info, ext = link.splitext()
            if not ext:
                if link not in self.logged_links:
                    logger.debug('Skipping link %s; not a file' % link)
                    self.logged_links.add(link)
                return []
            if egg_info.endswith('.tar'):
                # Special double-extension case:
                egg_info = egg_info[:-4]
                ext = '.tar' + ext
            if ext not in ('.tar.gz', '.tar.bz2', '.tar', '.tgz', '.zip'):
                if link not in self.logged_links:
                    logger.debug('Skipping link %s; unknown archive format: %s' % (link, ext))
                    self.logged_links.add(link)
                return []
            if ""macosx10"" in link.path and ext in ('.zip'):
                if link not in self.logged_links:
                    logger.debug('Skipping link %s; macosx10 one' % (link))
                    self.logged_links.add(link)
                return []
        version = self._egg_info_matches(egg_info, search_name, link)
        if version is None:
            logger.debug('Skipping link %s; wrong project name (not %s)' % (link, search_name))
            return []
        match = self._py_version_re.search(version)
        if match:
            version = version[:match.start()]
            py_version = match.group(1)
            if py_version != sys.version[:3]:
                logger.debug('Skipping %s because Python version is incorrect' % link)
                return []
        logger.debug('Found link %s, version: %s' % (link, version))
        return [(pkg_resources.parse_version(version),
               link,
               version)]
",https://github.com/pypa/pip/issues/57,Improper Check for Unusual or Exceptional Conditions,"If macosx10, then another logic.",pip/index.py,PackageFinder._link_package_versions,[27],"zyv@mypride:~/Desktop$ pip install pygame
Downloading/unpacking pygame
Downloading pygame-1.9.1release-py2.6-macosx10.5.zip (10.8Mb): 10.8Mb downloaded
Running setup.py egg_info for package pygame
Traceback (most recent call last):
File ""<string>"", line 14, in <module>
IOError: [Errno 2] No such file or directory: '/home/zyv/Desktop/build/pygame/setup.py'
Complete output from command python setup.py egg_info:
Traceback (most recent call last):

File ""<string>"", line 14, in <module>

IOError: [Errno 2] No such file or directory: '/home/zyv/Desktop/build/pygame/setup.py'

----------------------------------------
Command python setup.py egg_info failed with error code 1
Storing complete log in /home/zyv/.pip/pip.log",IOError,buggy_snippets_files/a50d45787ec4de8efbeada07c6347a53121e164ddd05d94c7a58b794d264d985_before_merge.py,buggy_snippets_files/a50d45787ec4de8efbeada07c6347a53121e164ddd05d94c7a58b794d264d985_after_merge.py
377361,"def main(args):
    """"""
    Store the defect results in the specified input list as bug reports in the
    database.
    """"""

    if not host_check.check_zlib():
        raise Exception(""zlib is not available on the system!"")

    # To ensure the help message prints the default folder properly,
    # the 'default' for 'args.input' is a string, not a list.
    # But we need lists for the foreach here to work.
    if isinstance(args.input, str):
        args.input = [args.input]

    if 'name' not in args:
        LOG.debug(""Generating name for analysis..."")
        generated = __get_run_name(args.input)
        if generated:
            setattr(args, 'name', generated)
        else:
            LOG.error(""No suitable name was found in the inputs for the ""
                      ""analysis run. Please specify one by passing argument ""
                      ""--name run_name in the invocation."")
            sys.exit(2)  # argparse returns error code 2 for bad invocations.

    LOG.info(""Storing analysis results for run '"" + args.name + ""'"")

    if 'force' in args:
        LOG.info(""argument --force was specified: the run with name '"" +
                 args.name + ""' will be deleted."")

    _, host, port, product_name = split_product_url(args.product_url)

    # Before any transmission happens, check if we have the PRODUCT_STORE
    # permission to prevent a possibly long ZIP operation only to get an
    # error later on.
    product_client = libclient.setup_product_client(host, port, product_name)
    product_id = product_client.getCurrentProduct().id

    auth_client, _ = libclient.setup_auth_client(host, port)
    has_perm = libclient.check_permission(
        auth_client, Permission.PRODUCT_STORE, {'productID': product_id})
    if not has_perm:
        LOG.error(""You are not authorised to store analysis results in ""
                  ""product '{0}'"".format(product_name))
        sys.exit(1)

    # Setup connection to the remote server.
    client = libclient.setup_client(args.product_url)

    LOG.debug(""Initializing client connecting to {0}:{1}/{2} done.""
              .format(host, port, product_name))

    _, zip_file = tempfile.mkstemp('.zip')
    LOG.debug(""Will write mass store ZIP to '{0}'..."".format(zip_file))

    try:
        assemble_zip(args.input, zip_file, client)
        with open(zip_file, 'rb') as zf:
            b64zip = base64.b64encode(zf.read())

        context = generic_package_context.get_context()

        client.massStoreRun(args.name,
                            context.version,
                            b64zip,
                            'force' in args)

        LOG.info(""Storage finished successfully."")
    except Exception as ex:
        LOG.info(""Storage failed: "" + str(ex))
    finally:
        os.remove(zip_file)
","def main(args):
    """"""
    Store the defect results in the specified input list as bug reports in the
    database.
    """"""

    if not host_check.check_zlib():
        raise Exception(""zlib is not available on the system!"")

    # To ensure the help message prints the default folder properly,
    # the 'default' for 'args.input' is a string, not a list.
    # But we need lists for the foreach here to work.
    if isinstance(args.input, str):
        args.input = [args.input]

    if 'name' not in args:
        LOG.debug(""Generating name for analysis..."")
        generated = __get_run_name(args.input)
        if generated:
            setattr(args, 'name', generated)
        else:
            LOG.error(""No suitable name was found in the inputs for the ""
                      ""analysis run. Please specify one by passing argument ""
                      ""--name run_name in the invocation."")
            sys.exit(2)  # argparse returns error code 2 for bad invocations.

    LOG.info(""Storing analysis results for run '"" + args.name + ""'"")

    if 'force' in args:
        LOG.info(""argument --force was specified: the run with name '"" +
                 args.name + ""' will be deleted."")

    _, host, port, product_name = split_product_url(args.product_url)

    # Before any transmission happens, check if we have the PRODUCT_STORE
    # permission to prevent a possibly long ZIP operation only to get an
    # error later on.
    product_client = libclient.setup_product_client(host, port, product_name)
    product_id = product_client.getCurrentProduct().id

    auth_client, _ = libclient.setup_auth_client(host, port)
    has_perm = libclient.check_permission(
        auth_client, Permission.PRODUCT_STORE, {'productID': product_id})
    if not has_perm:
        LOG.error(""You are not authorised to store analysis results in ""
                  ""product '{0}'"".format(product_name))
        sys.exit(1)

    # Setup connection to the remote server.
    client = libclient.setup_client(args.product_url)

    LOG.debug(""Initializing client connecting to {0}:{1}/{2} done.""
              .format(host, port, product_name))

    _, zip_file = tempfile.mkstemp('.zip')
    LOG.debug(""Will write mass store ZIP to '{0}'..."".format(zip_file))

    try:
        assemble_zip(args.input, zip_file, client)
        with open(zip_file, 'rb') as zf:
            b64zip = base64.b64encode(zf.read())

        context = generic_package_context.get_context()

        client.massStoreRun(args.name,
                            context.version,
                            b64zip,
                            'force' in args)

        LOG.info(""Storage finished successfully."")
    except Exception as ex:
        LOG.info(""Storage failed: "" + str(ex))
        sys.exit(1)
    finally:
        os.remove(zip_file)
",https://github.com/Ericsson/codechecker/issues/878,Non-exit on Failed Initialization,Exit(1) if storage failed.,libcodechecker/libhandlers/store.py,main,[73],"[14:00] - 127.0.0.1:55233 -- [Anonymous] POST /Xerces/CodeCheckerService
Traceback (most recent call last):
File ""/home/compass/srv/CodeCheckerv6/workspace/install/CodeChecker/lib/python2.7/gencodechecker/codeCheckerDBAccess/codeCheckerDBAccess.py"", line 2478, in
process_massStoreRun
result.success = self._handler.massStoreRun(args.run_name, args.version, args.zipfile, args.force)
File ""/home/compass/srv/CodeCheckerv6/workspace/install/CodeChecker/lib/python2.7/libcodechecker/profiler.py"", line 45, in release_wrapper
res = function(*args, **kwargs)
File ""/home/compass/srv/CodeCheckerv6/workspace/install/CodeChecker/lib/python2.7/libcodechecker/server/client_db_access_handler.py"", line 2355, in massStor
eRun
store_handler.collect_paths_events(report, file_ids)
File ""/home/compass/srv/CodeCheckerv6/workspace/install/CodeChecker/lib/python2.7/libcodechecker/analyze/store_handler.py"", line 106, in collect_paths_event
s
file_path = files[event['location']['file']]
IndexError: list index out of range
[14:02] - Session timeout for run 4",IndexError,buggy_snippets_files/afdf10704f100eddbb335c8a7f8791f7dd02262bb4c619f37c99d429735c0b61_before_merge.py,buggy_snippets_files/afdf10704f100eddbb335c8a7f8791f7dd02262bb4c619f37c99d429735c0b61_after_merge.py
148206,"    def handle_compare(self, args):
        if len(args) < 2:
            print(""Error: missing argument"")
            return 1
        addr = self.convert_value(args[0])
        if len(args) < 3:
            filename = args[1]
            length = None
        else:
            filename = args[2]
            length = self.convert_value(args[1])

        region = self.session.target.memory_map.get_region_for_address(addr)
        flash_init_required =  region is not None and region.is_flash and not region.is_powered_on_boot and region.flash is not None
        if flash_init_required:
            region.flash.init(region.flash.Operation.VERIFY)

        with open(filename, 'rb') as f:
            file_data = f.read(length)
        
        if length is None:
            length = len(file_data)
        elif len(file_data) < length:
            print(""File is %d bytes long; reducing comparison length to match."" % len(file_data))
            length = len(file_data)
        
        # Divide into 32 kB chunks.
        CHUNK_SIZE = 32 * 1024
        chunk_count = (length + CHUNK_SIZE - 1) // CHUNK_SIZE
        
        end_addr = addr + length
        offset = 0
        mismatch = False
        
        for chunk in range(chunk_count):
            # Get this chunk's size.
            chunk_size = min(end_addr - addr, CHUNK_SIZE)
            print(""Comparing %d bytes @ 0x%08x"" % (chunk_size, addr))
            
            data = bytearray(self.target.aps[self.selected_ap].read_memory_block8(addr, chunk_size))
            
            for i in range(chunk_size):
                if data[i] != file_data[offset+i]:
                    mismatch = True
                    print(""Mismatched byte at 0x%08x (offset 0x%x): 0x%02x (memory) != 0x%02x (file)"" %
                        (addr + i, offset + i, data[i], file_data[offset+i]))
                    break
        
            if mismatch:
                break
        
            offset += chunk_size
            addr += chunk_size
        
        if not mismatch:
            print(""All %d bytes match."" % length)

        if flash_init_required:
            region.flash.cleanup()
    ","    def handle_compare(self, args):
        if len(args) < 2:
            print(""Error: missing argument"")
            return 1
        addr = self.convert_value(args[0])
        if len(args) < 3:
            filename = args[1]
            length = None
        else:
            filename = args[2]
            length = self.convert_value(args[1])

        region = self.session.target.memory_map.get_region_for_address(addr)
        flash_init_required =  region is not None and region.is_flash and not region.is_powered_on_boot and region.flash is not None
        if flash_init_required:
            try:
                region.flash.init(region.flash.Operation.VERIFY)
            except exceptions.FlashFailure:
                region.flash.init(region.flash.Operation.ERASE)

        with open(filename, 'rb') as f:
            file_data = f.read(length)
        
        if length is None:
            length = len(file_data)
        elif len(file_data) < length:
            print(""File is %d bytes long; reducing comparison length to match."" % len(file_data))
            length = len(file_data)
        
        # Divide into 32 kB chunks.
        CHUNK_SIZE = 32 * 1024
        chunk_count = (length + CHUNK_SIZE - 1) // CHUNK_SIZE
        
        end_addr = addr + length
        offset = 0
        mismatch = False
        
        for chunk in range(chunk_count):
            # Get this chunk's size.
            chunk_size = min(end_addr - addr, CHUNK_SIZE)
            print(""Comparing %d bytes @ 0x%08x"" % (chunk_size, addr))
            
            data = bytearray(self.target.aps[self.selected_ap].read_memory_block8(addr, chunk_size))
            
            for i in range(chunk_size):
                if data[i] != file_data[offset+i]:
                    mismatch = True
                    print(""Mismatched byte at 0x%08x (offset 0x%x): 0x%02x (memory) != 0x%02x (file)"" %
                        (addr + i, offset + i, data[i], file_data[offset+i]))
                    break
        
            if mismatch:
                break
        
            offset += chunk_size
            addr += chunk_size
        
        if not mismatch:
            print(""All %d bytes match."" % length)

        if flash_init_required:
            region.flash.cleanup()
    ",https://github.com/pyocd/pyOCD/issues/865,Uncaught Exception,Added exception handling (try).,pyocd/tools/pyocd.py,PyOCDCommander.handle_compare,[16],"$ pyocd gdbserver --trust-crc --uid 0231000026294e450024000e58a900543ee1000097969900 --port 7100
0000675:WARNING:common:STLink and CMSIS-DAPv2 probes are not supported because no libusb library was found.
0001093:INFO:board:Target type is k22f
0001108:INFO:coresight_target:Performing connect pre-reset
0001342:INFO:dap:DP IDR = 0x2ba01477 (v1 rev2)
0001367:INFO:ap:AP#0 IDR = 0x24770011 (AHB-AP var1 rev2)
0001386:INFO:ap:AP#1 IDR = 0x001c0000 (proprietary)
0003197:WARNING:target_kinetis:K22F in secure state: will try to unlock via mass erase
0003426:WARNING:target_kinetis:K22F secure state: unlocked successfully
0003440:INFO:rom_table:AP#0 Class 0x1 ROM table #0 @ 0xe00ff000 (designer=43b part=4c4)
0003451:INFO:rom_table:[0]<e000e000:SCS-M4 class=14 designer=43b part=00c>
0003459:INFO:rom_table:[1]<e0001000:DWT class=14 designer=43b part=002>
0003467:INFO:rom_table:[2]<e0002000:FPB class=14 designer=43b part=003>
0003476:INFO:rom_table:[3]<e0000000:ITM class=14 designer=43b part=001>
0003484:INFO:rom_table:[4]<e0040000:TPIU-M4 class=9 designer=43b part=9a1 devtype=11 archid=0000 devid=ca0:0:0>
0003488:INFO:cortex_m:CPU core #0 is Cortex-M4 r0p1
0003501:INFO:dwt:4 hardware watchpoints
0003506:INFO:fpb:6 hardware breakpoints, 4 literal comparators
0003538:INFO:server:Semihost server started on port 4444 (core 0)
0003539:INFO:gdbserver:GDB server started on port 7100 (core 0)
0004069:INFO:gdbserver:One client connected!
0004102:ERROR:gdbserver:Unhandled exception in handle_message: type object 'CortexM' has no attribute 'DFSR_PMU'
Traceback (most recent call last):
File ""c:\python27\lib\site-packages\pyocd\gdbserver\gdbserver.py"", line 381, in handle_message
reply = handler()
File ""c:\python27\lib\site-packages\pyocd\gdbserver\gdbserver.py"", line 516, in stop_reason_query
return self.create_rsp_packet(self.get_t_response())
File ""c:\python27\lib\site-packages\pyocd\gdbserver\gdbserver.py"", line 1116, in get_t_response
response = self.target_facade.get_t_response(forceSignal)
File ""c:\python27\lib\site-packages\pyocd\gdbserver\context_facade.py"", line 138, in get_t_response
response = six.b('T' + conversion.byte_to_hex2(self.get_signal_value()))
File ""c:\python27\lib\site-packages\pyocd\gdbserver\context_facade.py"", line 152, in get_signal_value
if self._context.core.is_vector_catch():
File ""c:\python27\lib\site-packages\pyocd\coresight\cortex_m.py"", line 1335, in is_vector_catch
return self.get_halt_reason() == Target.HaltReason.VECTOR_CATCH
File ""c:\python27\lib\site-packages\pyocd\coresight\cortex_m.py"", line 1349, in get_halt_reason
elif dfsr &amp; CortexM.DFSR_PMU:
AttributeError: type object 'CortexM' has no attribute 'DFSR_PMU'
0008320:ERROR:gdbserver:Unhandled exception in handle_message: init error: 536870913
Traceback (most recent call last):
File ""c:\python27\lib\site-packages\pyocd\gdbserver\gdbserver.py"", line 383, in handle_message
reply = handler(msg[msgStart:])
File ""c:\python27\lib\site-packages\pyocd\gdbserver\gdbserver.py"", line 602, in v_command
return self.flash_op(data)
File ""c:\python27\lib\site-packages\pyocd\gdbserver\gdbserver.py"", line 718, in flash_op
self.flash_loader.commit()
File ""c:\python27\lib\site-packages\pyocd\flash\loader.py"", line 168, in commit
keep_unwritten=self._keep_unwritten)
File ""c:\python27\lib\site-packages\pyocd\flash\builder.py"", line 417, in program
self._build_sectors_and_pages(keep_unwritten)
File ""c:\python27\lib\site-packages\pyocd\flash\builder.py"", line 315, in _build_sectors_and_pages
fill_end_of_page_gap()
File ""c:\python27\lib\site-packages\pyocd\flash\builder.py"", line 259, in fill_end_of_page_gap
self._enable_read_access()
File ""c:\python27\lib\site-packages\pyocd\flash\builder.py"", line 221, in _enable_read_access
self.flash.init(self.flash.Operation.VERIFY)
File ""c:\python27\lib\site-packages\pyocd\flash\flash.py"", line 250, in init
raise FlashFailure('init error: %i' % result, result_code=result)
FlashFailure: init error: 536870913
0008342:INFO:gdbserver:Client detached
0008342:INFO:gdbserver:Client disconnected!
0008546:INFO:server:Semihost server stopped",AttributeError,buggy_snippets_files/61aa21e92e51d6c3897f47c6ade2dc08a8d493a0f4d5ddc88a041875b0ae2b2e_before_merge.py,buggy_snippets_files/61aa21e92e51d6c3897f47c6ade2dc08a8d493a0f4d5ddc88a041875b0ae2b2e_after_merge.py
58219,"    def query(self, view_kwargs):
        """"""
        query method for Attendees List
        :param view_kwargs:
        :return:
        """"""
        query_ = self.session.query(TicketHolder)

        if view_kwargs.get('order_identifier'):
            order = safe_query(self, Order, 'identifier', view_kwargs['order_identifier'], 'order_identifier')
            if not has_access('is_registrar', event_id=order.event_id) or not has_access('is_user_itself',
                                                                                         user_id=order.user_id):
                raise ForbiddenException({'source': ''}, 'Access Forbidden')
            query_ = query_.join(Order).filter(Order.id == order.id)

        if view_kwargs.get('ticket_id'):
            ticket = safe_query(self, Ticket, 'id', view_kwargs['ticket_id'], 'ticket_id')
            if not has_access('is_registrar', event_id=ticket.event_id):
                raise ForbiddenException({'source': ''}, 'Access Forbidden')
            query_ = query_.join(Ticket).filter(Ticket.id == ticket.id)

        if view_kwargs.get('user_id'):
            user = safe_query(self, User, 'id', view_kwargs['user_id'], 'user_id')
            if not has_access('is_user_itself', user_id=user.id):
                raise ForbiddenException({'source': ''}, 'Access Forbidden')
            query_ = query_.join(User, User.email == TicketHolder.email).filter(User.id == user.id)

        query_ = event_query(self, query_, view_kwargs, permission='is_registrar')
        return query_
","    def query(self, view_kwargs):
        """"""
        query method for Attendees List
        :param view_kwargs:
        :return:
        """"""
        query_ = self.session.query(TicketHolder)

        if view_kwargs.get('order_identifier'):
            order = safe_query(self, Order, 'identifier', view_kwargs['order_identifier'], 'order_identifier')
            if not has_access('is_registrar', event_id=order.event_id) and not has_access('is_user_itself',
                                                                                         user_id=order.user_id):
                raise ForbiddenException({'source': ''}, 'Access Forbidden')
            query_ = query_.join(Order).filter(Order.id == order.id)

        if view_kwargs.get('ticket_id'):
            ticket = safe_query(self, Ticket, 'id', view_kwargs['ticket_id'], 'ticket_id')
            if not has_access('is_registrar', event_id=ticket.event_id):
                raise ForbiddenException({'source': ''}, 'Access Forbidden')
            query_ = query_.join(Ticket).filter(Ticket.id == ticket.id)

        if view_kwargs.get('user_id'):
            user = safe_query(self, User, 'id', view_kwargs['user_id'], 'user_id')
            if not has_access('is_user_itself', user_id=user.id):
                raise ForbiddenException({'source': ''}, 'Access Forbidden')
            query_ = query_.join(User, User.email == TicketHolder.email).filter(User.id == user.id)

        query_ = event_query(self, query_, view_kwargs, permission='is_registrar')
        return query_
",https://github.com/fossasia/open-event-server/issues/4899,Incorrect Calculation,Wrong logical operator.,app/api/attendees.py,AttendeeList.query,[11],"Exception on /v1/orders/fe5a8c1d-ac6d-4bd3-ac4b-0540d4bcc03b/attendees [GET]
Traceback (most recent call last):
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
response = self.full_dispatch_request()
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
rv = self.handle_user_exception(e)
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask_cors/extension.py"", line 161, in wrapped_function
return cors_after_request(app.make_response(f(*args, **kwargs)))
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
reraise(exc_type, exc_value, tb)
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
raise value
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
rv = self.dispatch_request()
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
return self.view_functions[rule.endpoint](**req.view_args)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/src/flask-rest-jsonapi/flask_rest_jsonapi/decorators.py"", line 32, in wrapper
return func(*args, **kwargs)
File ""/home/srv_twry/anaconda3/envs/open-event-server/lib/python3.6/site-packages/flask/views.py"", line 88, in view
return self.dispatch_request(*args, **kwargs)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/src/flask-rest-jsonapi/flask_rest_jsonapi/resource.py"", line 68, in dispatch_request
response = method(*args, **kwargs)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/src/flask-rest-jsonapi/flask_rest_jsonapi/decorators.py"", line 56, in wrapper
return func(*args, **kwargs)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/src/flask-rest-jsonapi/flask_rest_jsonapi/resource.py"", line 145, in get
objects_count, objects = self._data_layer.get_collection(qs, kwargs)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/src/flask-rest-jsonapi/flask_rest_jsonapi/data_layers/alchemy.py"", line 92, in get_collection
query = self.query(view_kwargs)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/app/api/attendees.py"", line 81, in query
id=order.user_id):
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/app/api/helpers/permission_manager.py"", line 445, in has_access
auth = permissions[access_level](lambda *a, **b: True, (), {}, (), **kwargs)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/app/api/helpers/permissions.py"", line 45, in decorator
return fn(*args, **kwargs)
File ""/media/srv_twry/work/Projects/Community-Projects/Fossasia/open-event-server/app/api/helpers/permission_manager.py"", line 109, in is_user_itself
if not user.is_admin and not user.is_super_admin and user.id != kwargs['user_id']:
KeyError: 'user_id'",KeyError,buggy_snippets_files/4654a8063a12c7fb2e2321aca7e8078ab6a1fe61cc8e113ef5ea8ce7e30c2d9c_before_merge.py,buggy_snippets_files/4654a8063a12c7fb2e2321aca7e8078ab6a1fe61cc8e113ef5ea8ce7e30c2d9c_after_merge.py
137023,"    @blocking_call_on_reactor_thread
    def load_communities(self):
        self._logger.info(""tribler: Preparing communities..."")
        now_time = timemod.time()
        default_kwargs = {'tribler_session': self.session}

        # Search Community
        if self.session.get_enable_torrent_search():
            from Tribler.community.search.community import SearchCommunity
            self.dispersy.define_auto_load(SearchCommunity, self.session.dispersy_member, load=True,
                                           kargs=default_kwargs)

        # AllChannel Community
        if self.session.get_enable_channel_search():
            from Tribler.community.allchannel.community import AllChannelCommunity
            self.dispersy.define_auto_load(AllChannelCommunity, self.session.dispersy_member, load=True,
                                           kargs=default_kwargs)

        # Channel Community
        if self.session.get_channel_community_enabled():
            from Tribler.community.channel.community import ChannelCommunity
            self.dispersy.define_auto_load(ChannelCommunity,
                                           self.session.dispersy_member, load=True, kargs=default_kwargs)

        # PreviewChannel Community
        if self.session.get_preview_channel_community_enabled():
            from Tribler.community.channel.preview import PreviewChannelCommunity
            self.dispersy.define_auto_load(PreviewChannelCommunity,
                                           self.session.dispersy_member, kargs=default_kwargs)

        if self.session.get_tunnel_community_enabled():
            tunnel_settings = TunnelSettings(tribler_session=self.session)
            tunnel_kwargs = {'tribler_session': self.session, 'settings': tunnel_settings}

            if self.session.get_enable_multichain():
                multichain_kwargs = {'tribler_session': self.session}

                # If the multichain is enabled, we use the permanent multichain keypair
                # for both the multichain and the tunnel community
                keypair = self.session.multichain_keypair
                dispersy_member = self.dispersy.get_member(private_key=keypair.key_to_bin())

                from Tribler.community.multichain.community import MultiChainCommunity
                self.dispersy.define_auto_load(MultiChainCommunity,
                                               dispersy_member,
                                               load=True,
                                               kargs=multichain_kwargs)

            else:
                keypair = self.dispersy.crypto.generate_key(u""curve25519"")
                dispersy_member = self.dispersy.get_member(private_key=self.dispersy.crypto.key_to_bin(keypair))

            from Tribler.community.tunnel.hidden_community import HiddenTunnelCommunity
            self.tunnel_community = self.dispersy.define_auto_load(HiddenTunnelCommunity, dispersy_member,
                                                                   load=True, kargs=tunnel_kwargs)[0]

        self._logger.info(""tribler: communities are ready in %.2f seconds"", timemod.time() - now_time)
","    @blocking_call_on_reactor_thread
    def load_communities(self):
        self._logger.info(""tribler: Preparing communities..."")
        now_time = timemod.time()
        default_kwargs = {'tribler_session': self.session}

        # Search Community
        if self.session.get_enable_torrent_search():
            from Tribler.community.search.community import SearchCommunity
            self.dispersy.define_auto_load(SearchCommunity, self.session.dispersy_member, load=True,
                                           kargs=default_kwargs)

        # AllChannel Community
        if self.session.get_enable_channel_search():
            from Tribler.community.allchannel.community import AllChannelCommunity
            self.dispersy.define_auto_load(AllChannelCommunity, self.session.dispersy_member, load=True,
                                           kargs=default_kwargs)

        # Channel Community
        if self.session.get_channel_community_enabled():
            from Tribler.community.channel.community import ChannelCommunity
            self.dispersy.define_auto_load(ChannelCommunity,
                                           self.session.dispersy_member, load=True, kargs=default_kwargs)

        # PreviewChannel Community
        if self.session.get_preview_channel_community_enabled():
            from Tribler.community.channel.preview import PreviewChannelCommunity
            self.dispersy.define_auto_load(PreviewChannelCommunity,
                                           self.session.dispersy_member, kargs=default_kwargs)

        if self.session.get_tunnel_community_enabled():
            tunnel_settings = TunnelSettings(tribler_session=self.session)
            tunnel_kwargs = {'tribler_session': self.session, 'settings': tunnel_settings}

            if self.session.get_enable_multichain():
                multichain_kwargs = {'tribler_session': self.session}

                # If the multichain is enabled, we use the permanent multichain keypair
                # for both the multichain and the tunnel community
                keypair = self.session.multichain_keypair
                dispersy_member = self.dispersy.get_member(private_key=keypair.key_to_bin())

                from Tribler.community.multichain.community import MultiChainCommunity
                self.dispersy.define_auto_load(MultiChainCommunity,
                                               dispersy_member,
                                               load=True,
                                               kargs=multichain_kwargs)

            else:
                keypair = self.dispersy.crypto.generate_key(u""curve25519"")
                dispersy_member = self.dispersy.get_member(private_key=self.dispersy.crypto.key_to_bin(keypair))

            from Tribler.community.tunnel.hidden_community import HiddenTunnelCommunity
            self.tunnel_community = self.dispersy.define_auto_load(HiddenTunnelCommunity, dispersy_member,
                                                                   load=True, kargs=tunnel_kwargs)[0]

            # We don't want to automatically load other instances of this community with other master members.
            self.dispersy.undefine_auto_load(HiddenTunnelCommunity)

        self._logger.info(""tribler: communities are ready in %.2f seconds"", timemod.time() - now_time)
",https://github.com/Tribler/tribler/issues/2885,Improper Interaction Between Multiple Correctly-Behaving Entities,Project doesn't want to automatically load other instances of the community with other master members.,Tribler/Core/APIImplementation/LaunchManyCore.py,TriblerLaunchMany.load_communities,[57],"Traceback (most recent call last):
File ""TriblerGUI\event_request_manager.py"", line 92, in on_read_data
RuntimeError: Unhandled Error
Traceback (most recent call last):
File ""lib\multiprocessing\process.py"", line 114, in run

File ""TriblerGUI\core_manager.py"", line 63, in start_tribler_core

File ""lib\site-packages\twisted\internet\base.py"", line 1195, in run

File ""lib\site-packages\twisted\internet\base.py"", line 1204, in mainLoop

--- <exception caught here> ---
File ""lib\site-packages\twisted\internet\base.py"", line 798, in runUntilCurrent

File ""Tribler\dispersy\endpoint.py"", line 253, in dispersythread_data_came_in

File ""Tribler\dispersy\dispersy.py"", line 1488, in on_incoming_packets

File ""Tribler\dispersy\dispersy.py"", line 685, in get_community

File ""Tribler\dispersy\community.py"", line 226, in init_community

File ""Tribler\community\tunnel\tunnel_community.py"", line 316, in initialize

File ""Tribler\community\tunnel\Socks5\server.py"", line 309, in start

File ""lib\site-packages\twisted\internet\posixbase.py"", line 478, in listenTCP

File ""lib\site-packages\twisted\internet\tcp.py"", line 983, in startListening

twisted.internet.error.CannotListenError: Couldn't listen on any:55661: [Errno 10048] Elk socketadres (protocol/netwerkadres/poort) kan normaal slechts n keer worden gebruikt.",RuntimeError,buggy_snippets_files/bf42b0cfa94115faea0cf5cd29eefe38138486f9ac6489a75ddf5fe947495b5b_before_merge.py,buggy_snippets_files/bf42b0cfa94115faea0cf5cd29eefe38138486f9ac6489a75ddf5fe947495b5b_after_merge.py
221836,"    def single_string(self):
        """"""Creates a long string with the ascii art.

        Returns:
            str: The lines joined by a newline (``\\n``)
        """"""
        return ""\n"".join(self.lines())
","    def single_string(self):
        """"""Creates a long string with the ascii art.
        Returns:
            str: The lines joined by a newline (``\\n``)
        """"""
        try:
            return ""\n"".join(self.lines()).encode(self.encoding).decode(self.encoding)
        except (UnicodeEncodeError, UnicodeDecodeError):
            warn('The encoding %s has a limited charset. Consider a different encoding in your '
                 'environment. UTF-8 is being used instead' % self.encoding, RuntimeWarning)
            self.encoding = 'utf-8'
            return ""\n"".join(self.lines()).encode(self.encoding).decode(self.encoding)
",https://github.com/Qiskit/qiskit-terra/issues/2616,Uncaught Exception,Added exception handling (try).,qiskit/visualization/text.py,TextDrawing.single_string,[7],"Traceback (most recent call last):'
File ""C:\\projects\\qiskit-sdk-py\\test\\python\\test_examples.py"", line 50, in test_all_examples'
self.assertEqual(run_example.returncode, 0, error_string)'
File ""C:\\Python37\\lib\\unittest\\case.py"", line 839, in assertEqual'
assertion_func(first, second, msg=msg)'
File ""C:\\Python37\\lib\\unittest\\case.py"", line 832, in _baseAssertEqual'
raise self.failureException(msg)'
AssertionError: 1 != 0 : Running example circuit_draw.py failed with return code 1'
'stdout:'
'stderr: Traceback (most recent call last):\r'
File ""C:\\projects\\qiskit-sdk-py\\examples\\python\\circuit_draw.py"", line 34, in <module>\r'
print(bell_circuit)\r'
File ""C:\\Python37\\lib\\encodings\\cp1252.py"", line 19, in encode\r'
return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r'
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8-12: character maps to <undefined>\r""",AssertionError,buggy_snippets_files/60577f83a171b24b05e902e721a4936dfa0e6318a7cdb99a25b950e2da037800_before_merge.py,buggy_snippets_files/60577f83a171b24b05e902e721a4936dfa0e6318a7cdb99a25b950e2da037800_after_merge.py
187041,"  def EqualityString(self):
    """"""Return a string describing the EventObject in terms of object equality.

    The details of this function must match the logic of __eq__. EqualityStrings
    of two event objects should be the same if and only if the EventObjects are
    equal as described in __eq__.

    Returns:
      String: will match another EventObject's Equality String if and only if
              the EventObjects are equal
    """"""
    fields = sorted(list(self.GetAttributes().difference(self.COMPARE_EXCLUDE)))

    # TODO: Review this (after 1.1.0 release). Is there a better/more clean
    # method of removing the timestamp description field out of the fields list?
    parser = getattr(self, u'parser', u'')
    if parser == u'filestat':
      # We don't want to compare the timestamp description field when comparing
      # filestat events. This is done to be able to join together FILE events
      # that have the same timestamp, yet different description field (as in an
      # event that has for instance the same timestamp for mtime and atime,
      # joining it together into a single event).
      try:
        timestamp_desc_index = fields.index(u'timestamp_desc')
        del fields[timestamp_desc_index]
      except ValueError:
        pass

    basic = [self.timestamp, self.data_type]
    attributes = []
    for attribute in fields:
      value = getattr(self, attribute)
      if isinstance(value, dict):
        attributes.append(sorted(value.items()))
      elif isinstance(value, set):
        attributes.append(sorted(list(value)))
      else:
        attributes.append(value)
    identity = basic + [x for pair in zip(fields, attributes) for x in pair]

    if parser == u'filestat':
      inode = getattr(self, u'inode', u'a')
      if inode == u'a':
        inode = u'_{0:s}'.format(uuid.uuid4())
      identity.append(u'inode')
      identity.append(inode)

    return u'|'.join(map(unicode, identity))
","  def EqualityString(self):
    """"""Return a string describing the EventObject in terms of object equality.

    The details of this function must match the logic of __eq__. EqualityStrings
    of two event objects should be the same if and only if the EventObjects are
    equal as described in __eq__.

    Returns:
      String: will match another EventObject's Equality String if and only if
              the EventObjects are equal
    """"""
    fields = sorted(list(self.GetAttributes().difference(self.COMPARE_EXCLUDE)))

    # TODO: Review this (after 1.1.0 release). Is there a better/more clean
    # method of removing the timestamp description field out of the fields list?
    parser = getattr(self, u'parser', u'')
    if parser == u'filestat':
      # We don't want to compare the timestamp description field when comparing
      # filestat events. This is done to be able to join together FILE events
      # that have the same timestamp, yet different description field (as in an
      # event that has for instance the same timestamp for mtime and atime,
      # joining it together into a single event).
      try:
        timestamp_desc_index = fields.index(u'timestamp_desc')
        del fields[timestamp_desc_index]
      except ValueError:
        pass

    basic = [self.timestamp, self.data_type]
    attributes = []
    for attribute in fields:
      value = getattr(self, attribute)
      if isinstance(value, dict):
        attributes.append(sorted(value.items()))
      elif isinstance(value, set):
        attributes.append(sorted(list(value)))
      else:
        attributes.append(value)
    identity = basic + [x for pair in zip(fields, attributes) for x in pair]

    if parser == u'filestat':
      inode = getattr(self, u'inode', u'a')
      if inode == u'a':
        inode = u'_{0:s}'.format(uuid.uuid4())
      identity.append(u'inode')
      identity.append(inode)

    try:
      text = u'|'.join(map(unicode, identity))
      return text
    except UnicodeDecodeError:
      # If we cannot properly decode the equality string we give back the UUID
      # which is unique to this event and thus will not trigger an equal string
      # with another event.
      return self.uuid
",https://github.com/log2timeline/plaso/issues/220,Uncaught Exception,Added exception handling (try).,plaso/lib/event.py,EventObject.EqualityString,[48],"[pe] Unable to process file: type: OS, location:<PATH>/test_images/win7-64-nfury-10.3.58.6/win7-64-nfury-c-drive/win7-64-nfury-c-drive.E01
type: EWF
type: TSK, inode: 75147, location: /Windows/assembly/NativeImages_v2.0.50727_32/UIAutomationClient/d0972fea9e965a565c3cff76982709db/UIAutomationClient.ni.dll
with error: 'ascii' codec can't decode byte 0x90 in position 7: ordinal not in range(128).

2015-06-02 10:40:54,309 [ERROR] (Worker_08 ) PID:16459 <worker> 'ascii' codec can't decode byte 0x90 in position 7: ordinal not in range(128)
Traceback (most recent call last):
File ""/usr/local/lib/python2.7/dist-packages/plaso-1.2.1_20150602-py2.7.egg/plaso/engine/worker.py"", line 169, in _ParseFileEntryWithParser
File ""/usr/local/lib/python2.7/dist-packages/plaso-1.2.1_20150602-py2.7.egg/plaso/parsers/interface.py"", line 72, in UpdateChainAndParse
File ""/usr/local/lib/python2.7/dist-packages/plaso-1.2.1_20150602-py2.7.egg/plaso/parsers/interface.py"", line 256, in Parse
File ""/usr/local/lib/python2.7/dist-packages/plaso-1.2.1_20150602-py2.7.egg/plaso/parsers/pe.py"", line 259, in ParseFileObject
File ""/usr/local/lib/python2.7/dist-packages/plaso-1.2.1_20150602-py2.7.egg/plaso/parsers/pe.py"", line 132, in _GetSectionNames
UnicodeDecodeError: 'ascii' codec can't decode byte 0x90 in position 7: ordinal not in range(128)",UnicodeDecodeError,buggy_snippets_files/835d6ea4aab191471d79ee8f54509cf6bb787cceff26709c7e2d1bb30e328303_before_merge.py,buggy_snippets_files/835d6ea4aab191471d79ee8f54509cf6bb787cceff26709c7e2d1bb30e328303_after_merge.py
45588,"def write_PN(fp, data_element, padding=b' ', encoding=None):
    if not encoding:
        encoding = [default_encoding] * 3

    if data_element.VM == 1:
        val = [data_element.value, ]
    else:
        val = data_element.value

    if isinstance(val[0], compat.text_type) or not in_py2:
        val = [elem.encode(encoding) for elem in val]

    val = b'\\'.join(val)

    if len(val) % 2 != 0:
        val = val + padding

    fp.write(val)
","def write_PN(fp, data_element, padding=b' ', encoding=None):
    if not encoding:
        encoding = [default_encoding] * 3

    if data_element.VM == 1:
        val = [data_element.value, ]
    else:
        val = data_element.value

    if isinstance(val[0], compat.text_type) or not in_py2:
        try:
            val = [elem.encode(encoding) for elem in val]
        except TypeError:
            val = [elem.encode(encoding[0]) for elem in val]

    val = b'\\'.join(val)

    if len(val) % 2 != 0:
        val = val + padding

    fp.write(val)
",https://github.com/pydicom/pydicom/issues/489,Uncaught Exception,Added exception handling (try).,pydicom/filewriter.py,write_PN,[11],"Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""../pydicom/pydicom/filewriter.py"", line 228, in write_PN
val = [elem.encode(encoding) for elem in val]
TypeError: encode() argument 1 must be string, not list",TypeError,buggy_snippets_files/87ed20aa12d4e178c9723135b31cf086d1f36bb0f3c5d6c3d2193ed7751dbb1b_before_merge.py,buggy_snippets_files/87ed20aa12d4e178c9723135b31cf086d1f36bb0f3c5d6c3d2193ed7751dbb1b_after_merge.py
322833,"    def _connect(self, client, host, port, sock=None, retries=1,
                 user=None, password=None, pkey=None,
                 **paramiko_kwargs):
        """"""Connect to host

        :raises: :py:class:`pssh.exceptions.AuthenticationException`
          on authentication error
        :raises: :py:class:`pssh.exceptions.UnknownHostException`
          on DNS resolution error
        :raises: :py:class:`pssh.exceptions.ConnectionErrorException`
          on error connecting
        :raises: :py:class:`pssh.exceptions.SSHException` on other undefined
          SSH errors
        """"""
        try:
            client.connect(host,
                           username=user if user else self.user,
                           password=password if password else self.password,
                           port=port, pkey=pkey if pkey else self.pkey,
                           sock=sock, timeout=self.timeout,
                           allow_agent=self.allow_agent,
                           **paramiko_kwargs)
        except sock_gaierror as ex:
            logger.error(""Could not resolve host '%s' - retry %s/%s"",
                         host, retries, self.num_retries)
            while retries < self.num_retries:
                sleep(5)
                return self._connect(client, host, port,
                                     sock=sock,
                                     retries=retries+1,
                                     **paramiko_kwargs)
            raise UnknownHostException(""Unknown host %s - %s - retry %s/%s"",
                                       host, str(ex.args[1]), retries,
                                       self.num_retries)
        except sock_error as ex:
            logger.error(""Error connecting to host '%s:%s' - retry %s/%s"",
                         self.host, self.port, retries, self.num_retries)
            while retries < self.num_retries:
                sleep(5)
                return self._connect(client, host, port,
                                     sock=sock,
                                     retries=retries+1,
                                     **paramiko_kwargs)
            error_type = ex.args[1] if len(ex.args) > 1 else ex.args[0]
            raise ConnectionErrorException(
                ""Error connecting to host '%s:%s' - %s - retry %s/%s"",
                self.host, self.port, str(error_type), retries,
                self.num_retries,)
        except paramiko.AuthenticationException as ex:
            msg = ""Authentication error while connecting to %s:%s.""
            raise AuthenticationException(msg, host, port)
        # SSHException is more general so should be below other types
        # of SSH failure
        except paramiko.SSHException as ex:
            msg = ""General SSH error - %s"" % (ex,)
            logger.error(msg)
            raise SSHException(msg, host, port)
","    def _connect(self, client, host, port, sock=None, retries=1,
                 user=None, password=None, pkey=None,
                 **paramiko_kwargs):
        """"""Connect to host

        :raises: :py:class:`pssh.exceptions.AuthenticationException`
          on authentication error
        :raises: :py:class:`pssh.exceptions.UnknownHostException`
          on DNS resolution error
        :raises: :py:class:`pssh.exceptions.ConnectionErrorException`
          on error connecting
        :raises: :py:class:`pssh.exceptions.SSHException` on other undefined
          SSH errors
        """"""
        logger.debug(""Connecting to %s.."", host)
        try:
            client.connect(host,
                           username=user if user else self.user,
                           password=password if password else self.password,
                           port=port, pkey=pkey if pkey else self.pkey,
                           sock=sock, timeout=self.timeout,
                           allow_agent=self.allow_agent,
                           **paramiko_kwargs)
        except sock_gaierror as ex:
            logger.error(""Could not resolve host '%s' - retry %s/%s"",
                         host, retries, self.num_retries)
            while retries < self.num_retries:
                sleep(5)
                return self._connect(client, host, port,
                                     sock=sock,
                                     retries=retries+1,
                                     **paramiko_kwargs)
            raise UnknownHostException(""Unknown host %s - %s - retry %s/%s"",
                                       host, str(ex.args[1]), retries,
                                       self.num_retries)
        except sock_error as ex:
            logger.error(""Error connecting to host '%s:%s' - retry %s/%s"",
                         host, self.port, retries, self.num_retries)
            while retries < self.num_retries:
                sleep(5)
                return self._connect(client, host, port,
                                     sock=sock,
                                     retries=retries+1,
                                     **paramiko_kwargs)
            error_type = ex.args[1] if len(ex.args) > 1 else ex.args[0]
            raise ConnectionErrorException(
                ""Error connecting to host '%s:%s' - %s - retry %s/%s"",
                host, self.port, str(error_type), retries,
                self.num_retries,)
        except paramiko.AuthenticationException as ex:
            msg = ""Authentication error while connecting to %s:%s.""
            raise AuthenticationException(msg, host, port)
        # SSHException is more general so should be below other types
        # of SSH failure
        except paramiko.SSHException as ex:
            msg = ""General SSH error - %s"" % (ex,)
            logger.error(msg)
            raise SSHException(msg, host, port)
",https://github.com/ParallelSSH/parallel-ssh/issues/93,Use of Redundant Code,Use variable self.host instead of host variable.,pssh/ssh_client.py,SSHClient._connect,[37; 47],"Traceback (most recent call last):
File ""./test.py"", line 11, in <module>
output = client.run_command('ls')
File ""pssh/pssh_client.py"", line 689, in run_command
self.get_output(cmd, output, encoding=encoding)
File ""pssh/pssh_client.py"", line 778, in get_output
stdout = self.host_clients[host].read_output_buffer(
KeyError: '10.0.2.4'",KeyError,buggy_snippets_files/78736e711e4506bcb296c78e0c6cabacbb6540ce3b5c95a3ab84508bd65adf98_before_merge.py,buggy_snippets_files/78736e711e4506bcb296c78e0c6cabacbb6540ce3b5c95a3ab84508bd65adf98_after_merge.py
71160,"def _infer_xy_labels_3d(darray, x, y, rgb):
    """"""
    Determine x and y labels for showing RGB images.

    Attempts to infer which dimension is RGB/RGBA by size and order of dims.

    """"""
    assert rgb is None or rgb != x
    assert rgb is None or rgb != y
    # Start by detecting and reporting invalid combinations of arguments
    assert darray.ndim == 3
    not_none = [a for a in (x, y, rgb) if a is not None]
    if len(set(not_none)) < len(not_none):
        raise ValueError(
            'Dimension names must be None or unique strings, but imshow was '
            'passed x=%r, y=%r, and rgb=%r.' % (x, y, rgb))
    for label in not_none:
        if label not in darray.dims:
            raise ValueError('%r is not a dimension' % (label,))

    # Then calculate rgb dimension if certain and check validity
    could_be_color = [label for label in darray.dims
                      if darray[label].size in (3, 4) and label not in (x, y)]
    if rgb is None and not could_be_color:
        raise ValueError(
            'A 3-dimensional array was passed to imshow(), but there is no '
            'dimension that could be color.  At least one dimension must be '
            'of size 3 (RGB) or 4 (RGBA), and not given as x or y.')
    if rgb is None and len(could_be_color) == 1:
        rgb = could_be_color[0]
    if rgb is not None and darray[rgb].size not in (3, 4):
        raise ValueError('Cannot interpret dim %r of size %s as RGB or RGBA.'
                         % (rgb, darray[rgb].size))

    # If rgb dimension is still unknown, there must be two or three dimensions
    # in could_be_color.  We therefore warn, and use a heuristic to break ties.
    if rgb is None:
        assert len(could_be_color) in (2, 3)
        rgb = could_be_color[-1]
        warnings.warn(
            'Several dimensions of this array could be colors.  Xarray '
            'will use the last possible dimension (%r) to match '
            'matplotlib.pyplot.imshow.  You can pass names of x, y, '
            'and/or rgb dimensions to override this guess.' % rgb)
    assert rgb is not None

    # Finally, we pick out the red slice and delegate to the 2D version:
    return _infer_xy_labels(darray.isel(**{rgb: 0}).squeeze(), x, y)
","def _infer_xy_labels_3d(darray, x, y, rgb):
    """"""
    Determine x and y labels for showing RGB images.

    Attempts to infer which dimension is RGB/RGBA by size and order of dims.

    """"""
    assert rgb is None or rgb != x
    assert rgb is None or rgb != y
    # Start by detecting and reporting invalid combinations of arguments
    assert darray.ndim == 3
    not_none = [a for a in (x, y, rgb) if a is not None]
    if len(set(not_none)) < len(not_none):
        raise ValueError(
            'Dimension names must be None or unique strings, but imshow was '
            'passed x=%r, y=%r, and rgb=%r.' % (x, y, rgb))
    for label in not_none:
        if label not in darray.dims:
            raise ValueError('%r is not a dimension' % (label,))

    # Then calculate rgb dimension if certain and check validity
    could_be_color = [label for label in darray.dims
                      if darray[label].size in (3, 4) and label not in (x, y)]
    if rgb is None and not could_be_color:
        raise ValueError(
            'A 3-dimensional array was passed to imshow(), but there is no '
            'dimension that could be color.  At least one dimension must be '
            'of size 3 (RGB) or 4 (RGBA), and not given as x or y.')
    if rgb is None and len(could_be_color) == 1:
        rgb = could_be_color[0]
    if rgb is not None and darray[rgb].size not in (3, 4):
        raise ValueError('Cannot interpret dim %r of size %s as RGB or RGBA.'
                         % (rgb, darray[rgb].size))

    # If rgb dimension is still unknown, there must be two or three dimensions
    # in could_be_color.  We therefore warn, and use a heuristic to break ties.
    if rgb is None:
        assert len(could_be_color) in (2, 3)
        rgb = could_be_color[-1]
        warnings.warn(
            'Several dimensions of this array could be colors.  Xarray '
            'will use the last possible dimension (%r) to match '
            'matplotlib.pyplot.imshow.  You can pass names of x, y, '
            'and/or rgb dimensions to override this guess.' % rgb)
    assert rgb is not None

    # Finally, we pick out the red slice and delegate to the 2D version:
    return _infer_xy_labels(darray.isel(**{rgb: 0}), x, y)
",https://github.com/pydata/xarray/issues/1966,Improper Following of Specification by Caller,Invalid shape (forgotten squeeze).,xarray/plot/utils.py,_infer_xy_labels_3d,[48],"/home/mowglie/Documents/git/xarray/xarray/plot/utils.py:295: UserWarning: Several dimensions of this array could be colors.  Xarray will use the last possible dimension ('dim_2') to match matplotlib.pyplot.imshow.  You can pass names of x, y, and/or rgb dimensions to override this guess.
'and/or rgb dimensions to override this guess.' % rgb)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-a0592d3e8758> in <module>()
----> 1 da.plot.imshow()

~/Documents/git/xarray/xarray/plot/plot.py in plotmethod(_PlotMethods_obj, x, y, figsize, size, aspect, ax, row, col, col_wrap, xincrease, yincrease, add_colorbar, add_labels, vmin, vmax, cmap, colors, center, robust, extend, levels, infer_intervals, subplot_kws, cbar_ax, cbar_kwargs, **kwargs)
679         for arg in ['_PlotMethods_obj', 'newplotfunc', 'kwargs']:
680             del allargs[arg]
--> 681         return newplotfunc(**allargs)
682
683     # Add to class _PlotMethods

~/Documents/git/xarray/xarray/plot/plot.py in newplotfunc(darray, x, y, figsize, size, aspect, ax, row, col, col_wrap, xincrease, yincrease, add_colorbar, add_labels, vmin, vmax, cmap, center, robust, extend, levels, infer_intervals, colors, subplot_kws, cbar_ax, cbar_kwargs, **kwargs)
553         rgb = kwargs.pop('rgb', None)
554         xlab, ylab = _infer_xy_labels(
--> 555             darray=darray, x=x, y=y, imshow=imshow_rgb, rgb=rgb)
556
557         if rgb is not None and plotfunc.__name__ != 'imshow':

~/Documents/git/xarray/xarray/plot/utils.py in _infer_xy_labels(darray, x, y, imshow, rgb)
308     assert x is None or x != y
309     if imshow and darray.ndim == 3:
--> 310         return _infer_xy_labels_3d(darray, x, y, rgb)
311
312     if x is None and y is None:

~/Documents/git/xarray/xarray/plot/utils.py in _infer_xy_labels_3d(darray, x, y, rgb)
297
298     # Finally, we pick out the red slice and delegate to the 2D version:
--> 299     return _infer_xy_labels(darray.isel(**{rgb: 0}).squeeze(), x, y)
300
301

~/Documents/git/xarray/xarray/plot/utils.py in _infer_xy_labels(darray, x, y, imshow, rgb)
312     if x is None and y is None:
313         if darray.ndim != 2:
--> 314             raise ValueError('DataArray must be 2d')
315         y, x = darray.dims
316     elif x is None:

ValueError: DataArray must be 2d",ValueError,buggy_snippets_files/438c6714fbfa72d47a90b2fbcaa5e1db8e6bee5535e151f15c4e68088925fe68_before_merge.py,buggy_snippets_files/438c6714fbfa72d47a90b2fbcaa5e1db8e6bee5535e151f15c4e68088925fe68_after_merge.py
73188,"def guess_format(filename: Text) -> Text:
    """"""Applies heuristics to guess the data format of a file.

    Args:
        filename: file whose type should be guessed

    Returns:
        Guessed file format.
    """"""
    guess = UNK
    content = rasa.utils.io.read_file(filename)
    try:
        js = json.loads(content)
    except ValueError:
        if any([marker in content for marker in _markdown_section_markers]):
            guess = MARKDOWN
    else:
        for fformat, format_heuristic in _json_format_heuristics.items():
            if format_heuristic(js, filename):
                guess = fformat
                break

    return guess
","def guess_format(filename: Text) -> Text:
    """"""Applies heuristics to guess the data format of a file.

    Args:
        filename: file whose type should be guessed

    Returns:
        Guessed file format.
    """"""
    guess = UNK

    content = """"
    try:
        content = rasa.utils.io.read_file(filename)
        js = json.loads(content)
    except ValueError:
        if any([marker in content for marker in _markdown_section_markers]):
            guess = MARKDOWN
    else:
        for fformat, format_heuristic in _json_format_heuristics.items():
            if format_heuristic(js, filename):
                guess = fformat
                break

    return guess
",https://github.com/RasaHQ/rasa/issues/3812,Uncaught Exception,Added exception handling (try).,rasa/nlu/training_data/loading.py,guess_format,[11],"? Export stories to (if file exists, this will append the stories)
? Export NLU data to (if file exists, this will merge learned data with previous training examples)
? Export domain file to (if file exists, this will be overwritten) domain.yml
2019-06-18 08:16:35 ERROR    rasa.core.training.interactive  - An exception occurred while recording messages.
Traceback (most recent call last):
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 1367, in record_messages
endpoint, sender_id, sender_ids, plot_file
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 885, in _predict_till_next_listen
action_name, policy, confidence, predictions, endpoint, sender_id
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 1024, in _validate_action
is_correct = await _ask_questions(question, sender_id, endpoint)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 312, in _ask_questions
should_retry = await _ask_if_quit(sender_id, endpoint)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 591, in _ask_if_quit
await _write_data_to_file(sender_id, endpoint)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 566, in _write_data_to_file
await _write_stories_to_file(story_path, events)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 764, in _write_stories_to_file
with open(export_story_path, append_write, encoding=""utf-8"") as f:
FileNotFoundError: [Errno 2] No such file or directory: ''
2019-06-18 08:16:35 ERROR    asyncio  - Task exception was never retrieved
future: <Task finished coro=<_serve_application.<locals>.run_interactive_io() done, defined at /Users/erohmensing/rasa/rasa/core/training/interactive.py:1409> exception=FileNotFoundError(2, 'No such file or directory')>
Traceback (most recent call last):
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 1416, in run_interactive_io
sender_id=uuid.uuid4().hex,
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 1367, in record_messages
endpoint, sender_id, sender_ids, plot_file
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 885, in _predict_till_next_listen
action_name, policy, confidence, predictions, endpoint, sender_id
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 1024, in _validate_action
is_correct = await _ask_questions(question, sender_id, endpoint)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 312, in _ask_questions
should_retry = await _ask_if_quit(sender_id, endpoint)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 591, in _ask_if_quit
await _write_data_to_file(sender_id, endpoint)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 566, in _write_data_to_file
await _write_stories_to_file(story_path, events)
File ""/Users/erohmensing/rasa/rasa/core/training/interactive.py"", line 764, in _write_stories_to_file
with open(export_story_path, append_write, encoding=""utf-8"") as f:
FileNotFoundError: [Errno 2] No such file or directory: ''",FileNotFoundError,buggy_snippets_files/e773e337a86d5dc9df35e32a06ec8471909e36abbc7265dc0a807b5e8bf47efb_before_merge.py,buggy_snippets_files/e773e337a86d5dc9df35e32a06ec8471909e36abbc7265dc0a807b5e8bf47efb_after_merge.py
346312,"    def prepare(self):
        """""" Initialize important variables """"""
        if not hasattr(self, ""HOSTER_NAME""):
            self.HOSTER_NAME = re.search(self.__pattern__, self.pyfile.url).group(1)
        if not hasattr(self, ""DIRECT_LINK_PATTERN""):
            self.DIRECT_LINK_PATTERN = r'(http://([^/]*?%s|\d+\.\d+\.\d+\.\d+)(:\d+/d/|/files/\d+/\w+/)[^""\'<]+)' % self.HOSTER_NAME

        self.captcha = self.errmsg = None
        self.passwords = self.getPassword().splitlines()
","    def prepare(self):
        """""" Initialize important variables """"""
        if not hasattr(self, ""HOSTER_NAME""):
            self.HOSTER_NAME = re.search(self.__pattern__, self.pyfile.url).group(1)
        if not hasattr(self, ""DIRECT_LINK_PATTERN""):
            self.DIRECT_LINK_PATTERN = r'(http://([^/]*?%s|\d+\.\d+\.\d+\.\d+)(:\d+/d/|(?:/files)?/\d+/\w+/)[^""\'<]+)' % self.HOSTER_NAME

        self.captcha = self.errmsg = None
        self.passwords = self.getPassword().splitlines()
",https://github.com/pyload/pyload/issues/256,Incorrect Regular Expression,Incorrect regex.,module/plugins/hoster/XFileSharingPro.py,XFileSharingPro.prepare,[6],"04.09.2013 23:38:59 DEBUG     XFileSharingPro: Pattern loaded - handling 81 hosters
04.09.2013 23:38:59 INFO      pyLoad is up and running
04.09.2013 23:39:01 INFO      UpdateManager: No Updates for pyLoad
04.09.2013 23:39:01 INFO      UpdateManager: No plugin updates available
04.09.2013 23:39:13 DEBUG     Redirected import module.plugins.hoster.XFileSharingPro -> userplugins.hoster.XFileSharingPro
04.09.2013 23:39:13 DEBUG     Redirected import module.plugins.internal.CaptchaService -> userplugins.internal.CaptchaService
04.09.2013 23:39:13 INFO      Download starts: Zone-Telechargement.comNCIS.S10E14.FRENCH.LD.HDTV.XviD-MiND.avi
04.09.2013 23:39:21 DEBUG     UptoboxCom: URL: http://uptobox.com/d507fg2b4w9b
04.09.2013 23:39:21 DEBUG     UptoboxCom: {'status': 3, 'name': '', 'size': 0}
04.09.2013 23:39:21 DEBUG     UptoboxCom: uptobox.com | {u'rand': u'v5pcjltbkge65ychada5iy62rrttmf6tnvlpo4y', u'method_premium': u'1', u'down_direct': u'1', u'referer': u'http://uptobox.com/d507fg2b4w9b', u'method_free': '', u'id': u'd507fg2b4w9b', u'op': u'download2'}
04.09.2013 23:39:21 WARNING   Download failed: Zone-Telechargement.comNCIS.S10E14.FRENCH.LD.HDTV.XviD-MiND.avi | 'Parse error (DIRECT LINK) - plugin may be out of date'
Traceback (most recent call last):
File ""/usr/local/share/pyload/module/PluginThread.py"", line 187, in run
pyfile.plugin.preprocessing(self)
File ""/usr/local/share/pyload/module/plugins/Plugin.py"", line 237, in preprocessing
return self.process(self.pyfile)
File ""/root/pyload/userplugins/hoster/XFileSharingPro.py"", line 95, in process
self.handlePremium()
File ""/root/pyload/userplugins/hoster/XFileSharingPro.py"", line 160, in handlePremium
self.parseError('DIRECT LINK')
File ""/root/pyload/userplugins/internal/SimpleHoster.py"", line 226, in parseError
raise PluginParseError(msg)
PluginParseError: 'Parse error (DIRECT LINK) - plugin may be out of date'
04.09.2013 23:39:21 INFO      Debug Report written to debug_UptoboxCom_04-09-2013_23-39-21.zip
04.09.2013 23:39:21 INFO      UnSkipOnFail: look for skipped duplicates for Zone-Telechargement.comNCIS.S10E14.FRENCH.LD.HDTV.XviD-MiND.avi (pid:4)...
04.09.2013 23:39:21 DEBUG     All downloads processed
..
..
..
04.09.2013 23:51:53 WARNING   Download failed: http://uptobox.com/d507fg2b4w9b | 'Parse error (DIRECT LINK) - plugin may be out of date'
Traceback (most recent call last):
File ""/usr/local/share/pyload/module/PluginThread.py"", line 187, in run
pyfile.plugin.preprocessing(self)
File ""/usr/local/share/pyload/module/plugins/Plugin.py"", line 237, in preprocessing
return self.process(self.pyfile)
File ""/root/pyload/userplugins/hoster/XFileSharingPro.py"", line 95, in process
self.handlePremium()
File ""/root/pyload/userplugins/hoster/XFileSharingPro.py"", line 160, in handlePremium
self.parseError('DIRECT LINK')
File ""/root/pyload/userplugins/internal/SimpleHoster.py"", line 226, in parseError
raise PluginParseError(msg)
PluginParseError: 'Parse error (DIRECT LINK) - plugin may be out of date'
04.09.2013 23:51:53 INFO      Debug Report written to debug_UptoboxCom_04-09-2013_23-51-53.zip
04.09.2013 23:51:53 INFO      UnSkipOnFail: look for skipped duplicates for d507fg2b4w9b (pid:4)...
04.09.2013 23:51:53 DEBUG     All downloads processed
04.09.2013 23:52:24 DEBUG     Run Info Fetching for UptoboxCom
04.09.2013 23:52:24 INFO      Added package ncis containing 1 links
04.09.2013 23:52:24 DEBUG     Finished Info Fetching for UptoboxCom
04.09.2013 23:52:25 INFO      Download starts: Zone-Telechargement.comNCIS.S10E14.FRENCH.LD.HDTV.XviD-MiND.avi
04.09.2013 23:52:25 DEBUG     UptoboxCom: URL: http://uptobox.com/d507fg2b4w9b
04.09.2013 23:52:25 DEBUG     UptoboxCom: {'status': 3, 'name': '', 'size': 0}
04.09.2013 23:52:25 DEBUG     UptoboxCom: uptobox.com | {u'rand': u'tisa6omstximt6fbfdyde7hp23iwxqsqnmymbly', u'method_premium': u'1', u'down_direct': u'1', u'referer': u'http://uptobox.com/d507fg2b4w9b', u'method_free': '', u'id': u'd507fg2b4w9b', u'op': u'download2'}
04.09.2013 23:52:26 DEBUG     UptoboxCom: DIRECT LINK: http://www5.uptobox.com:8080/d/uyyyv54jf4csmtsqkbyzmw3heem4lbpapk2uxggvw5e4lpffsqpijogg/Zone-Telechargement.comNCIS.S10E14.FRENCH.LD.HDTV.XviD-MiND.avi",PluginParseError,buggy_snippets_files/f9123d95bcca0b1cc8a031ae122d45713ea23b140ef305fe970e747773f8297e_before_merge.py,buggy_snippets_files/f9123d95bcca0b1cc8a031ae122d45713ea23b140ef305fe970e747773f8297e_after_merge.py
76105,"                def mapper_fn(x):
                    return mapper(x)
","                def mapper_fn(x):
                    if x in mapper:
                        return mapper[x]
                    else:
                        if errors == ""raise"":
                            raise KeyError(""Index include value which is not in the `mapper`"")
                        return x
",https://github.com/databricks/koalas/issues/1770,Improper Input Validation,Added key value validation.,databricks/koalas/frame.py,DataFrame.rename.gen_mapper_fn.mapper_fn,[2],"---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-36-43ac3f9768b4> in <module>
----> 1 toy_ks1.join(toy_ks2, on = ['day','item'], rsuffix='r')

~\AppData\Local\Continuum\anaconda3\envs\spark_env\lib\site-packages\databricks\koalas\frame.py in join(self, right, on, how, lsuffix, rsuffix)
6928             )
6929         if on:
-> 6930             self = self.set_index(on)
6931             join_kdf = self.merge(
6932                 right, left_index=True, right_index=True, how=how, suffixes=(lsuffix, rsuffix)

~\AppData\Local\Continuum\anaconda3\envs\spark_env\lib\site-packages\databricks\koalas\frame.py in set_index(self, keys, drop, append, inplace)
3182         for key in keys:
3183             if key not in columns:
-> 3184                 raise KeyError(key)
3185         keys = [key if isinstance(key, tuple) else (key,) for key in keys]
3186

KeyError: 'day'",KeyError,buggy_snippets_files/e2d080b4fcf5d8dfce136e5f895c0a4ba148c116012f8cc80ea7e114872ae337_before_merge.py,buggy_snippets_files/e2d080b4fcf5d8dfce136e5f895c0a4ba148c116012f8cc80ea7e114872ae337_after_merge.py
189362,"    def __call__(self, data, **metadata):
        opts = jsbeautifier.default_options()
        opts.indent_size = 2
        res = jsbeautifier.beautify(data, opts)
        return ""JavaScript"", format_text(res)
","    def __call__(self, data, **metadata):
        opts = jsbeautifier.default_options()
        opts.indent_size = 2
        data = data.decode(""utf-8"", ""replace"")
        res = jsbeautifier.beautify(data, opts)
        return ""JavaScript"", format_text(res)
",https://github.com/mitmproxy/mitmproxy/issues/1431,Improper Handling of Unicode Encoding,Missed utf-8 decoding.,mitmproxy/contentviews.py,ViewJavaScript.__call__,[4],"Content viewer failed:
Traceback (most recent call last):
File ""/Users/cortesi/mitmproxy/mitmproxy/mitmproxy/contentviews.py"", line 634, in get_content_view
ret = viewmode(data, **metadata)
File ""/Users/cortesi/mitmproxy/mitmproxy/mitmproxy/contentviews.py"", line 151, in __call__
return content_types_map[ct][0](data, **metadata)
File ""/Users/cortesi/mitmproxy/mitmproxy/mitmproxy/contentviews.py"", line 401, in __call__
res = jsbeautifier.beautify(data, opts)
File ""/Users/cortesi/mitmproxy/mitmproxy/mitmproxy/contrib/jsbeautifier/__init__.py"", line 93, in beautify
return b.beautify(string, opts)
File ""/Users/cortesi/mitmproxy/mitmproxy/mitmproxy/contrib/jsbeautifier/__init__.py"", line 218, in beautify
token_text, token_type = self.get_next_token()
File ""/Users/cortesi/mitmproxy/mitmproxy/mitmproxy/contrib/jsbeautifier/__init__.py"", line 443, in
get_next_token
if c in self.wordchar:
TypeError: 'in <string>' requires string as left operand, not int",TypeError,buggy_snippets_files/6a3212079fca2da1cab5c2695f23cedde774f36928e0fdbc3230bf973896c2f6_before_merge.py,buggy_snippets_files/6a3212079fca2da1cab5c2695f23cedde774f36928e0fdbc3230bf973896c2f6_after_merge.py
321032,"  def decode(self,
             inputs,
             sequence_length,
             vocab_size=None,
             initial_state=None,
             sampling_probability=None,
             embedding=None,
             output_layer=None,
             mode=tf.estimator.ModeKeys.TRAIN,
             memory=None,
             memory_sequence_length=None):
    _ = memory
    _ = memory_sequence_length

    batch_size = tf.shape(inputs)[0]

    if (sampling_probability is not None
        and (tf.contrib.framework.is_tensor(sampling_probability)
             or sampling_probability > 0.0)):
      if embedding is None:
        raise ValueError(""embedding argument must be set when using scheduled sampling"")

      tf.summary.scalar(""sampling_probability"", sampling_probability)
      helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(
          inputs,
          sequence_length,
          embedding,
          sampling_probability)
    else:
      helper = tf.contrib.seq2seq.TrainingHelper(inputs, sequence_length)

    cell, initial_state = self._build_cell(
        mode,
        batch_size,
        initial_state=initial_state,
        memory=memory,
        memory_sequence_length=memory_sequence_length,
        dtype=inputs.dtype)

    if output_layer is None:
      output_layer = build_output_layer(self.num_units, vocab_size, dtype=inputs.dtype)

    # With TrainingHelper, project all timesteps at once.
    fused_projection = isinstance(helper, tf.contrib.seq2seq.TrainingHelper)

    decoder = tf.contrib.seq2seq.BasicDecoder(
        cell,
        helper,
        initial_state,
        output_layer=output_layer if not fused_projection else None)

    outputs, state, length = tf.contrib.seq2seq.dynamic_decode(decoder)

    if fused_projection and output_layer is not None:
      logits = output_layer(outputs.rnn_output)
    else:
      logits = outputs.rnn_output

    return (logits, state, length)
","  def decode(self,
             inputs,
             sequence_length,
             vocab_size=None,
             initial_state=None,
             sampling_probability=None,
             embedding=None,
             output_layer=None,
             mode=tf.estimator.ModeKeys.TRAIN,
             memory=None,
             memory_sequence_length=None):
    _ = memory
    _ = memory_sequence_length

    batch_size = tf.shape(inputs)[0]

    if (sampling_probability is not None
        and (tf.contrib.framework.is_tensor(sampling_probability)
             or sampling_probability > 0.0)):
      if embedding is None:
        raise ValueError(""embedding argument must be set when using scheduled sampling"")

      tf.summary.scalar(""sampling_probability"", sampling_probability)
      helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(
          inputs,
          sequence_length,
          embedding,
          sampling_probability)
    else:
      helper = tf.contrib.seq2seq.TrainingHelper(inputs, sequence_length)

    cell, initial_state = self._build_cell(
        mode,
        batch_size,
        initial_state=initial_state,
        memory=memory,
        memory_sequence_length=memory_sequence_length,
        dtype=inputs.dtype)

    if output_layer is None:
      output_layer = build_output_layer(self.num_units, vocab_size, dtype=inputs.dtype)

    # With TrainingHelper, project all timesteps at once.
    fused_projection = isinstance(helper, tf.contrib.seq2seq.TrainingHelper)

    decoder = tf.contrib.seq2seq.BasicDecoder(
        cell,
        helper,
        initial_state,
        output_layer=output_layer if not fused_projection else None)

    outputs, state, length = tf.contrib.seq2seq.dynamic_decode(decoder)

    if fused_projection and output_layer is not None:
      logits = output_layer(outputs.rnn_output)
    else:
      logits = outputs.rnn_output
    # Make sure outputs have the same time_dim as inputs
    inputs_len = tf.shape(inputs)[1]
    logits = align_in_time(logits, inputs_len)

    return (logits, state, length)
",https://github.com/OpenNMT/OpenNMT-tf/issues/137,Improper Input Validation,Issue with size: make sure outputs have the same time_dim as inputs.,opennmt/decoders/rnn_decoder.py,RNNDecoder.decode,[58],"Traceback (most recent call last):
File ""/root/tf-1.4/bin/onmt-main"", line 11, in <module>
sys.exit(main())
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/bin/main.py"", line 120, in main
runner.train_and_evaluate()
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/runner.py"", line 141, in train_and_evaluate
tf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 430, in train_and_evaluate
executor.run_local()
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 609, in run_local
hooks=train_hooks)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 783, in _train_model
_, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 521, in run
run_metadata=run_metadata)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 892, in run
run_metadata=run_metadata)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 967, in run
raise six.reraise(*original_exc_info)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 952, in run
return self._sess.run(*args, **kwargs)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1024, in run
run_metadata=run_metadata)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 827, in run
return self._sess.run(*args, **kwargs)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
run_metadata_ptr)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
feed_dict_tensor, options, run_metadata)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
options, run_metadata)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[196,101] labels_size=[203,101]
[[Node: seq2seq/parallel_1/seq2seq/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](seq2seq/parallel_1/seq2seq/Reshape, seq2seq/parallel_1/seq2seq/Reshape_1)]]

Caused by op u'seq2seq/parallel_1/seq2seq/SoftmaxCrossEntropyWithLogits', defined at:
File ""/root/tf-1.4/bin/onmt-main"", line 11, in <module>
sys.exit(main())
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/bin/main.py"", line 120, in main
runner.train_and_evaluate()
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/runner.py"", line 141, in train_and_evaluate
tf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 430, in train_and_evaluate
executor.run_local()
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 609, in run_local
hooks=train_hooks)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
model_fn_results = self._model_fn(features=features, **kwargs)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/models/model.py"", line 88, in _model_fn
_loss_op, features_shards, labels_shards, params, mode, config)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/utils/parallel.py"", line 150, in __call__
outputs.append(funs[i](*args[i], **kwargs[i]))
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/models/model.py"", line 52, in _loss_op
return self._compute_loss(features, labels, logits, params, mode)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/models/sequence_to_sequence.py"", line 229, in _compute_loss
mode=mode)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/utils/losses.py"", line 51, in cross_entropy_sequence_loss
cross_entropy = _softmax_cross_entropy(logits, labels, label_smoothing, mode)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/opennmt/utils/losses.py"", line 24, in _softmax_cross_entropy
logits=logits, labels=smoothed_labels)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
precise_logits, labels, name=name)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
name=name)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
op_def=op_def)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
op_def=op_def)
File ""/root/tf-1.4/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[196,101] labels_size=[203,101]
[[Node: seq2seq/parallel_1/seq2seq/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](seq2seq/parallel_1/seq2seq/Reshape, seq2seq/parallel_1/seq2seq/Reshape_1)]]",tensorflow.python.framework.errors_impl.InvalidArgumentError,buggy_snippets_files/3410e6d8b6006e554504d5b6b1c3516e0ea5213c0b8f6687f38325e7f3ac6e41_before_merge.py,buggy_snippets_files/3410e6d8b6006e554504d5b6b1c3516e0ea5213c0b8f6687f38325e7f3ac6e41_after_merge.py
252421,"    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
                                residual, pred, sample_weight):
        terminal_region = np.where(terminal_regions == leaf)[0]
        pred = pred.take(terminal_region, axis=0)
        y = y.take(terminal_region, axis=0)
        sample_weight = sample_weight.take(terminal_region, axis=0)

        y_ = 2. * y - 1.

        numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))
        denominator = np.sum(sample_weight * np.exp(-y_ * pred))

        if denominator == 0.0:
            tree.value[leaf, 0, 0] = 0.0
        else:
            tree.value[leaf, 0, 0] = numerator / denominator
","    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
                                residual, pred, sample_weight):
        terminal_region = np.where(terminal_regions == leaf)[0]
        pred = pred.take(terminal_region, axis=0)
        y = y.take(terminal_region, axis=0)
        sample_weight = sample_weight.take(terminal_region, axis=0)

        y_ = 2. * y - 1.

        numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))
        denominator = np.sum(sample_weight * np.exp(-y_ * pred))

        # prevents overflow and division by zero
        if abs(denominator) < 1e-150:
            tree.value[leaf, 0, 0] = 0.0
        else:
            tree.value[leaf, 0, 0] = numerator / denominator
",https://github.com/scikit-learn/scikit-learn/issues/7717,Divide By Zero,Division by zero (overflow).,sklearn/ensemble/gradient_boosting.py,ExponentialLoss._update_terminal_region,[13],"Traceback (most recent call last):
File ""double_cv.py"", line 68, in <module>
double_cv(**vars(args))
File ""double_cv.py"", line 49, in double_cv
rs.fit(X_train, y_train)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/model_selection/_search.py"", line 1185, in fit
return self._fit(X, y, groups, sampled_params)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/model_selection/_search.py"", line 562, in _fit
for parameters in parameter_iterable
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 758, in __call__
while self.dispatch_one_batch(iterator):
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 608, in dispatch_one_batch
self._dispatch(tasks)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 571, in _dispatch
job = self._backend.apply_async(batch, callback=cb)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 109, in apply_async
result = ImmediateResult(func)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 322, in __init__
self.results = batch()
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
return [func(*args, **kwargs) for func, args, kwargs in self.items]
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/model_selection/_validation.py"", line 260, in _fit_and_score
test_score = _score(estimator, X_test, y_test, scorer)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/model_selection/_validation.py"", line 287, in _score
score = scorer(estimator, X_test, y_test)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/metrics/scorer.py"", line 196, in __call__
return self._sign * self._score_func(y, y_pred, **self._kwargs)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/metrics/ranking.py"", line 260, in roc_auc_score
sample_weight=sample_weight)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/metrics/base.py"", line 84, in _average_binary_score
return binary_metric(y_true, y_score, sample_weight=sample_weight)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/metrics/ranking.py"", line 255, in _binary_roc_auc_score
sample_weight=sample_weight)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/metrics/ranking.py"", line 505, in roc_curve
y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/metrics/ranking.py"", line 301, in _binary_clf_curve
assert_all_finite(y_score)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 65, in assert_all_finite
_assert_all_finite(X.data if sp.issparse(X) else X)
File ""/home/podda/.venvs/vermont/local/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 58, in _assert_all_finite
"" or a value too large for %r."" % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').",ValueError,buggy_snippets_files/ac335b5c42bbd01f0982e6c46a40c64dd131370e9d4941948e9426a5ddfe3111_before_merge.py,buggy_snippets_files/ac335b5c42bbd01f0982e6c46a40c64dd131370e9d4941948e9426a5ddfe3111_after_merge.py
213610,"    def install(self):
        # the file is a tar, so open it that way and extract it
        # to the specified (or default) roles directory
        local_file = False

        if self.scm:
            # create tar file from scm url
            tmp_file = RoleRequirement.scm_archive_role(**self.spec)
        elif self.src:
            if os.path.isfile(self.src):
                # installing a local tar.gz
                local_file = True
                tmp_file = self.src
            elif '://' in self.src:
                role_data = self.src
                tmp_file = self.fetch(role_data)
            else:
                api = GalaxyAPI(self.galaxy)
                role_data = api.lookup_role_by_name(self.src)
                if not role_data:
                    raise AnsibleError(""- sorry, %s was not found on %s."" % (self.src, api.api_server))

                if role_data.get('role_type') == 'APP':
                    # Container Role
                    display.warning(""%s is a Container App role, and should only be installed using Ansible ""
                                    ""Container"" % self.name)

                role_versions = api.fetch_role_related('versions', role_data['id'])
                if not self.version:
                    # convert the version names to LooseVersion objects
                    # and sort them to get the latest version. If there
                    # are no versions in the list, we'll grab the head
                    # of the master branch
                    if len(role_versions) > 0:
                        loose_versions = [LooseVersion(a.get('name', None)) for a in role_versions]
                        loose_versions.sort()
                        self.version = str(loose_versions[-1])
                    elif role_data.get('github_branch', None):
                        self.version = role_data['github_branch']
                    else:
                        self.version = 'master'
                elif self.version != 'master':
                    if role_versions and str(self.version) not in [a.get('name', None) for a in role_versions]:
                        raise AnsibleError(""- the specified version (%s) of %s was not found in the list of available versions (%s)."" % (self.version,
                                                                                                                                         self.name,
                                                                                                                                         role_versions))

                tmp_file = self.fetch(role_data)

        else:
            raise AnsibleError(""No valid role data found"")

        if tmp_file:

            display.debug(""installing from %s"" % tmp_file)

            if not tarfile.is_tarfile(tmp_file):
                raise AnsibleError(""the file downloaded was not a tar.gz"")
            else:
                if tmp_file.endswith('.gz'):
                    role_tar_file = tarfile.open(tmp_file, ""r:gz"")
                else:
                    role_tar_file = tarfile.open(tmp_file, ""r"")
                # verify the role's meta file
                meta_file = None
                members = role_tar_file.getmembers()
                # next find the metadata file
                for member in members:
                    if self.META_MAIN in member.name:
                        # Look for parent of meta/main.yml
                        # Due to possibility of sub roles each containing meta/main.yml
                        # look for shortest length parent
                        meta_parent_dir = os.path.dirname(os.path.dirname(member.name))
                        if not meta_file:
                            archive_parent_dir = meta_parent_dir
                            meta_file = member
                        else:
                            if len(meta_parent_dir) < len(archive_parent_dir):
                                archive_parent_dir = meta_parent_dir
                                meta_file = member
                if not meta_file:
                    raise AnsibleError(""this role does not appear to have a meta/main.yml file."")
                else:
                    try:
                        self._metadata = yaml.safe_load(role_tar_file.extractfile(meta_file))
                    except:
                        raise AnsibleError(""this role does not appear to have a valid meta/main.yml file."")

                # we strip off any higher-level directories for all of the files contained within
                # the tar file here. The default is 'github_repo-target'. Gerrit instances, on the other
                # hand, does not have a parent directory at all.
                installed = False
                while not installed:
                    display.display(""- extracting %s to %s"" % (self.name, self.path))
                    try:
                        if os.path.exists(self.path):
                            if not os.path.isdir(self.path):
                                raise AnsibleError(""the specified roles path exists and is not a directory."")
                            elif not getattr(self.options, ""force"", False):
                                raise AnsibleError(""the specified role %s appears to already exist. Use --force to replace it."" % self.name)
                            else:
                                # using --force, remove the old path
                                if not self.remove():
                                    raise AnsibleError(""%s doesn't appear to contain a role.\n  please remove this directory manually if you really ""
                                                       ""want to put the role here."" % self.path)
                        else:
                            os.makedirs(self.path)

                        # now we do the actual extraction to the path
                        for member in members:
                            # we only extract files, and remove any relative path
                            # bits that might be in the file for security purposes
                            # and drop any containing directory, as mentioned above
                            if member.isreg() or member.issym():
                                parts = member.name.replace(archive_parent_dir, """", 1).split(os.sep)
                                final_parts = []
                                for part in parts:
                                    if part != '..' and '~' not in part and '$' not in part:
                                        final_parts.append(part)
                                member.name = os.path.join(*final_parts)
                                role_tar_file.extract(member, self.path)

                        # write out the install info file for later use
                        self._write_galaxy_install_info()
                        installed = True
                    except OSError as e:
                        error = True
                        if e.errno == errno.EACCES and len(self.paths) > 1:
                            current = self.paths.index(self.path)
                            if len(self.paths) > current:
                                self.path = self.paths[current + 1]
                                error = False
                        if error:
                            raise AnsibleError(""Could not update files in %s: %s"" % (self.path, str(e)))

                # return the parsed yaml metadata
                display.display(""- %s was installed successfully"" % str(self))
                if not local_file:
                    try:
                        os.unlink(tmp_file)
                    except (OSError, IOError) as e:
                        display.warning(""Unable to remove tmp file (%s): %s"" % (tmp_file, str(e)))
                return True

        return False
","    def install(self):
        # the file is a tar, so open it that way and extract it
        # to the specified (or default) roles directory
        local_file = False

        if self.scm:
            # create tar file from scm url
            tmp_file = RoleRequirement.scm_archive_role(**self.spec)
        elif self.src:
            if os.path.isfile(self.src):
                # installing a local tar.gz
                local_file = True
                tmp_file = self.src
            elif '://' in self.src:
                role_data = self.src
                tmp_file = self.fetch(role_data)
            else:
                api = GalaxyAPI(self.galaxy)
                role_data = api.lookup_role_by_name(self.src)
                if not role_data:
                    raise AnsibleError(""- sorry, %s was not found on %s."" % (self.src, api.api_server))

                if role_data.get('role_type') == 'APP':
                    # Container Role
                    display.warning(""%s is a Container App role, and should only be installed using Ansible ""
                                    ""Container"" % self.name)

                role_versions = api.fetch_role_related('versions', role_data['id'])
                if not self.version:
                    # convert the version names to LooseVersion objects
                    # and sort them to get the latest version. If there
                    # are no versions in the list, we'll grab the head
                    # of the master branch
                    if len(role_versions) > 0:
                        loose_versions = [LooseVersion(a.get('name', None)) for a in role_versions]
                        try:
                            loose_versions.sort()
                        except TypeError:
                            raise AnsibleError(
                                'Unable to compare role versions (%s) to determine the most recent version due to incompatible version formats. '
                                'Please contact the role author to resolve versioning conflicts, or specify an explicit role version to '
                                'install.' % ', '.join([v.vstring for v in loose_versions])
                            )
                        self.version = str(loose_versions[-1])
                    elif role_data.get('github_branch', None):
                        self.version = role_data['github_branch']
                    else:
                        self.version = 'master'
                elif self.version != 'master':
                    if role_versions and str(self.version) not in [a.get('name', None) for a in role_versions]:
                        raise AnsibleError(""- the specified version (%s) of %s was not found in the list of available versions (%s)."" % (self.version,
                                                                                                                                         self.name,
                                                                                                                                         role_versions))

                tmp_file = self.fetch(role_data)

        else:
            raise AnsibleError(""No valid role data found"")

        if tmp_file:

            display.debug(""installing from %s"" % tmp_file)

            if not tarfile.is_tarfile(tmp_file):
                raise AnsibleError(""the file downloaded was not a tar.gz"")
            else:
                if tmp_file.endswith('.gz'):
                    role_tar_file = tarfile.open(tmp_file, ""r:gz"")
                else:
                    role_tar_file = tarfile.open(tmp_file, ""r"")
                # verify the role's meta file
                meta_file = None
                members = role_tar_file.getmembers()
                # next find the metadata file
                for member in members:
                    if self.META_MAIN in member.name:
                        # Look for parent of meta/main.yml
                        # Due to possibility of sub roles each containing meta/main.yml
                        # look for shortest length parent
                        meta_parent_dir = os.path.dirname(os.path.dirname(member.name))
                        if not meta_file:
                            archive_parent_dir = meta_parent_dir
                            meta_file = member
                        else:
                            if len(meta_parent_dir) < len(archive_parent_dir):
                                archive_parent_dir = meta_parent_dir
                                meta_file = member
                if not meta_file:
                    raise AnsibleError(""this role does not appear to have a meta/main.yml file."")
                else:
                    try:
                        self._metadata = yaml.safe_load(role_tar_file.extractfile(meta_file))
                    except:
                        raise AnsibleError(""this role does not appear to have a valid meta/main.yml file."")

                # we strip off any higher-level directories for all of the files contained within
                # the tar file here. The default is 'github_repo-target'. Gerrit instances, on the other
                # hand, does not have a parent directory at all.
                installed = False
                while not installed:
                    display.display(""- extracting %s to %s"" % (self.name, self.path))
                    try:
                        if os.path.exists(self.path):
                            if not os.path.isdir(self.path):
                                raise AnsibleError(""the specified roles path exists and is not a directory."")
                            elif not getattr(self.options, ""force"", False):
                                raise AnsibleError(""the specified role %s appears to already exist. Use --force to replace it."" % self.name)
                            else:
                                # using --force, remove the old path
                                if not self.remove():
                                    raise AnsibleError(""%s doesn't appear to contain a role.\n  please remove this directory manually if you really ""
                                                       ""want to put the role here."" % self.path)
                        else:
                            os.makedirs(self.path)

                        # now we do the actual extraction to the path
                        for member in members:
                            # we only extract files, and remove any relative path
                            # bits that might be in the file for security purposes
                            # and drop any containing directory, as mentioned above
                            if member.isreg() or member.issym():
                                parts = member.name.replace(archive_parent_dir, """", 1).split(os.sep)
                                final_parts = []
                                for part in parts:
                                    if part != '..' and '~' not in part and '$' not in part:
                                        final_parts.append(part)
                                member.name = os.path.join(*final_parts)
                                role_tar_file.extract(member, self.path)

                        # write out the install info file for later use
                        self._write_galaxy_install_info()
                        installed = True
                    except OSError as e:
                        error = True
                        if e.errno == errno.EACCES and len(self.paths) > 1:
                            current = self.paths.index(self.path)
                            if len(self.paths) > current:
                                self.path = self.paths[current + 1]
                                error = False
                        if error:
                            raise AnsibleError(""Could not update files in %s: %s"" % (self.path, str(e)))

                # return the parsed yaml metadata
                display.display(""- %s was installed successfully"" % str(self))
                if not local_file:
                    try:
                        os.unlink(tmp_file)
                    except (OSError, IOError) as e:
                        display.warning(""Unable to remove tmp file (%s): %s"" % (tmp_file, str(e)))
                return True

        return False
",https://github.com/ansible/ansible/issues/32301,Uncaught Exception,Added exception handling.,lib/ansible/galaxy/role.py,GalaxyRole.install,[36],"$ ansible-galaxy install --force --role-file=requirements.yml --roles-path=roles -vvv
ansible-galaxy 2.4.1.0
config file = None
configured module search path = ['/Users/jodewey/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
ansible python module location = /Users/jodewey/.pyenv/versions/3.6.1/lib/python3.6/site-packages/ansible
executable location = /Users/jodewey/.pyenv/versions/3.6.1/bin/ansible-galaxy
python version = 3.6.1 (default, May  4 2017, 15:46:43) [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)]
No config file found; using defaults
Opened /Users/jodewey/.ansible_galaxy
found role {'name': 'timezone', 'src': 'yatesr.timezone', 'version': '', 'scm': None} in yaml file
Processing role timezone
Opened /Users/jodewey/.ansible_galaxy
- downloading role 'timezone', owned by yatesr
https://galaxy.ansible.com/api/v1/roles/?owner__username=yatesr&amp;name=timezone
https://galaxy.ansible.com/api/v1/roles/1429/versions/?page_size=50
ERROR! Unexpected Exception, this is probably a bug: '<' not supported between instances of 'int' and 'str'
the full traceback was:

Traceback (most recent call last):
File ""/Users/jodewey/.pyenv/versions/3.6.1/bin/ansible-galaxy"", line 106, in <module>
exit_code = cli.run()
File ""/Users/jodewey/.pyenv/versions/3.6.1/lib/python3.6/site-packages/ansible/cli/galaxy.py"", line 150, in run
self.execute()
File ""/Users/jodewey/.pyenv/versions/3.6.1/lib/python3.6/site-packages/ansible/cli/__init__.py"", line 154, in execute
fn()
File ""/Users/jodewey/.pyenv/versions/3.6.1/lib/python3.6/site-packages/ansible/cli/galaxy.py"", line 392, in execute_install
installed = role.install()
File ""/Users/jodewey/.pyenv/versions/3.6.1/lib/python3.6/site-packages/ansible/galaxy/role.py"", line 236, in install
loose_versions.sort()
File ""/Users/jodewey/.pyenv/versions/3.6.1/lib/python3.6/distutils/version.py"", line 52, in __lt__
c = self._cmp(other)
File ""/Users/jodewey/.pyenv/versions/3.6.1/lib/python3.6/distutils/version.py"", line 337, in _cmp
if self.version < other.version:
TypeError: '<' not supported between instances of 'int' and 'str'",TypeError,buggy_snippets_files/8393d8e7be1d317cd1fc234f5fa5d8616d5b3741b1dccfc22f391fe5b443888a_before_merge.py,buggy_snippets_files/8393d8e7be1d317cd1fc234f5fa5d8616d5b3741b1dccfc22f391fe5b443888a_after_merge.py
0,"def rescale(image, scale, order=None, mode='reflect', cval=0, clip=True,
            preserve_range=False, multichannel=False,
            anti_aliasing=None, anti_aliasing_sigma=None):
    """"""Scale image by a certain factor.

    Performs interpolation to up-scale or down-scale N-dimensional images.
    Note that anti-aliasing should be enabled when down-sizing images to avoid
    aliasing artifacts. For down-sampling with an integer factor also see
    `skimage.transform.downscale_local_mean`.

    Parameters
    ----------
    image : ndarray
        Input image.
    scale : {float, tuple of floats}
        Scale factors. Separate scale factors can be defined as
        `(rows, cols[, ...][, dim])`.

    Returns
    -------
    scaled : ndarray
        Scaled version of the input.

    Other parameters
    ----------------
    order : int, optional
        The order of the spline interpolation, default is 0 if
        image.dtype is bool and 1 otherwise. The order has to be in
        the range 0-5. See `skimage.transform.warp` for detail.
    mode : {'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional
        Points outside the boundaries of the input are filled according
        to the given mode.  Modes match the behaviour of `numpy.pad`.
    cval : float, optional
        Used in conjunction with mode 'constant', the value outside
        the image boundaries.
    clip : bool, optional
        Whether to clip the output to the range of values of the input image.
        This is enabled by default, since higher order interpolation may
        produce values outside the given input range.
    preserve_range : bool, optional
        Whether to keep the original range of values. Otherwise, the input
        image is converted according to the conventions of `img_as_float`.
        Also see
        https://scikit-image.org/docs/dev/user_guide/data_types.html
    multichannel : bool, optional
        Whether the last axis of the image is to be interpreted as multiple
        channels or another spatial dimension.
    anti_aliasing : bool, optional
        Whether to apply a Gaussian filter to smooth the image prior
        to down-scaling. It is crucial to filter when down-sampling
        the image to avoid aliasing artifacts. If input image data
        type is bool, no anti-aliasing is applied.
    anti_aliasing_sigma : {float, tuple of floats}, optional
        Standard deviation for Gaussian filtering to avoid aliasing artifacts.
        By default, this value is chosen as (s - 1) / 2 where s is the
        down-scaling factor.

    Notes
    -----
    Modes 'reflect' and 'symmetric' are similar, but differ in whether the edge
    pixels are duplicated during the reflection.  As an example, if an array
    has values [0, 1, 2] and was padded to the right by four values using
    symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it
    would be [0, 1, 2, 1, 0, 1, 2].

    Examples
    --------
    >>> from skimage import data
    >>> from skimage.transform import rescale
    >>> image = data.camera()
    >>> rescale(image, 0.1).shape
    (51, 51)
    >>> rescale(image, 0.5).shape
    (256, 256)

    """"""
    scale = np.atleast_1d(scale)
    if len(scale) > 1:
        if ((not multichannel and len(scale) != image.ndim) or
                (multichannel and len(scale) != image.ndim - 1)):
            raise ValueError(""Supply a single scale, or one value per spatial ""
                             ""axis"")
        if multichannel:
            scale = np.concatenate((scale, [1]))
    orig_shape = np.asarray(image.shape)
    output_shape = np.round(scale * orig_shape)
    if multichannel:  # don't scale channel dimension
        output_shape[-1] = orig_shape[-1]

    return resize(image, output_shape, order=order, mode=mode, cval=cval,
                  clip=clip, preserve_range=preserve_range,
                  anti_aliasing=anti_aliasing,
                  anti_aliasing_sigma=anti_aliasing_sigma)
","def rescale(image, scale, order=None, mode='reflect', cval=0, clip=True,
            preserve_range=False, multichannel=False,
            anti_aliasing=None, anti_aliasing_sigma=None):
    """"""Scale image by a certain factor.

    Performs interpolation to up-scale or down-scale N-dimensional images.
    Note that anti-aliasing should be enabled when down-sizing images to avoid
    aliasing artifacts. For down-sampling with an integer factor also see
    `skimage.transform.downscale_local_mean`.

    Parameters
    ----------
    image : ndarray
        Input image.
    scale : {float, tuple of floats}
        Scale factors. Separate scale factors can be defined as
        `(rows, cols[, ...][, dim])`.

    Returns
    -------
    scaled : ndarray
        Scaled version of the input.

    Other parameters
    ----------------
    order : int, optional
        The order of the spline interpolation, default is 0 if
        image.dtype is bool and 1 otherwise. The order has to be in
        the range 0-5. See `skimage.transform.warp` for detail.
    mode : {'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional
        Points outside the boundaries of the input are filled according
        to the given mode.  Modes match the behaviour of `numpy.pad`.
    cval : float, optional
        Used in conjunction with mode 'constant', the value outside
        the image boundaries.
    clip : bool, optional
        Whether to clip the output to the range of values of the input image.
        This is enabled by default, since higher order interpolation may
        produce values outside the given input range.
    preserve_range : bool, optional
        Whether to keep the original range of values. Otherwise, the input
        image is converted according to the conventions of `img_as_float`.
        Also see
        https://scikit-image.org/docs/dev/user_guide/data_types.html
    multichannel : bool, optional
        Whether the last axis of the image is to be interpreted as multiple
        channels or another spatial dimension.
    anti_aliasing : bool, optional
        Whether to apply a Gaussian filter to smooth the image prior
        to down-scaling. It is crucial to filter when down-sampling
        the image to avoid aliasing artifacts. If input image data
        type is bool, no anti-aliasing is applied.
    anti_aliasing_sigma : {float, tuple of floats}, optional
        Standard deviation for Gaussian filtering to avoid aliasing artifacts.
        By default, this value is chosen as (s - 1) / 2 where s is the
        down-scaling factor.

    Notes
    -----
    Modes 'reflect' and 'symmetric' are similar, but differ in whether the edge
    pixels are duplicated during the reflection.  As an example, if an array
    has values [0, 1, 2] and was padded to the right by four values using
    symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it
    would be [0, 1, 2, 1, 0, 1, 2].

    Examples
    --------
    >>> from skimage import data
    >>> from skimage.transform import rescale
    >>> image = data.camera()
    >>> rescale(image, 0.1).shape
    (51, 51)
    >>> rescale(image, 0.5).shape
    (256, 256)

    """"""
    scale = np.atleast_1d(scale)
    if len(scale) > 1:
        if ((not multichannel and len(scale) != image.ndim) or
                (multichannel and len(scale) != image.ndim - 1)):
            raise ValueError(""Supply a single scale, or one value per spatial ""
                             ""axis"")
        if multichannel:
            scale = np.concatenate((scale, [1]))
    orig_shape = np.asarray(image.shape)
    output_shape = np.maximum(np.round(scale * orig_shape), 1)
    if multichannel:  # don't scale channel dimension
        output_shape[-1] = orig_shape[-1]

    return resize(image, output_shape, order=order, mode=mode, cval=cval,
                  clip=clip, preserve_range=preserve_range,
                  anti_aliasing=anti_aliasing,
                  anti_aliasing_sigma=anti_aliasing_sigma)
",https://github.com/scikit-image/scikit-image/issues/5252,CWE-369: Divide By Zero,output_shape array in rescale function may contain zero components. It is passed as an argument to resize function where it becomes a denominator in the expression for factors array,skimage/transform/_warps.py,rescale,[86],"SOME_PATH\skimage\transform\_warps.py:115: RuntimeWarning: divide by zero encountered in true_divide
factors = (np.asarray(input_shape, dtype=float) /
Traceback (most recent call last):
File ""PATH_TO\my_script.py"", line 5, in <module>
img = transform.rescale(img, scale=0.1)  # error
File ""SOME_PATH\skimage\transform\_warps.py"", line 290, in rescale
return resize(image, output_shape, order=order, mode=mode, cval=cval,
File ""SOME_PATH\skimage\transform\_warps.py"", line 147, in resize
image = ndi.gaussian_filter(image, anti_aliasing_sigma,
File ""SOME_PATH\scipy\ndimage\filters.py"", line 341, in gaussian_filter
gaussian_filter1d(input, sigma, axis, order, output,
File ""SOME_PATH\scipy\ndimage\filters.py"", line 257, in gaussian_filter1d
lw = int(truncate * sd + 0.5)
OverflowError: cannot convert float infinity to integer
Process finished with exit code 1",OverflowError,buggy_snippets_files/b82d3bc69380ce2d9d59011f19004361ea8a3cb128410cacf11d7778d13a1353_before_merge.py,buggy_snippets_files/b82d3bc69380ce2d9d59011f19004361ea8a3cb128410cacf11d7778d13a1353_after_merge.py
1,"def _build_laplacian(data, spacing, mask, beta, multichannel):
    l_x, l_y, l_z = data.shape[:3]
    edges = _make_graph_edges_3d(l_x, l_y, l_z)
    weights = _compute_weights_3d(data, spacing, beta=beta, eps=1.e-10,
                                  multichannel=multichannel)
    if mask is not None:
        # Remove edges of the graph connected to masked nodes, as well
        # as corresponding weights of the edges.
        mask0 = np.hstack([mask[..., :-1].ravel(), mask[:, :-1].ravel(),
                           mask[:-1].ravel()])
        mask1 = np.hstack([mask[..., 1:].ravel(), mask[:, 1:].ravel(),
                           mask[1:].ravel()])
        ind_mask = np.logical_and(mask0, mask1)
        edges, weights = edges[:, ind_mask], weights[ind_mask]

        # Reassign edges labels to 0, 1, ... edges_number - 1
        _, inv_idx = np.unique(edges, return_inverse=True)
        edges = inv_idx.reshape(edges.shape)

    # Build the sparse linear system
    pixel_nb = edges.shape[1]
    i_indices = edges.ravel()
    j_indices = edges[::-1].ravel()
    data = np.hstack((weights, weights))
    lap = sparse.coo_matrix((data, (i_indices, j_indices)),
                            shape=(pixel_nb, pixel_nb))
    lap.setdiag(-np.ravel(lap.sum(axis=0)))
    return lap.tocsr()
","def _build_laplacian(data, spacing, mask, beta, multichannel):
    l_x, l_y, l_z = data.shape[:3]
    edges = _make_graph_edges_3d(l_x, l_y, l_z)
    weights = _compute_weights_3d(data, spacing, beta=beta, eps=1.e-10,
                                  multichannel=multichannel)
    if mask is not None:
        # Remove edges of the graph connected to masked nodes, as well
        # as corresponding weights of the edges.
        mask0 = np.hstack([mask[..., :-1].ravel(), mask[:, :-1].ravel(),
                           mask[:-1].ravel()])
        mask1 = np.hstack([mask[..., 1:].ravel(), mask[:, 1:].ravel(),
                           mask[1:].ravel()])
        ind_mask = np.logical_and(mask0, mask1)
        edges, weights = edges[:, ind_mask], weights[ind_mask]

        # Reassign edges labels to 0, 1, ... edges_number - 1
        _, inv_idx = np.unique(edges, return_inverse=True)
        edges = inv_idx.reshape(edges.shape)

    # Build the sparse linear system
    pixel_nb = l_x * l_y * l_z
    i_indices = edges.ravel()
    j_indices = edges[::-1].ravel()
    data = np.hstack((weights, weights))
    lap = sparse.coo_matrix((data, (i_indices, j_indices)),
                            shape=(pixel_nb, pixel_nb))
    lap.setdiag(-np.ravel(lap.sum(axis=0)))
    return lap.tocsr()
",https://github.com/scikit-image/scikit-image/issues/5034,CWE-126: Buffer Over-read,"pixel_nb variable in _build_laplacian function is responsible for size of lap sparse matrix. This matrix is then indexed with unlabeled_indices in _build_linear_system function. As pixel_nb underestimates size of lap, it leads to IndexError during performing the indexing operation with unlabeled_indices in _build_linear_system function",skimage/segmentation/random_walker_segmentation.py,_build_laplacian,[21],"Traceback (most recent call last):
File ""rw.py"", line 6, in <module>
labels = random_walker(data, markers, beta=10)
File ""/Users/jni/projects/scikit-image/skimage/segmentation/random_walker_segmentation.py"", line 483, in random_walker
lap_sparse, B = _build_linear_system(data, spacing, labels, nlabels, mask,
File ""/Users/jni/projects/scikit-image/skimage/segmentation/random_walker_segmentation.py"", line 157, in _build_linear_system
rows = lap_sparse[unlabeled_indices, :]
File ""/Users/jni/conda/envs/all/lib/python3.8/site-packages/scipy/sparse/_index.py"", line 33, in __getitem__
row, col = self._validate_indices(key)
File ""/Users/jni/conda/envs/all/lib/python3.8/site-packages/scipy/sparse/_index.py"", line 137, in _validate_indices
row = self._asindices(row, M)
File ""/Users/jni/conda/envs/all/lib/python3.8/site-packages/scipy/sparse/_index.py"", line 169, in _asindices
raise IndexError('index (%d) out of range' % max_indx)
IndexError: index (6) out of range",IndexError,buggy_snippets_files/f8d2a2d78243f8b2762893e9414050d74e39ad70f707661fc2205e0f1952872f_before_merge.py,buggy_snippets_files/f8d2a2d78243f8b2762893e9414050d74e39ad70f707661fc2205e0f1952872f_after_merge.py
77,"def hough_line_peaks(hspace, angles, dists, min_distance=9, min_angle=10,
                     threshold=None, num_peaks=np.inf):
    """"""Return peaks in a straight line Hough transform.

    Identifies most prominent lines separated by a certain angle and distance
    in a Hough transform. Non-maximum suppression with different sizes is
    applied separately in the first (distances) and second (angles) dimension
    of the Hough space to identify peaks.

    Parameters
    ----------
    hspace : (N, M) array
        Hough space returned by the `hough_line` function.
    angles : (M,) array
        Angles returned by the `hough_line` function. Assumed to be continuous.
        (`angles[-1] - angles[0] == PI`).
    dists : (N, ) array
        Distances returned by the `hough_line` function.
    min_distance : int, optional
        Minimum distance separating lines (maximum filter size for first
        dimension of hough space).
    min_angle : int, optional
        Minimum angle separating lines (maximum filter size for second
        dimension of hough space).
    threshold : float, optional
        Minimum intensity of peaks. Default is `0.5 * max(hspace)`.
    num_peaks : int, optional
        Maximum number of peaks. When the number of peaks exceeds `num_peaks`,
        return `num_peaks` coordinates based on peak intensity.

    Returns
    -------
    accum, angles, dists : tuple of array
        Peak values in Hough space, angles and distances.

    Examples
    --------
    >>> from skimage.transform import hough_line, hough_line_peaks
    >>> from skimage.draw import line
    >>> img = np.zeros((15, 15), dtype=np.bool_)
    >>> rr, cc = line(0, 0, 14, 14)
    >>> img[rr, cc] = 1
    >>> rr, cc = line(0, 14, 14, 0)
    >>> img[cc, rr] = 1
    >>> hspace, angles, dists = hough_line(img)
    >>> hspace, angles, dists = hough_line_peaks(hspace, angles, dists)
    >>> len(angles)
    2

    """"""
    from ..feature.peak import _prominent_peaks

    h, a, d = _prominent_peaks(hspace, min_xdistance=min_angle,
                               min_ydistance=min_distance,
                               threshold=threshold,
                               num_peaks=num_peaks)
    if a.any():
        return (h, angles[a], dists[d])
    else:
        return (h, np.array([]), np.array([]))
","def hough_line_peaks(hspace, angles, dists, min_distance=9, min_angle=10,
                     threshold=None, num_peaks=np.inf):
    """"""Return peaks in a straight line Hough transform.

    Identifies most prominent lines separated by a certain angle and distance
    in a Hough transform. Non-maximum suppression with different sizes is
    applied separately in the first (distances) and second (angles) dimension
    of the Hough space to identify peaks.

    Parameters
    ----------
    hspace : (N, M) array
        Hough space returned by the `hough_line` function.
    angles : (M,) array
        Angles returned by the `hough_line` function. Assumed to be continuous.
        (`angles[-1] - angles[0] == PI`).
    dists : (N, ) array
        Distances returned by the `hough_line` function.
    min_distance : int, optional
        Minimum distance separating lines (maximum filter size for first
        dimension of hough space).
    min_angle : int, optional
        Minimum angle separating lines (maximum filter size for second
        dimension of hough space).
    threshold : float, optional
        Minimum intensity of peaks. Default is `0.5 * max(hspace)`.
    num_peaks : int, optional
        Maximum number of peaks. When the number of peaks exceeds `num_peaks`,
        return `num_peaks` coordinates based on peak intensity.

    Returns
    -------
    accum, angles, dists : tuple of array
        Peak values in Hough space, angles and distances.

    Examples
    --------
    >>> from skimage.transform import hough_line, hough_line_peaks
    >>> from skimage.draw import line
    >>> img = np.zeros((15, 15), dtype=np.bool_)
    >>> rr, cc = line(0, 0, 14, 14)
    >>> img[rr, cc] = 1
    >>> rr, cc = line(0, 14, 14, 0)
    >>> img[cc, rr] = 1
    >>> hspace, angles, dists = hough_line(img)
    >>> hspace, angles, dists = hough_line_peaks(hspace, angles, dists)
    >>> len(angles)
    2

    """"""
    from ..feature.peak import _prominent_peaks

    min_angle = min(min_angle, hspace.shape[1])
    h, a, d = _prominent_peaks(hspace, min_xdistance=min_angle,
                               min_ydistance=min_distance,
                               threshold=threshold,
                               num_peaks=num_peaks)
    if a.any():
        return (h, angles[a], dists[d])
    else:
        return (h, np.array([]), np.array([]))
",https://github.com/scikit-image/scikit-image/issues/4814,CWE-127: Buffer Under-read,"keyword argument min_angle is assigned to its default value of 10 in hough_line_peaks function arguments list declaration. When the function _prominent_peaks is called from the hough_line_peaks function, an IndexError occurs as xcoords_nh contains -9 whereas size of img_max is 1 along the axis=1. Here min_angle value is contained in ",skimage/transform/hough_transform.py,hough_line_peaks,[53],"Traceback (most recent call last):
File ""<ipython-input-147-b875ad1f50fd>"", line 3, in <module>
accum, angles, dists = hough_line_peaks(h, theta, d, threshold=2)
File ""C:\Users\Mark4\anaconda3\lib\site-packages\skimage\transform\hough_transform.py"", line 63, in hough_line_peaks
num_peaks=num_peaks)
File ""C:\Users\Mark4\anaconda3\lib\site-packages\skimage\feature\peak.py"", line 316, in _prominent_peaks
img_max[ycoords_nh, xcoords_nh] = 0
IndexError: index -9 is out of bounds for axis 1 with size 1",IndexError,buggy_snippets_files/ec63fd2194c7ec8b838c9a9f1143617439b63b0d3c556a3e4317d9642f55527a_before_merge.py,buggy_snippets_files/ec63fd2194c7ec8b838c9a9f1143617439b63b0d3c556a3e4317d9642f55527a_after_merge.py
1200,"def _find_min_diff(image):
    """"""
    Find the minimal difference of grey levels inside the image.
    """"""
    img_vec = np.unique(image.flatten())
    min_diff = np.min(img_vec[1:] - img_vec[:-1])
    return min_diff
","def _find_min_diff(image):
    """"""
    Find the minimal difference of grey levels inside the image.
    """"""
    img_vec = np.unique(image.flatten())

    if img_vec.size == 1:
        return 0

    min_diff = np.min(img_vec[1:] - img_vec[:-1])
    return min_diff
",https://github.com/scikit-image/scikit-image/issues/2691,"CWE-126: Buffer Over-read, CWE-127: Buffer Under-read, CWE-248: Uncaught Exception",the extreme situation of background image=0 is not processed,skimage/morphology/extrema.py,_find_min_diff,[6],"import numpy as np
from skimage import morphology
a = np.ones((10,10), dtype=float)
morphology.local_maxima(a)
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""/usr/local/lib/python3.5/dist-packages/skimage/morphology/extrema.py"", line 280, in local_maxima
h = _find_min_diff(img)
File ""/usr/local/lib/python3.5/dist-packages/skimage/morphology/extrema.py"", line 215, in _find_min_diff
min_diff = np.min(img_vec[1:] - img_vec[:-1])
File ""/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py"", line 2372, in amin
out=out, **kwargs)
File ""/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py"", line 29, in _amin
return umr_minimum(a, axis, None, out, keepdims)
ValueError: zero-size array to reduction operation minimum which has no identity",ValueError,buggy_snippets_files/b4ebf1d65a3f3426c610cddf4d0619bbc0304568bf67d5abad3e04a4ed18c217_before_merge.py,buggy_snippets_files/b4ebf1d65a3f3426c610cddf4d0619bbc0304568bf67d5abad3e04a4ed18c217_after_merge.py
1344,"    def set_image(self, *, url):
        """"""Sets the image for the embed content.

        This function returns the class instance to allow for fluent-style
        chaining.

        .. versionchanged:: 1.4
            Passing :attr:`Empty` removes the image.

        Parameters
        -----------
        url: :class:`str`
            The source URL for the image. Only HTTP(S) is supported.
        """"""

        if url is EmptyEmbed:
            del self._image
        else:
            self._image = {
                'url': str(url)
            }

        return self
","    def set_image(self, *, url):
        """"""Sets the image for the embed content.

        This function returns the class instance to allow for fluent-style
        chaining.

        .. versionchanged:: 1.4
            Passing :attr:`Empty` removes the image.

        Parameters
        -----------
        url: :class:`str`
            The source URL for the image. Only HTTP(S) is supported.
        """"""

        if url is EmptyEmbed:
            try:
                del self._image
            except AttributeError:
                pass
        else:
            self._image = {
                'url': str(url)
            }

        return self
",https://github.com/Rapptz/discord.py/issues/5210,"CWE-415: Double Free, CWE-248: Uncaught Exception",the atribute _image is deleted twice from the object self,discord/embeds.py,Embed.set_image,[17],"_ClientEventTask exception was never retrieved
future: <ClientEventTask state=finished event=on_command_error coro=<bound method Bot.on_command_error of <s4.bot.bot.Bot object at 0x000001E116E814F0>> exception=AttributeError('_thumbnail')>
Traceback (most recent call last):
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\client.py"", line 317, in _run_event
await self.on_error(event_name, *args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\bot.py"", line 103, in on_error
await error.error(err, *args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\client.py"", line 312, in _run_event
await coro(*args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\bot.py"", line 107, in on_command_error
await error.command_error(ctx, exc)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\cogs\error.py"", line 124, in command_error
raise original
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\ext\commands\core.py"", line 85, in wrapped
ret = await coro(*args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\cogs\meta.py"", line 160, in botinfo_command
embed=self.bot.embed.build(
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\utils\embed.py"", line 35, in build
embed.set_thumbnail(url=Embed.Empty)
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\embeds.py"", line 328, in set_thumbnail
del self._thumbnail
AttributeError: _thumbnail",AttributeError,buggy_snippets_files/99f1f17c34c2c99c5fbc5d356d30078edb3666cfe190aa15e207e72aa058caca_before_merge.py,buggy_snippets_files/99f1f17c34c2c99c5fbc5d356d30078edb3666cfe190aa15e207e72aa058caca_after_merge.py
1345,"    def set_thumbnail(self, *, url):
        """"""Sets the thumbnail for the embed content.

        This function returns the class instance to allow for fluent-style
        chaining.

        .. versionchanged:: 1.4
            Passing :attr:`Empty` removes the thumbnail.

        Parameters
        -----------
        url: :class:`str`
            The source URL for the thumbnail. Only HTTP(S) is supported.
        """"""

        if url is EmptyEmbed:
            del self._thumbnail
        else:
            self._thumbnail = {
                'url': str(url)
            }

        return self
","    def set_thumbnail(self, *, url):
        """"""Sets the thumbnail for the embed content.

        This function returns the class instance to allow for fluent-style
        chaining.

        .. versionchanged:: 1.4
            Passing :attr:`Empty` removes the thumbnail.

        Parameters
        -----------
        url: :class:`str`
            The source URL for the thumbnail. Only HTTP(S) is supported.
        """"""

        if url is EmptyEmbed:
            try:
                del self._thumbnail
            except AttributeError:
                pass
        else:
            self._thumbnail = {
                'url': str(url)
            }

        return self
",https://github.com/Rapptz/discord.py/issues/5210,"CWE-415: Double Free, CWE-248: Uncaught Exception",the atribute _thumbnail is deleted twice from the object self,discord/embeds.py,Embed.set_thumbnail,[17],"_ClientEventTask exception was never retrieved
future: <ClientEventTask state=finished event=on_command_error coro=<bound method Bot.on_command_error of <s4.bot.bot.Bot object at 0x000001E116E814F0>> exception=AttributeError('_thumbnail')>
Traceback (most recent call last):
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\client.py"", line 317, in _run_event
await self.on_error(event_name, *args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\bot.py"", line 103, in on_error
await error.error(err, *args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\client.py"", line 312, in _run_event
await coro(*args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\bot.py"", line 107, in on_command_error
await error.command_error(ctx, exc)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\cogs\error.py"", line 124, in command_error
raise original
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\ext\commands\core.py"", line 85, in wrapped
ret = await coro(*args, **kwargs)
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\bot\cogs\meta.py"", line 160, in botinfo_command
embed=self.bot.embed.build(
File ""D:\Programs\Python\Discord Bots\Bots\S4\s4\utils\embed.py"", line 35, in build
embed.set_thumbnail(url=Embed.Empty)
File ""D:\Programs\Python\Discord Bots\Bots\S4\.venv\lib\site-packages\discord\embeds.py"", line 328, in set_thumbnail
del self._thumbnail
AttributeError: _thumbnail",AttributeError,buggy_snippets_files/95aa5669d1262c7e680e2f2b7d526130522bb5ff2f1727721dff2f57d6ffbf96_before_merge.py,buggy_snippets_files/95aa5669d1262c7e680e2f2b7d526130522bb5ff2f1727721dff2f57d6ffbf96_after_merge.py
1905,"    def get_staticfiles_finders(self):
        """"""
        Returns a sorted mapping between the finder path and the list
        of relative and file system paths which that finder was able
        to find.
        """"""
        finders_mapping = OrderedDict()
        for finder in finders.get_finders():
            for path, finder_storage in finder.list([]):
                if getattr(finder_storage, ""prefix"", None):
                    prefixed_path = join(finder_storage.prefix, path)
                else:
                    prefixed_path = path
                finder_cls = finder.__class__
                finder_path = ""."".join([finder_cls.__module__, finder_cls.__name__])
                real_path = finder_storage.path(path)
                payload = (prefixed_path, real_path)
                finders_mapping.setdefault(finder_path, []).append(payload)
                self.num_found += 1
        return finders_mapping
","    def get_staticfiles_finders(self):
        """"""
        Returns a sorted mapping between the finder path and the list
        of relative and file system paths which that finder was able
        to find.
        """"""
        finders_mapping = OrderedDict()
        for finder in finders.get_finders():
            try:
                for path, finder_storage in finder.list([]):
                    if getattr(finder_storage, ""prefix"", None):
                        prefixed_path = join(finder_storage.prefix, path)
                    else:
                        prefixed_path = path
                    finder_cls = finder.__class__
                    finder_path = ""."".join([finder_cls.__module__, finder_cls.__name__])
                    real_path = finder_storage.path(path)
                    payload = (prefixed_path, real_path)
                    finders_mapping.setdefault(finder_path, []).append(payload)
                    self.num_found += 1
            except OSError:
                # This error should be captured and presented as a part of run_checks.
                pass
        return finders_mapping
",https://github.com/jazzband/django-debug-toolbar/issues/1318,CWE-248: Uncaught Exception,the situation is not handled where a directory do not exist when applying finder.list([]),debug_toolbar/panels/staticfiles.py,StaticFilesPanel.get_staticfiles_finders,[9],"Internal Server Error: /
Traceback (most recent call last):
File ""/Users/paul/.dev/police-records/.direnv/python-3.8.5/lib/python3.8/site-packages/django/core/handlers/exception.py"", line 34, in inner
response = get_response(request)
File ""/Users/paul/.dev/police-records/.direnv/python-3.8.5/lib/python3.8/site-packages/debug_toolbar/middleware.py"", line 92, in __call__
panel.generate_stats(request, response)
File ""/Users/paul/.dev/police-records/.direnv/python-3.8.5/lib/python3.8/site-packages/debug_toolbar/panels/staticfiles.py"", line 128, in generate_stats
""staticfiles_finders"": self.get_staticfiles_finders(),
File ""/Users/paul/.dev/police-records/.direnv/python-3.8.5/lib/python3.8/site-packages/debug_toolbar/panels/staticfiles.py"", line 140, in get_staticfiles_finders
for path, finder_storage in finder.list([]):
File ""/Users/paul/.dev/police-records/.direnv/python-3.8.5/lib/python3.8/site-packages/django/contrib/staticfiles/finders.py"", line 130, in list
for path in utils.get_files(storage, ignore_patterns):
File ""/Users/paul/.dev/police-records/.direnv/python-3.8.5/lib/python3.8/site-packages/django/contrib/staticfiles/utils.py"", line 23, in get_files
directories, files = storage.listdir(location)
File ""/Users/paul/.dev/police-records/.direnv/python-3.8.5/lib/python3.8/site-packages/django/core/files/storage.py"", line 316, in listdir
for entry in os.scandir(path):
FileNotFoundError: [Errno 2] No such file or directory: '/Users/paul/.dev/police-records/static'",FileNotFoundError,buggy_snippets_files/ac371f6476f75c3a75d1df71b442b6503b6bd26cad4109e99d16713c4c8661ed_before_merge.py,buggy_snippets_files/ac371f6476f75c3a75d1df71b442b6503b6bd26cad4109e99d16713c4c8661ed_after_merge.py
1166,"def invert(image, signed_float=False):
    """"""Invert an image.

    Invert the intensity range of the input image, so that the dtype maximum
    is now the dtype minimum, and vice-versa. This operation is
    slightly different depending on the input dtype:

    - unsigned integers: subtract the image from the dtype maximum
    - signed integers: subtract the image from -1 (see Notes)
    - floats: subtract the image from 1 (if signed_float is False, so we
      assume the image is unsigned), or from 0 (if signed_float is True).

    See the examples for clarification.

    Parameters
    ----------
    image : ndarray
        Input image.
    signed_float : bool, optional
        If True and the image is of type float, the range is assumed to
        be [-1, 1]. If False and the image is of type float, the range is
        assumed to be [0, 1].

    Returns
    -------
    inverted : ndarray
        Inverted image.

    Notes
    -----
    Ideally, for signed integers we would simply multiply by -1. However,
    signed integer ranges are asymmetric. For example, for np.int8, the range
    of possible values is [-128, 127], so that -128 * -1 equals -128! By
    subtracting from -1, we correctly map the maximum dtype value to the
    minimum.

    Examples
    --------
    >>> img = np.array([[100,  0, 200],
    ...                 [  0, 50,   0],
    ...                 [ 30,  0, 255]], np.uint8)
    >>> invert(img)
    array([[155, 255,  55],
           [255, 205, 255],
           [225, 255,   0]], dtype=uint8)
    >>> img2 = np.array([[ -2, 0, -128],
    ...                  [127, 0,    5]], np.int8)
    >>> invert(img2)
    array([[   1,   -1,  127],
           [-128,   -1,   -6]], dtype=int8)
    >>> img3 = np.array([[ 0., 1., 0.5, 0.75]])
    >>> invert(img3)
    array([[ 1.  ,  0.  ,  0.5 ,  0.25]])
    >>> img4 = np.array([[ 0., 1., -1., -0.25]])
    >>> invert(img4, signed_float=True)
    array([[-0.  , -1.  ,  1.  ,  0.25]])
    """"""
    if image.dtype == 'bool':
        inverted = ~image
    elif np.issubdtype(image.dtype, np.unsignedinteger):
        max_val = dtype_limits(image, clip_negative=False)[1]
        inverted = max_val - image
    elif np.issubdtype(image.dtype, np.signedinteger):
        inverted = -1 - image
    else:  # float dtype
        if signed_float:
            inverted = -image
        else:
            inverted = 1 - image
    return inverted
","def invert(image, signed_float=False):
    """"""Invert an image.

    Invert the intensity range of the input image, so that the dtype maximum
    is now the dtype minimum, and vice-versa. This operation is
    slightly different depending on the input dtype:

    - unsigned integers: subtract the image from the dtype maximum
    - signed integers: subtract the image from -1 (see Notes)
    - floats: subtract the image from 1 (if signed_float is False, so we
      assume the image is unsigned), or from 0 (if signed_float is True).

    See the examples for clarification.

    Parameters
    ----------
    image : ndarray
        Input image.
    signed_float : bool, optional
        If True and the image is of type float, the range is assumed to
        be [-1, 1]. If False and the image is of type float, the range is
        assumed to be [0, 1].

    Returns
    -------
    inverted : ndarray
        Inverted image.

    Notes
    -----
    Ideally, for signed integers we would simply multiply by -1. However,
    signed integer ranges are asymmetric. For example, for np.int8, the range
    of possible values is [-128, 127], so that -128 * -1 equals -128! By
    subtracting from -1, we correctly map the maximum dtype value to the
    minimum.

    Examples
    --------
    >>> img = np.array([[100,  0, 200],
    ...                 [  0, 50,   0],
    ...                 [ 30,  0, 255]], np.uint8)
    >>> invert(img)
    array([[155, 255,  55],
           [255, 205, 255],
           [225, 255,   0]], dtype=uint8)
    >>> img2 = np.array([[ -2, 0, -128],
    ...                  [127, 0,    5]], np.int8)
    >>> invert(img2)
    array([[   1,   -1,  127],
           [-128,   -1,   -6]], dtype=int8)
    >>> img3 = np.array([[ 0., 1., 0.5, 0.75]])
    >>> invert(img3)
    array([[ 1.  ,  0.  ,  0.5 ,  0.25]])
    >>> img4 = np.array([[ 0., 1., -1., -0.25]])
    >>> invert(img4, signed_float=True)
    array([[-0.  , -1.  ,  1.  ,  0.25]])
    """"""
    if image.dtype == 'bool':
        inverted = ~image
    elif np.issubdtype(image.dtype, np.unsignedinteger):
        max_val = dtype_limits(image, clip_negative=False)[1]
        inverted = np.subtract(max_val, image, dtype=image.dtype)
    elif np.issubdtype(image.dtype, np.signedinteger):
        inverted = np.subtract(-1, image, dtype=image.dtype)
    else:  # float dtype
        if signed_float:
            inverted = -image
        else:
            inverted = np.subtract(1, image, dtype=image.dtype)
    return inverted
",https://github.com/scikit-image/scikit-image/issues/3043,"CWE-704: Incorrect Type Conversion or Cast, CWE-241: Improper Handling of Unexpected Data Type, CWE-248: Uncaught Exception",type of pixels in input image does not equal to type of pixels in inverted image. Subtraction like 256-np.array() does not always keep the same type for resulting np.array,skimage/util/_invert.py,invert,[62; 64; 69],"In [1]: import numpy as np

In [2]: from skimage.util import invert

In [3]: imin = np.array([0, 1], np.uint64)

In [4]: imout = invert(imin)

In [5]: imtest = invert(imout)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-5-ab8c45624279> in <module>()
----> 1 imtest = invert(imout)

/Users/twalter/pycharm/scikit-image/skimage/util/_invert.pyc in invert(image, signed_float)
63         inverted = ~image
64     elif np.issubdtype(image.dtype, np.unsignedinteger):
---> 65         max_val = dtype_limits(image, clip_negative=False)[1]
66         inverted = max_val - image
67     elif np.issubdtype(image.dtype, np.signedinteger):

/Users/twalter/pycharm/scikit-image/skimage/util/dtype.pyc in dtype_limits(image, clip_negative)
48         warn('The default of `clip_negative` in `skimage.util.dtype_limits` '
49              'will change to `False` in version 0.15.')
---> 50     imin, imax = dtype_range[image.dtype.type]
51     if clip_negative:
52         imin = 0

KeyError: <type 'numpy.uint64'>",KeyError,buggy_snippets_files/00dde44f282557d0e695b117016e41021555ec48403745bad6af18a05d25b052_before_merge.py,buggy_snippets_files/00dde44f282557d0e695b117016e41021555ec48403745bad6af18a05d25b052_after_merge.py
1206,"    def detect_and_extract(self, image):
        """"""Detect oriented FAST keypoints and extract rBRIEF descriptors.

        Note that this is faster than first calling `detect` and then
        `extract`.

        Parameters
        ----------
        image : 2D array
            Input image.

        """"""
        assert_nD(image, 2)

        pyramid = self._build_pyramid(image)

        keypoints_list = []
        responses_list = []
        scales_list = []
        orientations_list = []
        descriptors_list = []

        for octave in range(len(pyramid)):

            octave_image = np.ascontiguousarray(pyramid[octave])

            keypoints, orientations, responses = \
                self._detect_octave(octave_image)

            if len(keypoints) == 0:
                keypoints_list.append(keypoints)
                responses_list.append(responses)
                descriptors_list.append(np.zeros((0, 256), dtype=np.bool))
                continue

            descriptors, mask = self._extract_octave(octave_image, keypoints,
                                                     orientations)

            keypoints_list.append(keypoints[mask] * self.downscale ** octave)
            responses_list.append(responses[mask])
            orientations_list.append(orientations[mask])
            scales_list.append(self.downscale ** octave
                               * np.ones(keypoints.shape[0], dtype=np.intp))
            descriptors_list.append(descriptors)

        keypoints = np.vstack(keypoints_list)
        responses = np.hstack(responses_list)
        scales = np.hstack(scales_list)
        orientations = np.hstack(orientations_list)
        descriptors = np.vstack(descriptors_list).view(np.bool)

        if keypoints.shape[0] < self.n_keypoints:
            self.keypoints = keypoints
            self.scales = scales
            self.orientations = orientations
            self.responses = responses
            self.descriptors = descriptors
        else:
            # Choose best n_keypoints according to Harris corner response
            best_indices = responses.argsort()[::-1][:self.n_keypoints]
            self.keypoints = keypoints[best_indices]
            self.scales = scales[best_indices]
            self.orientations = orientations[best_indices]
            self.responses = responses[best_indices]
            self.descriptors = descriptors[best_indices]
","    def detect_and_extract(self, image):
        """"""Detect oriented FAST keypoints and extract rBRIEF descriptors.

        Note that this is faster than first calling `detect` and then
        `extract`.

        Parameters
        ----------
        image : 2D array
            Input image.

        """"""
        assert_nD(image, 2)

        pyramid = self._build_pyramid(image)

        keypoints_list = []
        responses_list = []
        scales_list = []
        orientations_list = []
        descriptors_list = []

        for octave in range(len(pyramid)):

            octave_image = np.ascontiguousarray(pyramid[octave])

            keypoints, orientations, responses = \
                self._detect_octave(octave_image)

            if len(keypoints) == 0:
                keypoints_list.append(keypoints)
                responses_list.append(responses)
                descriptors_list.append(np.zeros((0, 256), dtype=np.bool))
                continue

            descriptors, mask = self._extract_octave(octave_image, keypoints,
                                                     orientations)

            keypoints_list.append(keypoints[mask] * self.downscale ** octave)
            responses_list.append(responses[mask])
            orientations_list.append(orientations[mask])
            scales_list.append(self.downscale ** octave
                               * np.ones(keypoints.shape[0], dtype=np.intp))
            descriptors_list.append(descriptors)

        if len(scales_list) == 0:
            raise RuntimeError(""ORB found no features. Try passing in an image ""
                               ""containing greater intensity contrasts between ""
                               ""adjacent pixels."")

        keypoints = np.vstack(keypoints_list)
        responses = np.hstack(responses_list)
        scales = np.hstack(scales_list)
        orientations = np.hstack(orientations_list)
        descriptors = np.vstack(descriptors_list).view(np.bool)

        if keypoints.shape[0] < self.n_keypoints:
            self.keypoints = keypoints
            self.scales = scales
            self.orientations = orientations
            self.responses = responses
            self.descriptors = descriptors
        else:
            # Choose best n_keypoints according to Harris corner response
            best_indices = responses.argsort()[::-1][:self.n_keypoints]
            self.keypoints = keypoints[best_indices]
            self.scales = scales[best_indices]
            self.orientations = orientations[best_indices]
            self.responses = responses[best_indices]
            self.descriptors = descriptors[best_indices]
",https://github.com/scikit-image/scikit-image/issues/2253,CWE-248: Uncaught Exception,the case is unprocessed where scales_list is an empty array,skimage/feature/orb.py,ORB.detect_and_extract,[48],"---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-37-caefda1c8359> in <module>()
----> 1 extractor_em.detect_and_extract(simlight8)

/Users/jni/conda/envs/ana3/lib/python3.5/site-packages/skimage/feature/orb.py in detect_and_extract(self, image)
321         keypoints = np.vstack(keypoints_list)
322         responses = np.hstack(responses_list)
--> 323         scales = np.hstack(scales_list)
324         orientations = np.hstack(orientations_list)
325         descriptors = np.vstack(descriptors_list).view(np.bool)

/Users/jni/conda/envs/ana3/lib/python3.5/site-packages/numpy/core/shape_base.py in hstack(tup)
275     arrs = [atleast_1d(_m) for _m in tup]
276     # As a special case, dimension 0 of 1-dimensional arrays is ""horizontal""
--> 277     if arrs[0].ndim == 1:
278         return _nx.concatenate(arrs, 0)
279     else:

IndexError: list index out of range",IndexError,buggy_snippets_files/8e92b6b8ae2abf1ff58754e115e5a2af0479f19e295aed603d5a5ceaf87bce8b_before_merge.py,buggy_snippets_files/8e92b6b8ae2abf1ff58754e115e5a2af0479f19e295aed603d5a5ceaf87bce8b_after_merge.py
1238,"def blob_log(image, min_sigma=1, max_sigma=50, num_sigma=10, threshold=.2,
             overlap=.5, log_scale=False):
    """"""Finds blobs in the given grayscale image.

    Blobs are found using the Laplacian of Gaussian (LoG) method [1]_.
    For each blob found, the method returns its coordinates and the standard
    deviation of the Gaussian kernel that detected the blob.

    Parameters
    ----------
    image : ndarray
        Input grayscale image, blobs are assumed to be light on dark
        background (white on black).
    min_sigma : float, optional
        The minimum standard deviation for Gaussian Kernel. Keep this low to
        detect smaller blobs.
    max_sigma : float, optional
        The maximum standard deviation for Gaussian Kernel. Keep this high to
        detect larger blobs.
    num_sigma : int, optional
        The number of intermediate values of standard deviations to consider
        between `min_sigma` and `max_sigma`.
    threshold : float, optional.
        The absolute lower bound for scale space maxima. Local maxima smaller
        than thresh are ignored. Reduce this to detect blobs with less
        intensities.
    overlap : float, optional
        A value between 0 and 1. If the area of two blobs overlaps by a
        fraction greater than `threshold`, the smaller blob is eliminated.
    log_scale : bool, optional
        If set intermediate values of standard deviations are interpolated
        using a logarithmic scale to the base `10`. If not, linear
        interpolation is used.

    Returns
    -------
    A : (n, 3) ndarray
        A 2d array with each row representing 3 values, ``(y,x,sigma)``
        where ``(y,x)`` are coordinates of the blob and ``sigma`` is the
        standard deviation of the Gaussian kernel which detected the blob.

    References
    ----------
    .. [1] http://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian

    Examples
    --------
    >>> from skimage import data, feature, exposure
    >>> img = data.coins()
    >>> img = exposure.equalize_hist(img)  # improves detection
    >>> feature.blob_log(img, threshold = .3)
    array([[ 113.        ,  323.        ,    1.        ],
           [ 121.        ,  272.        ,   17.33333333],
           [ 124.        ,  336.        ,   11.88888889],
           [ 126.        ,   46.        ,   11.88888889],
           [ 126.        ,  208.        ,   11.88888889],
           [ 127.        ,  102.        ,   11.88888889],
           [ 128.        ,  154.        ,   11.88888889],
           [ 185.        ,  344.        ,   17.33333333],
           [ 194.        ,  213.        ,   17.33333333],
           [ 194.        ,  276.        ,   17.33333333],
           [ 197.        ,   44.        ,   11.88888889],
           [ 198.        ,  103.        ,   11.88888889],
           [ 198.        ,  155.        ,   11.88888889],
           [ 260.        ,  174.        ,   17.33333333],
           [ 263.        ,  244.        ,   17.33333333],
           [ 263.        ,  302.        ,   17.33333333],
           [ 266.        ,  115.        ,   11.88888889]])

    Notes
    -----
    The radius of each blob is approximately :math:`\sqrt{2}sigma`.
    """"""

    assert_nD(image, 2)

    image = img_as_float(image)

    if log_scale:
        start, stop = log(min_sigma, 10), log(max_sigma, 10)
        sigma_list = np.logspace(start, stop, num_sigma)
    else:
        sigma_list = np.linspace(min_sigma, max_sigma, num_sigma)

    # computing gaussian laplace
    # s**2 provides scale invariance
    gl_images = [-gaussian_laplace(image, s) * s ** 2 for s in sigma_list]
    image_cube = np.dstack(gl_images)

    local_maxima = peak_local_max(image_cube, threshold_abs=threshold,
                                  footprint=np.ones((3, 3, 3)),
                                  threshold_rel=0.0,
                                  exclude_border=False)

    # Convert local_maxima to float64
    lm = local_maxima.astype(np.float64)
    # Convert the last index to its corresponding scale value
    lm[:, 2] = sigma_list[local_maxima[:, 2]]
    local_maxima = lm
    return _prune_blobs(local_maxima, overlap)
","def blob_log(image, min_sigma=1, max_sigma=50, num_sigma=10, threshold=.2,
             overlap=.5, log_scale=False):
    """"""Finds blobs in the given grayscale image.

    Blobs are found using the Laplacian of Gaussian (LoG) method [1]_.
    For each blob found, the method returns its coordinates and the standard
    deviation of the Gaussian kernel that detected the blob.

    Parameters
    ----------
    image : ndarray
        Input grayscale image, blobs are assumed to be light on dark
        background (white on black).
    min_sigma : float, optional
        The minimum standard deviation for Gaussian Kernel. Keep this low to
        detect smaller blobs.
    max_sigma : float, optional
        The maximum standard deviation for Gaussian Kernel. Keep this high to
        detect larger blobs.
    num_sigma : int, optional
        The number of intermediate values of standard deviations to consider
        between `min_sigma` and `max_sigma`.
    threshold : float, optional.
        The absolute lower bound for scale space maxima. Local maxima smaller
        than thresh are ignored. Reduce this to detect blobs with less
        intensities.
    overlap : float, optional
        A value between 0 and 1. If the area of two blobs overlaps by a
        fraction greater than `threshold`, the smaller blob is eliminated.
    log_scale : bool, optional
        If set intermediate values of standard deviations are interpolated
        using a logarithmic scale to the base `10`. If not, linear
        interpolation is used.

    Returns
    -------
    A : (n, 3) ndarray
        A 2d array with each row representing 3 values, ``(y,x,sigma)``
        where ``(y,x)`` are coordinates of the blob and ``sigma`` is the
        standard deviation of the Gaussian kernel which detected the blob.

    References
    ----------
    .. [1] http://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian

    Examples
    --------
    >>> from skimage import data, feature, exposure
    >>> img = data.coins()
    >>> img = exposure.equalize_hist(img)  # improves detection
    >>> feature.blob_log(img, threshold = .3)
    array([[ 113.        ,  323.        ,    1.        ],
           [ 121.        ,  272.        ,   17.33333333],
           [ 124.        ,  336.        ,   11.88888889],
           [ 126.        ,   46.        ,   11.88888889],
           [ 126.        ,  208.        ,   11.88888889],
           [ 127.        ,  102.        ,   11.88888889],
           [ 128.        ,  154.        ,   11.88888889],
           [ 185.        ,  344.        ,   17.33333333],
           [ 194.        ,  213.        ,   17.33333333],
           [ 194.        ,  276.        ,   17.33333333],
           [ 197.        ,   44.        ,   11.88888889],
           [ 198.        ,  103.        ,   11.88888889],
           [ 198.        ,  155.        ,   11.88888889],
           [ 260.        ,  174.        ,   17.33333333],
           [ 263.        ,  244.        ,   17.33333333],
           [ 263.        ,  302.        ,   17.33333333],
           [ 266.        ,  115.        ,   11.88888889]])

    Notes
    -----
    The radius of each blob is approximately :math:`\sqrt{2}sigma`.
    """"""

    assert_nD(image, 2)

    image = img_as_float(image)

    if log_scale:
        start, stop = log(min_sigma, 10), log(max_sigma, 10)
        sigma_list = np.logspace(start, stop, num_sigma)
    else:
        sigma_list = np.linspace(min_sigma, max_sigma, num_sigma)

    # computing gaussian laplace
    # s**2 provides scale invariance
    gl_images = [-gaussian_laplace(image, s) * s ** 2 for s in sigma_list]
    image_cube = np.dstack(gl_images)

    local_maxima = peak_local_max(image_cube, threshold_abs=threshold,
                                  footprint=np.ones((3, 3, 3)),
                                  threshold_rel=0.0,
                                  exclude_border=False)

    # Catch no peaks
    if local_maxima.size == 0:
        return np.empty((0,3))
    # Convert local_maxima to float64
    lm = local_maxima.astype(np.float64)
    # Convert the last index to its corresponding scale value
    lm[:, 2] = sigma_list[local_maxima[:, 2]]
    local_maxima = lm
    return _prune_blobs(local_maxima, overlap)
",https://github.com/scikit-image/scikit-image/issues/2334,"CWE-126: Buffer Over-read, CWE-248: Uncaught Exception","the case is unprocessed where local_maxima is empty. In this case local_maxima has shape (0,2)",skimage/feature/blob.py,blob_log,"[90,93]","IndexError                                Traceback (most recent call last)
<ipython-input-15-0eabae5c0e37> in <module>()
----> 1 blob_log(a)

/Users/jj/anaconda3/lib/python3.5/site-packages/skimage/feature/blob.py in blob_log(image, min_sigma, max_sigma, num_sigma, threshold, overlap, log_scale)
306     lm = local_maxima.astype(np.float64)
307     # Convert the last index to its corresponding scale value
--> 308     lm[:, 2] = sigma_list[local_maxima[:, 2]]
309     local_maxima = lm
310     return _prune_blobs(local_maxima, overlap)

IndexError: index 2 is out of bounds for axis 1 with size 2",IndexError,buggy_snippets_files/53e7e5e427c4184a7e47a30246dd270e1b8c9b0368b0d1f6e113cdf1a56d9393_before_merge.py,buggy_snippets_files/53e7e5e427c4184a7e47a30246dd270e1b8c9b0368b0d1f6e113cdf1a56d9393_after_merge.py
1237,"def blob_doh(image, min_sigma=1, max_sigma=30, num_sigma=10, threshold=0.01,
             overlap=.5, log_scale=False):
    """"""Finds blobs in the given grayscale image.

    Blobs are found using the Determinant of Hessian method [1]_. For each blob
    found, the method returns its coordinates and the standard deviation
    of the Gaussian Kernel used for the Hessian matrix whose determinant
    detected the blob. Determinant of Hessians is approximated using [2]_.

    Parameters
    ----------
    image : ndarray
        Input grayscale image.Blobs can either be light on dark or vice versa.
    min_sigma : float, optional
        The minimum standard deviation for Gaussian Kernel used to compute
        Hessian matrix. Keep this low to detect smaller blobs.
    max_sigma : float, optional
        The maximum standard deviation for Gaussian Kernel used to compute
        Hessian matrix. Keep this high to detect larger blobs.
    num_sigma : int, optional
        The number of intermediate values of standard deviations to consider
        between `min_sigma` and `max_sigma`.
    threshold : float, optional.
        The absolute lower bound for scale space maxima. Local maxima smaller
        than thresh are ignored. Reduce this to detect less prominent blobs.
    overlap : float, optional
        A value between 0 and 1. If the area of two blobs overlaps by a
        fraction greater than `threshold`, the smaller blob is eliminated.
    log_scale : bool, optional
        If set intermediate values of standard deviations are interpolated
        using a logarithmic scale to the base `10`. If not, linear
        interpolation is used.

    Returns
    -------
    A : (n, 3) ndarray
        A 2d array with each row representing 3 values, ``(y,x,sigma)``
        where ``(y,x)`` are coordinates of the blob and ``sigma`` is the
        standard deviation of the Gaussian kernel of the Hessian Matrix whose
        determinant detected the blob.

    References
    ----------
    .. [1] http://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian

    .. [2] Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool,
           ""SURF: Speeded Up Robust Features""
           ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf

    Examples
    --------
    >>> from skimage import data, feature
    >>> img = data.coins()
    >>> feature.blob_doh(img)
    array([[ 121.        ,  271.        ,   30.        ],
           [ 123.        ,   44.        ,   23.55555556],
           [ 123.        ,  205.        ,   20.33333333],
           [ 124.        ,  336.        ,   20.33333333],
           [ 126.        ,  101.        ,   20.33333333],
           [ 126.        ,  153.        ,   20.33333333],
           [ 156.        ,  302.        ,   30.        ],
           [ 185.        ,  348.        ,   30.        ],
           [ 192.        ,  212.        ,   23.55555556],
           [ 193.        ,  275.        ,   23.55555556],
           [ 195.        ,  100.        ,   23.55555556],
           [ 197.        ,   44.        ,   20.33333333],
           [ 197.        ,  153.        ,   20.33333333],
           [ 260.        ,  173.        ,   30.        ],
           [ 262.        ,  243.        ,   23.55555556],
           [ 265.        ,  113.        ,   23.55555556],
           [ 270.        ,  363.        ,   30.        ]])

    Notes
    -----
    The radius of each blob is approximately `sigma`.
    Computation of Determinant of Hessians is independent of the standard
    deviation. Therefore detecting larger blobs won't take more time. In
    methods line :py:meth:`blob_dog` and :py:meth:`blob_log` the computation
    of Gaussians for larger `sigma` takes more time. The downside is that
    this method can't be used for detecting blobs of radius less than `3px`
    due to the box filters used in the approximation of Hessian Determinant.
    """"""

    assert_nD(image, 2)

    image = img_as_float(image)
    image = integral_image(image)

    if log_scale:
        start, stop = log(min_sigma, 10), log(max_sigma, 10)
        sigma_list = np.logspace(start, stop, num_sigma)
    else:
        sigma_list = np.linspace(min_sigma, max_sigma, num_sigma)

    hessian_images = [_hessian_matrix_det(image, s) for s in sigma_list]
    image_cube = np.dstack(hessian_images)

    local_maxima = peak_local_max(image_cube, threshold_abs=threshold,
                                  footprint=np.ones((3, 3, 3)),
                                  threshold_rel=0.0,
                                  exclude_border=False)

    # Convert local_maxima to float64
    lm = local_maxima.astype(np.float64)
    # Convert the last index to its corresponding scale value
    lm[:, 2] = sigma_list[local_maxima[:, 2]]
    local_maxima = lm
    return _prune_blobs(local_maxima, overlap)
","def blob_doh(image, min_sigma=1, max_sigma=30, num_sigma=10, threshold=0.01,
             overlap=.5, log_scale=False):
    """"""Finds blobs in the given grayscale image.

    Blobs are found using the Determinant of Hessian method [1]_. For each blob
    found, the method returns its coordinates and the standard deviation
    of the Gaussian Kernel used for the Hessian matrix whose determinant
    detected the blob. Determinant of Hessians is approximated using [2]_.

    Parameters
    ----------
    image : ndarray
        Input grayscale image.Blobs can either be light on dark or vice versa.
    min_sigma : float, optional
        The minimum standard deviation for Gaussian Kernel used to compute
        Hessian matrix. Keep this low to detect smaller blobs.
    max_sigma : float, optional
        The maximum standard deviation for Gaussian Kernel used to compute
        Hessian matrix. Keep this high to detect larger blobs.
    num_sigma : int, optional
        The number of intermediate values of standard deviations to consider
        between `min_sigma` and `max_sigma`.
    threshold : float, optional.
        The absolute lower bound for scale space maxima. Local maxima smaller
        than thresh are ignored. Reduce this to detect less prominent blobs.
    overlap : float, optional
        A value between 0 and 1. If the area of two blobs overlaps by a
        fraction greater than `threshold`, the smaller blob is eliminated.
    log_scale : bool, optional
        If set intermediate values of standard deviations are interpolated
        using a logarithmic scale to the base `10`. If not, linear
        interpolation is used.

    Returns
    -------
    A : (n, 3) ndarray
        A 2d array with each row representing 3 values, ``(y,x,sigma)``
        where ``(y,x)`` are coordinates of the blob and ``sigma`` is the
        standard deviation of the Gaussian kernel of the Hessian Matrix whose
        determinant detected the blob.

    References
    ----------
    .. [1] http://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian

    .. [2] Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool,
           ""SURF: Speeded Up Robust Features""
           ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf

    Examples
    --------
    >>> from skimage import data, feature
    >>> img = data.coins()
    >>> feature.blob_doh(img)
    array([[ 121.        ,  271.        ,   30.        ],
           [ 123.        ,   44.        ,   23.55555556],
           [ 123.        ,  205.        ,   20.33333333],
           [ 124.        ,  336.        ,   20.33333333],
           [ 126.        ,  101.        ,   20.33333333],
           [ 126.        ,  153.        ,   20.33333333],
           [ 156.        ,  302.        ,   30.        ],
           [ 185.        ,  348.        ,   30.        ],
           [ 192.        ,  212.        ,   23.55555556],
           [ 193.        ,  275.        ,   23.55555556],
           [ 195.        ,  100.        ,   23.55555556],
           [ 197.        ,   44.        ,   20.33333333],
           [ 197.        ,  153.        ,   20.33333333],
           [ 260.        ,  173.        ,   30.        ],
           [ 262.        ,  243.        ,   23.55555556],
           [ 265.        ,  113.        ,   23.55555556],
           [ 270.        ,  363.        ,   30.        ]])

    Notes
    -----
    The radius of each blob is approximately `sigma`.
    Computation of Determinant of Hessians is independent of the standard
    deviation. Therefore detecting larger blobs won't take more time. In
    methods line :py:meth:`blob_dog` and :py:meth:`blob_log` the computation
    of Gaussians for larger `sigma` takes more time. The downside is that
    this method can't be used for detecting blobs of radius less than `3px`
    due to the box filters used in the approximation of Hessian Determinant.
    """"""

    assert_nD(image, 2)

    image = img_as_float(image)
    image = integral_image(image)

    if log_scale:
        start, stop = log(min_sigma, 10), log(max_sigma, 10)
        sigma_list = np.logspace(start, stop, num_sigma)
    else:
        sigma_list = np.linspace(min_sigma, max_sigma, num_sigma)

    hessian_images = [_hessian_matrix_det(image, s) for s in sigma_list]
    image_cube = np.dstack(hessian_images)

    local_maxima = peak_local_max(image_cube, threshold_abs=threshold,
                                  footprint=np.ones((3, 3, 3)),
                                  threshold_rel=0.0,
                                  exclude_border=False)

    # Catch no peaks
    if local_maxima.size == 0:
        return np.empty((0,3))
    # Convert local_maxima to float64
    lm = local_maxima.astype(np.float64)
    # Convert the last index to its corresponding scale value
    lm[:, 2] = sigma_list[local_maxima[:, 2]]
    local_maxima = lm
    return _prune_blobs(local_maxima, overlap)
",https://github.com/scikit-image/scikit-image/issues/2334,"CWE-126: Buffer Over-read, CWE-248: Uncaught Exception","the case is unprocessed where local_maxima is empty. In this case local_maxima has shape (0,2)",skimage/feature/blob.py,blob_doh,"[98, 101]","IndexError                                Traceback (most recent call last)
<ipython-input-15-0eabae5c0e37> in <module>()
----> 1 blob_log(a)

/Users/jj/anaconda3/lib/python3.5/site-packages/skimage/feature/blob.py in blob_log(image, min_sigma, max_sigma, num_sigma, threshold, overlap, log_scale)
306     lm = local_maxima.astype(np.float64)
307     # Convert the last index to its corresponding scale value
--> 308     lm[:, 2] = sigma_list[local_maxima[:, 2]]
309     local_maxima = lm
310     return _prune_blobs(local_maxima, overlap)

IndexError: index 2 is out of bounds for axis 1 with size 2",IndexError,buggy_snippets_files/810c198a3bd5bdcf85868432b5c10c875781f8e409a994f6dea65d0707ba6f2c_before_merge.py,buggy_snippets_files/810c198a3bd5bdcf85868432b5c10c875781f8e409a994f6dea65d0707ba6f2c_after_merge.py
1236,"def blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=2.0,
             overlap=.5,):
    """"""Finds blobs in the given grayscale image.

    Blobs are found using the Difference of Gaussian (DoG) method [1]_.
    For each blob found, the method returns its coordinates and the standard
    deviation of the Gaussian kernel that detected the blob.

    Parameters
    ----------
    image : ndarray
        Input grayscale image, blobs are assumed to be light on dark
        background (white on black).
    min_sigma : float, optional
        The minimum standard deviation for Gaussian Kernel. Keep this low to
        detect smaller blobs.
    max_sigma : float, optional
        The maximum standard deviation for Gaussian Kernel. Keep this high to
        detect larger blobs.
    sigma_ratio : float, optional
        The ratio between the standard deviation of Gaussian Kernels used for
        computing the Difference of Gaussians
    threshold : float, optional.
        The absolute lower bound for scale space maxima. Local maxima smaller
        than thresh are ignored. Reduce this to detect blobs with less
        intensities.
    overlap : float, optional
        A value between 0 and 1. If the area of two blobs overlaps by a
        fraction greater than `threshold`, the smaller blob is eliminated.

    Returns
    -------
    A : (n, 3) ndarray
        A 2d array with each row representing 3 values, ``(y,x,sigma)``
        where ``(y,x)`` are coordinates of the blob and ``sigma`` is the
        standard deviation of the Gaussian kernel which detected the blob.

    References
    ----------
    .. [1] http://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach

    Examples
    --------
    >>> from skimage import data, feature
    >>> feature.blob_dog(data.coins(), threshold=.5, max_sigma=40)
    array([[  45.      ,  336.      ,   16.777216],
           [  52.      ,  155.      ,   16.777216],
           [  52.      ,  216.      ,   16.777216],
           [  54.      ,   42.      ,   16.777216],
           [  54.      ,  276.      ,   10.48576 ],
           [  58.      ,  100.      ,   10.48576 ],
           [ 120.      ,  272.      ,   16.777216],
           [ 124.      ,  337.      ,   10.48576 ],
           [ 125.      ,   45.      ,   16.777216],
           [ 125.      ,  208.      ,   10.48576 ],
           [ 127.      ,  102.      ,   10.48576 ],
           [ 128.      ,  154.      ,   10.48576 ],
           [ 185.      ,  347.      ,   16.777216],
           [ 193.      ,  213.      ,   16.777216],
           [ 194.      ,  277.      ,   16.777216],
           [ 195.      ,  102.      ,   16.777216],
           [ 196.      ,   43.      ,   10.48576 ],
           [ 198.      ,  155.      ,   10.48576 ],
           [ 260.      ,   46.      ,   16.777216],
           [ 261.      ,  173.      ,   16.777216],
           [ 263.      ,  245.      ,   16.777216],
           [ 263.      ,  302.      ,   16.777216],
           [ 267.      ,  115.      ,   10.48576 ],
           [ 267.      ,  359.      ,   16.777216]])

    Notes
    -----
    The radius of each blob is approximately :math:`\sqrt{2}sigma`.
    """"""
    assert_nD(image, 2)

    image = img_as_float(image)

    # k such that min_sigma*(sigma_ratio**k) > max_sigma
    k = int(log(float(max_sigma) / min_sigma, sigma_ratio)) + 1

    # a geometric progression of standard deviations for gaussian kernels
    sigma_list = np.array([min_sigma * (sigma_ratio ** i)
                           for i in range(k + 1)])

    gaussian_images = [gaussian_filter(image, s) for s in sigma_list]

    # computing difference between two successive Gaussian blurred images
    # multiplying with standard deviation provides scale invariance
    dog_images = [(gaussian_images[i] - gaussian_images[i + 1])
                  * sigma_list[i] for i in range(k)]
    image_cube = np.dstack(dog_images)

    # local_maxima = get_local_maxima(image_cube, threshold)
    local_maxima = peak_local_max(image_cube, threshold_abs=threshold,
                                  footprint=np.ones((3, 3, 3)),
                                  threshold_rel=0.0,
                                  exclude_border=False)
    # Convert local_maxima to float64
    lm = local_maxima.astype(np.float64)
    # Convert the last index to its corresponding scale value
    lm[:, 2] = sigma_list[local_maxima[:, 2]]
    local_maxima = lm
    return _prune_blobs(local_maxima, overlap)
","def blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=2.0,
             overlap=.5,):
    """"""Finds blobs in the given grayscale image.

    Blobs are found using the Difference of Gaussian (DoG) method [1]_.
    For each blob found, the method returns its coordinates and the standard
    deviation of the Gaussian kernel that detected the blob.

    Parameters
    ----------
    image : ndarray
        Input grayscale image, blobs are assumed to be light on dark
        background (white on black).
    min_sigma : float, optional
        The minimum standard deviation for Gaussian Kernel. Keep this low to
        detect smaller blobs.
    max_sigma : float, optional
        The maximum standard deviation for Gaussian Kernel. Keep this high to
        detect larger blobs.
    sigma_ratio : float, optional
        The ratio between the standard deviation of Gaussian Kernels used for
        computing the Difference of Gaussians
    threshold : float, optional.
        The absolute lower bound for scale space maxima. Local maxima smaller
        than thresh are ignored. Reduce this to detect blobs with less
        intensities.
    overlap : float, optional
        A value between 0 and 1. If the area of two blobs overlaps by a
        fraction greater than `threshold`, the smaller blob is eliminated.

    Returns
    -------
    A : (n, 3) ndarray
        A 2d array with each row representing 3 values, ``(y,x,sigma)``
        where ``(y,x)`` are coordinates of the blob and ``sigma`` is the
        standard deviation of the Gaussian kernel which detected the blob.

    References
    ----------
    .. [1] http://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach

    Examples
    --------
    >>> from skimage import data, feature
    >>> feature.blob_dog(data.coins(), threshold=.5, max_sigma=40)
    array([[  45.      ,  336.      ,   16.777216],
           [  52.      ,  155.      ,   16.777216],
           [  52.      ,  216.      ,   16.777216],
           [  54.      ,   42.      ,   16.777216],
           [  54.      ,  276.      ,   10.48576 ],
           [  58.      ,  100.      ,   10.48576 ],
           [ 120.      ,  272.      ,   16.777216],
           [ 124.      ,  337.      ,   10.48576 ],
           [ 125.      ,   45.      ,   16.777216],
           [ 125.      ,  208.      ,   10.48576 ],
           [ 127.      ,  102.      ,   10.48576 ],
           [ 128.      ,  154.      ,   10.48576 ],
           [ 185.      ,  347.      ,   16.777216],
           [ 193.      ,  213.      ,   16.777216],
           [ 194.      ,  277.      ,   16.777216],
           [ 195.      ,  102.      ,   16.777216],
           [ 196.      ,   43.      ,   10.48576 ],
           [ 198.      ,  155.      ,   10.48576 ],
           [ 260.      ,   46.      ,   16.777216],
           [ 261.      ,  173.      ,   16.777216],
           [ 263.      ,  245.      ,   16.777216],
           [ 263.      ,  302.      ,   16.777216],
           [ 267.      ,  115.      ,   10.48576 ],
           [ 267.      ,  359.      ,   16.777216]])

    Notes
    -----
    The radius of each blob is approximately :math:`\sqrt{2}sigma`.
    """"""
    assert_nD(image, 2)

    image = img_as_float(image)

    # k such that min_sigma*(sigma_ratio**k) > max_sigma
    k = int(log(float(max_sigma) / min_sigma, sigma_ratio)) + 1

    # a geometric progression of standard deviations for gaussian kernels
    sigma_list = np.array([min_sigma * (sigma_ratio ** i)
                           for i in range(k + 1)])

    gaussian_images = [gaussian_filter(image, s) for s in sigma_list]

    # computing difference between two successive Gaussian blurred images
    # multiplying with standard deviation provides scale invariance
    dog_images = [(gaussian_images[i] - gaussian_images[i + 1])
                  * sigma_list[i] for i in range(k)]
    image_cube = np.dstack(dog_images)

    # local_maxima = get_local_maxima(image_cube, threshold)
    local_maxima = peak_local_max(image_cube, threshold_abs=threshold,
                                  footprint=np.ones((3, 3, 3)),
                                  threshold_rel=0.0,
                                  exclude_border=False)
    # Catch no peaks
    if local_maxima.size == 0:
        return np.empty((0,3))
    # Convert local_maxima to float64
    lm = local_maxima.astype(np.float64)
    # Convert the last index to its corresponding scale value
    lm[:, 2] = sigma_list[local_maxima[:, 2]]
    local_maxima = lm
    return _prune_blobs(local_maxima, overlap)
",https://github.com/scikit-image/scikit-image/issues/2334,"CWE-126: Buffer Over-read, CWE-248: Uncaught Exception","the case is unprocessed where local_maxima is empty. In this case local_maxima has shape (0,2)",skimage/feature/blob.py,blob_dog,"[95,98]","IndexError                                Traceback (most recent call last)
<ipython-input-15-0eabae5c0e37> in <module>()
----> 1 blob_log(a)

/Users/jj/anaconda3/lib/python3.5/site-packages/skimage/feature/blob.py in blob_log(image, min_sigma, max_sigma, num_sigma, threshold, overlap, log_scale)
306     lm = local_maxima.astype(np.float64)
307     # Convert the last index to its corresponding scale value
--> 308     lm[:, 2] = sigma_list[local_maxima[:, 2]]
309     local_maxima = lm
310     return _prune_blobs(local_maxima, overlap)

IndexError: index 2 is out of bounds for axis 1 with size 2",IndexError,buggy_snippets_files/1a4751beb7a38c2eeb7dabf5088e469a9d8d1b3905510fdc211a5ff215949d0b_before_merge.py,buggy_snippets_files/1a4751beb7a38c2eeb7dabf5088e469a9d8d1b3905510fdc211a5ff215949d0b_after_merge.py
1246,"def random_walker(data, labels, beta=130, mode='bf', tol=1.e-3, copy=True,
                  multichannel=False, return_full_prob=False, spacing=None):
    """"""Random walker algorithm for segmentation from markers.

    Random walker algorithm is implemented for gray-level or multichannel
    images.

    Parameters
    ----------
    data : array_like
        Image to be segmented in phases. Gray-level `data` can be two- or
        three-dimensional; multichannel data can be three- or four-
        dimensional (multichannel=True) with the highest dimension denoting
        channels. Data spacing is assumed isotropic unless the `spacing`
        keyword argument is used.
    labels : array of ints, of same shape as `data` without channels dimension
        Array of seed markers labeled with different positive integers
        for different phases. Zero-labeled pixels are unlabeled pixels.
        Negative labels correspond to inactive pixels that are not taken
        into account (they are removed from the graph). If labels are not
        consecutive integers, the labels array will be transformed so that
        labels are consecutive. In the multichannel case, `labels` should have
        the same shape as a single channel of `data`, i.e. without the final
        dimension denoting channels.
    beta : float
        Penalization coefficient for the random walker motion
        (the greater `beta`, the more difficult the diffusion).
    mode : string, available options {'cg_mg', 'cg', 'bf'}
        Mode for solving the linear system in the random walker algorithm.
        If no preference given, automatically attempt to use the fastest
        option available ('cg_mg' from pyamg >> 'cg' with UMFPACK > 'bf').

        - 'bf' (brute force): an LU factorization of the Laplacian is
          computed. This is fast for small images (<1024x1024), but very slow
          and memory-intensive for large images (e.g., 3-D volumes).
        - 'cg' (conjugate gradient): the linear system is solved iteratively
          using the Conjugate Gradient method from scipy.sparse.linalg. This is
          less memory-consuming than the brute force method for large images,
          but it is quite slow.
        - 'cg_mg' (conjugate gradient with multigrid preconditioner): a
          preconditioner is computed using a multigrid solver, then the
          solution is computed with the Conjugate Gradient method.  This mode
          requires that the pyamg module (http://pyamg.org/) is
          installed. For images of size > 512x512, this is the recommended
          (fastest) mode.

    tol : float
        tolerance to achieve when solving the linear system, in
        cg' and 'cg_mg' modes.
    copy : bool
        If copy is False, the `labels` array will be overwritten with
        the result of the segmentation. Use copy=False if you want to
        save on memory.
    multichannel : bool, default False
        If True, input data is parsed as multichannel data (see 'data' above
        for proper input format in this case)
    return_full_prob : bool, default False
        If True, the probability that a pixel belongs to each of the labels
        will be returned, instead of only the most likely label.
    spacing : iterable of floats
        Spacing between voxels in each spatial dimension. If `None`, then
        the spacing between pixels/voxels in each dimension is assumed 1.

    Returns
    -------
    output : ndarray
        * If `return_full_prob` is False, array of ints of same shape as
          `data`, in which each pixel has been labeled according to the marker
          that reached the pixel first by anisotropic diffusion.
        * If `return_full_prob` is True, array of floats of shape
          `(nlabels, data.shape)`. `output[label_nb, i, j]` is the probability
          that label `label_nb` reaches the pixel `(i, j)` first.

    See also
    --------
    skimage.morphology.watershed: watershed segmentation
        A segmentation algorithm based on mathematical morphology
        and ""flooding"" of regions from markers.

    Notes
    -----
    Multichannel inputs are scaled with all channel data combined. Ensure all
    channels are separately normalized prior to running this algorithm.

    The `spacing` argument is specifically for anisotropic datasets, where
    data points are spaced differently in one or more spatial dimensions.
    Anisotropic data is commonly encountered in medical imaging.

    The algorithm was first proposed in *Random walks for image
    segmentation*, Leo Grady, IEEE Trans Pattern Anal Mach Intell.
    2006 Nov;28(11):1768-83.

    The algorithm solves the diffusion equation at infinite times for
    sources placed on markers of each phase in turn. A pixel is labeled with
    the phase that has the greatest probability to diffuse first to the pixel.

    The diffusion equation is solved by minimizing x.T L x for each phase,
    where L is the Laplacian of the weighted graph of the image, and x is
    the probability that a marker of the given phase arrives first at a pixel
    by diffusion (x=1 on markers of the phase, x=0 on the other markers, and
    the other coefficients are looked for). Each pixel is attributed the label
    for which it has a maximal value of x. The Laplacian L of the image
    is defined as:

       - L_ii = d_i, the number of neighbors of pixel i (the degree of i)
       - L_ij = -w_ij if i and j are adjacent pixels

    The weight w_ij is a decreasing function of the norm of the local gradient.
    This ensures that diffusion is easier between pixels of similar values.

    When the Laplacian is decomposed into blocks of marked and unmarked
    pixels::

        L = M B.T
            B A

    with first indices corresponding to marked pixels, and then to unmarked
    pixels, minimizing x.T L x for one phase amount to solving::

        A x = - B x_m

    where x_m = 1 on markers of the given phase, and 0 on other markers.
    This linear system is solved in the algorithm using a direct method for
    small images, and an iterative method for larger images.

    Examples
    --------
    >>> np.random.seed(0)
    >>> a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)
    >>> a[5:8, 5:8] += 1
    >>> b = np.zeros_like(a)
    >>> b[3, 3] = 1  # Marker for first phase
    >>> b[6, 6] = 2  # Marker for second phase
    >>> random_walker(a, b)
    array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)

    """"""
    # Parse input data
    if mode is None:
        if amg_loaded:
            mode = 'cg_mg'
        elif UmfpackContext is not None:
            mode = 'cg'
        else:
            mode = 'bf'

    if UmfpackContext is None and mode == 'cg':
        warn('""cg"" mode will be used, but it may be slower than '
             '""bf"" because SciPy was built without UMFPACK. Consider'
             ' rebuilding SciPy with UMFPACK; this will greatly '
             'accelerate the conjugate gradient (""cg"") solver. '
             'You may also install pyamg and run the random_walker '
             'function in ""cg_mg"" mode (see docstring).')

    if (labels != 0).all():
        warn('Random walker only segments unlabeled areas, where '
             'labels == 0. No zero valued areas in labels were '
             'found. Returning provided labels.')

        if return_full_prob:
            # Find and iterate over valid labels
            unique_labels = np.unique(labels)
            unique_labels = unique_labels[unique_labels > 0]

            out_labels = np.empty(labels.shape + (len(unique_labels),),
                                  dtype=np.bool)
            for n, i in enumerate(unique_labels):
                out_labels[..., n] = (labels == i)

        else:
            out_labels = labels
        return out_labels

    # This algorithm expects 4-D arrays of floats, where the first three
    # dimensions are spatial and the final denotes channels. 2-D images have
    # a singleton placeholder dimension added for the third spatial dimension,
    # and single channel images likewise have a singleton added for channels.
    # The following block ensures valid input and coerces it to the correct
    # form.
    if not multichannel:
        if data.ndim < 2 or data.ndim > 3:
            raise ValueError('For non-multichannel input, data must be of '
                             'dimension 2 or 3.')
        dims = data.shape  # To reshape final labeled result
        data = np.atleast_3d(img_as_float(data))[..., np.newaxis]
    else:
        if data.ndim < 3:
            raise ValueError('For multichannel input, data must have 3 or 4 '
                             'dimensions.')
        dims = data[..., 0].shape  # To reshape final labeled result
        data = img_as_float(data)
        if data.ndim == 3:  # 2D multispectral, needs singleton in 3rd axis
            data = data[:, :, np.newaxis, :]

    # Spacing kwarg checks
    if spacing is None:
        spacing = np.asarray((1.,) * 3)
    elif len(spacing) == len(dims):
        if len(spacing) == 2:  # Need a dummy spacing for singleton 3rd dim
            spacing = np.r_[spacing, 1.]
        else:                  # Convert to array
            spacing = np.asarray(spacing)
    else:
        raise ValueError('Input argument `spacing` incorrect, should be an '
                         'iterable with one number per spatial dimension.')

    if copy:
        labels = np.copy(labels)
    label_values = np.unique(labels)

    # Reorder label values to have consecutive integers (no gaps)
    if np.any(np.diff(label_values) != 1):
        mask = labels >= 0
        labels[mask] = rank_order(labels[mask])[0].astype(labels.dtype)
    labels = labels.astype(np.int32)

    # If the array has pruned zones, be sure that no isolated pixels
    # exist between pruned zones (they could not be determined)
    if np.any(labels < 0):
        filled = ndi.binary_propagation(labels > 0, mask=labels >= 0)
        labels[np.logical_and(np.logical_not(filled), labels == 0)] = -1
        del filled
    labels = np.atleast_3d(labels)
    if np.any(labels < 0):
        lap_sparse = _build_laplacian(data, spacing, mask=labels >= 0,
                                      beta=beta, multichannel=multichannel)
    else:
        lap_sparse = _build_laplacian(data, spacing, beta=beta,
                                      multichannel=multichannel)
    lap_sparse, B = _buildAB(lap_sparse, labels)

    # We solve the linear system
    # lap_sparse X = B
    # where X[i, j] is the probability that a marker of label i arrives
    # first at pixel j by anisotropic diffusion.
    if mode == 'cg':
        X = _solve_cg(lap_sparse, B, tol=tol,
                      return_full_prob=return_full_prob)
    if mode == 'cg_mg':
        if not amg_loaded:
            warn(""""""pyamg (http://pyamg.org/)) is needed to use
                this mode, but is not installed. The 'cg' mode will be used
                instead."""""")
            X = _solve_cg(lap_sparse, B, tol=tol,
                          return_full_prob=return_full_prob)
        else:
            X = _solve_cg_mg(lap_sparse, B, tol=tol,
                             return_full_prob=return_full_prob)
    if mode == 'bf':
        X = _solve_bf(lap_sparse, B,
                      return_full_prob=return_full_prob)

    # Clean up results
    if return_full_prob:
        labels = labels.astype(np.float)
        X = np.array([_clean_labels_ar(Xline, labels, copy=True).reshape(dims)
                      for Xline in X])
        for i in range(1, int(labels.max()) + 1):
            mask_i = np.squeeze(labels == i)
            X[:, mask_i] = 0
            X[i - 1, mask_i] = 1
    else:
        X = _clean_labels_ar(X + 1, labels).reshape(dims)
    return X
","def random_walker(data, labels, beta=130, mode='bf', tol=1.e-3, copy=True,
                  multichannel=False, return_full_prob=False, spacing=None):
    """"""Random walker algorithm for segmentation from markers.

    Random walker algorithm is implemented for gray-level or multichannel
    images.

    Parameters
    ----------
    data : array_like
        Image to be segmented in phases. Gray-level `data` can be two- or
        three-dimensional; multichannel data can be three- or four-
        dimensional (multichannel=True) with the highest dimension denoting
        channels. Data spacing is assumed isotropic unless the `spacing`
        keyword argument is used.
    labels : array of ints, of same shape as `data` without channels dimension
        Array of seed markers labeled with different positive integers
        for different phases. Zero-labeled pixels are unlabeled pixels.
        Negative labels correspond to inactive pixels that are not taken
        into account (they are removed from the graph). If labels are not
        consecutive integers, the labels array will be transformed so that
        labels are consecutive. In the multichannel case, `labels` should have
        the same shape as a single channel of `data`, i.e. without the final
        dimension denoting channels.
    beta : float
        Penalization coefficient for the random walker motion
        (the greater `beta`, the more difficult the diffusion).
    mode : string, available options {'cg_mg', 'cg', 'bf'}
        Mode for solving the linear system in the random walker algorithm.
        If no preference given, automatically attempt to use the fastest
        option available ('cg_mg' from pyamg >> 'cg' with UMFPACK > 'bf').

        - 'bf' (brute force): an LU factorization of the Laplacian is
          computed. This is fast for small images (<1024x1024), but very slow
          and memory-intensive for large images (e.g., 3-D volumes).
        - 'cg' (conjugate gradient): the linear system is solved iteratively
          using the Conjugate Gradient method from scipy.sparse.linalg. This is
          less memory-consuming than the brute force method for large images,
          but it is quite slow.
        - 'cg_mg' (conjugate gradient with multigrid preconditioner): a
          preconditioner is computed using a multigrid solver, then the
          solution is computed with the Conjugate Gradient method.  This mode
          requires that the pyamg module (http://pyamg.org/) is
          installed. For images of size > 512x512, this is the recommended
          (fastest) mode.

    tol : float
        tolerance to achieve when solving the linear system, in
        cg' and 'cg_mg' modes.
    copy : bool
        If copy is False, the `labels` array will be overwritten with
        the result of the segmentation. Use copy=False if you want to
        save on memory.
    multichannel : bool, default False
        If True, input data is parsed as multichannel data (see 'data' above
        for proper input format in this case)
    return_full_prob : bool, default False
        If True, the probability that a pixel belongs to each of the labels
        will be returned, instead of only the most likely label.
    spacing : iterable of floats
        Spacing between voxels in each spatial dimension. If `None`, then
        the spacing between pixels/voxels in each dimension is assumed 1.

    Returns
    -------
    output : ndarray
        * If `return_full_prob` is False, array of ints of same shape as
          `data`, in which each pixel has been labeled according to the marker
          that reached the pixel first by anisotropic diffusion.
        * If `return_full_prob` is True, array of floats of shape
          `(nlabels, data.shape)`. `output[label_nb, i, j]` is the probability
          that label `label_nb` reaches the pixel `(i, j)` first.

    See also
    --------
    skimage.morphology.watershed: watershed segmentation
        A segmentation algorithm based on mathematical morphology
        and ""flooding"" of regions from markers.

    Notes
    -----
    Multichannel inputs are scaled with all channel data combined. Ensure all
    channels are separately normalized prior to running this algorithm.

    The `spacing` argument is specifically for anisotropic datasets, where
    data points are spaced differently in one or more spatial dimensions.
    Anisotropic data is commonly encountered in medical imaging.

    The algorithm was first proposed in *Random walks for image
    segmentation*, Leo Grady, IEEE Trans Pattern Anal Mach Intell.
    2006 Nov;28(11):1768-83.

    The algorithm solves the diffusion equation at infinite times for
    sources placed on markers of each phase in turn. A pixel is labeled with
    the phase that has the greatest probability to diffuse first to the pixel.

    The diffusion equation is solved by minimizing x.T L x for each phase,
    where L is the Laplacian of the weighted graph of the image, and x is
    the probability that a marker of the given phase arrives first at a pixel
    by diffusion (x=1 on markers of the phase, x=0 on the other markers, and
    the other coefficients are looked for). Each pixel is attributed the label
    for which it has a maximal value of x. The Laplacian L of the image
    is defined as:

       - L_ii = d_i, the number of neighbors of pixel i (the degree of i)
       - L_ij = -w_ij if i and j are adjacent pixels

    The weight w_ij is a decreasing function of the norm of the local gradient.
    This ensures that diffusion is easier between pixels of similar values.

    When the Laplacian is decomposed into blocks of marked and unmarked
    pixels::

        L = M B.T
            B A

    with first indices corresponding to marked pixels, and then to unmarked
    pixels, minimizing x.T L x for one phase amount to solving::

        A x = - B x_m

    where x_m = 1 on markers of the given phase, and 0 on other markers.
    This linear system is solved in the algorithm using a direct method for
    small images, and an iterative method for larger images.

    Examples
    --------
    >>> np.random.seed(0)
    >>> a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)
    >>> a[5:8, 5:8] += 1
    >>> b = np.zeros_like(a)
    >>> b[3, 3] = 1  # Marker for first phase
    >>> b[6, 6] = 2  # Marker for second phase
    >>> random_walker(a, b)
    array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)

    """"""
    # Parse input data
    if mode is None:
        if amg_loaded:
            mode = 'cg_mg'
        elif UmfpackContext is not None:
            mode = 'cg'
        else:
            mode = 'bf'
    elif mode not in ('cg_mg', 'cg', 'bf'):
        raise ValueError(""{mode} is not a valid mode. Valid modes are 'cg_mg',""
                         "" 'cg' and 'bf'"".format(mode=mode))

    if UmfpackContext is None and mode == 'cg':
        warn('""cg"" mode will be used, but it may be slower than '
             '""bf"" because SciPy was built without UMFPACK. Consider'
             ' rebuilding SciPy with UMFPACK; this will greatly '
             'accelerate the conjugate gradient (""cg"") solver. '
             'You may also install pyamg and run the random_walker '
             'function in ""cg_mg"" mode (see docstring).')

    if (labels != 0).all():
        warn('Random walker only segments unlabeled areas, where '
             'labels == 0. No zero valued areas in labels were '
             'found. Returning provided labels.')

        if return_full_prob:
            # Find and iterate over valid labels
            unique_labels = np.unique(labels)
            unique_labels = unique_labels[unique_labels > 0]

            out_labels = np.empty(labels.shape + (len(unique_labels),),
                                  dtype=np.bool)
            for n, i in enumerate(unique_labels):
                out_labels[..., n] = (labels == i)

        else:
            out_labels = labels
        return out_labels

    # This algorithm expects 4-D arrays of floats, where the first three
    # dimensions are spatial and the final denotes channels. 2-D images have
    # a singleton placeholder dimension added for the third spatial dimension,
    # and single channel images likewise have a singleton added for channels.
    # The following block ensures valid input and coerces it to the correct
    # form.
    if not multichannel:
        if data.ndim < 2 or data.ndim > 3:
            raise ValueError('For non-multichannel input, data must be of '
                             'dimension 2 or 3.')
        dims = data.shape  # To reshape final labeled result
        data = np.atleast_3d(img_as_float(data))[..., np.newaxis]
    else:
        if data.ndim < 3:
            raise ValueError('For multichannel input, data must have 3 or 4 '
                             'dimensions.')
        dims = data[..., 0].shape  # To reshape final labeled result
        data = img_as_float(data)
        if data.ndim == 3:  # 2D multispectral, needs singleton in 3rd axis
            data = data[:, :, np.newaxis, :]

    # Spacing kwarg checks
    if spacing is None:
        spacing = np.asarray((1.,) * 3)
    elif len(spacing) == len(dims):
        if len(spacing) == 2:  # Need a dummy spacing for singleton 3rd dim
            spacing = np.r_[spacing, 1.]
        else:                  # Convert to array
            spacing = np.asarray(spacing)
    else:
        raise ValueError('Input argument `spacing` incorrect, should be an '
                         'iterable with one number per spatial dimension.')

    if copy:
        labels = np.copy(labels)
    label_values = np.unique(labels)

    # Reorder label values to have consecutive integers (no gaps)
    if np.any(np.diff(label_values) != 1):
        mask = labels >= 0
        labels[mask] = rank_order(labels[mask])[0].astype(labels.dtype)
    labels = labels.astype(np.int32)

    # If the array has pruned zones, be sure that no isolated pixels
    # exist between pruned zones (they could not be determined)
    if np.any(labels < 0):
        filled = ndi.binary_propagation(labels > 0, mask=labels >= 0)
        labels[np.logical_and(np.logical_not(filled), labels == 0)] = -1
        del filled
    labels = np.atleast_3d(labels)
    if np.any(labels < 0):
        lap_sparse = _build_laplacian(data, spacing, mask=labels >= 0,
                                      beta=beta, multichannel=multichannel)
    else:
        lap_sparse = _build_laplacian(data, spacing, beta=beta,
                                      multichannel=multichannel)
    lap_sparse, B = _buildAB(lap_sparse, labels)

    # We solve the linear system
    # lap_sparse X = B
    # where X[i, j] is the probability that a marker of label i arrives
    # first at pixel j by anisotropic diffusion.
    if mode == 'cg':
        X = _solve_cg(lap_sparse, B, tol=tol,
                      return_full_prob=return_full_prob)
    if mode == 'cg_mg':
        if not amg_loaded:
            warn(""""""pyamg (http://pyamg.org/)) is needed to use
                this mode, but is not installed. The 'cg' mode will be used
                instead."""""")
            X = _solve_cg(lap_sparse, B, tol=tol,
                          return_full_prob=return_full_prob)
        else:
            X = _solve_cg_mg(lap_sparse, B, tol=tol,
                             return_full_prob=return_full_prob)
    if mode == 'bf':
        X = _solve_bf(lap_sparse, B,
                      return_full_prob=return_full_prob)

    # Clean up results
    if return_full_prob:
        labels = labels.astype(np.float)
        X = np.array([_clean_labels_ar(Xline, labels, copy=True).reshape(dims)
                      for Xline in X])
        for i in range(1, int(labels.max()) + 1):
            mask_i = np.squeeze(labels == i)
            X[:, mask_i] = 0
            X[i - 1, mask_i] = 1
    else:
        X = _clean_labels_ar(X + 1, labels).reshape(dims)
    return X
",https://github.com/scikit-image/scikit-image/issues/2361,CWE-703: Improper Check or Handling of Exceptional Conditions,"if mode variable does not take one of its prescribed values, the program work is unpredictable",skimage/segmentation/random_walker_segmentation.py,random_walker,"[148, 154]","UnboundLocalError                         Traceback (most recent call last)
/Volumes/work/Research/Emmanuelle/rw_inaccuracies.py in <module>()
16 markers3d[100,:,:]=markers3d[:,100,:] = markers3d[:,:,100] = 2 # border of the box cont.
17
---> 18 grow3d = random_walker(blob3ds, markers3d, beta=5e4, mode='toto', tol=1e-5, return_full_prob=True)
19
20 print ""Should be between 0 and 1: [%f ; %f]"" % (np.min(grow3d[1]), np.max(grow3d[1]))

/opt/anaconda/lib/python2.7/site-packages/skimage/segmentation/random_walker_segmentation.pyc in random_walker(data, labels, beta, mode, tol, copy, multichannel, return_full_prob, spacing)
457         labels = labels.astype(np.float)
458         X = np.array([_clean_labels_ar(Xline, labels, copy=True).reshape(dims)
--> 459                       for Xline in X])
460         for i in range(1, int(labels.max()) + 1):
461             mask_i = np.squeeze(labels == i)

UnboundLocalError: local variable 'X' referenced before assignment",UnboundLocalError,buggy_snippets_files/d9ab819a2ef2615dc6cad690b3973b78490f1aa35bdf24821efff68780518042_before_merge.py,buggy_snippets_files/d9ab819a2ef2615dc6cad690b3973b78490f1aa35bdf24821efff68780518042_after_merge.py
1283,"    def _load(self, jwt):
        if isinstance(jwt, text_type):
            jwt = jwt.encode('utf-8')

        try:
            signing_input, crypto_segment = jwt.rsplit(b'.', 1)
            header_segment, payload_segment = signing_input.split(b'.', 1)
        except ValueError:
            raise DecodeError('Not enough segments')

        try:
            header_data = base64url_decode(header_segment)
        except (TypeError, binascii.Error):
            raise DecodeError('Invalid header padding')

        try:
            header = json.loads(header_data.decode('utf-8'))
        except ValueError as e:
            raise DecodeError('Invalid header string: %s' % e)

        if not isinstance(header, Mapping):
            raise DecodeError('Invalid header string: must be a json object')

        try:
            payload = base64url_decode(payload_segment)
        except (TypeError, binascii.Error):
            raise DecodeError('Invalid payload padding')

        try:
            signature = base64url_decode(crypto_segment)
        except (TypeError, binascii.Error):
            raise DecodeError('Invalid crypto padding')

        return (payload, signing_input, header, signature)
","    def _load(self, jwt):
        if isinstance(jwt, text_type):
            jwt = jwt.encode('utf-8')

        if not issubclass(type(jwt), binary_type):
            raise DecodeError(""Invalid token type. Token must be a {0}"".format(
                binary_type))

        try:
            signing_input, crypto_segment = jwt.rsplit(b'.', 1)
            header_segment, payload_segment = signing_input.split(b'.', 1)
        except ValueError:
            raise DecodeError('Not enough segments')

        try:
            header_data = base64url_decode(header_segment)
        except (TypeError, binascii.Error):
            raise DecodeError('Invalid header padding')

        try:
            header = json.loads(header_data.decode('utf-8'))
        except ValueError as e:
            raise DecodeError('Invalid header string: %s' % e)

        if not isinstance(header, Mapping):
            raise DecodeError('Invalid header string: must be a json object')

        try:
            payload = base64url_decode(payload_segment)
        except (TypeError, binascii.Error):
            raise DecodeError('Invalid payload padding')

        try:
            signature = base64url_decode(crypto_segment)
        except (TypeError, binascii.Error):
            raise DecodeError('Invalid crypto padding')

        return (payload, signing_input, header, signature)
",https://github.com/jpadilla/pyjwt/issues/183,"CWE-824: Access of Uninitialized Pointer, CWE-248: Uncaught Exception",the case is unprocessed where jwt object is None,jwt/api_jws.py,PyJWS._load,[3],"import jwt
jwt.decode(None)
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""C:\Python27\lib\site-packages\jwt\api_jwt.py"", line 61, in decode
payload, signing_input, header, signature = self._load(jwt)
File ""C:\Python27\lib\site-packages\jwt\api_jws.py"", line 135, in _load
signing_input, crypto_segment = jwt.rsplit(b'.', 1)
AttributeError: 'NoneType' object has no attribute 'rsplit'
",AttributeError,buggy_snippets_files/de312e3e4979b592b0da5a63d5e22d1f2e3631623f320f490458d5125aa3e5d3_before_merge.py,buggy_snippets_files/de312e3e4979b592b0da5a63d5e22d1f2e3631623f320f490458d5125aa3e5d3_after_merge.py
1285,"    def _verify_signature(self, payload, signing_input, header, signature,
                          key='', algorithms=None):

        alg = header['alg']

        if algorithms is not None and alg not in algorithms:
            raise InvalidAlgorithmError('The specified alg value is not allowed')

        try:
            alg_obj = self._algorithms[alg]
            key = alg_obj.prepare_key(key)

            if not alg_obj.verify(signing_input, key, signature):
                raise DecodeError('Signature verification failed')

        except KeyError:
            raise InvalidAlgorithmError('Algorithm not supported')
","    def _verify_signature(self, payload, signing_input, header, signature,
                          key='', algorithms=None):

        alg = header.get('alg')

        if algorithms is not None and alg not in algorithms:
            raise InvalidAlgorithmError('The specified alg value is not allowed')

        try:
            alg_obj = self._algorithms[alg]
            key = alg_obj.prepare_key(key)

            if not alg_obj.verify(signing_input, key, signature):
                raise DecodeError('Signature verification failed')

        except KeyError:
            raise InvalidAlgorithmError('Algorithm not supported')
",https://github.com/jpadilla/pyjwt/issues/167,CWE-248: Uncaught Exception,the case is unhandled where 'alg' key is not present in headers,jwt/api_jws.py,PyJWS._verify_signature,[4],"j
'e30.eyJhIjo1fQ.KEh186CjVw_Q8FadjJcaVnE7hO5Z9nHBbU8TgbhHcBY'
jwt.decode(j, 'secret')
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File "".../site-packages/jwt/api_jwt.py"", line 61, in decode
options, **kwargs)
File "".../site-packages/jwt/api_jws.py"", line 115, in decode
key, algorithms)
File "".../site-packages/jwt/api_jws.py"", line 168, in _verify_signature
alg = header['alg']
KeyError: 'alg'
",KeyError,buggy_snippets_files/79fbfb422347c18d3655d1159194330f47c53fb304ad59e12bfab8b437344397_before_merge.py,buggy_snippets_files/79fbfb422347c18d3655d1159194330f47c53fb304ad59e12bfab8b437344397_after_merge.py
1305,"    def _from_data(self, guild):
        # according to Stan, this is always available even if the guild is unavailable
        # I don't have this guarantee when someone updates the guild.
        member_count = guild.get('member_count', None)
        if member_count is not None:
            self._member_count = member_count

        self.name = guild.get('name')
        self.region = try_enum(VoiceRegion, guild.get('region'))
        self.verification_level = try_enum(VerificationLevel, guild.get('verification_level'))
        self.default_notifications = try_enum(NotificationLevel, guild.get('default_message_notifications'))
        self.explicit_content_filter = try_enum(ContentFilter, guild.get('explicit_content_filter', 0))
        self.afk_timeout = guild.get('afk_timeout')
        self.icon = guild.get('icon')
        self.banner = guild.get('banner')
        self.unavailable = guild.get('unavailable', False)
        self.id = int(guild['id'])
        self._roles = {}
        state = self._state # speed up attribute access
        for r in guild.get('roles', []):
            role = Role(guild=self, data=r, state=state)
            self._roles[role.id] = role

        self.mfa_level = guild.get('mfa_level')
        self.emojis = tuple(map(lambda d: state.store_emoji(self, d), guild.get('emojis', [])))
        self.features = guild.get('features', [])
        self.splash = guild.get('splash')
        self._system_channel_id = utils._get_as_snowflake(guild, 'system_channel_id')
        self.description = guild.get('description')
        self.max_presences = guild.get('max_presences')
        self.max_members = guild.get('max_members')
        self.max_video_channel_users = guild.get('max_video_channel_users')
        self.premium_tier = guild.get('premium_tier', 0)
        self.premium_subscription_count = guild.get('premium_subscription_count') or 0
        self._system_channel_flags = guild.get('system_channel_flags', 0)
        self.preferred_locale = guild.get('preferred_locale')
        self.discovery_splash = guild.get('discovery_splash')
        self._rules_channel_id = utils._get_as_snowflake(guild, 'rules_channel_id')
        self._public_updates_channel_id = utils._get_as_snowflake(guild, 'public_updates_channel_id')

        cache_online_members = self._state._member_cache_flags.online
        cache_joined = self._state._member_cache_flags.joined
        self_id = self._state.self_id
        for mdata in guild.get('members', []):
            member = Member(data=mdata, guild=self, state=state)
            if cache_joined or (cache_online_members and member.raw_status != 'offline') or member.id == self_id:
                self._add_member(member)

        self._sync(guild)
        self._large = None if member_count is None else self._member_count >= 250

        self.owner_id = utils._get_as_snowflake(guild, 'owner_id')
        self.afk_channel = self.get_channel(utils._get_as_snowflake(guild, 'afk_channel_id'))

        for obj in guild.get('voice_states', []):
            self._update_voice_state(obj, int(obj['channel_id']))
","    def _from_data(self, guild):
        # according to Stan, this is always available even if the guild is unavailable
        # I don't have this guarantee when someone updates the guild.
        member_count = guild.get('member_count', None)
        if member_count is not None:
            self._member_count = member_count

        self.name = guild.get('name')
        self.region = try_enum(VoiceRegion, guild.get('region'))
        self.verification_level = try_enum(VerificationLevel, guild.get('verification_level'))
        self.default_notifications = try_enum(NotificationLevel, guild.get('default_message_notifications'))
        self.explicit_content_filter = try_enum(ContentFilter, guild.get('explicit_content_filter', 0))
        self.afk_timeout = guild.get('afk_timeout')
        self.icon = guild.get('icon')
        self.banner = guild.get('banner')
        self.unavailable = guild.get('unavailable', False)
        self.id = int(guild['id'])
        self._roles = {}
        state = self._state # speed up attribute access
        for r in guild.get('roles', []):
            role = Role(guild=self, data=r, state=state)
            self._roles[role.id] = role

        self.mfa_level = guild.get('mfa_level')
        self.emojis = tuple(map(lambda d: state.store_emoji(self, d), guild.get('emojis', [])))
        self.features = guild.get('features', [])
        self.splash = guild.get('splash')
        self._system_channel_id = utils._get_as_snowflake(guild, 'system_channel_id')
        self.description = guild.get('description')
        self.max_presences = guild.get('max_presences')
        self.max_members = guild.get('max_members')
        self.max_video_channel_users = guild.get('max_video_channel_users')
        self.premium_tier = guild.get('premium_tier', 0)
        self.premium_subscription_count = guild.get('premium_subscription_count') or 0
        self._system_channel_flags = guild.get('system_channel_flags', 0)
        self.preferred_locale = guild.get('preferred_locale')
        self.discovery_splash = guild.get('discovery_splash')
        self._rules_channel_id = utils._get_as_snowflake(guild, 'rules_channel_id')
        self._public_updates_channel_id = utils._get_as_snowflake(guild, 'public_updates_channel_id')

        cache_online_members = self._state.member_cache_flags.online
        cache_joined = self._state.member_cache_flags.joined
        self_id = self._state.self_id
        for mdata in guild.get('members', []):
            member = Member(data=mdata, guild=self, state=state)
            if cache_joined or (cache_online_members and member.raw_status != 'offline') or member.id == self_id:
                self._add_member(member)

        self._sync(guild)
        self._large = None if member_count is None else self._member_count >= 250

        self.owner_id = utils._get_as_snowflake(guild, 'owner_id')
        self.afk_channel = self.get_channel(utils._get_as_snowflake(guild, 'afk_channel_id'))

        for obj in guild.get('voice_states', []):
            self._update_voice_state(obj, int(obj['channel_id']))
",https://github.com/Rapptz/discord.py/issues/5986,"CWE-236: Improper Handling of Undefined Parameters, CWE-166: Improper Handling of Missing Special Element, CWE-248: Uncaught Exception",wrong name for attribute member_cache_flags,discord/guild.py,Guild._from_data,"[41, 42]","Ignoring exception in command guild:
Traceback (most recent call last):
File ""\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\ext\commands\core.py"", line 85, in wrapped
ret = await coro(*args, **kwargs)
File ""C:\Users\\Desktop\Coding\Python\Lost... Bot\Cogs\staff.py"", line 36, in guild
temp = await self.bot.fetch_template(code)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\client.py"", line 1144, in fetch_template
return Template(data=data, state=self._connection)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\template.py"", line 120, in __init__
self.source_guild = Guild(data=source_serialised, state=state)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\guild.py"", line 190, in __init__
self._from_data(data)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\guild.py"", line 308, in _from_data
cache_online_members = self._state._member_cache_flags.online
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\template.py"", line 75, in __getattr__
raise AttributeError('PartialTemplateState does not support {0!r}.'.format(attr))
AttributeError: PartialTemplateState does not support '_member_cache_flags'.",AttributeError,buggy_snippets_files/4a7ce80f4f3cc4b4db3b3341ccd8defb8f024ecbf0ec0ff5b17c757f643ac035_before_merge.py,buggy_snippets_files/4a7ce80f4f3cc4b4db3b3341ccd8defb8f024ecbf0ec0ff5b17c757f643ac035_after_merge.py
1304,"    async def query_member_named(self, guild, argument):
        cache = guild._state._member_cache_flags.joined
        if len(argument) > 5 and argument[-5] == '#':
            username, _, discriminator = argument.rpartition('#')
            members = await guild.query_members(username, limit=100, cache=cache)
            return discord.utils.get(members, name=username, discriminator=discriminator)
        else:
            members = await guild.query_members(argument, limit=100, cache=cache)
            return discord.utils.find(lambda m: m.name == argument or m.nick == argument, members)
","    async def query_member_named(self, guild, argument):
        cache = guild._state.member_cache_flags.joined
        if len(argument) > 5 and argument[-5] == '#':
            username, _, discriminator = argument.rpartition('#')
            members = await guild.query_members(username, limit=100, cache=cache)
            return discord.utils.get(members, name=username, discriminator=discriminator)
        else:
            members = await guild.query_members(argument, limit=100, cache=cache)
            return discord.utils.find(lambda m: m.name == argument or m.nick == argument, members)
",https://github.com/Rapptz/discord.py/issues/5986,"CWE-236: Improper Handling of Undefined Parameters, CWE-166: Improper Handling of Missing Special Element, CWE-248: Uncaught Exception",wrong name for attribute member_cache_flags,discord/ext/commands/converter.py,MemberConverter.query_member_named,[2],"Ignoring exception in command guild:
Traceback (most recent call last):
File ""\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\ext\commands\core.py"", line 85, in wrapped
ret = await coro(*args, **kwargs)
File ""C:\Users\\Desktop\Coding\Python\Lost... Bot\Cogs\staff.py"", line 36, in guild
temp = await self.bot.fetch_template(code)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\client.py"", line 1144, in fetch_template
return Template(data=data, state=self._connection)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\template.py"", line 120, in __init__
self.source_guild = Guild(data=source_serialised, state=state)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\guild.py"", line 190, in __init__
self._from_data(data)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\guild.py"", line 308, in _from_data
cache_online_members = self._state._member_cache_flags.online
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\template.py"", line 75, in __getattr__
raise AttributeError('PartialTemplateState does not support {0!r}.'.format(attr))
AttributeError: PartialTemplateState does not support '_member_cache_flags'.",AttributeError,buggy_snippets_files/3fa3f513064ec4346e5a5fa2703d780e3e7c266c79b69b02f8d54f1e4fb7d675_before_merge.py,buggy_snippets_files/3fa3f513064ec4346e5a5fa2703d780e3e7c266c79b69b02f8d54f1e4fb7d675_after_merge.py
1303,"    async def query_member_by_id(self, bot, guild, user_id):
        ws = bot._get_websocket(shard_id=guild.shard_id)
        cache = guild._state._member_cache_flags.joined
        if ws.is_ratelimited():
            # If we're being rate limited on the WS, then fall back to using the HTTP API
            # So we don't have to wait ~60 seconds for the query to finish
            try:
                member = await guild.fetch_member(user_id)
            except discord.HTTPException:
                return None

            if cache:
                guild._add_member(member)
            return member

        # If we're not being rate limited then we can use the websocket to actually query
        members = await guild.query_members(limit=1, user_ids=[user_id], cache=cache)
        if not members:
            return None
        return members[0]
","    async def query_member_by_id(self, bot, guild, user_id):
        ws = bot._get_websocket(shard_id=guild.shard_id)
        cache = guild._state.member_cache_flags.joined
        if ws.is_ratelimited():
            # If we're being rate limited on the WS, then fall back to using the HTTP API
            # So we don't have to wait ~60 seconds for the query to finish
            try:
                member = await guild.fetch_member(user_id)
            except discord.HTTPException:
                return None

            if cache:
                guild._add_member(member)
            return member

        # If we're not being rate limited then we can use the websocket to actually query
        members = await guild.query_members(limit=1, user_ids=[user_id], cache=cache)
        if not members:
            return None
        return members[0]
",https://github.com/Rapptz/discord.py/issues/5986,"CWE-236: Improper Handling of Undefined Parameters, CWE-166: Improper Handling of Missing Special Element, CWE-248: Uncaught Exception",wrong name for attribute member_cache_flags,discord/ext/commands/converter.py,MemberConverter.query_member_by_id,[3],"Ignoring exception in command guild:
Traceback (most recent call last):
File ""\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\ext\commands\core.py"", line 85, in wrapped
ret = await coro(*args, **kwargs)
File ""C:\Users\\Desktop\Coding\Python\Lost... Bot\Cogs\staff.py"", line 36, in guild
temp = await self.bot.fetch_template(code)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\client.py"", line 1144, in fetch_template
return Template(data=data, state=self._connection)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\template.py"", line 120, in __init__
self.source_guild = Guild(data=source_serialised, state=state)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\guild.py"", line 190, in __init__
self._from_data(data)
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\guild.py"", line 308, in _from_data
cache_online_members = self._state._member_cache_flags.online
File ""C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\discord\template.py"", line 75, in __getattr__
raise AttributeError('PartialTemplateState does not support {0!r}.'.format(attr))
AttributeError: PartialTemplateState does not support '_member_cache_flags'.",AttributeError,buggy_snippets_files/5ab5a137a829180e5cb3a10c3299d0a9a3752b65b935cc9f29e6a4c27d1a0b68_before_merge.py,buggy_snippets_files/5ab5a137a829180e5cb3a10c3299d0a9a3752b65b935cc9f29e6a4c27d1a0b68_after_merge.py
1349,"    async def _create_socket(self, server_id, data):
        async with self._handshake_check:
            if self._handshaking:
                log.info(""Ignoring voice server update while handshake is in progress"")
                return
            self._handshaking = True

        self._connected.clear()
        self.session_id = self.main_ws.session_id
        self.server_id = server_id
        self.token = data.get('token')
        endpoint = data.get('endpoint')

        if endpoint is None or self.token is None:
            log.warning('Awaiting endpoint... This requires waiting. ' \
                        'If timeout occurred considering raising the timeout and reconnecting.')
            return

        self.endpoint = endpoint.replace(':80', '')
        self.endpoint_ip = socket.gethostbyname(self.endpoint)

        if self.socket:
            try:
                self.socket.close()
            except Exception:
                pass

        self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.socket.setblocking(False)

        if self._handshake_complete.is_set():
            # terminate the websocket and handle the reconnect loop if necessary.
            self._handshake_complete.clear()
            self._handshaking = False
            await self.ws.close(4000)
            return

        self._handshake_complete.set()
","    async def _create_socket(self, server_id, data):
        async with self._handshake_check:
            if self._handshaking:
                log.info(""Ignoring voice server update while handshake is in progress"")
                return
            self._handshaking = True

        self._connected.clear()
        self.session_id = self.main_ws.session_id
        self.server_id = server_id
        self.token = data.get('token')
        endpoint = data.get('endpoint')

        if endpoint is None or self.token is None:
            log.warning('Awaiting endpoint... This requires waiting. ' \
                        'If timeout occurred considering raising the timeout and reconnecting.')
            return

        self.endpoint, _, _ = endpoint.rpartition(':')
        # This gets set later
        self.endpoint_ip = None

        if self.socket:
            try:
                self.socket.close()
            except Exception:
                pass

        self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.socket.setblocking(False)

        if self._handshake_complete.is_set():
            # terminate the websocket and handle the reconnect loop if necessary.
            self._handshake_complete.clear()
            self._handshaking = False
            await self.ws.close(4000)
            return

        self._handshake_complete.set()
",https://github.com/Rapptz/discord.py/issues/5191,"CWE-168: Improper Handling of Inconsistent Special Elements, CWE-159: Improper Handling of Invalid Use of Special Elements",the case where web link string contains ':443' is unprocessed,discord/voice_client.py,VoiceClient._create_socket,"[19, 20]","Task exception was never retrieved
future: <Task finished coro=<VoiceClient._create_socket() done, defined at D:\Python\lib\site-
packages\discord\voice_client.py:172> exception=gaierror(11001, 'getaddrinfo failed')>
Traceback (most recent call last):
File ""D:\Python\lib\site-packages\discord\voice_client.py"", line 192, in _create_socket
self.endpoint_ip = socket.gethostbyname(self.endpoint)
socket.gaierror: [Errno 11001] getaddrinfo failed",socket.gaierror,buggy_snippets_files/de098cd45e4a2260207f038d5e94c0fbcbc19d61b8c3442b3a5645b558c77abd_before_merge.py,buggy_snippets_files/de098cd45e4a2260207f038d5e94c0fbcbc19d61b8c3442b3a5645b558c77abd_after_merge.py
1361,"    @classmethod
    def from_incomplete(cls, *, state, data):
        guild_id = int(data['guild']['id'])
        channel_id = int(data['channel']['id'])
        guild = state._get_guild(guild_id)
        if guild is not None:
            channel = guild.get_channel(channel_id)
        else:
            channel_data = data['channel']
            guild_data = data['guild']
            channel_type = try_enum(ChannelType, channel_data['type'])
            channel = PartialInviteChannel(id=channel_id, name=channel_data['name'], type=channel_type)
            guild = PartialInviteGuild(state, guild_data, guild_id)
        data['guild'] = guild
        data['channel'] = channel
        return cls(state=state, data=data)
","    @classmethod
    def from_incomplete(cls, *, state, data):
        try:
            guild_id = int(data['guild']['id'])
        except KeyError:
            # If we're here, then this is a group DM
            guild = None
        else:
            guild = state._get_guild(guild_id)
            if guild is None:
                # If it's not cached, then it has to be a partial guild
                guild_data = data['guild']
                guild = PartialInviteGuild(state, guild_data, guild_id)

        # As far as I know, invites always need a channel
        # So this should never raise.
        channel_data = data['channel']
        channel_id = int(channel_data['id'])
        channel_type = try_enum(ChannelType, channel_data['type'])
        channel = PartialInviteChannel(id=channel_id, name=channel_data['name'], type=channel_type)
        if guild is not None:
            # Upgrade the partial data if applicable
            channel = guild.get_channel(channel_id) or channel

        data['guild'] = guild
        data['channel'] = channel
        return cls(state=state, data=data)
",https://github.com/Rapptz/discord.py/issues/2383,CWE-248: Uncaught Exception,the situation is unhandled where key guild is non-existent,discord/invite.py,Invite.from_incomplete,[3],"await self.bot.fetch_invite(""YXHVVE"")
Traceback (most recent call last):
File ""/home/alexflipnote/Desktop/xelA/cogs/eval.py"", line 62, in _eval
res = await func()
File ""<string>"", line 5, in func
File ""/usr/local/lib/python3.7/site-packages/discord/client.py"", line 1054, in fetch_invite
return Invite.from_incomplete(state=self._connection, data=data)
File ""/usr/local/lib/python3.7/site-packages/discord/invite.py"", line 278, in from_incomplete
guild_id = int(data['guild']['id'])
KeyError: 'guild'",KeyError,buggy_snippets_files/1276dcd94915688d68b2a398789b7c3e4897067c37b8f7691195979513fc4579_before_merge.py,buggy_snippets_files/1276dcd94915688d68b2a398789b7c3e4897067c37b8f7691195979513fc4579_after_merge.py
1368,"    def execute_webhook(self, *, payload, wait=False, file=None, files=None):
        if file is not None:
            multipart = {
                'file': file,
                'payload_json': utils.to_json(payload)
            }
            data = None
        elif files is not None:
            multipart = {
                'payload_json': utils.to_json(payload)
            }
            for i, file in enumerate(files, start=1):
                multipart['file%i' % i] = file
            data = None
        else:
            data = payload
            multipart = None

        url = '%s?wait=%d' % (self._request_url, wait)
        maybe_coro = self.request('POST', url, multipart=multipart, payload=data)
        return self.handle_execution_response(maybe_coro, wait=wait)
","    def execute_webhook(self, *, payload, wait=False, file=None, files=None):
        cleanup = None
        if file is not None:
            multipart = {
                'file': (file.filename, file.open_file(), 'application/octet-stream'),
                'payload_json': utils.to_json(payload)
            }
            data = None
            cleanup = file.close
        elif files is not None:
            multipart = {
                'payload_json': utils.to_json(payload)
            }
            for i, file in enumerate(files, start=1):
                multipart['file%i' % i] = (file.filename, file.open_file(), 'application/octet-stream')
            data = None

            def _anon():
                for f in files:
                    f.close()

            cleanup = _anon
        else:
            data = payload
            multipart = None

        url = '%s?wait=%d' % (self._request_url, wait)
        try:
            maybe_coro = self.request('POST', url, multipart=multipart, payload=data)
        finally:
            if cleanup is not None:
                if not asyncio.iscoroutine(maybe_coro):
                    cleanup()
                else:
                    maybe_coro = self._wrap_coroutine_and_cleanup(maybe_coro, cleanup)
        return self.handle_execution_response(maybe_coro, wait=wait)
",https://github.com/Rapptz/discord.py/issues/1770,"CWE-672: Operation on a Resource after Expiration or Release, CWE-372: Incomplete Internal State Distinction",during async call of execute_webhook a file is closed before it finishes working with it,discord/webhook.py,WebhookAdapter.execute_webhook,"[2,14;20]","Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/discord/ext/commands/core.py"", line 62, in wrapped
ret = await coro(*args, **kwargs)
File ""/home/fedora/Thanatos/modules/test.py"", line 9, in test
await webhook.send(file=discord.File(""file.txt""))
File ""/usr/local/lib/python3.7/site-packages/discord/webhook.py"", line 194, in handle_execution_response
data = await response
File ""/usr/local/lib/python3.7/site-packages/discord/webhook.py"", line 162, in request
async with self.session.request(verb, url, headers=headers, data=data) as r:
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client.py"", line 855, in __aenter__
self._resp = await self._coro
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client.py"", line 361, in _request
ssl=ssl, proxy_headers=proxy_headers, traces=traces)
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client_reqrep.py"", line 233, in __init__
self.update_body_from_data(data)
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client_reqrep.py"", line 405, in update_body_from_data
body = body()
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/formdata.py"", line 141, in __call__
return self._gen_form_data()
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/formdata.py"", line 135, in _gen_form_data
self._writer.append_payload(part)
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/multipart.py"", line 745, in append_payload
size = payload.size
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/payload.py"", line 284, in size
return os.fstat(self._value.fileno()).st_size - self._value.tell()
ValueError: I/O operation on closed file",ValueError,buggy_snippets_files/8d4674a7b2a54a7fe084e5edf24a91459a5e49eefcea0afb7ee13898a48ecd6c_before_merge.py,buggy_snippets_files/8d4674a7b2a54a7fe084e5edf24a91459a5e49eefcea0afb7ee13898a48ecd6c_after_merge.py
1367,"    def send(self, content=None, *, wait=False, username=None, avatar_url=None, tts=False,
                                    file=None, files=None, embed=None, embeds=None):
        """"""|maybecoro|

        Sends a message using the webhook.

        If the webhook is constructed with a :class:`RequestsWebhookAdapter` then this is
        not a coroutine.

        The content must be a type that can convert to a string through ``str(content)``.

        To upload a single file, the ``file`` parameter should be used with a
        single :class:`File` object.

        If the ``embed`` parameter is provided, it must be of type :class:`Embed` and
        it must be a rich embed type. You cannot mix the ``embed`` parameter with the
        ``embeds`` parameter, which must be a :class:`list` of :class:`Embed` objects to send.

        Parameters
        ------------
        content
            The content of the message to send.
        wait: bool
            Whether the server should wait before sending a response. This essentially
            means that the return type of this function changes from ``None`` to
            a :class:`Message` if set to ``True``.
        username: str
            The username to send with this message. If no username is provided
            then the default username for the webhook is used.
        avatar_url: str
            The avatar URL to send with this message. If no avatar URL is provided
            then the default avatar for the webhook is used.
        tts: bool
            Indicates if the message should be sent using text-to-speech.
        file: :class:`File`
            The file to upload. This cannot be mixed with ``files`` parameter.
        files: List[:class:`File`]
            A list of files to send with the content. This cannot be mixed with the
            ``file`` parameter.
        embed: :class:`Embed`
            The rich embed for the content to send. This cannot be mixed with
            ``embeds`` parameter.
        embeds: List[:class:`Embed`]
            A list of embeds to send with the content. Maximum of 10. This cannot
            be mixed with the ``embed`` parameter.

        Raises
        --------
        HTTPException
            Sending the message failed.
        NotFound
            This webhook was not found.
        Forbidden
            The authorization token for the webhook is incorrect.
        InvalidArgument
            You specified both ``embed`` and ``embeds`` or the length of
            ``embeds`` was invalid.

        Returns
        ---------
        Optional[:class:`Message`]
            The message that was sent.
        """"""

        payload = {}

        if files is not None and file is not None:
            raise InvalidArgument('Cannot mix file and files keyword arguments.')
        if embeds is not None and embed is not None:
            raise InvalidArgument('Cannot mix embed and embeds keyword arguments.')

        if embeds is not None:
            if len(embeds) > 10:
                raise InvalidArgument('embeds has a maximum of 10 elements.')
            payload['embeds'] = [e.to_dict() for e in embeds]

        if embed is not None:
            payload['embeds'] = [embed.to_dict()]

        if content is not None:
            payload['content'] = str(content)

        payload['tts'] = tts
        if avatar_url:
            payload['avatar_url'] = avatar_url
        if username:
            payload['username'] = username

        if file is not None:
            try:
                to_pass = (file.filename, file.open_file(), 'application/octet-stream')
                return self._adapter.execute_webhook(wait=wait, file=to_pass, payload=payload)
            finally:
                file.close()
        elif files is not None:
            try:
                to_pass = [(file.filename, file.open_file(), 'application/octet-stream')
                           for file in files]
                return self._adapter.execute_webhook(wait=wait, files=to_pass, payload=payload)
            finally:
                for file in files:
                    file.close()
        else:
            return self._adapter.execute_webhook(wait=wait, payload=payload)
","    def send(self, content=None, *, wait=False, username=None, avatar_url=None, tts=False,
                                    file=None, files=None, embed=None, embeds=None):
        """"""|maybecoro|

        Sends a message using the webhook.

        If the webhook is constructed with a :class:`RequestsWebhookAdapter` then this is
        not a coroutine.

        The content must be a type that can convert to a string through ``str(content)``.

        To upload a single file, the ``file`` parameter should be used with a
        single :class:`File` object.

        If the ``embed`` parameter is provided, it must be of type :class:`Embed` and
        it must be a rich embed type. You cannot mix the ``embed`` parameter with the
        ``embeds`` parameter, which must be a :class:`list` of :class:`Embed` objects to send.

        Parameters
        ------------
        content
            The content of the message to send.
        wait: bool
            Whether the server should wait before sending a response. This essentially
            means that the return type of this function changes from ``None`` to
            a :class:`Message` if set to ``True``.
        username: str
            The username to send with this message. If no username is provided
            then the default username for the webhook is used.
        avatar_url: str
            The avatar URL to send with this message. If no avatar URL is provided
            then the default avatar for the webhook is used.
        tts: bool
            Indicates if the message should be sent using text-to-speech.
        file: :class:`File`
            The file to upload. This cannot be mixed with ``files`` parameter.
        files: List[:class:`File`]
            A list of files to send with the content. This cannot be mixed with the
            ``file`` parameter.
        embed: :class:`Embed`
            The rich embed for the content to send. This cannot be mixed with
            ``embeds`` parameter.
        embeds: List[:class:`Embed`]
            A list of embeds to send with the content. Maximum of 10. This cannot
            be mixed with the ``embed`` parameter.

        Raises
        --------
        HTTPException
            Sending the message failed.
        NotFound
            This webhook was not found.
        Forbidden
            The authorization token for the webhook is incorrect.
        InvalidArgument
            You specified both ``embed`` and ``embeds`` or the length of
            ``embeds`` was invalid.

        Returns
        ---------
        Optional[:class:`Message`]
            The message that was sent.
        """"""

        payload = {}

        if files is not None and file is not None:
            raise InvalidArgument('Cannot mix file and files keyword arguments.')
        if embeds is not None and embed is not None:
            raise InvalidArgument('Cannot mix embed and embeds keyword arguments.')

        if embeds is not None:
            if len(embeds) > 10:
                raise InvalidArgument('embeds has a maximum of 10 elements.')
            payload['embeds'] = [e.to_dict() for e in embeds]

        if embed is not None:
            payload['embeds'] = [embed.to_dict()]

        if content is not None:
            payload['content'] = str(content)

        payload['tts'] = tts
        if avatar_url:
            payload['avatar_url'] = avatar_url
        if username:
            payload['username'] = username

        return self._adapter.execute_webhook(wait=wait, file=file, files=files, payload=payload)
",https://github.com/Rapptz/discord.py/issues/1770,"CWE-672: Operation on a Resource after Expiration or Release, CWE-372: Incomplete Internal State Distinction",during async call of execute_webhook a file is closed before it finishes working with it,discord/webhook.py,Webhook.send,"[90,102]","Traceback (most recent call last):
File ""/usr/local/lib/python3.7/site-packages/discord/ext/commands/core.py"", line 62, in wrapped
ret = await coro(*args, **kwargs)
File ""/home/fedora/Thanatos/modules/test.py"", line 9, in test
await webhook.send(file=discord.File(""file.txt""))
File ""/usr/local/lib/python3.7/site-packages/discord/webhook.py"", line 194, in handle_execution_response
data = await response
File ""/usr/local/lib/python3.7/site-packages/discord/webhook.py"", line 162, in request
async with self.session.request(verb, url, headers=headers, data=data) as r:
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client.py"", line 855, in __aenter__
self._resp = await self._coro
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client.py"", line 361, in _request
ssl=ssl, proxy_headers=proxy_headers, traces=traces)
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client_reqrep.py"", line 233, in __init__
self.update_body_from_data(data)
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/client_reqrep.py"", line 405, in update_body_from_data
body = body()
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/formdata.py"", line 141, in __call__
return self._gen_form_data()
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/formdata.py"", line 135, in _gen_form_data
self._writer.append_payload(part)
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/multipart.py"", line 745, in append_payload
size = payload.size
File ""/usr/local/lib64/python3.7/site-packages/aiohttp/payload.py"", line 284, in size
return os.fstat(self._value.fileno()).st_size - self._value.tell()
ValueError: I/O operation on closed file",ValueError,buggy_snippets_files/3215d37dbe8ee809ce870ebe05f147a936bb7dac71191a5bfb0be622b1ff8429_before_merge.py,buggy_snippets_files/3215d37dbe8ee809ce870ebe05f147a936bb7dac71191a5bfb0be622b1ff8429_after_merge.py
1429,"def main(args=None):
    if args is None:
        args = sys.argv[1:]

    options = parse_args(args)
    setup_logging(options)

    # /dev/urandom may be slow, so we cache the data first
    log.info('Preparing test data...')
    rnd_fh = tempfile.TemporaryFile()
    with open('/dev/urandom', 'rb', 0) as src:
        copied = 0
        while copied < 50 * 1024 * 1024:
            buf = src.read(BUFSIZE)
            rnd_fh.write(buf)
            copied += len(buf)

    log.info('Measuring throughput to cache...')
    backend_dir = tempfile.mkdtemp(prefix='s3ql-benchmark-')
    mnt_dir = tempfile.mkdtemp(prefix='s3ql-mnt')
    atexit.register(shutil.rmtree, backend_dir)
    atexit.register(shutil.rmtree, mnt_dir)

    block_sizes = [ 2**b for b in range(12, 18) ]
    for blocksize in block_sizes:
        write_time = 0
        size = 50 * 1024 * 1024
        while write_time < 3:
            log.debug('Write took %.3g seconds, retrying', write_time)
            subprocess.check_call([exec_prefix + 'mkfs.s3ql', '--plain', 'local://%s' % backend_dir,
                                   '--quiet', '--force', '--cachedir', options.cachedir])
            subprocess.check_call([exec_prefix + 'mount.s3ql', '--threads', '1', '--quiet',
                                   '--cachesize', '%d' % (2 * size / 1024), '--log',
                                   '%s/mount.log' % backend_dir, '--cachedir', options.cachedir,
                                   'local://%s' % backend_dir, mnt_dir])
            try:
                size *= 2
                with open('%s/bigfile' % mnt_dir, 'wb', 0) as dst:
                    rnd_fh.seek(0)
                    write_time = time.time()
                    copied = 0
                    while copied < size:
                        buf = rnd_fh.read(blocksize)
                        if not buf:
                            rnd_fh.seek(0)
                            continue
                        dst.write(buf)
                        copied += len(buf)

                write_time = time.time() - write_time
                os.unlink('%s/bigfile' % mnt_dir)
            finally:
                subprocess.check_call([exec_prefix + 'umount.s3ql', mnt_dir])

        fuse_speed = copied / write_time
        log.info('Cache throughput with %3d KiB blocks: %d KiB/sec',
                 blocksize / 1024, fuse_speed / 1024)

    # Upload random data to prevent effects of compression
    # on the network layer
    log.info('Measuring raw backend throughput..')
    try:
        backend = get_backend(options, raw=True)
    except DanglingStorageURLError as exc:
        raise QuietError(str(exc)) from None

    upload_time = 0
    size = 512 * 1024
    while upload_time < 10:
        size *= 2
        def do_write(dst):
            rnd_fh.seek(0)
            stamp = time.time()
            copied = 0
            while copied < size:
                buf = rnd_fh.read(BUFSIZE)
                if not buf:
                    rnd_fh.seek(0)
                    continue
                dst.write(buf)
                copied += len(buf)
            return (copied, stamp)
        (upload_size, upload_time) = backend.perform_write(do_write, 's3ql_testdata')
        upload_time = time.time() - upload_time
    backend_speed = upload_size / upload_time
    log.info('Backend throughput: %d KiB/sec', backend_speed / 1024)
    backend.delete('s3ql_testdata')

    src = options.file
    size = os.fstat(options.file.fileno()).st_size
    log.info('Test file size: %.2f MiB', (size / 1024 ** 2))

    in_speed = dict()
    out_speed = dict()
    for alg in ALGS:
        log.info('compressing with %s-6...', alg)
        backend = ComprencBackend(b'pass', (alg, 6), Backend('local://' + backend_dir, None, None))
        def do_write(dst): #pylint: disable=E0102
            src.seek(0)
            stamp = time.time()
            while True:
                buf = src.read(BUFSIZE)
                if not buf:
                    break
                dst.write(buf)
            return (dst, stamp)
        (dst_fh, stamp) = backend.perform_write(do_write, 's3ql_testdata')
        dt = time.time() - stamp
        in_speed[alg] = size / dt
        out_speed[alg] = dst_fh.get_obj_size() / dt
        log.info('%s compression speed: %d KiB/sec per thread (in)', alg, in_speed[alg] / 1024)
        log.info('%s compression speed: %d KiB/sec per thread (out)', alg, out_speed[alg] / 1024)

    print('')
    print('With %d KiB blocks, maximum performance for different compression'
          % (block_sizes[-1]/1024), 'algorithms and thread counts is:', '', sep='\n')

    threads = set([1,2,4,8])
    cores = os.sysconf('SC_NPROCESSORS_ONLN')
    if cores != -1:
        threads.add(cores)
    if options.threads:
        threads.add(options.threads)

    print('%-26s' % 'Threads:',
          ('%12d' * len(threads)) % tuple(sorted(threads)))

    for alg in ALGS:
        speeds = []
        limits = []
        for t in sorted(threads):
            if fuse_speed > t * in_speed[alg]:
                limit = 'CPU'
                speed = t * in_speed[alg]
            else:
                limit = 'S3QL/FUSE'
                speed = fuse_speed

            if speed / in_speed[alg] * out_speed[alg] > backend_speed:
                limit = 'uplink'
                speed = backend_speed * in_speed[alg] / out_speed[alg]

            limits.append(limit)
            speeds.append(speed / 1024)

        print('%-26s' % ('Max FS throughput (%s):' % alg),
              ('%7d KiB/s' * len(threads)) % tuple(speeds))
        print('%-26s' % '..limited by:',
              ('%12s' * len(threads)) % tuple(limits))

    print('')
    print('All numbers assume that the test file is representative and that',
          'there are enough processor cores to run all active threads in parallel.',
          'To compensate for network latency, you should use about twice as',
          'many upload threads as indicated by the above table.\n', sep='\n')
","def main(args=None):
    if args is None:
        args = sys.argv[1:]

    options = parse_args(args)
    setup_logging(options)

    # /dev/urandom may be slow, so we cache the data first
    log.info('Preparing test data...')
    rnd_fh = tempfile.TemporaryFile()
    with open('/dev/urandom', 'rb', 0) as src:
        copied = 0
        while copied < 50 * 1024 * 1024:
            buf = src.read(BUFSIZE)
            rnd_fh.write(buf)
            copied += len(buf)

    log.info('Measuring throughput to cache...')
    backend_dir = tempfile.mkdtemp(prefix='s3ql-benchmark-')
    mnt_dir = tempfile.mkdtemp(prefix='s3ql-mnt')
    atexit.register(shutil.rmtree, backend_dir)
    atexit.register(shutil.rmtree, mnt_dir)

    block_sizes = [ 2**b for b in range(12, 18) ]
    for blocksize in block_sizes:
        write_time = 0
        size = 50 * 1024 * 1024
        while write_time < 3:
            log.debug('Write took %.3g seconds, retrying', write_time)
            subprocess.check_call([exec_prefix + 'mkfs.s3ql', '--plain', 'local://%s' % backend_dir,
                                   '--quiet', '--force', '--cachedir', options.cachedir])
            subprocess.check_call([exec_prefix + 'mount.s3ql', '--threads', '1', '--quiet',
                                   '--cachesize', '%d' % (2 * size / 1024), '--log',
                                   '%s/mount.log' % backend_dir, '--cachedir', options.cachedir,
                                   'local://%s' % backend_dir, mnt_dir])
            try:
                size *= 2
                with open('%s/bigfile' % mnt_dir, 'wb', 0) as dst:
                    rnd_fh.seek(0)
                    write_time = time.time()
                    copied = 0
                    while copied < size:
                        buf = rnd_fh.read(blocksize)
                        if not buf:
                            rnd_fh.seek(0)
                            continue
                        dst.write(buf)
                        copied += len(buf)

                write_time = time.time() - write_time
                os.unlink('%s/bigfile' % mnt_dir)
            finally:
                subprocess.check_call([exec_prefix + 'umount.s3ql', mnt_dir])

        fuse_speed = copied / write_time
        log.info('Cache throughput with %3d KiB blocks: %d KiB/sec',
                 blocksize / 1024, fuse_speed / 1024)

    # Upload random data to prevent effects of compression
    # on the network layer
    log.info('Measuring raw backend throughput..')
    try:
        backend = get_backend(options, raw=True)
    except DanglingStorageURLError as exc:
        raise QuietError(str(exc)) from None

    upload_time = 0
    size = 512 * 1024
    while upload_time < 10:
        size *= 2
        def do_write(dst):
            rnd_fh.seek(0)
            stamp = time.time()
            copied = 0
            while copied < size:
                buf = rnd_fh.read(BUFSIZE)
                if not buf:
                    rnd_fh.seek(0)
                    continue
                dst.write(buf)
                copied += len(buf)
            return (copied, stamp)
        (upload_size, upload_time) = backend.perform_write(do_write, 's3ql_testdata')
        upload_time = time.time() - upload_time
    backend_speed = upload_size / upload_time
    log.info('Backend throughput: %d KiB/sec', backend_speed / 1024)
    backend.delete('s3ql_testdata')

    src = options.file
    size = os.fstat(options.file.fileno()).st_size
    log.info('Test file size: %.2f MiB', (size / 1024 ** 2))

    in_speed = dict()
    out_speed = dict()
    for alg in ALGS:
        log.info('compressing with %s-6...', alg)
        backend = ComprencBackend(b'pass', (alg, 6),Backend(argparse.Namespace(storage_url='local://' + backend_dir)))
        def do_write(dst): #pylint: disable=E0102
            src.seek(0)
            stamp = time.time()
            while True:
                buf = src.read(BUFSIZE)
                if not buf:
                    break
                dst.write(buf)
            return (dst, stamp)
        (dst_fh, stamp) = backend.perform_write(do_write, 's3ql_testdata')
        dt = time.time() - stamp
        in_speed[alg] = size / dt
        out_speed[alg] = dst_fh.get_obj_size() / dt
        log.info('%s compression speed: %d KiB/sec per thread (in)', alg, in_speed[alg] / 1024)
        log.info('%s compression speed: %d KiB/sec per thread (out)', alg, out_speed[alg] / 1024)

    print('')
    print('With %d KiB blocks, maximum performance for different compression'
          % (block_sizes[-1]/1024), 'algorithms and thread counts is:', '', sep='\n')

    threads = set([1,2,4,8])
    cores = os.sysconf('SC_NPROCESSORS_ONLN')
    if cores != -1:
        threads.add(cores)
    if options.threads:
        threads.add(options.threads)

    print('%-26s' % 'Threads:',
          ('%12d' * len(threads)) % tuple(sorted(threads)))

    for alg in ALGS:
        speeds = []
        limits = []
        for t in sorted(threads):
            if fuse_speed > t * in_speed[alg]:
                limit = 'CPU'
                speed = t * in_speed[alg]
            else:
                limit = 'S3QL/FUSE'
                speed = fuse_speed

            if speed / in_speed[alg] * out_speed[alg] > backend_speed:
                limit = 'uplink'
                speed = backend_speed * in_speed[alg] / out_speed[alg]

            limits.append(limit)
            speeds.append(speed / 1024)

        print('%-26s' % ('Max FS throughput (%s):' % alg),
              ('%7d KiB/s' * len(threads)) % tuple(speeds))
        print('%-26s' % '..limited by:',
              ('%12s' * len(threads)) % tuple(limits))

    print('')
    print('All numbers assume that the test file is representative and that',
          'there are enough processor cores to run all active threads in parallel.',
          'To compensate for network latency, you should use about twice as',
          'many upload threads as indicated by the above table.\n', sep='\n')
",https://github.com/s3ql/s3ql/issues/22,CWE-235: Improper Handling of Extra Parameters,Backend constructor has two arguments (self and string parameter) but not 4 as provided; the situation is unhandled where the number of arguments is incompatible in the call of Backend constructor in main function,contrib/benchmark.py,main,[97],"Test file size: 500.00 MiB
compressing with lzma-6...
ERROR: Uncaught top-level exception:
Traceback (most recent call last):
File ""/usr/share/doc/s3ql/contrib/benchmark.py"", line 225, in <module>
main(sys.argv[1:])
File ""/usr/share/doc/s3ql/contrib/benchmark.py"", line 163, in main
backend = ComprencBackend(b'pass', (alg, 6), Backend('local://' + backend_dir, None, None))
TypeError: __init__() takes 2 positional arguments but 4 were given",TypeError,buggy_snippets_files/c656c6d990548c868018a316e7d330271460eeca32b1b3049d6222b39c793955_before_merge.py,buggy_snippets_files/c656c6d990548c868018a316e7d330271460eeca32b1b3049d6222b39c793955_after_merge.py
1556,"    def post_process(
        self,
        i: int,
        maxlen: int,
        maxlenratio: float,
        running_hyps: BatchHypothesis,
        ended_hyps: List[Hypothesis],
    ) -> BatchHypothesis:
        """"""Perform post-processing of beam search iterations.

        Args:
            i (int): The length of hypothesis tokens.
            maxlen (int): The maximum length of tokens in beam search.
            maxlenratio (int): The maximum length ratio in beam search.
            running_hyps (BatchHypothesis): The running hypotheses in beam search.
            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.

        Returns:
            BatchHypothesis: The new running hypotheses.

        """"""
        n_batch = running_hyps.yseq.shape[0]
        logging.debug(f""the number of running hypothes: {n_batch}"")
        if self.token_list is not None:
            logging.debug(
                ""best hypo: ""
                + """".join(
                    [
                        self.token_list[x]
                        for x in running_hyps.yseq[0, 1 : running_hyps.length[0]]
                    ]
                )
            )
        # add eos in the final loop to avoid that there are no ended hyps
        if i == maxlen - 1:
            logging.info(""adding <eos> in the last position in the loop"")
            yseq_eos = torch.cat(
                (
                    running_hyps.yseq,
                    torch.full(
                        (n_batch, 1),
                        self.eos,
                        device=running_hyps.yseq.device,
                        dtype=torch.int64,
                    ),
                ),
                1,
            )
            running_hyps.yseq.resize_as_(yseq_eos)
            running_hyps.yseq[:] = yseq_eos
            running_hyps.length[:] = len(yseq_eos)

        # add ended hypotheses to a final list, and removed them from current hypotheses
        # (this will be a probmlem, number of hyps < beam)
        is_eos = (
            running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]
            == self.eos
        )
        for b in torch.nonzero(is_eos).view(-1):
            hyp = self._select(running_hyps, b)
            ended_hyps.append(hyp)
        remained_ids = torch.nonzero(is_eos == 0).view(-1)
        return self._batch_select(running_hyps, remained_ids)
","    def post_process(
        self,
        i: int,
        maxlen: int,
        maxlenratio: float,
        running_hyps: BatchHypothesis,
        ended_hyps: List[Hypothesis],
    ) -> BatchHypothesis:
        """"""Perform post-processing of beam search iterations.

        Args:
            i (int): The length of hypothesis tokens.
            maxlen (int): The maximum length of tokens in beam search.
            maxlenratio (int): The maximum length ratio in beam search.
            running_hyps (BatchHypothesis): The running hypotheses in beam search.
            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.

        Returns:
            BatchHypothesis: The new running hypotheses.

        """"""
        n_batch = running_hyps.yseq.shape[0]
        logging.debug(f""the number of running hypothes: {n_batch}"")
        if self.token_list is not None:
            logging.debug(
                ""best hypo: ""
                + """".join(
                    [
                        self.token_list[x]
                        for x in running_hyps.yseq[0, 1 : running_hyps.length[0]]
                    ]
                )
            )
        # add eos in the final loop to avoid that there are no ended hyps
        if i == maxlen - 1:
            logging.info(""adding <eos> in the last position in the loop"")
            yseq_eos = torch.cat(
                (
                    running_hyps.yseq,
                    torch.full(
                        (n_batch, 1),
                        self.eos,
                        device=running_hyps.yseq.device,
                        dtype=torch.int64,
                    ),
                ),
                1,
            )
            running_hyps.yseq.resize_as_(yseq_eos)
            running_hyps.yseq[:] = yseq_eos
            running_hyps.length[:] = yseq_eos.shape[1]

        # add ended hypotheses to a final list, and removed them from current hypotheses
        # (this will be a probmlem, number of hyps < beam)
        is_eos = (
            running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]
            == self.eos
        )
        for b in torch.nonzero(is_eos).view(-1):
            hyp = self._select(running_hyps, b)
            ended_hyps.append(hyp)
        remained_ids = torch.nonzero(is_eos == 0).view(-1)
        return self._batch_select(running_hyps, remained_ids)
",https://github.com/espnet/espnet/issues/2483,CWE-126: Buffer Over-read,running_hyps.length is not correctly computed,espnet/nets/batch_beam_search.py,BatchBeamSearch.post_process,[51],"# python3 -m espnet2.bin.asr_inference --ngpu 0 --data_path_and_name_and_type dump/raw/eval1/wav.scp,speech,sound --key_file exp/asr_train_asr_transformer_raw_char_sp/decode_asr_lm_lm_train_lm_char_valid.loss.ave_asr_model_valid.acc.best/eval1/logdir/keys.10.scp --asr_train_config exp/asr_train_asr_transformer_raw_char_sp/config.yaml --asr_model_file exp/asr_train_asr_transformer_raw_char_sp/valid.acc.best.pth --output_dir exp/asr_train_asr_transformer_raw_char_sp/decode_asr_lm_lm_train_lm_char_valid.loss.ave_asr_model_valid.acc.best/eval1/logdir/output.10 --config conf/decode_asr.yaml --lm_train_config exp/lm_train_lm_char/config.yaml --lm_file exp/lm_train_lm_char/valid.loss.ave.pth
# Started at Wed Sep 16 11:49:54 JST 2020
#
/home/z43614a/work/espnet/tools/venv/bin/python3 /home/z43614a/work/espnet/espnet2/bin/asr_inference.py --ngpu 0 --data_path_and_name_and_type dump/raw/eval1/wav.scp,speech,sound --key_file exp/asr_train_asr_transformer_raw_char_sp/decode_asr_lm_lm_train_lm_char_valid.loss.ave_asr_model_valid.acc.best/eval1/logdir/keys.10.scp --asr_train_config exp/asr_train_asr_transformer_raw_char_sp/config.yaml --asr_model_file exp/asr_train_asr_transformer_raw_char_sp/valid.acc.best.pth --output_dir exp/asr_train_asr_transformer_raw_char_sp/decode_asr_lm_lm_train_lm_char_valid.loss.ave_asr_model_valid.acc.best/eval1/logdir/output.10 --config conf/decode_asr.yaml --lm_train_config exp/lm_train_lm_char/config.yaml --lm_file exp/lm_train_lm_char/valid.loss.ave.pth
2020-09-16 11:51:07,297 (asr:281) INFO: Vocabulary size: 3262
2020-09-16 11:51:11,029 (lm:198) INFO: Vocabulary size: 3262
2020-09-16 11:51:11,260 (asr_inference:119) INFO: BatchBeamSearch implementation is selected.
2020-09-16 11:51:11,265 (asr_inference:129) INFO: Beam_search: BatchBeamSearch(
(nn_dict): ModuleDict(
(decoder): TransformerDecoder(
(embed): Sequential(
(0): Embedding(3262, 512)
(1): PositionalEncoding(
(dropout): Dropout(p=0.1, inplace=False)
)
)
(after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(output_layer): Linear(in_features=512, out_features=3262, bias=True)
(decoders): MultiSequential(
(0): DecoderLayer(
(self_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(src_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(feed_forward): PositionwiseFeedForward(
(w_1): Linear(in_features=512, out_features=2048, bias=True)
(w_2): Linear(in_features=2048, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
(activation): ReLU()
)
(norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(1): DecoderLayer(
(self_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(src_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(feed_forward): PositionwiseFeedForward(
(w_1): Linear(in_features=512, out_features=2048, bias=True)
(w_2): Linear(in_features=2048, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
(activation): ReLU()
)
(norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(2): DecoderLayer(
(self_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(src_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(feed_forward): PositionwiseFeedForward(
(w_1): Linear(in_features=512, out_features=2048, bias=True)
(w_2): Linear(in_features=2048, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
(activation): ReLU()
)
(norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(3): DecoderLayer(
(self_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(src_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(feed_forward): PositionwiseFeedForward(
(w_1): Linear(in_features=512, out_features=2048, bias=True)
(w_2): Linear(in_features=2048, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
(activation): ReLU()
)
(norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(4): DecoderLayer(
(self_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(src_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(feed_forward): PositionwiseFeedForward(
(w_1): Linear(in_features=512, out_features=2048, bias=True)
(w_2): Linear(in_features=2048, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
(activation): ReLU()
)
(norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(5): DecoderLayer(
(self_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(src_attn): MultiHeadedAttention(
(linear_q): Linear(in_features=512, out_features=512, bias=True)
(linear_k): Linear(in_features=512, out_features=512, bias=True)
(linear_v): Linear(in_features=512, out_features=512, bias=True)
(linear_out): Linear(in_features=512, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
)
(feed_forward): PositionwiseFeedForward(
(w_1): Linear(in_features=512, out_features=2048, bias=True)
(w_2): Linear(in_features=2048, out_features=512, bias=True)
(dropout): Dropout(p=0.1, inplace=False)
(activation): ReLU()
)
(norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
(dropout): Dropout(p=0.1, inplace=False)
)
)
)
(lm): SequentialRNNLM(
(drop): Dropout(p=0.0, inplace=False)
(encoder): Embedding(3262, 650, padding_idx=0)
(rnn): LSTM(650, 650, num_layers=2, batch_first=True)
(decoder): Linear(in_features=650, out_features=3262, bias=True)
)
)
)
2020-09-16 11:51:11,265 (asr_inference:130) INFO: Decoding device=cpu, dtype=float32
2020-09-16 11:51:11,322 (asr_inference:148) INFO: Text tokenizer: CharTokenizer(space_symbol=""<space>""non_linguistic_symbols=""set()"")
2020-09-16 11:51:13,432 (beam_search:358) INFO: decoder input length: 156
2020-09-16 11:51:13,433 (beam_search:359) INFO: max output length: 156
2020-09-16 11:51:13,433 (beam_search:360) INFO: min output length: 0
/opt/conda/conda-bld/pytorch_1591914880026/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:
nonzero(Tensor input, *, Tensor out)
Consider using one of the following signatures instead:
nonzero(Tensor input, *, bool as_tuple)
2020-09-16 11:51:36,910 (beam_search:372) INFO: end detected at 65
2020-09-16 11:51:36,911 (beam_search:397) INFO:  -5.10 * 0.7 =  -3.57 for decoder
2020-09-16 11:51:36,911 (beam_search:397) INFO:  -6.90 * 0.3 =  -2.07 for ctc
2020-09-16 11:51:36,911 (beam_search:397) INFO: -154.84 * 0.3 = -46.45 for lm
2020-09-16 11:51:36,911 (beam_search:399) INFO: total log probability: -52.09
2020-09-16 11:51:36,911 (beam_search:400) INFO: normalized log probability: -0.85
2020-09-16 11:51:36,911 (beam_search:401) INFO: total number of ended hypotheses: 79
2020-09-16 11:51:36,911 (beam_search:406) INFO: best hypo: 

2020-09-16 11:51:38,032 (beam_search:358) INFO: decoder input length: 222
2020-09-16 11:51:38,033 (beam_search:359) INFO: max output length: 222
2020-09-16 11:51:38,033 (beam_search:360) INFO: min output length: 0
2020-09-16 11:52:25,634 (beam_search:372) INFO: end detected at 101
2020-09-16 11:52:25,635 (beam_search:397) INFO:  -9.83 * 0.7 =  -6.88 for decoder
2020-09-16 11:52:25,636 (beam_search:397) INFO: -11.97 * 0.3 =  -3.59 for ctc
2020-09-16 11:52:25,636 (beam_search:397) INFO: -201.95 * 0.3 = -60.59 for lm
2020-09-16 11:52:25,636 (beam_search:399) INFO: total log probability: -71.06
2020-09-16 11:52:25,636 (beam_search:400) INFO: normalized log probability: -0.74
2020-09-16 11:52:25,636 (beam_search:401) INFO: total number of ended hypotheses: 81
2020-09-16 11:52:25,636 (beam_search:406) INFO: best hypo: 

2020-09-16 11:52:26,598 (beam_search:358) INFO: decoder input length: 209
2020-09-16 11:52:26,598 (beam_search:359) INFO: max output length: 209
2020-09-16 11:52:26,598 (beam_search:360) INFO: min output length: 0
2020-09-16 11:53:02,819 (beam_search:372) INFO: end detected at 80
2020-09-16 11:53:02,820 (beam_search:397) INFO: -12.95 * 0.7 =  -9.07 for decoder
2020-09-16 11:53:02,820 (beam_search:397) INFO: -18.95 * 0.3 =  -5.68 for ctc
2020-09-16 11:53:02,820 (beam_search:397) INFO: -214.31 * 0.3 = -64.29 for lm
2020-09-16 11:53:02,820 (beam_search:399) INFO: total log probability: -79.05
2020-09-16 11:53:02,820 (beam_search:400) INFO: normalized log probability: -1.07
2020-09-16 11:53:02,820 (beam_search:401) INFO: total number of ended hypotheses: 69
2020-09-16 11:53:02,821 (beam_search:406) INFO: best hypo: 

2020-09-16 11:53:03,856 (beam_search:358) INFO: decoder input length: 230
2020-09-16 11:53:03,856 (beam_search:359) INFO: max output length: 230
2020-09-16 11:53:03,856 (beam_search:360) INFO: min output length: 0
2020-09-16 11:53:58,540 (beam_search:372) INFO: end detected at 113
2020-09-16 11:53:58,541 (beam_search:397) INFO: -18.32 * 0.7 = -12.83 for decoder
2020-09-16 11:53:58,541 (beam_search:397) INFO: -39.31 * 0.3 = -11.79 for ctc
2020-09-16 11:53:58,541 (beam_search:397) INFO: -293.19 * 0.3 = -87.96 for lm
2020-09-16 11:53:58,541 (beam_search:399) INFO: total log probability: -112.58
2020-09-16 11:53:58,542 (beam_search:400) INFO: normalized log probability: -1.05
2020-09-16 11:53:58,542 (beam_search:401) INFO: total number of ended hypotheses: 74
2020-09-16 11:53:58,542 (beam_search:406) INFO: best hypo: 

2020-09-16 11:53:58,932 (beam_search:358) INFO: decoder input length: 81
2020-09-16 11:53:58,932 (beam_search:359) INFO: max output length: 81
2020-09-16 11:53:58,932 (beam_search:360) INFO: min output length: 0
2020-09-16 11:54:10,401 (beam_search:372) INFO: end detected at 53
2020-09-16 11:54:10,402 (beam_search:397) INFO:  -9.26 * 0.7 =  -6.48 for decoder
2020-09-16 11:54:10,402 (beam_search:397) INFO: -17.02 * 0.3 =  -5.10 for ctc
2020-09-16 11:54:10,402 (beam_search:397) INFO: -132.29 * 0.3 = -39.69 for lm
2020-09-16 11:54:10,402 (beam_search:399) INFO: total log probability: -51.28
2020-09-16 11:54:10,402 (beam_search:400) INFO: normalized log probability: -1.09
2020-09-16 11:54:10,402 (beam_search:401) INFO: total number of ended hypotheses: 78
2020-09-16 11:54:10,403 (beam_search:406) INFO: best hypo: 

2020-09-16 11:54:11,404 (beam_search:358) INFO: decoder input length: 235
2020-09-16 11:54:11,404 (beam_search:359) INFO: max output length: 235
2020-09-16 11:54:11,404 (beam_search:360) INFO: min output length: 0
2020-09-16 11:54:59,835 (beam_search:372) INFO: end detected at 108
2020-09-16 11:54:59,836 (beam_search:397) INFO: -15.15 * 0.7 = -10.60 for decoder
2020-09-16 11:54:59,836 (beam_search:397) INFO: -21.38 * 0.3 =  -6.42 for ctc
2020-09-16 11:54:59,836 (beam_search:397) INFO: -248.58 * 0.3 = -74.57 for lm
2020-09-16 11:54:59,836 (beam_search:399) INFO: total log probability: -91.59
2020-09-16 11:54:59,836 (beam_search:400) INFO: normalized log probability: -0.91
2020-09-16 11:54:59,836 (beam_search:401) INFO: total number of ended hypotheses: 85
2020-09-16 11:54:59,837 (beam_search:406) INFO: best hypo: 

2020-09-16 11:54:59,973 (beam_search:358) INFO: decoder input length: 19
2020-09-16 11:54:59,973 (beam_search:359) INFO: max output length: 19
2020-09-16 11:54:59,973 (beam_search:360) INFO: min output length: 0
2020-09-16 11:55:01,277 (beam_search:372) INFO: end detected at 15
2020-09-16 11:55:01,278 (beam_search:397) INFO:  -1.13 * 0.7 =  -0.79 for decoder
2020-09-16 11:55:01,278 (beam_search:397) INFO:  -0.26 * 0.3 =  -0.08 for ctc
2020-09-16 11:55:01,278 (beam_search:397) INFO: -21.19 * 0.3 =  -6.36 for lm
2020-09-16 11:55:01,278 (beam_search:399) INFO: total log probability: -7.23
2020-09-16 11:55:01,278 (beam_search:400) INFO: normalized log probability: -0.72
2020-09-16 11:55:01,278 (beam_search:401) INFO: total number of ended hypotheses: 82
2020-09-16 11:55:01,278 (beam_search:406) INFO: best hypo: 

2020-09-16 11:55:01,904 (beam_search:358) INFO: decoder input length: 152
2020-09-16 11:55:01,904 (beam_search:359) INFO: max output length: 152
2020-09-16 11:55:01,904 (beam_search:360) INFO: min output length: 0
2020-09-16 11:55:28,522 (beam_search:372) INFO: end detected at 87
2020-09-16 11:55:28,523 (beam_search:397) INFO: -10.70 * 0.7 =  -7.49 for decoder
2020-09-16 11:55:28,523 (beam_search:397) INFO: -12.28 * 0.3 =  -3.68 for ctc
2020-09-16 11:55:28,523 (beam_search:397) INFO: -222.42 * 0.3 = -66.73 for lm
2020-09-16 11:55:28,523 (beam_search:399) INFO: total log probability: -77.90
2020-09-16 11:55:28,523 (beam_search:400) INFO: normalized log probability: -0.96
2020-09-16 11:55:28,523 (beam_search:401) INFO: total number of ended hypotheses: 80
2020-09-16 11:55:28,523 (beam_search:406) INFO: best hypo: 

2020-09-16 11:55:28,622 (beam_search:358) INFO: decoder input length: 8
2020-09-16 11:55:28,622 (beam_search:359) INFO: max output length: 8
2020-09-16 11:55:28,622 (beam_search:360) INFO: min output length: 0
2020-09-16 11:55:29,079 (batch_beam_search:321) INFO: adding <eos> in the last position in the loop
Traceback (most recent call last):
File ""/home/kan-bayashi/work/espnet/tools/venv/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
""__main__"", mod_spec)
File ""/home/kan-bayashi/work/espnet/tools/venv/lib/python3.7/runpy.py"", line 85, in _run_code
exec(code, run_globals)
File ""/home/kan-bayashi/work/espnet/espnet2/bin/asr_inference.py"", line 447, in <module>
main()
File ""/home/kan-bayashi/work/espnet/espnet2/bin/asr_inference.py"", line 443, in main
inference(**kwargs)
File ""/home/kan-bayashi/work/espnet/espnet2/bin/asr_inference.py"", line 312, in inference
results = speech2text(**batch)
File ""/home/kan-bayashi/work/espnet/tools/venv/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 15, in decorate_context
return func(*args, **kwargs)
File ""/home/kan-bayashi/work/espnet/espnet2/bin/asr_inference.py"", line 194, in __call__
x=enc[0], maxlenratio=self.maxlenratio, minlenratio=self.minlenratio
File ""/home/kan-bayashi/work/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
result = self.forward(*input, **kwargs)
File ""/home/kan-bayashi/work/espnet/espnet/nets/beam_search.py"", line 369, in forward
running_hyps = self.post_process(i, maxlen, maxlenratio, best, ended_hyps)
File ""/home/kan-bayashi/work/espnet/espnet/nets/batch_beam_search.py"", line 341, in post_process
running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]
IndexError: index 19 is out of bounds for dimension 1 with size 10
# Accounting: time=335 threads=1
# Ended (code 1) at Wed Sep 16 11:55:29 JST 2020, elapsed time 335 seconds",IndexError,buggy_snippets_files/c0071697828eb5e3385e5f8482ca6ff5ffc82ae2a5ccbfcaa4b18d84ec393a67_before_merge.py,buggy_snippets_files/c0071697828eb5e3385e5f8482ca6ff5ffc82ae2a5ccbfcaa4b18d84ec393a67_after_merge.py
1828,"def train(args):
    """"""Train with the given args.

    Args:
        args (namespace): The program arguments.

    """"""
    # display chainer version
    logging.info('chainer version = ' + chainer.__version__)

    set_deterministic_chainer(args)

    # check cuda and cudnn availability
    if not chainer.cuda.available:
        logging.warning('cuda is not available')
    if not chainer.cuda.cudnn_enabled:
        logging.warning('cudnn is not available')

    # get input and output dimension info
    with open(args.valid_json, 'rb') as f:
        valid_json = json.load(f)['utts']
    utts = list(valid_json.keys())
    idim = int(valid_json[utts[0]]['input'][0]['shape'][1])
    odim = int(valid_json[utts[0]]['output'][0]['shape'][1])
    logging.info('#input dims : ' + str(idim))
    logging.info('#output dims: ' + str(odim))

    # check attention type
    if args.atype not in ['noatt', 'dot', 'location']:
        raise NotImplementedError('chainer supports only noatt, dot, and location attention.')

    # specify attention, CTC, hybrid mode
    if args.mtlalpha == 1.0:
        mtl_mode = 'ctc'
        logging.info('Pure CTC mode')
    elif args.mtlalpha == 0.0:
        mtl_mode = 'att'
        logging.info('Pure attention mode')
    else:
        mtl_mode = 'mtl'
        logging.info('Multitask learning mode')

    # specify model architecture
    logging.info('import model module: ' + args.model_module)
    model_class = dynamic_import(args.model_module)
    model = model_class(idim, odim, args, flag_return=False)
    assert isinstance(model, ASRInterface)

    # write model config
    if not os.path.exists(args.outdir):
        os.makedirs(args.outdir)
    model_conf = args.outdir + '/model.json'
    with open(model_conf, 'wb') as f:
        logging.info('writing a model config file to ' + model_conf)
        f.write(json.dumps((idim, odim, vars(args)),
                           indent=4, ensure_ascii=False, sort_keys=True).encode('utf_8'))
    for key in sorted(vars(args).keys()):
        logging.info('ARGS: ' + key + ': ' + str(vars(args)[key]))

    # Set gpu
    ngpu = args.ngpu
    if ngpu == 1:
        gpu_id = 0
        # Make a specified GPU current
        chainer.cuda.get_device_from_id(gpu_id).use()
        model.to_gpu()  # Copy the model to the GPU
        logging.info('single gpu calculation.')
    elif ngpu > 1:
        gpu_id = 0
        devices = {'main': gpu_id}
        for gid in six.moves.xrange(1, ngpu):
            devices['sub_%d' % gid] = gid
        logging.info('multi gpu calculation (#gpus = %d).' % ngpu)
        logging.info('batch size is automatically increased (%d -> %d)' % (
            args.batch_size, args.batch_size * args.ngpu))
    else:
        gpu_id = -1
        logging.info('cpu calculation')

    # Setup an optimizer
    if args.opt == 'adadelta':
        optimizer = chainer.optimizers.AdaDelta(eps=args.eps)
    elif args.opt == 'adam':
        optimizer = chainer.optimizers.Adam()
    elif args.opt == 'noam':
        optimizer = chainer.optimizers.Adam(alpha=0, beta1=0.9, beta2=0.98, eps=1e-9)
    else:
        raise NotImplementedError('args.opt={}'.format(args.opt))

    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.GradientClipping(args.grad_clip))

    # Setup Training Extensions
    if 'transformer' in args.model_module:
        from espnet.nets.chainer_backend.transformer.training import CustomConverter
        from espnet.nets.chainer_backend.transformer.training import CustomParallelUpdater
        from espnet.nets.chainer_backend.transformer.training import CustomUpdater
    else:
        from espnet.nets.chainer_backend.rnn.training import CustomConverter
        from espnet.nets.chainer_backend.rnn.training import CustomParallelUpdater
        from espnet.nets.chainer_backend.rnn.training import CustomUpdater

    # Setup a converter
    converter = CustomConverter(subsampling_factor=model.subsample[0])

    # read json data
    with open(args.train_json, 'rb') as f:
        train_json = json.load(f)['utts']
    with open(args.valid_json, 'rb') as f:
        valid_json = json.load(f)['utts']

    # set up training iterator and updater
    load_tr = LoadInputsAndTargets(
        mode='asr', load_output=True, preprocess_conf=args.preprocess_conf,
        preprocess_args={'train': True}  # Switch the mode of preprocessing
    )
    load_cv = LoadInputsAndTargets(
        mode='asr', load_output=True, preprocess_conf=args.preprocess_conf,
        preprocess_args={'train': False}  # Switch the mode of preprocessing
    )

    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0
    accum_grad = args.accum_grad
    if ngpu <= 1:
        # make minibatch list (variable length)
        train = make_batchset(train_json, args.batch_size,
                              args.maxlen_in, args.maxlen_out, args.minibatches,
                              min_batch_size=args.ngpu if args.ngpu > 1 else 1,
                              shortest_first=use_sortagrad,
                              count=args.batch_count,
                              batch_bins=args.batch_bins,
                              batch_frames_in=args.batch_frames_in,
                              batch_frames_out=args.batch_frames_out,
                              batch_frames_inout=args.batch_frames_inout)
        # hack to make batchsize argument as 1
        # actual batchsize is included in a list
        if args.n_iter_processes > 0:
            train_iters = [ToggleableShufflingMultiprocessIterator(
                TransformDataset(train, load_tr),
                batch_size=1, n_processes=args.n_iter_processes, n_prefetch=8, maxtasksperchild=20,
                shuffle=not use_sortagrad)]
        else:
            train_iters = [ToggleableShufflingSerialIterator(
                TransformDataset(train, load_tr),
                batch_size=1, shuffle=not use_sortagrad)]

        # set up updater
        updater = CustomUpdater(
            train_iters[0], optimizer, converter=converter, device=gpu_id, accum_grad=accum_grad)
    else:
        if args.batch_count not in (""auto"", ""seq"") and args.batch_size == 0:
            raise NotImplementedError(""--batch-count 'bin' and 'frame' are not implemented in chainer multi gpu"")
        # set up minibatches
        train_subsets = []
        for gid in six.moves.xrange(ngpu):
            # make subset
            train_json_subset = {k: v for i, (k, v) in enumerate(train_json.items())
                                 if i % ngpu == gid}
            # make minibatch list (variable length)
            train_subsets += [make_batchset(train_json_subset, args.batch_size,
                                            args.maxlen_in, args.maxlen_out, args.minibatches)]

        # each subset must have same length for MultiprocessParallelUpdater
        maxlen = max([len(train_subset) for train_subset in train_subsets])
        for train_subset in train_subsets:
            if maxlen != len(train_subset):
                for i in six.moves.xrange(maxlen - len(train_subset)):
                    train_subset += [train_subset[i]]

        # hack to make batchsize argument as 1
        # actual batchsize is included in a list
        if args.n_iter_processes > 0:
            train_iters = [ToggleableShufflingMultiprocessIterator(
                TransformDataset(train_subsets[gid], load_tr),
                batch_size=1, n_processes=args.n_iter_processes, n_prefetch=8, maxtasksperchild=20,
                shuffle=not use_sortagrad)
                for gid in six.moves.xrange(ngpu)]
        else:
            train_iters = [ToggleableShufflingSerialIterator(
                TransformDataset(train_subsets[gid], load_tr),
                batch_size=1, shuffle=not use_sortagrad)
                for gid in six.moves.xrange(ngpu)]

        # set up updater
        updater = CustomParallelUpdater(
            train_iters, optimizer, converter=converter, devices=devices)

    # Set up a trainer
    trainer = training.Trainer(
        updater, (args.epochs, 'epoch'), out=args.outdir)

    if use_sortagrad:
        trainer.extend(ShufflingEnabler(train_iters),
                       trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, 'epoch'))
    if args.opt == 'noam':
        from espnet.nets.chainer_backend.transformer.training import VaswaniRule
        trainer.extend(VaswaniRule('alpha', d=args.adim, warmup_steps=args.transformer_warmup_steps,
                                   scale=args.transformer_lr), trigger=(1, 'iteration'))
    # Resume from a snapshot
    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    # set up validation iterator
    valid = make_batchset(valid_json, args.batch_size,
                          args.maxlen_in, args.maxlen_out, args.minibatches,
                          min_batch_size=args.ngpu if args.ngpu > 1 else 1,
                          count=args.batch_count,
                          batch_bins=args.batch_bins,
                          batch_frames_in=args.batch_frames_in,
                          batch_frames_out=args.batch_frames_out,
                          batch_frames_inout=args.batch_frames_inout)

    if args.n_iter_processes > 0:
        valid_iter = chainer.iterators.MultiprocessIterator(
            TransformDataset(valid, load_cv),
            batch_size=1, repeat=False, shuffle=False,
            n_processes=args.n_iter_processes, n_prefetch=8, maxtasksperchild=20)
    else:
        valid_iter = chainer.iterators.SerialIterator(
            TransformDataset(valid, load_cv),
            batch_size=1, repeat=False, shuffle=False)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(BaseEvaluator(
        valid_iter, model, converter=converter, device=gpu_id))

    # Save attention weight each epoch
    if args.num_save_attention > 0 and args.mtlalpha != 1.0:
        data = sorted(list(valid_json.items())[:args.num_save_attention],
                      key=lambda x: int(x[1]['input'][0]['shape'][1]), reverse=True)
        if hasattr(model, ""module""):
            att_vis_fn = model.module.calculate_all_attentions
            plot_class = model.module.attention_plot_class
        else:
            att_vis_fn = model.calculate_all_attentions
            plot_class = model.attention_plot_class
        logging.info('Using custom PlotAttentionReport')
        att_reporter = plot_class(
            att_vis_fn, data, args.outdir + ""/att_ws"",
            converter=converter, transform=load_cv, device=gpu_id)
        trainer.extend(att_reporter, trigger=(1, 'epoch'))
    else:
        att_reporter = None

    # Take a snapshot for each specified epoch
    trainer.extend(extensions.snapshot(filename='snapshot.ep.{.updater.epoch}'), trigger=(1, 'epoch'))

    # Make a plot for training and validation values
    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss',
                                          'main/loss_ctc', 'validation/main/loss_ctc',
                                          'main/loss_att', 'validation/main/loss_att'],
                                         'epoch', file_name='loss.png'))
    trainer.extend(extensions.PlotReport(['main/acc', 'validation/main/acc'],
                                         'epoch', file_name='acc.png'))

    # Save best models
    trainer.extend(extensions.snapshot_object(model, 'model.loss.best'),
                   trigger=training.triggers.MinValueTrigger('validation/main/loss'))
    if mtl_mode != 'ctc':
        trainer.extend(extensions.snapshot_object(model, 'model.acc.best'),
                       trigger=training.triggers.MaxValueTrigger('validation/main/acc'))

    # epsilon decay in the optimizer
    if args.opt == 'adadelta':
        if args.criterion == 'acc' and mtl_mode != 'ctc':
            trainer.extend(restore_snapshot(model, args.outdir + '/model.acc.best'),
                           trigger=CompareValueTrigger(
                               'validation/main/acc',
                               lambda best_value, current_value: best_value > current_value))
            trainer.extend(adadelta_eps_decay(args.eps_decay),
                           trigger=CompareValueTrigger(
                               'validation/main/acc',
                               lambda best_value, current_value: best_value > current_value))
        elif args.criterion == 'loss':
            trainer.extend(restore_snapshot(model, args.outdir + '/model.loss.best'),
                           trigger=CompareValueTrigger(
                               'validation/main/loss',
                               lambda best_value, current_value: best_value < current_value))
            trainer.extend(adadelta_eps_decay(args.eps_decay),
                           trigger=CompareValueTrigger(
                               'validation/main/loss',
                               lambda best_value, current_value: best_value < current_value))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport(trigger=(args.report_interval_iters, 'iteration')))
    report_keys = ['epoch', 'iteration', 'main/loss', 'main/loss_ctc', 'main/loss_att',
                   'validation/main/loss', 'validation/main/loss_ctc', 'validation/main/loss_att',
                   'main/acc', 'validation/main/acc', 'elapsed_time']
    if args.opt == 'adadelta':
        trainer.extend(extensions.observe_value(
            'eps', lambda trainer: trainer.updater.get_optimizer('main').eps),
            trigger=(args.report_interval_iters, 'iteration'))
        report_keys.append('eps')
    trainer.extend(extensions.PrintReport(
        report_keys), trigger=(args.report_interval_iters, 'iteration'))

    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))

    set_early_stop(trainer, args)
    if args.tensorboard_dir is not None and args.tensorboard_dir != """":
        writer = SummaryWriter(args.tensorboard_dir)
        trainer.extend(TensorboardLogger(writer, att_reporter),
                       trigger=(args.report_interval_iters, 'iteration'))

    # Run the training
    trainer.run()
    check_early_stop(trainer, args.epochs)
","def train(args):
    """"""Train with the given args.

    Args:
        args (namespace): The program arguments.

    """"""
    # display chainer version
    logging.info('chainer version = ' + chainer.__version__)

    set_deterministic_chainer(args)

    # check cuda and cudnn availability
    if not chainer.cuda.available:
        logging.warning('cuda is not available')
    if not chainer.cuda.cudnn_enabled:
        logging.warning('cudnn is not available')

    # get input and output dimension info
    with open(args.valid_json, 'rb') as f:
        valid_json = json.load(f)['utts']
    utts = list(valid_json.keys())
    idim = int(valid_json[utts[0]]['input'][0]['shape'][1])
    odim = int(valid_json[utts[0]]['output'][0]['shape'][1])
    logging.info('#input dims : ' + str(idim))
    logging.info('#output dims: ' + str(odim))

    # specify attention, CTC, hybrid mode
    if args.mtlalpha == 1.0:
        mtl_mode = 'ctc'
        logging.info('Pure CTC mode')
    elif args.mtlalpha == 0.0:
        mtl_mode = 'att'
        logging.info('Pure attention mode')
    else:
        mtl_mode = 'mtl'
        logging.info('Multitask learning mode')

    # specify model architecture
    logging.info('import model module: ' + args.model_module)
    model_class = dynamic_import(args.model_module)
    model = model_class(idim, odim, args, flag_return=False)
    assert isinstance(model, ASRInterface)

    # write model config
    if not os.path.exists(args.outdir):
        os.makedirs(args.outdir)
    model_conf = args.outdir + '/model.json'
    with open(model_conf, 'wb') as f:
        logging.info('writing a model config file to ' + model_conf)
        f.write(json.dumps((idim, odim, vars(args)),
                           indent=4, ensure_ascii=False, sort_keys=True).encode('utf_8'))
    for key in sorted(vars(args).keys()):
        logging.info('ARGS: ' + key + ': ' + str(vars(args)[key]))

    # Set gpu
    ngpu = args.ngpu
    if ngpu == 1:
        gpu_id = 0
        # Make a specified GPU current
        chainer.cuda.get_device_from_id(gpu_id).use()
        model.to_gpu()  # Copy the model to the GPU
        logging.info('single gpu calculation.')
    elif ngpu > 1:
        gpu_id = 0
        devices = {'main': gpu_id}
        for gid in six.moves.xrange(1, ngpu):
            devices['sub_%d' % gid] = gid
        logging.info('multi gpu calculation (#gpus = %d).' % ngpu)
        logging.info('batch size is automatically increased (%d -> %d)' % (
            args.batch_size, args.batch_size * args.ngpu))
    else:
        gpu_id = -1
        logging.info('cpu calculation')

    # Setup an optimizer
    if args.opt == 'adadelta':
        optimizer = chainer.optimizers.AdaDelta(eps=args.eps)
    elif args.opt == 'adam':
        optimizer = chainer.optimizers.Adam()
    elif args.opt == 'noam':
        optimizer = chainer.optimizers.Adam(alpha=0, beta1=0.9, beta2=0.98, eps=1e-9)
    else:
        raise NotImplementedError('args.opt={}'.format(args.opt))

    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.GradientClipping(args.grad_clip))

    # Setup Training Extensions
    if 'transformer' in args.model_module:
        from espnet.nets.chainer_backend.transformer.training import CustomConverter
        from espnet.nets.chainer_backend.transformer.training import CustomParallelUpdater
        from espnet.nets.chainer_backend.transformer.training import CustomUpdater
    else:
        from espnet.nets.chainer_backend.rnn.training import CustomConverter
        from espnet.nets.chainer_backend.rnn.training import CustomParallelUpdater
        from espnet.nets.chainer_backend.rnn.training import CustomUpdater

    # Setup a converter
    converter = CustomConverter(subsampling_factor=model.subsample[0])

    # read json data
    with open(args.train_json, 'rb') as f:
        train_json = json.load(f)['utts']
    with open(args.valid_json, 'rb') as f:
        valid_json = json.load(f)['utts']

    # set up training iterator and updater
    load_tr = LoadInputsAndTargets(
        mode='asr', load_output=True, preprocess_conf=args.preprocess_conf,
        preprocess_args={'train': True}  # Switch the mode of preprocessing
    )
    load_cv = LoadInputsAndTargets(
        mode='asr', load_output=True, preprocess_conf=args.preprocess_conf,
        preprocess_args={'train': False}  # Switch the mode of preprocessing
    )

    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0
    accum_grad = args.accum_grad
    if ngpu <= 1:
        # make minibatch list (variable length)
        train = make_batchset(train_json, args.batch_size,
                              args.maxlen_in, args.maxlen_out, args.minibatches,
                              min_batch_size=args.ngpu if args.ngpu > 1 else 1,
                              shortest_first=use_sortagrad,
                              count=args.batch_count,
                              batch_bins=args.batch_bins,
                              batch_frames_in=args.batch_frames_in,
                              batch_frames_out=args.batch_frames_out,
                              batch_frames_inout=args.batch_frames_inout)
        # hack to make batchsize argument as 1
        # actual batchsize is included in a list
        if args.n_iter_processes > 0:
            train_iters = [ToggleableShufflingMultiprocessIterator(
                TransformDataset(train, load_tr),
                batch_size=1, n_processes=args.n_iter_processes, n_prefetch=8, maxtasksperchild=20,
                shuffle=not use_sortagrad)]
        else:
            train_iters = [ToggleableShufflingSerialIterator(
                TransformDataset(train, load_tr),
                batch_size=1, shuffle=not use_sortagrad)]

        # set up updater
        updater = CustomUpdater(
            train_iters[0], optimizer, converter=converter, device=gpu_id, accum_grad=accum_grad)
    else:
        if args.batch_count not in (""auto"", ""seq"") and args.batch_size == 0:
            raise NotImplementedError(""--batch-count 'bin' and 'frame' are not implemented in chainer multi gpu"")
        # set up minibatches
        train_subsets = []
        for gid in six.moves.xrange(ngpu):
            # make subset
            train_json_subset = {k: v for i, (k, v) in enumerate(train_json.items())
                                 if i % ngpu == gid}
            # make minibatch list (variable length)
            train_subsets += [make_batchset(train_json_subset, args.batch_size,
                                            args.maxlen_in, args.maxlen_out, args.minibatches)]

        # each subset must have same length for MultiprocessParallelUpdater
        maxlen = max([len(train_subset) for train_subset in train_subsets])
        for train_subset in train_subsets:
            if maxlen != len(train_subset):
                for i in six.moves.xrange(maxlen - len(train_subset)):
                    train_subset += [train_subset[i]]

        # hack to make batchsize argument as 1
        # actual batchsize is included in a list
        if args.n_iter_processes > 0:
            train_iters = [ToggleableShufflingMultiprocessIterator(
                TransformDataset(train_subsets[gid], load_tr),
                batch_size=1, n_processes=args.n_iter_processes, n_prefetch=8, maxtasksperchild=20,
                shuffle=not use_sortagrad)
                for gid in six.moves.xrange(ngpu)]
        else:
            train_iters = [ToggleableShufflingSerialIterator(
                TransformDataset(train_subsets[gid], load_tr),
                batch_size=1, shuffle=not use_sortagrad)
                for gid in six.moves.xrange(ngpu)]

        # set up updater
        updater = CustomParallelUpdater(
            train_iters, optimizer, converter=converter, devices=devices)

    # Set up a trainer
    trainer = training.Trainer(
        updater, (args.epochs, 'epoch'), out=args.outdir)

    if use_sortagrad:
        trainer.extend(ShufflingEnabler(train_iters),
                       trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, 'epoch'))
    if args.opt == 'noam':
        from espnet.nets.chainer_backend.transformer.training import VaswaniRule
        trainer.extend(VaswaniRule('alpha', d=args.adim, warmup_steps=args.transformer_warmup_steps,
                                   scale=args.transformer_lr), trigger=(1, 'iteration'))
    # Resume from a snapshot
    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    # set up validation iterator
    valid = make_batchset(valid_json, args.batch_size,
                          args.maxlen_in, args.maxlen_out, args.minibatches,
                          min_batch_size=args.ngpu if args.ngpu > 1 else 1,
                          count=args.batch_count,
                          batch_bins=args.batch_bins,
                          batch_frames_in=args.batch_frames_in,
                          batch_frames_out=args.batch_frames_out,
                          batch_frames_inout=args.batch_frames_inout)

    if args.n_iter_processes > 0:
        valid_iter = chainer.iterators.MultiprocessIterator(
            TransformDataset(valid, load_cv),
            batch_size=1, repeat=False, shuffle=False,
            n_processes=args.n_iter_processes, n_prefetch=8, maxtasksperchild=20)
    else:
        valid_iter = chainer.iterators.SerialIterator(
            TransformDataset(valid, load_cv),
            batch_size=1, repeat=False, shuffle=False)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(BaseEvaluator(
        valid_iter, model, converter=converter, device=gpu_id))

    # Save attention weight each epoch
    if args.num_save_attention > 0 and args.mtlalpha != 1.0:
        data = sorted(list(valid_json.items())[:args.num_save_attention],
                      key=lambda x: int(x[1]['input'][0]['shape'][1]), reverse=True)
        if hasattr(model, ""module""):
            att_vis_fn = model.module.calculate_all_attentions
            plot_class = model.module.attention_plot_class
        else:
            att_vis_fn = model.calculate_all_attentions
            plot_class = model.attention_plot_class
        logging.info('Using custom PlotAttentionReport')
        att_reporter = plot_class(
            att_vis_fn, data, args.outdir + ""/att_ws"",
            converter=converter, transform=load_cv, device=gpu_id)
        trainer.extend(att_reporter, trigger=(1, 'epoch'))
    else:
        att_reporter = None

    # Take a snapshot for each specified epoch
    trainer.extend(extensions.snapshot(filename='snapshot.ep.{.updater.epoch}'), trigger=(1, 'epoch'))

    # Make a plot for training and validation values
    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss',
                                          'main/loss_ctc', 'validation/main/loss_ctc',
                                          'main/loss_att', 'validation/main/loss_att'],
                                         'epoch', file_name='loss.png'))
    trainer.extend(extensions.PlotReport(['main/acc', 'validation/main/acc'],
                                         'epoch', file_name='acc.png'))

    # Save best models
    trainer.extend(extensions.snapshot_object(model, 'model.loss.best'),
                   trigger=training.triggers.MinValueTrigger('validation/main/loss'))
    if mtl_mode != 'ctc':
        trainer.extend(extensions.snapshot_object(model, 'model.acc.best'),
                       trigger=training.triggers.MaxValueTrigger('validation/main/acc'))

    # epsilon decay in the optimizer
    if args.opt == 'adadelta':
        if args.criterion == 'acc' and mtl_mode != 'ctc':
            trainer.extend(restore_snapshot(model, args.outdir + '/model.acc.best'),
                           trigger=CompareValueTrigger(
                               'validation/main/acc',
                               lambda best_value, current_value: best_value > current_value))
            trainer.extend(adadelta_eps_decay(args.eps_decay),
                           trigger=CompareValueTrigger(
                               'validation/main/acc',
                               lambda best_value, current_value: best_value > current_value))
        elif args.criterion == 'loss':
            trainer.extend(restore_snapshot(model, args.outdir + '/model.loss.best'),
                           trigger=CompareValueTrigger(
                               'validation/main/loss',
                               lambda best_value, current_value: best_value < current_value))
            trainer.extend(adadelta_eps_decay(args.eps_decay),
                           trigger=CompareValueTrigger(
                               'validation/main/loss',
                               lambda best_value, current_value: best_value < current_value))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport(trigger=(args.report_interval_iters, 'iteration')))
    report_keys = ['epoch', 'iteration', 'main/loss', 'main/loss_ctc', 'main/loss_att',
                   'validation/main/loss', 'validation/main/loss_ctc', 'validation/main/loss_att',
                   'main/acc', 'validation/main/acc', 'elapsed_time']
    if args.opt == 'adadelta':
        trainer.extend(extensions.observe_value(
            'eps', lambda trainer: trainer.updater.get_optimizer('main').eps),
            trigger=(args.report_interval_iters, 'iteration'))
        report_keys.append('eps')
    trainer.extend(extensions.PrintReport(
        report_keys), trigger=(args.report_interval_iters, 'iteration'))

    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))

    set_early_stop(trainer, args)
    if args.tensorboard_dir is not None and args.tensorboard_dir != """":
        writer = SummaryWriter(args.tensorboard_dir)
        trainer.extend(TensorboardLogger(writer, att_reporter),
                       trigger=(args.report_interval_iters, 'iteration'))

    # Run the training
    trainer.run()
    check_early_stop(trainer, args.epochs)
",https://github.com/espnet/espnet/issues/1093,"CWE-236: Improper Handling of Undefined Parameters, CWE-166: Improper Handling of Missing Special Element, CWE-248: Uncaught Exception",args does not have an attribute 'atype'. The case is unhandled where args object does not have an attribute atype e.g. in __getattr__,espnet/asr/chainer_backend/asr.py,train,"[29, 30]","Traceback (most recent call last):
File ""/export/db/espnet/egs/voxforge/asr1/../../../espnet/bin/asr_train.py"", line 341, in <module>
main(sys.argv[1:])
File ""/export/db/espnet/egs/voxforge/asr1/../../../espnet/bin/asr_train.py"", line 325, in main
train(args)
File ""/export/db/espnet/espnet/asr/chainer_backend/asr.py"", line 81, in train
if args.atype not in ['noatt', 'dot', 'location']:
AttributeError: 'Namespace' object has no attribute 'atype'",AttributeError,buggy_snippets_files/9157244cccea8d97c81339cb24fdb107b86b53099f215c603eb38e27c5fac890_before_merge.py,buggy_snippets_files/9157244cccea8d97c81339cb24fdb107b86b53099f215c603eb38e27c5fac890_after_merge.py
1829,"def att_for(args):
    """"""Returns an attention layer given the program arguments.

    Args:
        args (Namespace): The arguments.

    Returns:
        chainer.Chain: The corresponding attention module.

    """"""
    if args.atype == 'dot':
        att = AttDot(args.eprojs, args.dunits, args.adim)
    elif args.atype == 'location':
        att = AttLoc(args.eprojs, args.dunits,
                     args.adim, args.aconv_chans, args.aconv_filts)
    elif args.atype == 'noatt':
        att = NoAtt()
    else:
        logging.error(
            ""Error: need to specify an appropriate attention architecture"")
        sys.exit()
    return att
","def att_for(args):
    """"""Returns an attention layer given the program arguments.

    Args:
        args (Namespace): The arguments.

    Returns:
        chainer.Chain: The corresponding attention module.

    """"""
    if args.atype == 'dot':
        att = AttDot(args.eprojs, args.dunits, args.adim)
    elif args.atype == 'location':
        att = AttLoc(args.eprojs, args.dunits,
                     args.adim, args.aconv_chans, args.aconv_filts)
    elif args.atype == 'noatt':
        att = NoAtt()
    else:
        raise NotImplementedError('chainer supports only noatt, dot, and location attention.')
    return att
",https://github.com/espnet/espnet/issues/1093,CWE-691: Insufficient Control Flow Management,The case is not properly handled where args.atype takes some unprescribed value,espnet/nets/chainer_backend/rnn/attentions.py,att_for,"[11,21]","Traceback (most recent call last):
File ""/export/db/espnet/egs/voxforge/asr1/../../../espnet/bin/asr_train.py"", line 341, in <module>
main(sys.argv[1:])
File ""/export/db/espnet/egs/voxforge/asr1/../../../espnet/bin/asr_train.py"", line 325, in main
train(args)
File ""/export/db/espnet/espnet/asr/chainer_backend/asr.py"", line 81, in train
if args.atype not in ['noatt', 'dot', 'location']:
AttributeError: 'Namespace' object has no attribute 'atype'",AttributeError,buggy_snippets_files/df90f510341c203dbccc4f4218bef8bd062c5ef62c855962af9b4b68e37e97c5_before_merge.py,buggy_snippets_files/df90f510341c203dbccc4f4218bef8bd062c5ef62c855962af9b4b68e37e97c5_after_merge.py
1832,"    def forward(self, xs, ilens, ys, labels, olens, spembs=None, spcs=None, *args, **kwargs):
        """"""Calculate forward propagation.

        Args:
            xs (Tensor): Batch of padded character ids (B, Tmax).
            ilens (LongTensor): Batch of lengths of each input batch (B,).
            ys (Tensor): Batch of padded target features (B, Lmax, odim).
            olens (LongTensor): Batch of the lengths of each target (B,).
            spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim).
            spcs (Tensor, optional): Batch of groundtruth spectrograms (B, Lmax, spc_dim).

        Returns:
            Tensor: Loss value.

        """"""
        # remove unnecessary padded part (for multi-gpus)
        max_in = max(ilens)
        max_out = max(olens)
        if max_in != xs.shape[1]:
            xs = xs[:, :max_in]
        if max_out != ys.shape[1]:
            ys = ys[:, :max_out]
            labels = labels[:, :max_out]

        # calculate tacotron2 outputs
        hs, hlens = self.enc(xs, ilens)
        if self.spk_embed_dim is not None:
            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)
            hs = torch.cat([hs, spembs], dim=-1)
        after_outs, before_outs, logits, att_ws = self.dec(hs, hlens, ys)

        # modifiy mod part of groundtruth
        if self.reduction_factor > 1:
            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])
            max_out = max(olens)
            ys = ys[:, :max_out]
            labels = labels[:, :max_out]
            labels[:, -1] = 1.0  # make sure at least one frame has 1

        # caluculate taco2 loss
        l1_loss, mse_loss, bce_loss = self.taco2_loss(
            after_outs, before_outs, logits, ys, labels, olens)
        loss = l1_loss + mse_loss + bce_loss
        report_keys = [
            {'l1_loss': l1_loss.item()},
            {'mse_loss': mse_loss.item()},
            {'bce_loss': bce_loss.item()},
        ]

        # caluculate attention loss
        if self.use_guided_attn_loss:
            attn_loss = self.attn_loss(att_ws, ilens, olens)
            loss = loss + attn_loss
            report_keys += [
                {'attn_loss': attn_loss.item()},
            ]

        # caluculate cbhg loss
        if self.use_cbhg:
            # remove unnecessary padded part (for multi-gpus)
            if max_out != spcs.shape[1]:
                spcs = spcs[:, :max_out]

            # caluculate cbhg outputs & loss and report them
            cbhg_outs, _ = self.cbhg(after_outs, olens)
            cbhg_l1_loss, cbhg_mse_loss = self.cbhg_loss(cbhg_outs, spcs, olens)
            loss = loss + cbhg_l1_loss + cbhg_mse_loss
            report_keys += [
                {'cbhg_l1_loss': cbhg_l1_loss.item()},
                {'cbhg_mse_loss': cbhg_mse_loss.item()},
            ]

        report_keys += [{'loss': loss.item()}]
        self.reporter.report(report_keys)

        return loss
","    def forward(self, xs, ilens, ys, labels, olens, spembs=None, spcs=None, *args, **kwargs):
        """"""Calculate forward propagation.

        Args:
            xs (Tensor): Batch of padded character ids (B, Tmax).
            ilens (LongTensor): Batch of lengths of each input batch (B,).
            ys (Tensor): Batch of padded target features (B, Lmax, odim).
            olens (LongTensor): Batch of the lengths of each target (B,).
            spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim).
            spcs (Tensor, optional): Batch of groundtruth spectrograms (B, Lmax, spc_dim).

        Returns:
            Tensor: Loss value.

        """"""
        # remove unnecessary padded part (for multi-gpus)
        max_in = max(ilens)
        max_out = max(olens)
        if max_in != xs.shape[1]:
            xs = xs[:, :max_in]
        if max_out != ys.shape[1]:
            ys = ys[:, :max_out]
            labels = labels[:, :max_out]

        # calculate tacotron2 outputs
        hs, hlens = self.enc(xs, ilens)
        if self.spk_embed_dim is not None:
            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)
            hs = torch.cat([hs, spembs], dim=-1)
        after_outs, before_outs, logits, att_ws = self.dec(hs, hlens, ys)

        # modifiy mod part of groundtruth
        if self.reduction_factor > 1:
            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])
            max_out = max(olens)
            ys = ys[:, :max_out]
            labels = labels[:, :max_out]
            labels[:, -1] = 1.0  # make sure at least one frame has 1

        # caluculate taco2 loss
        l1_loss, mse_loss, bce_loss = self.taco2_loss(
            after_outs, before_outs, logits, ys, labels, olens)
        loss = l1_loss + mse_loss + bce_loss
        report_keys = [
            {'l1_loss': l1_loss.item()},
            {'mse_loss': mse_loss.item()},
            {'bce_loss': bce_loss.item()},
        ]

        # caluculate attention loss
        if self.use_guided_attn_loss:
            # NOTE(kan-bayashi): length of output for auto-regressive input will be changed when r > 1
            if self.reduction_factor > 1:
                olens_in = olens.new([olen // self.reduction_factor for olen in olens])
            else:
                olens_in = olens
            attn_loss = self.attn_loss(att_ws, ilens, olens_in)
            loss = loss + attn_loss
            report_keys += [
                {'attn_loss': attn_loss.item()},
            ]

        # caluculate cbhg loss
        if self.use_cbhg:
            # remove unnecessary padded part (for multi-gpus)
            if max_out != spcs.shape[1]:
                spcs = spcs[:, :max_out]

            # caluculate cbhg outputs & loss and report them
            cbhg_outs, _ = self.cbhg(after_outs, olens)
            cbhg_l1_loss, cbhg_mse_loss = self.cbhg_loss(cbhg_outs, spcs, olens)
            loss = loss + cbhg_l1_loss + cbhg_mse_loss
            report_keys += [
                {'cbhg_l1_loss': cbhg_l1_loss.item()},
                {'cbhg_mse_loss': cbhg_mse_loss.item()},
            ]

        report_keys += [{'loss': loss.item()}]
        self.reporter.report(report_keys)

        return loss
",https://github.com/espnet/espnet/issues/1086,"		
CWE-168: Improper Handling of Inconsistent Special Elements",arrays self.guided_attn_masks and att_ws are not properly resized in GuidedAttentionLoss.forward method (see https://github.com/espnet/espnet/pull/1087/commits/540cda6102edad90bcbff024c38036733e9ec86c),espnet/nets/pytorch_backend/e2e_tts_tacotron2.py,Tacotron2.forward,[52],"[JException in main training loop: The size of tensor a (864) must match the size of tensor b (432) at non-singleton dimension 1
Traceback (most recent call last):
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py"", line 315, in run
update()
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/tts/pytorch_backend/tts.py"", line 153, in update
self.update_core()
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/tts/pytorch_backend/tts.py"", line 134, in update_core
loss = self.model(**x).mean() / self.accum_grad
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__
result = self.forward(*input, **kwargs)
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py"", line 529, in forward
attn_loss = self.attn_loss(att_ws, ilens, olens)
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__
result = self.forward(*input, **kwargs)
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py"", line 68, in forward
losses = self.guided_attn_masks * att_ws
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
File ""/home/abelab/k_inoue/work/tool/espnet/egs/blizzard17/tts1/../../../espnet/bin/tts_train.py"", line 179, in <module>
main(sys.argv[1:])
File ""/home/abelab/k_inoue/work/tool/espnet/egs/blizzard17/tts1/../../../espnet/bin/tts_train.py"", line 173, in main
train(args)
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/tts/pytorch_backend/tts.py"", line 424, in train
trainer.run()
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py"", line 329, in run
six.reraise(*sys.exc_info())
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/six.py"", line 693, in reraise
raise value
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py"", line 315, in run
update()
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/tts/pytorch_backend/tts.py"", line 153, in update
self.update_core()
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/tts/pytorch_backend/tts.py"", line 134, in update_core
loss = self.model(**x).mean() / self.accum_grad
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__
result = self.forward(*input, **kwargs)
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py"", line 529, in forward
attn_loss = self.attn_loss(att_ws, ilens, olens)
File ""/home/abelab/k_inoue/work/tool/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__
result = self.forward(*input, **kwargs)
File ""/home/abelab/k_inoue/work/tool/espnet/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py"", line 68, in forward
losses = self.guided_attn_masks * att_ws
RuntimeError: The size of tensor a (864) must match the size of tensor b (432) at non-singleton dimension 1",RuntimeError,buggy_snippets_files/6162c65b28c2302bb1a573f6d05c7c6ab2e525d3515020b2906b7fd13daa4a0d_before_merge.py,buggy_snippets_files/6162c65b28c2302bb1a573f6d05c7c6ab2e525d3515020b2906b7fd13daa4a0d_after_merge.py
1875,"def train(args):
    # display chainer version
    logging.info('chainer version = ' + chainer.__version__)

    # seed setting (chainer seed may not need it)
    nseed = args.seed
    os.environ['CHAINER_SEED'] = str(nseed)
    logging.info('chainer seed = ' + os.environ['CHAINER_SEED'])

    # debug mode setting
    # 0 would be fastest, but 1 seems to be reasonable
    # by considering reproducability
    # revmoe type check
    if args.debugmode < 2:
        chainer.config.type_check = False
        logging.info('chainer type check is disabled')
    # use determinisitic computation or not
    if args.debugmode < 1:
        chainer.config.cudnn_deterministic = False
        logging.info('chainer cudnn deterministic is disabled')
    else:
        chainer.config.cudnn_deterministic = True

    # check cuda and cudnn availability
    if not chainer.cuda.available:
        logging.warning('cuda is not available')
    if not chainer.cuda.cudnn_enabled:
        logging.warning('cudnn is not available')

    with open(args.train_label, 'rb') as f:
        train = np.array([args.char_list_dict[char]
                          if char in args.char_list_dict else args.char_list_dict['<unk>']
                          for char in f.readline().decode('utf-8').split()], dtype=np.int32)
    with open(args.valid_label, 'rb') as f:
        valid = np.array([args.char_list_dict[char]
                          if char in args.char_list_dict else args.char_list_dict['<unk>']
                          for char in f.readline().decode('utf-8').split()], dtype=np.int32)

    logging.info('#vocab = ' + str(args.n_vocab))
    logging.info('#words in the training data = ' + str(len(train)))
    logging.info('#words in the validation data = ' + str(len(valid)))
    logging.info('#iterations per epoch = ' + str(len(train) // (args.batchsize * args.bproplen)))
    logging.info('#total iterations = ' + str(args.epoch * len(train) // (args.batchsize * args.bproplen)))

    # Create the dataset iterators
    train_iter = ParallelSequentialIterator(train, args.batchsize)
    valid_iter = ParallelSequentialIterator(valid, args.batchsize, repeat=False)

    # Prepare an RNNLM model
    rnn = RNNLM(args.n_vocab, args.unit)
    model = ClassifierWithState(rnn)
    model.compute_accuracy = False  # we only want the perplexity
    if args.ngpu > 1:
        logging.warn(""currently, multi-gpu is not supported. use single gpu."")
    if args.ngpu > 0:
        # Make the specified GPU current
        gpu_id = 0
        chainer.cuda.get_device_from_id(gpu_id).use()
        model.to_gpu()

    # Save model conf to json
    model_conf = args.outdir + '/model.json'
    with open(model_conf, 'wb') as f:
        logging.info('writing a model config file to ' + model_conf)
        f.write(json.dumps(vars(args), indent=4, sort_keys=True).encode('utf_8'))

    # Set up an optimizer
    optimizer = chainer.optimizers.SGD(lr=1.0)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.GradientClipping(args.gradclip))

    def evaluate(model, iter, bproplen=100):
        # Evaluation routine to be used for validation and test.
        model.predictor.train = False
        evaluator = model.copy()  # to use different state
        state = None
        evaluator.predictor.train = False  # dropout does nothing
        sum_perp = 0
        data_count = 0
        for batch in copy.copy(iter):
            x, t = convert.concat_examples(batch, gpu_id)
            state, loss = evaluator(state, x, t)
            sum_perp += loss.data
            if data_count % bproplen == 0:
                loss.unchain_backward()  # Truncate the graph
            data_count += 1
        model.predictor.train = True
        return np.exp(float(sum_perp) / data_count)

    sum_perp = 0
    count = 0
    iteration = 0
    epoch_now = 0
    best_valid = 100000000
    state = None
    while train_iter.epoch < args.epoch:
        loss = 0
        iteration += 1
        # Progress the dataset iterator for bprop_len words at each iteration.
        for i in range(args.bproplen):
            # Get the next batch (a list of tuples of two word IDs)
            batch = train_iter.__next__()
            # Concatenate the word IDs to matrices and send them to the device
            # self.converter does this job
            # (it is chainer.dataset.concat_examples by default)
            x, t = convert.concat_examples(batch, gpu_id)
            # Compute the loss at this time step and accumulate it
            state, loss_batch = optimizer.target(state, chainer.Variable(x), chainer.Variable(t))
            loss += loss_batch
            count += 1

        sum_perp += loss.data
        optimizer.target.cleargrads()  # Clear the parameter gradients
        loss.backward()  # Backprop
        loss.unchain_backward()  # Truncate the graph
        optimizer.update()  # Update the parameters

        if iteration % 100 == 0:
            logging.info('iteration: ' + str(iteration))
            logging.info('training perplexity: ' + str(np.exp(float(sum_perp) / count)))
            sum_perp = 0
            count = 0

        if train_iter.epoch > epoch_now:
            valid_perp = evaluate(model, valid_iter)
            logging.info('epoch: ' + str(train_iter.epoch))
            logging.info('validation perplexity: ' + str(valid_perp))

            # Save the model and the optimizer
            logging.info('save the model')
            serializers.save_npz(args.outdir + '/rnnlm.model.' + str(epoch_now), model)
            logging.info('save the optimizer')
            serializers.save_npz(args.outdir + '/rnnlm.state.' + str(epoch_now), optimizer)

            if valid_perp < best_valid:
                dest = args.outdir + '/rnnlm.model.best'
                if os.path.lexists(dest):
                    os.remove(dest)
                os.symlink('rnnlm.model.' + str(epoch_now), dest)
                best_valid = valid_perp

            epoch_now = train_iter.epoch
","def train(args):
    # display chainer version
    logging.info('chainer version = ' + chainer.__version__)

    # seed setting (chainer seed may not need it)
    nseed = args.seed
    os.environ['CHAINER_SEED'] = str(nseed)
    logging.info('chainer seed = ' + os.environ['CHAINER_SEED'])

    # debug mode setting
    # 0 would be fastest, but 1 seems to be reasonable
    # by considering reproducability
    # revmoe type check
    if args.debugmode < 2:
        chainer.config.type_check = False
        logging.info('chainer type check is disabled')
    # use determinisitic computation or not
    if args.debugmode < 1:
        chainer.config.cudnn_deterministic = False
        logging.info('chainer cudnn deterministic is disabled')
    else:
        chainer.config.cudnn_deterministic = True

    # check cuda and cudnn availability
    if not chainer.cuda.available:
        logging.warning('cuda is not available')
    if not chainer.cuda.cudnn_enabled:
        logging.warning('cudnn is not available')

    with open(args.train_label, 'rb') as f:
        train = np.array([args.char_list_dict[char]
                          if char in args.char_list_dict else args.char_list_dict['<unk>']
                          for char in f.readline().decode('utf-8').split()], dtype=np.int32)
    with open(args.valid_label, 'rb') as f:
        valid = np.array([args.char_list_dict[char]
                          if char in args.char_list_dict else args.char_list_dict['<unk>']
                          for char in f.readline().decode('utf-8').split()], dtype=np.int32)

    logging.info('#vocab = ' + str(args.n_vocab))
    logging.info('#words in the training data = ' + str(len(train)))
    logging.info('#words in the validation data = ' + str(len(valid)))
    logging.info('#iterations per epoch = ' + str(len(train) // (args.batchsize * args.bproplen)))
    logging.info('#total iterations = ' + str(args.epoch * len(train) // (args.batchsize * args.bproplen)))

    # Create the dataset iterators
    train_iter = ParallelSequentialIterator(train, args.batchsize)
    valid_iter = ParallelSequentialIterator(valid, args.batchsize, repeat=False)

    # Prepare an RNNLM model
    rnn = RNNLM(args.n_vocab, args.unit)
    model = ClassifierWithState(rnn)
    model.compute_accuracy = False  # we only want the perplexity
    if args.ngpu > 1:
        logging.warn(""currently, multi-gpu is not supported. use single gpu."")
    if args.ngpu > 0:
        # Make the specified GPU current
        gpu_id = 0
        chainer.cuda.get_device_from_id(gpu_id).use()
        model.to_gpu()
    else:
        gpu_id = -1

    # Save model conf to json
    model_conf = args.outdir + '/model.json'
    with open(model_conf, 'wb') as f:
        logging.info('writing a model config file to ' + model_conf)
        f.write(json.dumps(vars(args), indent=4, sort_keys=True).encode('utf_8'))

    # Set up an optimizer
    optimizer = chainer.optimizers.SGD(lr=1.0)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.GradientClipping(args.gradclip))

    def evaluate(model, iter, bproplen=100):
        # Evaluation routine to be used for validation and test.
        model.predictor.train = False
        evaluator = model.copy()  # to use different state
        state = None
        evaluator.predictor.train = False  # dropout does nothing
        sum_perp = 0
        data_count = 0
        for batch in copy.copy(iter):
            x, t = convert.concat_examples(batch, gpu_id)
            state, loss = evaluator(state, x, t)
            sum_perp += loss.data
            if data_count % bproplen == 0:
                loss.unchain_backward()  # Truncate the graph
            data_count += 1
        model.predictor.train = True
        return np.exp(float(sum_perp) / data_count)

    sum_perp = 0
    count = 0
    iteration = 0
    epoch_now = 0
    best_valid = 100000000
    state = None
    while train_iter.epoch < args.epoch:
        loss = 0
        iteration += 1
        # Progress the dataset iterator for bprop_len words at each iteration.
        for i in range(args.bproplen):
            # Get the next batch (a list of tuples of two word IDs)
            batch = train_iter.__next__()
            # Concatenate the word IDs to matrices and send them to the device
            # self.converter does this job
            # (it is chainer.dataset.concat_examples by default)
            x, t = convert.concat_examples(batch, gpu_id)
            # Compute the loss at this time step and accumulate it
            state, loss_batch = optimizer.target(state, chainer.Variable(x), chainer.Variable(t))
            loss += loss_batch
            count += 1

        sum_perp += loss.data
        optimizer.target.cleargrads()  # Clear the parameter gradients
        loss.backward()  # Backprop
        loss.unchain_backward()  # Truncate the graph
        optimizer.update()  # Update the parameters

        if iteration % 100 == 0:
            logging.info('iteration: ' + str(iteration))
            logging.info('training perplexity: ' + str(np.exp(float(sum_perp) / count)))
            sum_perp = 0
            count = 0

        if train_iter.epoch > epoch_now:
            valid_perp = evaluate(model, valid_iter)
            logging.info('epoch: ' + str(train_iter.epoch))
            logging.info('validation perplexity: ' + str(valid_perp))

            # Save the model and the optimizer
            logging.info('save the model')
            serializers.save_npz(args.outdir + '/rnnlm.model.' + str(epoch_now), model)
            logging.info('save the optimizer')
            serializers.save_npz(args.outdir + '/rnnlm.state.' + str(epoch_now), optimizer)

            if valid_perp < best_valid:
                dest = args.outdir + '/rnnlm.model.best'
                if os.path.lexists(dest):
                    os.remove(dest)
                os.symlink('rnnlm.model.' + str(epoch_now), dest)
                best_valid = valid_perp

            epoch_now = train_iter.epoch
",https://github.com/espnet/espnet/issues/317,"CWE-706: Use of Incorrectly-Resolved Name or Reference, CWE-248: Uncaught Exception, CWE-824: Access of Uninitialized Pointer, CWE-457: Use of Uninitialized Variable",there are branches in the train function where gpu_id is referenced before assignment,src/lm/lm_chainer.py,train,[106],"2018-07-27 22:33:54,922 (lm_chainer:145) INFO: chainer version = 4.3.0
2018-07-27 22:33:54,923 (lm_chainer:150) INFO: chainer seed = 1
2018-07-27 22:33:54,923 (lm_chainer:158) INFO: chainer type check is disabled
2018-07-27 22:33:54,923 (lm_chainer:170) WARNING: cudnn is not available
2018-07-27 22:34:00,204 (lm_chainer:181) INFO: #vocab = 5939
2018-07-27 22:34:00,204 (lm_chainer:182) INFO: #words in the training data = 6666818
2018-07-27 22:34:00,204 (lm_chainer:183) INFO: #words in the validation data = 6666818
2018-07-27 22:34:00,204 (lm_chainer:184) INFO: #iterations per epoch = 744
2018-07-27 22:34:00,204 (lm_chainer:185) INFO: #total iterations = 29762
Traceback (most recent call last):
File ""/home/christopher/gitrepos/espnet-1/egs/csj/asr1/../../../src/bin/lm_train.py"", line 131, in <module>
main()
File ""/home/christopher/gitrepos/espnet-1/egs/csj/asr1/../../../src/bin/lm_train.py"", line 122, in main
train(args)
File ""/home/christopher/gitrepos/espnet-1/src/lm/lm_chainer.py"", line 242, in train
x, t = convert.concat_examples(batch, gpu_id)
UnboundLocalError: local variable 'gpu_id' referenced before assignment
# Accounting: time=17 threads=1
# Ended (code 1) at Fri Jul 27 22:34:01 UTC 2018, elapsed time 17 seconds",UnboundLocalError,buggy_snippets_files/4944e9ec1a1c30770a92e2b06f8e29b3009376c4cf7814883fce906c87dcb860_before_merge.py,buggy_snippets_files/4944e9ec1a1c30770a92e2b06f8e29b3009376c4cf7814883fce906c87dcb860_after_merge.py
1911,"    def _decode(self, param):
        # make sure datetime, date and time are converted to string by force_text
        CONVERT_TYPES = (datetime.datetime, datetime.date, datetime.time)
        try:
            return force_text(param, strings_only=not isinstance(param, CONVERT_TYPES))
        except UnicodeDecodeError:
            return '(encoded string)'
","    def _decode(self, param):
        # If a sequence type, decode each element separately
        if isinstance(param, list) or isinstance(param, tuple):
            return [self._decode(element) for element in param]

        # If a dictionary type, decode each value separately
        if isinstance(param, dict):
            return {key: self._decode(value) for key, value in param.items()}

        # make sure datetime, date and time are converted to string by force_text
        CONVERT_TYPES = (datetime.datetime, datetime.date, datetime.time)
        try:
            return force_text(param, strings_only=not isinstance(param, CONVERT_TYPES))
        except UnicodeDecodeError:
            return '(encoded string)'
",https://github.com/jazzband/django-debug-toolbar/issues/796,CWE-241: Improper Handling of Unexpected Data Type,"force_text function accepts only strings but lists, tuples and dicts can be passed as its argument param in the _decode function when calling it from _record function",debug_toolbar/panels/sql/tracking.py,NormalCursorWrapper._decode,[5],"Internal Server Error: /__debug__/sql_select/
Traceback (most recent call last):
File ""/django/django/db/backends/utils.py"", line 64, in execute
return self.cursor.execute(sql, params)
psycopg2.DataError: invalid input syntax for type json
LINE 1: ...r"".""id"") WHERE ""app_model1"".""firmware_versions"" = '''""{''''x...
^
DETAIL:  Token ""'"" is invalid.
CONTEXT:  JSON data, line 1: '...


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
File ""/django/django/core/handlers/base.py"", line 149, in get_response
response = self.process_exception_by_middleware(e, request)
File ""/django/django/core/handlers/base.py"", line 147, in get_response
response = wrapped_callback(request, *callback_args, **callback_kwargs)
File ""/usr/lib64/python3.5/contextlib.py"", line 30, in inner
return func(*args, **kwds)
File ""/django/django/views/decorators/csrf.py"", line 58, in wrapped_view
return view_func(*args, **kwargs)
File ""/django-debug-toolbar/debug_toolbar/panels/sql/views.py"", line 19, in sql_select
cursor.execute(sql, params)
File ""/django/django/db/backends/utils.py"", line 79, in execute
return super(CursorDebugWrapper, self).execute(sql, params)
File ""/django/django/db/backends/utils.py"", line 64, in execute
return self.cursor.execute(sql, params)
File ""/django/django/db/utils.py"", line 95, in __exit__
six.reraise(dj_exc_type, dj_exc_value, traceback)
File ""/django/django/utils/six.py"", line 685, in reraise
raise value.with_traceback(tb)
File ""/django/django/db/backends/utils.py"", line 64, in execute
return self.cursor.execute(sql, params)
django.db.utils.DataError: invalid input syntax for type json
LINE 1: ...r"".""id"") WHERE ""app_model1"".""foo_versions"" = '''""{''''x...
^
DETAIL:  Token ""'"" is invalid.
CONTEXT:  JSON data, line 1: '...",psycopg2.DataError,buggy_snippets_files/1bec6b9f1717b256198f27799b6e1bb98b928b473f909d0cdb9223858a431f39_before_merge.py,buggy_snippets_files/1bec6b9f1717b256198f27799b6e1bb98b928b473f909d0cdb9223858a431f39_after_merge.py
2002,"    def _summary_coord_extra(self, coord, indent):
        # Returns the text needed to ensure this coordinate can be
        # distinguished from all others with the same name.
        extra = """"
        similar_coords = self.coords(coord.name())
        if len(similar_coords) > 1:
            # Find all the attribute keys
            keys = set()
            for similar_coord in similar_coords:
                keys.update(similar_coord.attributes.keys())
            # Look for any attributes that vary
            vary = set()
            attributes = {}
            for key in keys:
                for similar_coord in similar_coords:
                    if key not in similar_coord.attributes:
                        vary.add(key)
                        break
                    value = similar_coord.attributes[key]
                    if attributes.setdefault(key, value) != value:
                        vary.add(key)
                        break
            keys = sorted(vary & set(coord.attributes.keys()))
            bits = [
                ""{}={!r}"".format(key, coord.attributes[key]) for key in keys
            ]
            if bits:
                extra = indent + "", "".join(bits)
        return extra
","    def _summary_coord_extra(self, coord, indent):
        # Returns the text needed to ensure this coordinate can be
        # distinguished from all others with the same name.
        extra = """"
        similar_coords = self.coords(coord.name())
        if len(similar_coords) > 1:
            similar_coords.remove(coord)
            # Look for any attributes that vary.
            vary = set()
            for key, value in coord.attributes.items():
                for similar_coord in similar_coords:
                    if key not in similar_coord.attributes:
                        vary.add(key)
                        break
                    if not np.array_equal(
                        similar_coord.attributes[key], value
                    ):
                        vary.add(key)
                        break
            keys = sorted(vary)
            bits = [
                ""{}={!r}"".format(key, coord.attributes[key]) for key in keys
            ]
            if bits:
                extra = indent + "", "".join(bits)
        return extra
",https://github.com/SciTools/iris/issues/3921,CWE-241: Improper Handling of Unexpected Data Type,"in the logical expression attributes.setdefault(key, value) != value the value could be a numpy array; thus, the expression result is undefined. the case is unhandled where the expression attributes.setdefault(key, value) != value is of array type",lib/iris/cube.py,Cube._summary_coord_extra,[20],"Traceback (most recent call last):
File ""iris3_cube_print_fail.py"", line 13, in <module>
print(cube)
File ""[site-packages-path]/lib/python3.6/site-packages/iris/cube.py"", line 2599, in __str__
return self.summary()
File ""[site-packages-path]/lib/python3.6/site-packages/iris/cube.py"", line 2445, in summary
vector_dim_coords, cube_header, max_line_offset
File ""[site-packages-path]/lib/python3.6/site-packages/iris/cube.py"", line 2423, in vector_summary
vectors, vector_summary, extra_indent
File ""[site-packages-path]/lib/python3.6/site-packages/iris/cube.py"", line 2215, in _summary_extra
extra = self._summary_coord_extra(coord, indent)
File ""[site-packages-path]/lib/python3.6/site-packages/iris/cube.py"", line 2198, in _summary_coord_extra
if attributes.setdefault(key, value) != value:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",ValueError,buggy_snippets_files/ef3d68a71f73bd45e9752f854e743d052ffecbec562d8bb64e57aba90f4edd4f_before_merge.py,buggy_snippets_files/ef3d68a71f73bd45e9752f854e743d052ffecbec562d8bb64e57aba90f4edd4f_after_merge.py
2007,"    def __init__(
        self,
        latitude_of_projection_origin=0.0,
        longitude_of_central_meridian=0.0,
        false_easting=0.0,
        false_northing=0.0,
        standard_parallels=(20.0, 50.0),
        ellipsoid=None,
    ):
        """"""
        Constructs a Albers Conical Equal Area coord system.

        Kwargs:

            * latitude_of_projection_origin
                    True latitude of planar origin in degrees.
                    Defaults to 0.

            * longitude_of_central_meridian
                    True longitude of planar central meridian in degrees.
                    Defaults to 0.

            * false_easting
                    X offset from planar origin in metres. Defaults to 0.

            * false_northing
                    Y offset from planar origin in metres. Defaults to 0.

            * standard_parallels
                    The one or two latitudes of correct scale.
                    Defaults to (20,50).
            * ellipsoid
                    :class:`GeogCS` defining the ellipsoid.

        """"""
        #: True latitude of planar origin in degrees.
        self.latitude_of_projection_origin = latitude_of_projection_origin
        #: True longitude of planar central meridian in degrees.
        self.longitude_of_central_meridian = longitude_of_central_meridian
        #: X offset from planar origin in metres.
        self.false_easting = false_easting
        #: Y offset from planar origin in metres.
        self.false_northing = false_northing
        #: The one or two latitudes of correct scale.
        self.standard_parallels = standard_parallels
        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid
","    def __init__(
        self,
        latitude_of_projection_origin=None,
        longitude_of_central_meridian=None,
        false_easting=None,
        false_northing=None,
        standard_parallels=None,
        ellipsoid=None,
    ):
        """"""
        Constructs a Albers Conical Equal Area coord system.

        Kwargs:

        * latitude_of_projection_origin:
            True latitude of planar origin in degrees. Defaults to 0.0 .

        * longitude_of_central_meridian:
            True longitude of planar central meridian in degrees.
            Defaults to 0.0 .

        * false_easting:
            X offset from planar origin in metres. Defaults to 0.0 .

        * false_northing:
            Y offset from planar origin in metres. Defaults to 0.0 .

        * standard_parallels (number or iterable of 1 or 2 numbers):
            The one or two latitudes of correct scale.
            Defaults to (20.0, 50.0).

        * ellipsoid (:class:`GeogCS`):
            If given, defines the ellipsoid.

        """"""
        #: True latitude of planar origin in degrees.
        self.latitude_of_projection_origin = _arg_default(
            latitude_of_projection_origin, 0
        )

        #: True longitude of planar central meridian in degrees.
        self.longitude_of_central_meridian = _arg_default(
            longitude_of_central_meridian, 0
        )

        #: X offset from planar origin in metres.
        self.false_easting = _arg_default(false_easting, 0)

        #: Y offset from planar origin in metres.
        self.false_northing = _arg_default(false_northing, 0)

        #: The one or two latitudes of correct scale (tuple of 1 or 2 floats).
        self.standard_parallels = _arg_default(
            standard_parallels, (20, 50), cast_as=_1or2_parallels
        )

        #: Ellipsoid definition (:class:`GeogCS` or None).
        self.ellipsoid = ellipsoid
",https://github.com/SciTools/iris/issues/3675,CWE-241: Improper Handling of Unexpected Data Type,"the functions TransverseMercator.__init__ and AlbersEqualArea.__init__ contain keyword arguments initialized with their default values. But if those arguments get None values, it leads to incorrect object attributes initialization",lib/iris/coord_systems.py,AlbersEqualArea.__init__,[37;39;41;43;45],"iris.__version__
Out[25]: '2.3.0'

In [26]: c =  iris.load_cube('/our/own/local/directory/hierarchy/snw_EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day_19790101-19801231.nc')

In [27]: c.coord('projection_x_coordinate')
Out[27]:
DimCoord(array([   0. ,   12.5,   25. ,   37.5,   50. ,   62.5,   75. ,   87.5,
100. ,  112.5,  125. ,  137.5,  150. ,  162.5,  175. ,  187.5,
##   8>< snip    8>< snip
5500. , 5512.5, 5525. , 5537.5, 5550. , 5562.5, 5575. , 5587.5,
5600. , 5612.5, 5625. , 5637.5, 5650. ]), standard_name='projection_x_coordinate', units=Unit('km'), long_name='x coordinate of projection', var_name='x', coord_system=LambertConformal(central_lat=49.5, central_lon=10.5, false_easting=None, false_northing=None, secant_latitudes=(49.5,), ellipsoid=None))

In [28]: iris.save(c,'c.nc')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-28-4160b1a355b0> in <module>
----> 1 iris.save(c,'c.nc')

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/io/__init__.py in save(source, target, saver, **kwargs)
406     # Single cube?
407     if isinstance(source, iris.cube.Cube):
--> 408         saver(source, target, **kwargs)
409
410     # CubeList or sequence of cubes?

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in save(cube, filename, netcdf_format, local_keys, unlimited_dimensions, zlib, complevel, shuffle, fletcher32, contiguous, chunksizes, endian, least_significant_digit, packing, fill_value)
2386         # Iterate through the cubelist.
2387         for cube, packspec, fill_value in zip(cubes, packspecs, fill_values):
-> 2388             sman.write(cube, local_keys, unlimited_dimensions, zlib, complevel,
2389                        shuffle, fletcher32, contiguous, chunksizes, endian,
2390                        least_significant_digit, packing=packspec,

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in write(self, cube, local_keys, unlimited_dimensions, zlib, complevel, shuffle, fletcher32, contiguous, chunksizes, endian, least_significant_digit, packing, fill_value)
981
982         # Create the associated cube CF-netCDF data variable.
--> 983         cf_var_cube = self._create_cf_data_variable(
984             cube, dimension_names, local_keys, zlib=zlib, complevel=complevel,
985             shuffle=shuffle, fletcher32=fletcher32, contiguous=contiguous,

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in _create_cf_data_variable(self, cube, dimension_names, local_keys, packing, fill_value, **kwargs)
2131
2132         # Create the CF-netCDF grid mapping.
-> 2133         self._create_cf_grid_mapping(cube, cf_var)
2134
2135         return cf_var

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in _create_cf_grid_mapping(self, cube, cf_var_cube)
1848                     cf_var_grid.latitude_of_projection_origin = cs.central_lat
1849                     cf_var_grid.longitude_of_central_meridian = cs.central_lon
-> 1850                     cf_var_grid.false_easting = cs.false_easting
1851                     cf_var_grid.false_northing = cs.false_northing
1852

netCDF4/_netCDF4.pyx in netCDF4._netCDF4.Variable.__setattr__()

netCDF4/_netCDF4.pyx in netCDF4._netCDF4.Variable.setncattr()

netCDF4/_netCDF4.pyx in netCDF4._netCDF4._set_att()

TypeError: illegal data type for attribute b'false_easting', must be one of dict_keys(['S1', 'i1', 'u1', 'i2', 'u2', 'i4', 'u4', 'i8', 'u8', 'f4', 'f8']), got O


In [29]: !ncdump -h /our/own/local/directory/hierarchy/snw_EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day_19790101-19801231.nc
netcdf snw_EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day_19790101-19801231 {
dimensions:
y = 453 ;
x = 453 ;
nvertex = 4 ;
time = UNLIMITED ; // (731 currently)
axis_nbounds = 2 ;
variables:
double bounds_nav_lon(y, x, nvertex) ;
double bounds_nav_lat(y, x, nvertex) ;
double time(time) ;
time:axis = ""T"" ;
time:standard_name = ""time"" ;
time:long_name = ""Time axis"" ;
time:calendar = ""gregorian"" ;
time:units = ""days since 1949-01-01 00:00:00"" ;
time:time_origin = ""1949-01-01 00:00:00"" ;
time:bounds = ""time_bounds"" ;
double time_bounds(time, axis_nbounds) ;
float snw(time, y, x) ;
snw:standard_name = ""surface_snow_amount"" ;
snw:long_name = ""Surface Snow Amount"" ;
snw:units = ""kg m-2"" ;
snw:online_operation = ""average"" ;
snw:interval_operation = ""450 s"" ;
snw:interval_write = ""1 d"" ;
snw:cell_methods = ""time: mean"" ;
snw:_FillValue = 1.e+20f ;
snw:missing_value = 1.e+20f ;
snw:coordinates = ""lon lat"" ;
snw:grid_mapping = ""Lambert_Conformal"" ;
double lon(y, x) ;
lon:standard_name = ""longitude"" ;
lon:long_name = ""Longitude"" ;
lon:units = ""degrees_east"" ;
lon:bounds = ""bounds_nav_lon"" ;
double lat(y, x) ;
lat:standard_name = ""latitude"" ;
lat:long_name = ""Latitude"" ;
lat:units = ""degrees_north"" ;
lat:bounds = ""bounds_nav_lat"" ;
char Lambert_Conformal ;
Lambert_Conformal:latitude_of_projection_origin = 49.5f ;
Lambert_Conformal:standard_parallel = 49.5f ;
Lambert_Conformal:longitude_of_central_meridian = 10.5f ;
Lambert_Conformal:grid_mapping_name = ""lambert_conformal_conic"" ;
double x(x) ;
x:units = ""km"" ;
x:long_name = ""x coordinate of projection"" ;
x:standard_name = ""projection_x_coordinate"" ;
x:axis = ""X"" ;
double y(y) ;
y:units = ""km"" ;
y:long_name = ""y coordinate of projection"" ;
y:standard_name = ""projection_y_coordinate"" ;
y:axis = ""Y"" ;

// global attributes:
:description = ""Created by xios"" ;
:title = ""Created by xios"" ;
:Conventions = ""CF-1.6"" ;
:creation_date = ""2018-09-28T16:13:44Z"" ;
:name = ""EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day"" ;
:institute_id = ""CNRM"" ;
:institution = ""CNRM (Centre National de Recherches Meteorologiques, Toulouse 31057, France)"" ;
:model_id = ""CNRM-ALADIN63"" ;
:experiment_id = ""evaluation"" ;
:experiment = ""Evaluation run with reanalysis forcing"" ;
:contact = ""contact.aladin-cordex@meteo.fr"" ;
:product = ""output"" ;
:Convention = ""CF-1.6"" ;
:driving_model_id = ""ECMWF-ERAINT"" ;
:driving_model_ensemble_member = ""r1i1p1"" ;
:driving_experiment_name = ""evaluation"" ;
:driving_experiment = ""ERA-INTERIM, evaluation, r1i1p1"" ;
:rcm_version_id = ""v1"" ;
:project_id = ""CORDEX"" ;
:CORDEX_domain = ""EUR-11"" ;
:references = ""http://www.umr-cnrm.fr/spip.php?article125&amp;lang=en"" ;
:comment = ""CORDEX Europe EUR-11 CNRM-ALADIN 6.3 L91 ECMWF-ERAINT: EUC12v63-1.100. Reference : Daniel M., Lemonsu A., Dqu M., Somot S., Alias A., Masson V. (2018) Benefits of explicit urban parametrization in regional climate modelling to study climate and city interactions. Climate Dynamics, 1-20, doi:10.1007/s00382-018-4289-x"" ;
:frequency = ""day"" ;
:tracking_id = ""hdl:21.14103/0efca508-2ffb-420b-8093-fffe58a7ae75"" ;
}",TypeError,buggy_snippets_files/507e2a777fb81b3a768e237b357ccaa37f5d99a541bd0598f4e1a904bf8aa86d_before_merge.py,buggy_snippets_files/507e2a777fb81b3a768e237b357ccaa37f5d99a541bd0598f4e1a904bf8aa86d_after_merge.py
2006,"    def __init__(
        self,
        latitude_of_projection_origin,
        longitude_of_central_meridian,
        false_easting,
        false_northing,
        scale_factor_at_central_meridian,
        ellipsoid=None,
    ):
        """"""
        Constructs a TransverseMercator object.

        Args:

            * latitude_of_projection_origin
                    True latitude of planar origin in degrees.

            * longitude_of_central_meridian
                    True longitude of planar origin in degrees.

            * false_easting
                    X offset from planar origin in metres.

            * false_northing
                    Y offset from planar origin in metres.

            * scale_factor_at_central_meridian
                    Reduces the cylinder to slice through the ellipsoid
                    (secant form). Used to provide TWO longitudes of zero
                    distortion in the area of interest.

        Kwargs:

            * ellipsoid
                    Optional :class:`GeogCS` defining the ellipsoid.

        Example::

            airy1830 = GeogCS(6377563.396, 6356256.909)
            osgb = TransverseMercator(49, -2, 400000, -100000, 0.9996012717,
                                      ellipsoid=airy1830)

        """"""
        #: True latitude of planar origin in degrees.
        self.latitude_of_projection_origin = float(
            latitude_of_projection_origin
        )

        #: True longitude of planar origin in degrees.
        self.longitude_of_central_meridian = float(
            longitude_of_central_meridian
        )

        #: X offset from planar origin in metres.
        self.false_easting = float(false_easting)

        #: Y offset from planar origin in metres.
        self.false_northing = float(false_northing)

        #: Reduces the cylinder to slice through the ellipsoid (secant form).
        self.scale_factor_at_central_meridian = float(
            scale_factor_at_central_meridian
        )

        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid
","    def __init__(
        self,
        latitude_of_projection_origin,
        longitude_of_central_meridian,
        false_easting=None,
        false_northing=None,
        scale_factor_at_central_meridian=None,
        ellipsoid=None,
    ):
        """"""
        Constructs a TransverseMercator object.

        Args:

        * latitude_of_projection_origin:
                True latitude of planar origin in degrees.

        * longitude_of_central_meridian:
                True longitude of planar origin in degrees.

        Kwargs:

        * false_easting:
                X offset from planar origin in metres.
                Defaults to 0.0 .

        * false_northing:
                Y offset from planar origin in metres.
                Defaults to 0.0 .

        * scale_factor_at_central_meridian:
                Reduces the cylinder to slice through the ellipsoid
                (secant form). Used to provide TWO longitudes of zero
                distortion in the area of interest.
                Defaults to 1.0 .

        * ellipsoid (:class:`GeogCS`):
            If given, defines the ellipsoid.

        Example::

            airy1830 = GeogCS(6377563.396, 6356256.909)
            osgb = TransverseMercator(49, -2, 400000, -100000, 0.9996012717,
                                      ellipsoid=airy1830)

        """"""
        #: True latitude of planar origin in degrees.
        self.latitude_of_projection_origin = float(
            latitude_of_projection_origin
        )

        #: True longitude of planar origin in degrees.
        self.longitude_of_central_meridian = float(
            longitude_of_central_meridian
        )

        #: X offset from planar origin in metres.
        self.false_easting = _arg_default(false_easting, 0)

        #: Y offset from planar origin in metres.
        self.false_northing = _arg_default(false_northing, 0)

        #: Scale factor at the centre longitude.
        self.scale_factor_at_central_meridian = _arg_default(
            scale_factor_at_central_meridian, 1.0
        )

        #: Ellipsoid definition (:class:`GeogCS` or None).
        self.ellipsoid = ellipsoid
",https://github.com/SciTools/iris/issues/3675,CWE-241: Improper Handling of Unexpected Data Type,"the functions TransverseMercator.__init__ and AlbersEqualArea.__init__ contain keyword arguments initialized with their default values. But if those arguments get None values, it leads to incorrect object attributes initialization",lib/iris/coord_systems.py,TransverseMercator.__init__,[55;58],"iris.__version__
Out[25]: '2.3.0'

In [26]: c =  iris.load_cube('/our/own/local/directory/hierarchy/snw_EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day_19790101-19801231.nc')

In [27]: c.coord('projection_x_coordinate')
Out[27]:
DimCoord(array([   0. ,   12.5,   25. ,   37.5,   50. ,   62.5,   75. ,   87.5,
100. ,  112.5,  125. ,  137.5,  150. ,  162.5,  175. ,  187.5,
##   8>< snip    8>< snip
5500. , 5512.5, 5525. , 5537.5, 5550. , 5562.5, 5575. , 5587.5,
5600. , 5612.5, 5625. , 5637.5, 5650. ]), standard_name='projection_x_coordinate', units=Unit('km'), long_name='x coordinate of projection', var_name='x', coord_system=LambertConformal(central_lat=49.5, central_lon=10.5, false_easting=None, false_northing=None, secant_latitudes=(49.5,), ellipsoid=None))

In [28]: iris.save(c,'c.nc')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-28-4160b1a355b0> in <module>
----> 1 iris.save(c,'c.nc')

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/io/__init__.py in save(source, target, saver, **kwargs)
406     # Single cube?
407     if isinstance(source, iris.cube.Cube):
--> 408         saver(source, target, **kwargs)
409
410     # CubeList or sequence of cubes?

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in save(cube, filename, netcdf_format, local_keys, unlimited_dimensions, zlib, complevel, shuffle, fletcher32, contiguous, chunksizes, endian, least_significant_digit, packing, fill_value)
2386         # Iterate through the cubelist.
2387         for cube, packspec, fill_value in zip(cubes, packspecs, fill_values):
-> 2388             sman.write(cube, local_keys, unlimited_dimensions, zlib, complevel,
2389                        shuffle, fletcher32, contiguous, chunksizes, endian,
2390                        least_significant_digit, packing=packspec,

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in write(self, cube, local_keys, unlimited_dimensions, zlib, complevel, shuffle, fletcher32, contiguous, chunksizes, endian, least_significant_digit, packing, fill_value)
981
982         # Create the associated cube CF-netCDF data variable.
--> 983         cf_var_cube = self._create_cf_data_variable(
984             cube, dimension_names, local_keys, zlib=zlib, complevel=complevel,
985             shuffle=shuffle, fletcher32=fletcher32, contiguous=contiguous,

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in _create_cf_data_variable(self, cube, dimension_names, local_keys, packing, fill_value, **kwargs)
2131
2132         # Create the CF-netCDF grid mapping.
-> 2133         self._create_cf_grid_mapping(cube, cf_var)
2134
2135         return cf_var

~/.conda/envs/climix-devel-3/lib/python3.8/site-packages/iris/fileformats/netcdf.py in _create_cf_grid_mapping(self, cube, cf_var_cube)
1848                     cf_var_grid.latitude_of_projection_origin = cs.central_lat
1849                     cf_var_grid.longitude_of_central_meridian = cs.central_lon
-> 1850                     cf_var_grid.false_easting = cs.false_easting
1851                     cf_var_grid.false_northing = cs.false_northing
1852

netCDF4/_netCDF4.pyx in netCDF4._netCDF4.Variable.__setattr__()

netCDF4/_netCDF4.pyx in netCDF4._netCDF4.Variable.setncattr()

netCDF4/_netCDF4.pyx in netCDF4._netCDF4._set_att()

TypeError: illegal data type for attribute b'false_easting', must be one of dict_keys(['S1', 'i1', 'u1', 'i2', 'u2', 'i4', 'u4', 'i8', 'u8', 'f4', 'f8']), got O


In [29]: !ncdump -h /our/own/local/directory/hierarchy/snw_EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day_19790101-19801231.nc
netcdf snw_EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day_19790101-19801231 {
dimensions:
y = 453 ;
x = 453 ;
nvertex = 4 ;
time = UNLIMITED ; // (731 currently)
axis_nbounds = 2 ;
variables:
double bounds_nav_lon(y, x, nvertex) ;
double bounds_nav_lat(y, x, nvertex) ;
double time(time) ;
time:axis = ""T"" ;
time:standard_name = ""time"" ;
time:long_name = ""Time axis"" ;
time:calendar = ""gregorian"" ;
time:units = ""days since 1949-01-01 00:00:00"" ;
time:time_origin = ""1949-01-01 00:00:00"" ;
time:bounds = ""time_bounds"" ;
double time_bounds(time, axis_nbounds) ;
float snw(time, y, x) ;
snw:standard_name = ""surface_snow_amount"" ;
snw:long_name = ""Surface Snow Amount"" ;
snw:units = ""kg m-2"" ;
snw:online_operation = ""average"" ;
snw:interval_operation = ""450 s"" ;
snw:interval_write = ""1 d"" ;
snw:cell_methods = ""time: mean"" ;
snw:_FillValue = 1.e+20f ;
snw:missing_value = 1.e+20f ;
snw:coordinates = ""lon lat"" ;
snw:grid_mapping = ""Lambert_Conformal"" ;
double lon(y, x) ;
lon:standard_name = ""longitude"" ;
lon:long_name = ""Longitude"" ;
lon:units = ""degrees_east"" ;
lon:bounds = ""bounds_nav_lon"" ;
double lat(y, x) ;
lat:standard_name = ""latitude"" ;
lat:long_name = ""Latitude"" ;
lat:units = ""degrees_north"" ;
lat:bounds = ""bounds_nav_lat"" ;
char Lambert_Conformal ;
Lambert_Conformal:latitude_of_projection_origin = 49.5f ;
Lambert_Conformal:standard_parallel = 49.5f ;
Lambert_Conformal:longitude_of_central_meridian = 10.5f ;
Lambert_Conformal:grid_mapping_name = ""lambert_conformal_conic"" ;
double x(x) ;
x:units = ""km"" ;
x:long_name = ""x coordinate of projection"" ;
x:standard_name = ""projection_x_coordinate"" ;
x:axis = ""X"" ;
double y(y) ;
y:units = ""km"" ;
y:long_name = ""y coordinate of projection"" ;
y:standard_name = ""projection_y_coordinate"" ;
y:axis = ""Y"" ;

// global attributes:
:description = ""Created by xios"" ;
:title = ""Created by xios"" ;
:Conventions = ""CF-1.6"" ;
:creation_date = ""2018-09-28T16:13:44Z"" ;
:name = ""EUR-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_day"" ;
:institute_id = ""CNRM"" ;
:institution = ""CNRM (Centre National de Recherches Meteorologiques, Toulouse 31057, France)"" ;
:model_id = ""CNRM-ALADIN63"" ;
:experiment_id = ""evaluation"" ;
:experiment = ""Evaluation run with reanalysis forcing"" ;
:contact = ""contact.aladin-cordex@meteo.fr"" ;
:product = ""output"" ;
:Convention = ""CF-1.6"" ;
:driving_model_id = ""ECMWF-ERAINT"" ;
:driving_model_ensemble_member = ""r1i1p1"" ;
:driving_experiment_name = ""evaluation"" ;
:driving_experiment = ""ERA-INTERIM, evaluation, r1i1p1"" ;
:rcm_version_id = ""v1"" ;
:project_id = ""CORDEX"" ;
:CORDEX_domain = ""EUR-11"" ;
:references = ""http://www.umr-cnrm.fr/spip.php?article125&amp;lang=en"" ;
:comment = ""CORDEX Europe EUR-11 CNRM-ALADIN 6.3 L91 ECMWF-ERAINT: EUC12v63-1.100. Reference : Daniel M., Lemonsu A., Dqu M., Somot S., Alias A., Masson V. (2018) Benefits of explicit urban parametrization in regional climate modelling to study climate and city interactions. Climate Dynamics, 1-20, doi:10.1007/s00382-018-4289-x"" ;
:frequency = ""day"" ;
:tracking_id = ""hdl:21.14103/0efca508-2ffb-420b-8093-fffe58a7ae75"" ;
}",TypeError,buggy_snippets_files/bcc9bc018f8a51143b32426e21bec71c81707c960308868675c36e293b07571e_before_merge.py,buggy_snippets_files/bcc9bc018f8a51143b32426e21bec71c81707c960308868675c36e293b07571e_after_merge.py
2164,"    def get_lockfile_meta(self):
        from .vendor.plette.lockfiles import PIPFILE_SPEC_CURRENT
        if self.lockfile_exists:
            sources = self.lockfile_content.get(""_meta"", {}).get(""sources"", [])
        else:
            sources = [dict(source) for source in self.parsed_pipfile[""source""]]
        if not isinstance(sources, list):
            sources = [sources]
        return {
            ""hash"": {""sha256"": self.calculate_pipfile_hash()},
            ""pipfile-spec"": PIPFILE_SPEC_CURRENT,
            ""sources"": [self.populate_source(s) for s in sources],
            ""requires"": self.parsed_pipfile.get(""requires"", {})
        }","    def get_lockfile_meta(self):
        from .vendor.plette.lockfiles import PIPFILE_SPEC_CURRENT
        if self.lockfile_exists:
            sources = self.lockfile_content.get(""_meta"", {}).get(""sources"", [])
        elif ""source"" in self.parsed_pipfile:
            sources = [dict(source) for source in self.parsed_pipfile[""source""]]
        else:
            sources = self.pipfile_sources
        if not isinstance(sources, list):
            sources = [sources]
        return {
            ""hash"": {""sha256"": self.calculate_pipfile_hash()},
            ""pipfile-spec"": PIPFILE_SPEC_CURRENT,
            ""sources"": [self.populate_source(s) for s in sources],
            ""requires"": self.parsed_pipfile.get(""requires"", {})
        }",https://github.com/pypa/pipenv/issues/4141,"CWE-706: Use of Incorrectly-Resolved Name or Reference, CWE-248: Uncaught Exception, CWE-824: Access of Uninitialized Pointer, CWE-457: Use of Uninitialized Variable","when running pipenv install --skip-lock with the option --skip_lock one arrives at the case in the function get_lockfile_meta where sources = [dict(source) for source in self.parsed_pipfile[""source""]]. But if key 'source' is not present in the self.parsed_pipfile it fails to work. The case is unhandled where 'source' key is not present in the dict self.parsed_pipfile",pipenv/project.py,Project.get_lockfile_meta,[6],"Installing dependencies from Pipfile
Traceback (most recent call last):
File ""/opt/miniconda3/bin/pipenv"", line 8, in <module>
sys.exit(cli())
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 764, in __call__
return self.main(*args, **kwargs)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 717, in main
rv = self.invoke(ctx)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 1137, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 956, in invoke
return ctx.invoke(self.callback, **ctx.params)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 555, in invoke
return callback(*args, **kwargs)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/decorators.py"", line 64, in new_func
return ctx.invoke(f, obj, *args, **kwargs)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 555, in invoke
return callback(*args, **kwargs)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/click/decorators.py"", line 17, in new_func
return f(get_current_context(), *args, **kwargs)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/cli/command.py"", line 254, in install
editable_packages=state.installstate.editables,
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/core.py"", line 1874, in do_install
keep_outdated=keep_outdated
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/core.py"", line 1253, in do_init
pypi_mirror=pypi_mirror,
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/core.py"", line 795, in do_install_dependencies
lockfile = project.get_or_create_lockfile(from_pipfile=True)
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/project.py"", line 754, in get_or_create_lockfile
lockfile_dict.update({""_meta"": self.get_lockfile_meta()})
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/project.py"", line 790, in get_lockfile_meta
sources = [dict(source) for source in self.parsed_pipfile[""source""]]
File ""/opt/miniconda3/lib/python3.7/site-packages/pipenv/vendor/tomlkit/container.py"", line 500, in __getitem__
raise NonExistentKey(key)
tomlkit.exceptions.NonExistentKey: 'Key ""source"" does not exist.'",tomlkit.exceptions.NonExistentKey,buggy_snippets_files/368a693eb1ed2601b38350d407a8db82cfbc2b108ea71b208a08bea6a3bfa228_before_merge.py,buggy_snippets_files/368a693eb1ed2601b38350d407a8db82cfbc2b108ea71b208a08bea6a3bfa228_after_merge.py
2165,"    @classmethod
    def get_deps_from_req(cls, req, resolver=None):
        # type: (Requirement, Optional[""Resolver""]) -> Tuple[Set[str], Dict[str, Dict[str, Union[str, bool, List[str]]]]]
        from .vendor.requirementslib.models.utils import _requirement_to_str_lowercase_name
        from .vendor.requirementslib.models.requirements import Requirement
        from requirementslib.utils import is_installable_dir
        # TODO: this is way too complex, refactor this
        constraints = set()  # type: Set[str]
        locked_deps = dict()  # type: Dict[str, Dict[str, Union[str, bool, List[str]]]]
        if (req.is_file_or_url or req.is_vcs) and not req.is_wheel:
            # for local packages with setup.py files and potential direct url deps:
            if req.is_vcs:
                req_list, lockfile = get_vcs_deps(reqs=[req])
                req = next(iter(req for req in req_list if req is not None), req_list)
                entry = lockfile[pep423_name(req.normalized_name)]
            else:
                _, entry = req.pipfile_entry
            parsed_line = req.req.parsed_line  # type: Line
            setup_info = None  # type: Any
            try:
                name = req.normalized_name
            except TypeError:
                raise RequirementError(req=req)
            setup_info = req.req.setup_info
            setup_info.get_info()
            locked_deps[pep423_name(name)] = entry
            requirements = []
            # Allow users to toggle resolution off for non-editable VCS packages
            # but leave it on for local, installable folders on the filesystem
            if environments.PIPENV_RESOLVE_VCS or (
                req.editable or parsed_line.is_wheel or (
                    req.is_file_or_url and parsed_line.is_local
                    and is_installable_dir(parsed_line.path)
                )
            ):
                requirements = [v for v in getattr(setup_info, ""requires"", {}).values()]
            for r in requirements:
                if getattr(r, ""url"", None) and not getattr(r, ""editable"", False):
                    if r is not None:
                        if not r.url:
                            continue
                        line = _requirement_to_str_lowercase_name(r)
                        new_req, _, _ = cls.parse_line(line)
                        if r.marker and not r.marker.evaluate():
                            new_constraints = {}
                            _, new_entry = req.pipfile_entry
                            new_lock = {
                                pep423_name(new_req.normalized_name): new_entry
                            }
                        else:
                            new_constraints, new_lock = cls.get_deps_from_req(
                                new_req, resolver
                            )
                        locked_deps.update(new_lock)
                        constraints |= new_constraints
                # if there is no marker or there is a valid marker, add the constraint line
                elif r and (not r.marker or (r.marker and r.marker.evaluate())):
                    line = _requirement_to_str_lowercase_name(r)
                    constraints.add(line)
            # ensure the top level entry remains as provided
            # note that we shouldn't pin versions for editable vcs deps
            if not req.is_vcs:
                if req.specifiers:
                    locked_deps[name][""version""] = req.specifiers
                elif parsed_line.setup_info and parsed_line.setup_info.version:
                    locked_deps[name][""version""] = ""=={}"".format(
                        parsed_line.setup_info.version
                    )
            # if not req.is_vcs:
            locked_deps.update({name: entry})
            if req.is_vcs and req.editable:
                constraints.add(req.constraint_line)
            if req.is_file_or_url and req.req.is_local and req.editable and (
                    req.req.setup_path is not None and os.path.exists(req.req.setup_path)):
                constraints.add(req.constraint_line)
        else:
            # if the dependency isn't installable, don't add it to constraints
            # and instead add it directly to the lock
            if req and req.requirement and (
                req.requirement.marker and not req.requirement.marker.evaluate()
            ):
                pypi = resolver.repository if resolver else None
                best_match = pypi.find_best_match(req.ireq) if pypi else None
                if best_match:
                    hashes = resolver.collect_hashes(best_match) if resolver else []
                    new_req = Requirement.from_ireq(best_match)
                    new_req = new_req.add_hashes(hashes)
                    name, entry = new_req.pipfile_entry
                    locked_deps[pep423_name(name)] = translate_markers(entry)
                return constraints, locked_deps
            constraints.add(req.constraint_line)
            return constraints, locked_deps
        return constraints, locked_deps
","    @classmethod
    def get_deps_from_req(cls, req, resolver=None):
        # type: (Requirement, Optional[""Resolver""]) -> Tuple[Set[str], Dict[str, Dict[str, Union[str, bool, List[str]]]]]
        from .patched.piptools.exceptions import NoCandidateFound
        from .vendor.requirementslib.models.utils import _requirement_to_str_lowercase_name
        from .vendor.requirementslib.models.requirements import Requirement
        from .vendor.requirementslib.utils import is_installable_dir
        # TODO: this is way too complex, refactor this
        constraints = set()  # type: Set[str]
        locked_deps = dict()  # type: Dict[str, Dict[str, Union[str, bool, List[str]]]]
        if (req.is_file_or_url or req.is_vcs) and not req.is_wheel:
            # for local packages with setup.py files and potential direct url deps:
            if req.is_vcs:
                req_list, lockfile = get_vcs_deps(reqs=[req])
                req = next(iter(req for req in req_list if req is not None), req_list)
                entry = lockfile[pep423_name(req.normalized_name)]
            else:
                _, entry = req.pipfile_entry
            parsed_line = req.req.parsed_line  # type: Line
            setup_info = None  # type: Any
            try:
                name = req.normalized_name
            except TypeError:
                raise RequirementError(req=req)
            setup_info = req.req.setup_info
            setup_info.get_info()
            locked_deps[pep423_name(name)] = entry
            requirements = []
            # Allow users to toggle resolution off for non-editable VCS packages
            # but leave it on for local, installable folders on the filesystem
            if environments.PIPENV_RESOLVE_VCS or (
                req.editable or parsed_line.is_wheel or (
                    req.is_file_or_url and parsed_line.is_local
                    and is_installable_dir(parsed_line.path)
                )
            ):
                requirements = [v for v in getattr(setup_info, ""requires"", {}).values()]
            for r in requirements:
                if getattr(r, ""url"", None) and not getattr(r, ""editable"", False):
                    if r is not None:
                        if not r.url:
                            continue
                        line = _requirement_to_str_lowercase_name(r)
                        new_req, _, _ = cls.parse_line(line)
                        if r.marker and not r.marker.evaluate():
                            new_constraints = {}
                            _, new_entry = req.pipfile_entry
                            new_lock = {
                                pep423_name(new_req.normalized_name): new_entry
                            }
                        else:
                            new_constraints, new_lock = cls.get_deps_from_req(
                                new_req, resolver
                            )
                        locked_deps.update(new_lock)
                        constraints |= new_constraints
                # if there is no marker or there is a valid marker, add the constraint line
                elif r and (not r.marker or (r.marker and r.marker.evaluate())):
                    line = _requirement_to_str_lowercase_name(r)
                    constraints.add(line)
            # ensure the top level entry remains as provided
            # note that we shouldn't pin versions for editable vcs deps
            if not req.is_vcs:
                if req.specifiers:
                    locked_deps[name][""version""] = req.specifiers
                elif parsed_line.setup_info and parsed_line.setup_info.version:
                    locked_deps[name][""version""] = ""=={}"".format(
                        parsed_line.setup_info.version
                    )
            # if not req.is_vcs:
            locked_deps.update({name: entry})
            if req.is_vcs and req.editable:
                constraints.add(req.constraint_line)
            if req.is_file_or_url and req.req.is_local and req.editable and (
                    req.req.setup_path is not None and os.path.exists(req.req.setup_path)):
                constraints.add(req.constraint_line)
        else:
            # if the dependency isn't installable, don't add it to constraints
            # and instead add it directly to the lock
            if req and req.requirement and (
                req.requirement.marker and not req.requirement.marker.evaluate()
            ):
                pypi = resolver.repository if resolver else None
                try:
                    best_match = pypi.find_best_match(req.ireq) if pypi else None
                except NoCandidateFound:
                    best_match = None
                if best_match:
                    hashes = resolver.collect_hashes(best_match) if resolver else []
                    new_req = Requirement.from_ireq(best_match)
                    new_req = new_req.add_hashes(hashes)
                    name, entry = new_req.pipfile_entry
                    locked_deps[pep423_name(name)] = translate_markers(entry)
                    click_echo(
                        ""{} doesn't match your environment, ""
                        ""its dependencies won't be resolved."".format(req.as_line()),
                        err=True
                    )
                else:
                    click_echo(
                        ""Could not find a version of {} that matches your environment, ""
                        ""it will be skipped."".format(req.as_line()),
                        err=True
                    )
                return constraints, locked_deps
            constraints.add(req.constraint_line)
            return constraints, locked_deps
        return constraints, locked_deps
",https://github.com/pypa/pipenv/issues/4346,CWE-248: Uncaught Exception,"calling function pypi.find_best_match(req.ireq) in get_deps_from_req function throws a pipenv.patched.piptools.exceptions.NoCandidateFound exception, which is not processed in get_deps_from_req function",pipenv/utils.py,Resolver.get_deps_from_req,[83],"Pipfile.lock not found, creating
Locking [dev-packages] dependencies
Locking [packages] dependencies
Building requirements...
Resolving dependencies...
 Locking...Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 807, in <module>
 Locking Failed!
main()
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 802, in main
_main(parsed.pre, parsed.clear, parsed.verbose, parsed.system, parsed.write,
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 785, in _main
resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages)
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 746, in resolve_packages
results, resolver = resolve(
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 728, in resolve
return resolve_deps(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 1378, in resolve_deps
results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 1090, in actually_resolve_deps
resolver = Resolver.create(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 647, in create
constraints, skipped, index_lookup, markers_lookup = cls.get_metadata(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 462, in get_metadata
constraint_update, lockfile_update = cls.get_deps_from_req(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 610, in get_deps_from_req
best_match = pypi.find_best_match(req.ireq) if pypi else None
File ""/usr/lib/python3.8/site-packages/pipenv/patched/piptools/repositories/pypi.py"", line 202, in find_best_match
raise NoCandidateFound(ireq, all_candidates, self.finder)
pipenv.patched.piptools.exceptions.NoCandidateFound: Could not find a version that matches pywin32==227
No versions found
Were https://pypi.org/simple or https://mirrors.aliyun.com/pypi/simple/ or https://pypi.tuna.tsinghua.edu.cn/simple reachable?
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 807, in <module>
main()
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 802, in main
_main(parsed.pre, parsed.clear, parsed.verbose, parsed.system, parsed.write,
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 785, in _main
resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages)
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 746, in resolve_packages
results, resolver = resolve(
File ""/usr/lib/python3.8/site-packages/pipenv/resolver.py"", line 728, in resolve
return resolve_deps(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 1378, in resolve_deps
results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 1090, in actually_resolve_deps
resolver = Resolver.create(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 647, in create
constraints, skipped, index_lookup, markers_lookup = cls.get_metadata(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 462, in get_metadata
constraint_update, lockfile_update = cls.get_deps_from_req(
File ""/usr/lib/python3.8/site-packages/pipenv/utils.py"", line 610, in get_deps_from_req
best_match = pypi.find_best_match(req.ireq) if pypi else None
File ""/usr/lib/python3.8/site-packages/pipenv/patched/piptools/repositories/pypi.py"", line 202, in find_best_match
raise NoCandidateFound(ireq, all_candidates, self.finder)
pipenv.patched.piptools.exceptions.NoCandidateFound: Could not find a version that matches pywin32==227
No versions found
Were https://pypi.org/simple or https://mirrors.aliyun.com/pypi/simple/ or https://pypi.tuna.tsinghua.edu.cn/simple reachable?",pipenv.patched.piptools.exceptions.NoCandidateFound,buggy_snippets_files/af7e245f486c6e8b276b0c7254946ffd75ed2f520f46fb362bf53c2912090b3f_before_merge.py,buggy_snippets_files/af7e245f486c6e8b276b0c7254946ffd75ed2f520f46fb362bf53c2912090b3f_after_merge.py
2635,"def do_outdated(pypi_mirror=None, pre=False, clear=False):
    # TODO: Allow --skip-lock here?
    from .vendor.requirementslib.models.requirements import Requirement
    from .vendor.requirementslib.models.utils import get_version
    from .vendor.packaging.utils import canonicalize_name
    from .vendor.vistir.compat import Mapping
    from collections import namedtuple

    packages = {}
    package_info = namedtuple(""PackageInfo"", [""name"", ""installed"", ""available""])

    installed_packages = project.environment.get_installed_packages()
    outdated_packages = {
        canonicalize_name(pkg.project_name): package_info
        (pkg.project_name, pkg.parsed_version, pkg.latest_version)
        for pkg in project.environment.get_outdated_packages()
    }
    reverse_deps = project.environment.reverse_dependencies()
    for result in installed_packages:
        dep = Requirement.from_line(str(result.as_requirement()))
        packages.update(dep.as_pipfile())
    updated_packages = {}
    lockfile = do_lock(clear=clear, pre=pre, write=False, pypi_mirror=pypi_mirror)
    for section in (""develop"", ""default""):
        for package in lockfile[section]:
            try:
                updated_packages[package] = lockfile[section][package][""version""]
            except KeyError:
                pass
    outdated = []
    skipped = []
    for package in packages:
        norm_name = pep423_name(package)
        if norm_name in updated_packages:
            if updated_packages[norm_name] != packages[package]:
                outdated.append(
                    package_info(package, updated_packages[norm_name], packages[package])
                )
            elif canonicalize_name(package) in outdated_packages:
                skipped.append(outdated_packages[canonicalize_name(package)])
    for package, old_version, new_version in skipped:
        name_in_pipfile = project.get_package_name_in_pipfile(package)
        pipfile_version_text = """"
        required = """"
        version = None
        if name_in_pipfile:
            version = get_version(project.packages[name_in_pipfile])
            reverse_deps = reverse_deps.get(name_in_pipfile)
            if isinstance(reverse_deps, Mapping) and ""required"" in reverse_deps:
                required = "" {0} required"".format(reverse_deps[""required""])
            if version:
                pipfile_version_text = "" ({0} set in Pipfile)"".format(version)
            else:
                pipfile_version_text = "" (Unpinned in Pipfile)""
        click.echo(
            crayons.yellow(
                ""Skipped Update of Package {0!s}: {1!s} installed,{2!s}{3!s}, ""
                ""{4!s} available."".format(
                    package, old_version, required, pipfile_version_text, new_version
                )
            ), err=True
        )
    if not outdated:
        click.echo(crayons.green(""All packages are up to date!"", bold=True))
        sys.exit(0)
    for package, new_version, old_version in outdated:
        click.echo(
            ""Package {0!r} out-of-date: {1!r} installed, {2!r} available."".format(
                package, old_version, new_version
            )
        )
    sys.exit(bool(outdated))
","def do_outdated(pypi_mirror=None, pre=False, clear=False):
    # TODO: Allow --skip-lock here?
    from .vendor.requirementslib.models.requirements import Requirement
    from .vendor.requirementslib.models.utils import get_version
    from .vendor.packaging.utils import canonicalize_name
    from .vendor.vistir.compat import Mapping
    from collections import namedtuple

    packages = {}
    package_info = namedtuple(""PackageInfo"", [""name"", ""installed"", ""available""])

    installed_packages = project.environment.get_installed_packages()
    outdated_packages = {
        canonicalize_name(pkg.project_name): package_info
        (pkg.project_name, pkg.parsed_version, pkg.latest_version)
        for pkg in project.environment.get_outdated_packages()
    }
    reverse_deps = {
        canonicalize_name(name): deps
        for name, deps in project.environment.reverse_dependencies().items()
    }
    for result in installed_packages:
        dep = Requirement.from_line(str(result.as_requirement()))
        packages.update(dep.as_pipfile())
    updated_packages = {}
    lockfile = do_lock(clear=clear, pre=pre, write=False, pypi_mirror=pypi_mirror)
    for section in (""develop"", ""default""):
        for package in lockfile[section]:
            try:
                updated_packages[package] = lockfile[section][package][""version""]
            except KeyError:
                pass
    outdated = []
    skipped = []
    for package in packages:
        norm_name = pep423_name(package)
        if norm_name in updated_packages:
            if updated_packages[norm_name] != packages[package]:
                outdated.append(
                    package_info(package, updated_packages[norm_name], packages[package])
                )
            elif canonicalize_name(package) in outdated_packages:
                skipped.append(outdated_packages[canonicalize_name(package)])
    for package, old_version, new_version in skipped:
        name_in_pipfile = project.get_package_name_in_pipfile(package)
        pipfile_version_text = """"
        required = """"
        version = None
        if name_in_pipfile:
            version = get_version(project.packages[name_in_pipfile])
            rdeps = reverse_deps.get(canonicalize_name(package))
            if isinstance(rdeps, Mapping) and ""required"" in rdeps:
                required = "" {0} required"".format(rdeps[""required""])
            if version:
                pipfile_version_text = "" ({0} set in Pipfile)"".format(version)
            else:
                pipfile_version_text = "" (Unpinned in Pipfile)""
        click.echo(
            crayons.yellow(
                ""Skipped Update of Package {0!s}: {1!s} installed,{2!s}{3!s}, ""
                ""{4!s} available."".format(
                    package, old_version, required, pipfile_version_text, new_version
                )
            ), err=True
        )
    if not outdated:
        click.echo(crayons.green(""All packages are up to date!"", bold=True))
        sys.exit(0)
    for package, new_version, old_version in outdated:
        click.echo(
            ""Package {0!r} out-of-date: {1!r} installed, {2!r} available."".format(
                package, old_version, new_version
            )
        )
    sys.exit(bool(outdated))
",https://github.com/pypa/pipenv/issues/4229,"		
CWE-665: Improper Initialization, CWE-248: Uncaught Exception","reverse_deps, being a dictionary, gets assigned to its value within the loop for package, old_version, new_version in skipped, thus, after such an assignment, it does not have a get attribute. The situation is unhandled where reverse_deps does not have the get attribute",pipenv/core.py,do_outdated,[48],"pipenv update --dry-run
Locking...Building requirements...
Resolving dependencies...
Success!
Building requirements...
Resolving dependencies...
Success!
Skipped Update of Package yarl: 1.4.2 installed,, 1.4.2 available.
Skipped Update of Package wrapt: 1.12.1 installed,, 1.12.1 available.
Skipped Update of Package urllib3: 1.25.9 installed,, 1.25.9 available.
Skipped Update of Package tornado: 6.0.4 installed, 6.0.4 required (Unpinned in Pipfile), 6.0.4 available.
Skipped Update of Package toml: 0.10.0 installed,, 0.10.0 available.
Skipped Update of Package slackclient: 2.5.0 installed, (Unpinned in Pipfile), 2.5.0 available.
Skipped Update of Package six: 1.14.0 installed,, 1.14.0 available.
Traceback (most recent call last):
File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.752.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 193, in _run_module_as_main
return _run_code(code, main_globals, None,
File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.752.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 86, in _run_code
exec(code, run_globals)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\Scripts\pipenv.exe\__main__.py"", line 9, in <module>
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\core.py"", line 829, in __call__
return self.main(*args, **kwargs)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\core.py"", line 782, in main
rv = self.invoke(ctx)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\core.py"", line 1259, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\core.py"", line 1066, in invoke
return ctx.invoke(self.callback, **ctx.params)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\core.py"", line 610, in invoke
return callback(*args, **kwargs)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\decorators.py"", line 73, in new_func
return ctx.invoke(f, obj, *args, **kwargs)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\core.py"", line 610, in invoke
return callback(*args, **kwargs)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\vendor\click\decorators.py"", line 21, in new_func
return f(get_current_context(), *args, **kwargs)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\cli\command.py"", line 509, in update
do_outdated(clear=state.clear, pre=state.installstate.pre, pypi_mirror=state.pypi_mirror)
File ""C:\Users\gabri\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\pipenv\core.py"", line 1845, in do_outdated
reverse_deps = reverse_deps.get(name_in_pipfile)
AttributeError: 'NoneType' object has no attribute 'get'",AttributeError,buggy_snippets_files/53640bad14f3485bbb8d03a6de177737991f47d76a33c1e49a301a0fc1341816_before_merge.py,buggy_snippets_files/53640bad14f3485bbb8d03a6de177737991f47d76a33c1e49a301a0fc1341816_after_merge.py
3221,"def ensure_project(
    three=None,
    python=None,
    validate=True,
    system=False,
    warn=True,
    site_packages=False,
    deploy=False,
    skip_requirements=False,
    pypi_mirror=None,
    clear=False,
):
    """"""Ensures both Pipfile and virtualenv exist for the project.""""""
    from .environments import PIPENV_USE_SYSTEM

    # Clear the caches, if appropriate.
    if clear:
        print(""clearing"")
        sys.exit(1)

    # Automatically use an activated virtualenv.
    if PIPENV_USE_SYSTEM:
        system = True
    if not project.pipfile_exists and deploy:
        raise exceptions.PipfileNotFound
    # Fail if working under /
    if not project.name:
        click.echo(
            ""{0}: Pipenv is not intended to work under the root directory, ""
            ""please choose another path."".format(crayons.red(""ERROR"")),
            err=True
        )
        sys.exit(1)
    # Skip virtualenv creation when --system was used.
    if not system:
        ensure_virtualenv(
            three=three,
            python=python,
            site_packages=site_packages,
            pypi_mirror=pypi_mirror,
        )
        if warn:
            # Warn users if they are using the wrong version of Python.
            if project.required_python_version:
                path_to_python = which(""python"") or which(""py"")
                if path_to_python and project.required_python_version not in (
                    python_version(path_to_python) or """"
                ):
                    click.echo(
                        ""{0}: Your Pipfile requires {1} {2}, ""
                        ""but you are using {3} ({4})."".format(
                            crayons.red(""Warning"", bold=True),
                            crayons.normal(""python_version"", bold=True),
                            crayons.blue(project.required_python_version),
                            crayons.blue(python_version(path_to_python)),
                            crayons.green(shorten_path(path_to_python)),
                        ),
                        err=True,
                    )
                    click.echo(
                        ""  {0} and rebuilding the virtual environment ""
                        ""may resolve the issue."".format(crayons.green(""$ pipenv --rm"")),
                        err=True,
                    )
                    if not deploy:
                        click.echo(
                            ""  {0} will surely fail.""
                            """".format(crayons.red(""$ pipenv check"")),
                            err=True,
                        )
                    else:
                        raise exceptions.DeployException
    # Ensure the Pipfile exists.
    ensure_pipfile(
        validate=validate, skip_requirements=skip_requirements, system=system
    )","def ensure_project(
    three=None,
    python=None,
    validate=True,
    system=False,
    warn=True,
    site_packages=False,
    deploy=False,
    skip_requirements=False,
    pypi_mirror=None,
    clear=False,
):
    """"""Ensures both Pipfile and virtualenv exist for the project.""""""
    from .environments import PIPENV_USE_SYSTEM

    # Clear the caches, if appropriate.
    if clear:
        print(""clearing"")
        sys.exit(1)

    # Automatically use an activated virtualenv.
    if PIPENV_USE_SYSTEM:
        system = True
    if not project.pipfile_exists and deploy:
        raise exceptions.PipfileNotFound
    # Fail if working under /
    if not project.name:
        click.echo(
            ""{0}: Pipenv is not intended to work under the root directory, ""
            ""please choose another path."".format(crayons.red(""ERROR"")),
            err=True
        )
        sys.exit(1)
    # Skip virtualenv creation when --system was used.
    if not system:
        ensure_virtualenv(
            three=three,
            python=python,
            site_packages=site_packages,
            pypi_mirror=pypi_mirror,
        )
        if warn:
            # Warn users if they are using the wrong version of Python.
            if project.required_python_version:
                path_to_python = which(""python"") or which(""py"")
                if path_to_python and project.required_python_version not in (
                    python_version(path_to_python) or """"
                ):
                    click.echo(
                        ""{0}: Your Pipfile requires {1} {2}, ""
                        ""but you are using {3} ({4})."".format(
                            crayons.red(""Warning"", bold=True),
                            crayons.normal(""python_version"", bold=True),
                            crayons.blue(project.required_python_version),
                            crayons.blue(python_version(path_to_python) or ""unknown""),
                            crayons.green(shorten_path(path_to_python)),
                        ),
                        err=True,
                    )
                    click.echo(
                        ""  {0} and rebuilding the virtual environment ""
                        ""may resolve the issue."".format(crayons.green(""$ pipenv --rm"")),
                        err=True,
                    )
                    if not deploy:
                        click.echo(
                            ""  {0} will surely fail.""
                            """".format(crayons.red(""$ pipenv check"")),
                            err=True,
                        )
                    else:
                        raise exceptions.DeployException
    # Ensure the Pipfile exists.
    ensure_pipfile(
        validate=validate, skip_requirements=skip_requirements, system=system
    )",https://github.com/pypa/pipenv/issues/3407,CWE-241: Improper Handling of Unexpected Data Type,python_version(path_to_python) returns None. This causes crayons.blue(python_version(path_to_python)) to crash. The case is unhandled where crayons.blue argument is None ,pipenv/core.py,ensure_project,[55],"Traceback (most recent call last):
File ""/usr/bin/pipenv"", line 11, in <module>
load_entry_point('pipenv==2018.11.15.dev0', 'console_scripts', 'pipenv')()
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 764, in __call__
return self.main(*args, **kwargs)
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 717, in main
rv = self.invoke(ctx)
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 1137, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 956, in invoke
return ctx.invoke(self.callback, **ctx.params)
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 555, in invoke
return callback(*args, **kwargs)
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/decorators.py"", line 64, in new_func
return ctx.invoke(f, obj, *args, **kwargs)
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/core.py"", line 555, in invoke
return callback(*args, **kwargs)
File ""/usr/lib/python3.7/site-packages/pipenv/vendor/click/decorators.py"", line 17, in new_func
return f(get_current_context(), *args, **kwargs)
File ""/usr/lib/python3.7/site-packages/pipenv/cli/command.py"", line 319, in lock
ensure_project(three=state.three, python=state.python, pypi_mirror=state.pypi_mirror)
File ""/usr/lib/python3.7/site-packages/pipenv/core.py"", line 590, in ensure_project
crayons.green(shorten_path(path_to_python)),
TypeError: __str__ returned non-string (type NoneType)",TypeError,buggy_snippets_files/10694b1c5071fafca6957324664fe4632d781226c39bb0ca180a15a6b949e42f_before_merge.py,buggy_snippets_files/10694b1c5071fafca6957324664fe4632d781226c39bb0ca180a15a6b949e42f_after_merge.py
18782,"		def get_item_dict():
			default_uom = frappe.db.get_single_value(""Stock Settings"", ""stock_uom"") or _(""Nos"")
			cost_center = frappe.db.get_value(""Company"", self.company, ""cost_center"")
			if not cost_center:
				frappe.throw(_(""Please set the Default Cost Center in {0} company"").format(frappe.bold(self.company)))
			rate = flt(row.outstanding_amount) / row.qty

			return frappe._dict({
				""uom"": default_uom,
				""rate"": rate or 0.0,
				""qty"": row.qty,
				""conversion_factor"": 1.0,
				""item_name"": row.item_name or ""Opening Invoice Item"",
				""description"": row.item_name or ""Opening Invoice Item"",
				income_expense_account_field: row.temporary_opening_account,
				""cost_center"": cost_center
			})","		def get_item_dict():
			default_uom = frappe.db.get_single_value(""Stock Settings"", ""stock_uom"") or _(""Nos"")
			cost_center = frappe.db.get_value(""Company"", self.company, ""cost_center"")
			if not cost_center:
				frappe.throw(
					_(""Please set the Default Cost Center in {0} company"").format(frappe.bold(self.company))
				)
			rate = flt(row.outstanding_amount) / flt(row.qty)

			return frappe._dict({
				""uom"": default_uom,
				""rate"": rate or 0.0,
				""qty"": row.qty,
				""conversion_factor"": 1.0,
				""item_name"": row.item_name or ""Opening Invoice Item"",
				""description"": row.item_name or ""Opening Invoice Item"",
				income_expense_account_field: row.temporary_opening_account,
				""cost_center"": cost_center
			})",https://github.com/frappe/erpnext/issues/11938,CWE-241: Improper Handling of Unexpected Data Type, In the expression flt(row.outstanding_amount) / row.qty term row.qty is not correctly cast to type float,erpnext/accounts/doctype/opening_invoice_creation_tool/opening_invoice_creation_tool.py,OpeningInvoiceCreationTool.get_invoice_dict.get_item_dict,[6],"Traceback (most recent call last):
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/app.py"", line 62, in application
response = frappe.handler.handle()
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 22, in handle
data = execute_cmd(cmd)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 53, in execute_cmd
return frappe.call(method, **frappe.form_dict)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/__init__.py"", line 939, in call
return fn(*args, **newargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 81, in runserverobj
frappe.desk.form.run_method.runserverobj(method, docs=docs, dt=dt, dn=dn, arg=arg, args=args)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/desk/form/run_method.py"", line 36, in runserverobj
r = doc.run_method(method)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 755, in run_method
out = Document.hook(fn)(self, *args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 1024, in composer
return composed(self, method, *args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 1007, in runner
add_to_return_value(self, fn(self, *args, **kwargs))
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 749, in <lambda>
fn = lambda self, *args, **kwargs: getattr(self, method)(*args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/erpnext/erpnext/accounts/doctype/opening_invoice_creation_tool/opening_invoice_creation_tool.py"", line 68, in make_invoices
if not row.qty:
AttributeError: 'OpeningInvoiceCreationToolItem' object has no attribute 'qty'",AttributeError,buggy_snippets_files/0dad62880353b57c5811547c825097f11657ee3b91da2ff8b1581437032f7c09_before_merge.py,buggy_snippets_files/0dad62880353b57c5811547c825097f11657ee3b91da2ff8b1581437032f7c09_after_merge.py
18781,"	def get_invoice_dict(self, row=None):
		def get_item_dict():
			default_uom = frappe.db.get_single_value(""Stock Settings"", ""stock_uom"") or _(""Nos"")
			cost_center = frappe.db.get_value(""Company"", self.company, ""cost_center"")
			if not cost_center:
				frappe.throw(_(""Please set the Default Cost Center in {0} company"").format(frappe.bold(self.company)))
			rate = flt(row.outstanding_amount) / row.qty

			return frappe._dict({
				""uom"": default_uom,
				""rate"": rate or 0.0,
				""qty"": row.qty,
				""conversion_factor"": 1.0,
				""item_name"": row.item_name or ""Opening Invoice Item"",
				""description"": row.item_name or ""Opening Invoice Item"",
				income_expense_account_field: row.temporary_opening_account,
				""cost_center"": cost_center
			})

		if not row:
			return None

		party_type = ""Customer""
		income_expense_account_field = ""income_account""
		if self.invoice_type == ""Purchase"":
			party_type = ""Supplier""
			income_expense_account_field = ""expense_account""

		item = get_item_dict()
		return frappe._dict({
			""items"": [item],
			""is_opening"": ""Yes"",
			""set_posting_time"": 1,
			""company"": self.company,
			""due_date"": row.due_date,
			""posting_date"": row.posting_date,
			frappe.scrub(party_type): row.party,
			""doctype"": ""Sales Invoice"" if self.invoice_type == ""Sales"" \
				else ""Purchase Invoice"",
			""currency"": frappe.db.get_value(""Company"", self.company, ""default_currency"")
		})","	def get_invoice_dict(self, row=None):
		def get_item_dict():
			default_uom = frappe.db.get_single_value(""Stock Settings"", ""stock_uom"") or _(""Nos"")
			cost_center = frappe.db.get_value(""Company"", self.company, ""cost_center"")
			if not cost_center:
				frappe.throw(
					_(""Please set the Default Cost Center in {0} company"").format(frappe.bold(self.company))
				)
			rate = flt(row.outstanding_amount) / flt(row.qty)

			return frappe._dict({
				""uom"": default_uom,
				""rate"": rate or 0.0,
				""qty"": row.qty,
				""conversion_factor"": 1.0,
				""item_name"": row.item_name or ""Opening Invoice Item"",
				""description"": row.item_name or ""Opening Invoice Item"",
				income_expense_account_field: row.temporary_opening_account,
				""cost_center"": cost_center
			})

		if not row:
			return None

		party_type = ""Customer""
		income_expense_account_field = ""income_account""
		if self.invoice_type == ""Purchase"":
			party_type = ""Supplier""
			income_expense_account_field = ""expense_account""

		item = get_item_dict()
		return frappe._dict({
			""items"": [item],
			""is_opening"": ""Yes"",
			""set_posting_time"": 1,
			""company"": self.company,
			""due_date"": row.due_date,
			""posting_date"": row.posting_date,
			frappe.scrub(party_type): row.party,
			""doctype"": ""Sales Invoice"" if self.invoice_type == ""Sales"" else ""Purchase Invoice"",
			""currency"": frappe.db.get_value(""Company"", self.company, ""default_currency"")
		})",https://github.com/frappe/erpnext/issues/11938,CWE-241: Improper Handling of Unexpected Data Type, In the expression flt(row.outstanding_amount) / row.qty term row.qty is not correctly cast to type float,erpnext/accounts/doctype/opening_invoice_creation_tool/opening_invoice_creation_tool.py,OpeningInvoiceCreationTool.get_invoice_dict,[7],"Traceback (most recent call last):
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/app.py"", line 62, in application
response = frappe.handler.handle()
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 22, in handle
data = execute_cmd(cmd)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 53, in execute_cmd
return frappe.call(method, **frappe.form_dict)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/__init__.py"", line 939, in call
return fn(*args, **newargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 81, in runserverobj
frappe.desk.form.run_method.runserverobj(method, docs=docs, dt=dt, dn=dn, arg=arg, args=args)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/desk/form/run_method.py"", line 36, in runserverobj
r = doc.run_method(method)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 755, in run_method
out = Document.hook(fn)(self, *args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 1024, in composer
return composed(self, method, *args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 1007, in runner
add_to_return_value(self, fn(self, *args, **kwargs))
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 749, in <lambda>
fn = lambda self, *args, **kwargs: getattr(self, method)(*args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/erpnext/erpnext/accounts/doctype/opening_invoice_creation_tool/opening_invoice_creation_tool.py"", line 68, in make_invoices
if not row.qty:
AttributeError: 'OpeningInvoiceCreationToolItem' object has no attribute 'qty'",AttributeError,buggy_snippets_files/c6054b383785afa1f2225eff798e79e98020c0e0fd01f35524da7eba7860f61e_before_merge.py,buggy_snippets_files/c6054b383785afa1f2225eff798e79e98020c0e0fd01f35524da7eba7860f61e_after_merge.py
18783,"	def make_invoices(self):
		names = []
		mandatory_error_msg = _(""Row {idx}: {field} is required to create the Opening {invoice_type} Invoices"")
		if not self.company:
			frappe.throw(_(""Please select the Company""))

		for row in self.invoices:
			if not row.qty:
				row.qty = 1.0
			if not row.party:
				frappe.throw(mandatory_error_msg.format(
					idx=row.idx,
					field= _(""Party""),
					invoice_type=self.invoice_type
				))
			# set party type if not available
			if not row.party_type:
				row.party_type = ""Customer"" if self.invoice_type == ""Sales"" else ""Supplier""

			if not row.posting_date:
				frappe.throw(mandatory_error_msg.format(
					idx=row.idx,
					field= _(""Party""),
					invoice_type=self.invoice_type
				))

			if not row.outstanding_amount:
				frappe.throw(mandatory_error_msg.format(
					idx=row.idx,
					field= _(""Outstanding Amount""),
					invoice_type=self.invoice_type
				))

			args = self.get_invoice_dict(row=row)
			if not args:
				continue

			doc = frappe.get_doc(args).insert()
			doc.submit()
			names.append(doc.name)

			if(len(self.invoices) > 5):
				frappe.publish_realtime(""progress"",
					dict(progress=[row.idx, len(self.invoices)], title=_('Creating {0}').format(doc.doctype)),
					user=frappe.session.user)

		return names
","	def make_invoices(self):
		names = []
		mandatory_error_msg = _(""Row {idx}: {field} is required to create the Opening {invoice_type} Invoices"")
		if not self.company:
			frappe.throw(_(""Please select the Company""))

		for row in self.invoices:
			if not row.qty:
				row.qty = 1.0
			if not row.party:
				frappe.throw(mandatory_error_msg.format(
					idx=row.idx,
					field=_(""Party""),
					invoice_type=self.invoice_type
				))
			# set party type if not available
			if not row.party_type:
				row.party_type = ""Customer"" if self.invoice_type == ""Sales"" else ""Supplier""

			if not row.posting_date:
				frappe.throw(mandatory_error_msg.format(
					idx=row.idx,
					field=_(""Party""),
					invoice_type=self.invoice_type
				))

			if not row.outstanding_amount:
				frappe.throw(mandatory_error_msg.format(
					idx=row.idx,
					field=_(""Outstanding Amount""),
					invoice_type=self.invoice_type
				))

			args = self.get_invoice_dict(row=row)
			if not args:
				continue

			doc = frappe.get_doc(args).insert()
			doc.submit()
			names.append(doc.name)

			if len(self.invoices) > 5:
				frappe.publish_realtime(
					""progress"", dict(
						progress=[row.idx, len(self.invoices)],
						title=_('Creating {0}').format(doc.doctype)
					),
					user=frappe.session.user
				)

		return names
",https://github.com/frappe/erpnext/issues/11938,CWE-236: Improper Handling of Undefined Parameters,"A list of attributes for row in make_invoices function is stored in the json-file erpnext/accounts/doctype/opening_invoice_creation_tool_item/opening_invoice_creation_tool_item.json. This file does not contain qty attribute, which causes the AttributeError",erpnext/accounts/doctype/opening_invoice_creation_tool/opening_invoice_creation_tool.py,OpeningInvoiceCreationTool.make_invoices,[8],"Traceback (most recent call last):
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/app.py"", line 62, in application
response = frappe.handler.handle()
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 22, in handle
data = execute_cmd(cmd)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 53, in execute_cmd
return frappe.call(method, **frappe.form_dict)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/__init__.py"", line 939, in call
return fn(*args, **newargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/handler.py"", line 81, in runserverobj
frappe.desk.form.run_method.runserverobj(method, docs=docs, dt=dt, dn=dn, arg=arg, args=args)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/desk/form/run_method.py"", line 36, in runserverobj
r = doc.run_method(method)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 755, in run_method
out = Document.hook(fn)(self, *args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 1024, in composer
return composed(self, method, *args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 1007, in runner
add_to_return_value(self, fn(self, *args, **kwargs))
File ""/Users/umairsayyed/frappe-bench/apps/frappe/frappe/model/document.py"", line 749, in <lambda>
fn = lambda self, *args, **kwargs: getattr(self, method)(*args, **kwargs)
File ""/Users/umairsayyed/frappe-bench/apps/erpnext/erpnext/accounts/doctype/opening_invoice_creation_tool/opening_invoice_creation_tool.py"", line 68, in make_invoices
if not row.qty:
AttributeError: 'OpeningInvoiceCreationToolItem' object has no attribute 'qty'",AttributeError,buggy_snippets_files/3704aa5fc646f9fe8908e43e6a62a21b29fa9caa4dce26efc5a5a3f39813d66e_before_merge.py,buggy_snippets_files/3704aa5fc646f9fe8908e43e6a62a21b29fa9caa4dce26efc5a5a3f39813d66e_after_merge.py
17951,"    def handle(self):
        from clikit.utils.terminal import Terminal
        from poetry.repositories.installed_repository import InstalledRepository
        from poetry.semver import Version

        package = self.argument(""package"")

        if self.option(""tree""):
            self.init_styles(self.io)

        if self.option(""outdated""):
            self._args.set_option(""latest"", True)

        include_dev = not self.option(""no-dev"")
        locked_repo = self.poetry.locker.locked_repository(include_dev)

        # Show tree view if requested
        if self.option(""tree"") and not package:
            requires = self.poetry.package.requires + self.poetry.package.dev_requires
            packages = locked_repo.packages
            for package in packages:
                for require in requires:
                    if package.name == require.name:
                        self.display_package_tree(self._io, package, locked_repo)
                        break

            return 0

        table = self.table(style=""compact"")
        # table.style.line_vc_char = """"
        locked_packages = locked_repo.packages

        if package:
            pkg = None
            for locked in locked_packages:
                if package.lower() == locked.name:
                    pkg = locked
                    break

            if not pkg:
                raise ValueError(""Package {} not found"".format(package))

            if self.option(""tree""):
                self.display_package_tree(self.io, pkg, locked_repo)

                return 0

            rows = [
                [""<info>name</>"", "" : <c1>{}</>"".format(pkg.pretty_name)],
                [""<info>version</>"", "" : <b>{}</b>"".format(pkg.pretty_version)],
                [""<info>description</>"", "" : {}"".format(pkg.description)],
            ]

            table.add_rows(rows)
            table.render(self.io)

            if pkg.requires:
                self.line("""")
                self.line(""<info>dependencies</info>"")
                for dependency in pkg.requires:
                    self.line(
                        "" - <c1>{}</c1> <b>{}</b>"".format(
                            dependency.pretty_name, dependency.pretty_constraint
                        )
                    )

            return 0

        show_latest = self.option(""latest"")
        show_all = self.option(""all"")
        terminal = Terminal()
        width = terminal.width
        name_length = version_length = latest_length = 0
        latest_packages = {}
        latest_statuses = {}
        installed_repo = InstalledRepository.load(self.env)
        skipped = []

        python = Version.parse(""."".join([str(i) for i in self.env.version_info[:3]]))

        # Computing widths
        for locked in locked_packages:
            python_constraint = locked.python_constraint
            if not python_constraint.allows(python) or not self.env.is_valid_for_marker(
                locked.marker
            ):
                skipped.append(locked)

                if not show_all:
                    continue

            current_length = len(locked.pretty_name)
            if not self._io.output.supports_ansi():
                installed_status = self.get_installed_status(locked, installed_repo)

                if installed_status == ""not-installed"":
                    current_length += 4

            if show_latest:
                latest = self.find_latest_package(locked, include_dev)
                if not latest:
                    latest = locked

                latest_packages[locked.pretty_name] = latest
                update_status = latest_statuses[
                    locked.pretty_name
                ] = self.get_update_status(latest, locked)

                if not self.option(""outdated"") or update_status != ""up-to-date"":
                    name_length = max(name_length, current_length)
                    version_length = max(
                        version_length, len(locked.full_pretty_version)
                    )
                    latest_length = max(latest_length, len(latest.full_pretty_version))
            else:
                name_length = max(name_length, current_length)
                version_length = max(version_length, len(locked.full_pretty_version))

        write_version = name_length + version_length + 3 <= width
        write_latest = name_length + version_length + latest_length + 3 <= width
        write_description = name_length + version_length + latest_length + 24 <= width

        for locked in locked_packages:
            color = ""cyan""
            name = locked.pretty_name
            install_marker = """"
            if locked in skipped:
                if not show_all:
                    continue

                color = ""black;options=bold""
            else:
                installed_status = self.get_installed_status(locked, installed_repo)
                if installed_status == ""not-installed"":
                    color = ""red""

                    if not self._io.output.supports_ansi():
                        # Non installed in non decorated mode
                        install_marker = "" (!)""

            line = ""<fg={}>{:{}}{}</>"".format(
                color, name, name_length - len(install_marker), install_marker
            )
            if write_version:
                line += "" <b>{:{}}</b>"".format(
                    locked.full_pretty_version, version_length
                )
            if show_latest:
                latest = latest_packages[locked.pretty_name]
                update_status = latest_statuses[locked.pretty_name]

                if self.option(""outdated"") and update_status == ""up-to-date"":
                    continue

                if write_latest:
                    color = ""green""
                    if update_status == ""semver-safe-update"":
                        color = ""red""
                    elif update_status == ""update-possible"":
                        color = ""yellow""

                    line += "" <fg={}>{:{}}</>"".format(
                        color, latest.full_pretty_version, latest_length
                    )

            if write_description:
                description = locked.description
                remaining = width - name_length - version_length - 4
                if show_latest:
                    remaining -= latest_length

                if len(locked.description) > remaining:
                    description = description[: remaining - 3] + ""...""

                line += "" "" + description

            self.line(line)
","    def handle(self):
        from clikit.utils.terminal import Terminal
        from poetry.repositories.installed_repository import InstalledRepository
        from poetry.semver import Version

        package = self.argument(""package"")

        if self.option(""tree""):
            self.init_styles(self.io)

        if self.option(""outdated""):
            self._args.set_option(""latest"", True)

        include_dev = not self.option(""no-dev"")
        locked_repo = self.poetry.locker.locked_repository(include_dev)

        # Show tree view if requested
        if self.option(""tree"") and not package:
            requires = self.poetry.package.requires + self.poetry.package.dev_requires
            packages = locked_repo.packages
            for package in packages:
                for require in requires:
                    if package.name == require.name:
                        self.display_package_tree(self._io, package, locked_repo)
                        break

            return 0

        table = self.table(style=""compact"")
        # table.style.line_vc_char = """"
        locked_packages = locked_repo.packages

        if package:
            pkg = None
            for locked in locked_packages:
                if package.lower() == locked.name:
                    pkg = locked
                    break

            if not pkg:
                raise ValueError(""Package {} not found"".format(package))

            if self.option(""tree""):
                self.display_package_tree(self.io, pkg, locked_repo)

                return 0

            rows = [
                [""<info>name</>"", "" : <c1>{}</>"".format(pkg.pretty_name)],
                [""<info>version</>"", "" : <b>{}</b>"".format(pkg.pretty_version)],
                [""<info>description</>"", "" : {}"".format(pkg.description)],
            ]

            table.add_rows(rows)
            table.render(self.io)

            if pkg.requires:
                self.line("""")
                self.line(""<info>dependencies</info>"")
                for dependency in pkg.requires:
                    self.line(
                        "" - <c1>{}</c1> <b>{}</b>"".format(
                            dependency.pretty_name, dependency.pretty_constraint
                        )
                    )

            return 0

        show_latest = self.option(""latest"")
        show_all = self.option(""all"")
        terminal = Terminal()
        width = terminal.width
        name_length = version_length = latest_length = 0
        latest_packages = {}
        latest_statuses = {}
        installed_repo = InstalledRepository.load(self.env)
        skipped = []

        python = Version.parse(""."".join([str(i) for i in self.env.version_info[:3]]))

        # Computing widths
        for locked in locked_packages:
            python_constraint = locked.python_constraint
            if not python_constraint.allows(python) or not self.env.is_valid_for_marker(
                locked.marker
            ):
                skipped.append(locked)

                if not show_all:
                    continue

            current_length = len(locked.pretty_name)
            if not self._io.output.supports_ansi():
                installed_status = self.get_installed_status(locked, installed_repo)

                if installed_status == ""not-installed"":
                    current_length += 4

            if show_latest:
                latest = self.find_latest_package(locked, include_dev)
                if not latest:
                    latest = locked

                latest_packages[locked.pretty_name] = latest
                update_status = latest_statuses[
                    locked.pretty_name
                ] = self.get_update_status(latest, locked)

                if not self.option(""outdated"") or update_status != ""up-to-date"":
                    name_length = max(name_length, current_length)
                    version_length = max(
                        version_length, len(locked.full_pretty_version)
                    )
                    latest_length = max(latest_length, len(latest.full_pretty_version))
            else:
                name_length = max(name_length, current_length)
                version_length = max(version_length, len(locked.full_pretty_version))

        write_version = name_length + version_length + 3 <= width
        write_latest = name_length + version_length + latest_length + 3 <= width
        write_description = name_length + version_length + latest_length + 24 <= width

        for locked in locked_packages:
            color = ""cyan""
            name = locked.pretty_name
            install_marker = """"
            if locked in skipped:
                if not show_all:
                    continue

                color = ""black;options=bold""
            else:
                installed_status = self.get_installed_status(locked, installed_repo)
                if installed_status == ""not-installed"":
                    color = ""red""

                    if not self._io.output.supports_ansi():
                        # Non installed in non decorated mode
                        install_marker = "" (!)""

            if (
                show_latest
                and self.option(""outdated"")
                and latest_statuses[locked.pretty_name] == ""up-to-date""
            ):
                continue

            line = ""<fg={}>{:{}}{}</>"".format(
                color, name, name_length - len(install_marker), install_marker
            )
            if write_version:
                line += "" <b>{:{}}</b>"".format(
                    locked.full_pretty_version, version_length
                )
            if show_latest:
                latest = latest_packages[locked.pretty_name]
                update_status = latest_statuses[locked.pretty_name]

                if write_latest:
                    color = ""green""
                    if update_status == ""semver-safe-update"":
                        color = ""red""
                    elif update_status == ""update-possible"":
                        color = ""yellow""

                    line += "" <fg={}>{:{}}</>"".format(
                        color, latest.full_pretty_version, latest_length
                    )

            if write_description:
                description = locked.description
                remaining = width - name_length - version_length - 4
                if show_latest:
                    remaining -= latest_length

                if len(locked.description) > remaining:
                    description = description[: remaining - 3] + ""...""

                line += "" "" + description

            self.line(line)
",https://github.com/python-poetry/poetry/issues/1323,CWE-168: Improper Handling of Inconsistent Special Elements,"There are branches in the handle function in which name_length - len(install_marker)=0, which makes format string fail to work",poetry/console/commands/show.py,ShowCommand.handle,"[141,143]","Using virtualenv: /home/oakkitten/.cache/pypoetry/virtualenvs/woo-2dsX_qwR-py3.7

[ValueError]
'=' alignment not allowed in string format specifier

Traceback (most recent call last):
File ""/home/oakkitten/.poetry/lib/poetry/_vendor/py3.7/clikit/console_application.py"", line 132, in run
status_code = command.handle(parsed_args, io)
File ""/home/oakkitten/.poetry/lib/poetry/_vendor/py3.7/clikit/api/command/command.py"", line 119, in handle
status_code = self._do_handle(args, io)
File ""/home/oakkitten/.poetry/lib/poetry/_vendor/py3.7/clikit/api/command/command.py"", line 167, in _do_handle
return getattr(handler, handler_method)(args, io, self)
File ""/home/oakkitten/.poetry/lib/poetry/_vendor/py3.7/cleo/commands/command.py"", line 92, in wrap_handle
return self.handle()
File ""/home/oakkitten/.poetry/lib/poetry/console/commands/show.py"", line 177, in handle
color, name, name_length - len(install_marker), install_marker",ValueError,buggy_snippets_files/ca9ff055a513a6a6d236f47e7f046e2d89780f6ee5d771e645fb02527f9a3c1b_before_merge.py,buggy_snippets_files/ca9ff055a513a6a6d236f47e7f046e2d89780f6ee5d771e645fb02527f9a3c1b_after_merge.py
18026,"    def handle(self):
        from poetry.config.config import Config
        from poetry.config.config_source import ConfigSource
        from poetry.locations import CONFIG_DIR
        from poetry.utils._compat import Path
        from poetry.utils._compat import basestring
        from poetry.utils.toml_file import TomlFile

        config = Config()
        config_file = TomlFile(Path(CONFIG_DIR) / ""config.toml"")
        config_source = ConfigSource(config_file)
        config.merge(config_source.file.read())

        auth_config_file = TomlFile(Path(CONFIG_DIR) / ""auth.toml"")
        auth_config_source = ConfigSource(auth_config_file, auth_config=True)

        local_config_file = TomlFile(self.poetry.file.parent / ""poetry.toml"")
        if local_config_file.exists():
            config.merge(local_config_file.read())

        if self.option(""local""):
            config_source = ConfigSource(local_config_file)

        if not config_file.exists():
            config_file.path.parent.mkdir(parents=True, exist_ok=True)
            config_file.touch(mode=0o0600)

        if self.option(""list""):
            self._list_configuration(config.all(), config.raw())

            return 0

        setting_key = self.argument(""key"")
        if not setting_key:
            return 0

        if self.argument(""value"") and self.option(""unset""):
            raise RuntimeError(""You can not combine a setting value with --unset"")

        # show the value if no value is provided
        if not self.argument(""value"") and not self.option(""unset""):
            m = re.match(r""^repos?(?:itories)?(?:\.(.+))?"", self.argument(""key""))
            if m:
                if not m.group(1):
                    value = {}
                    if config.get(""repositories"") is not None:
                        value = config.get(""repositories"")
                else:
                    repo = config.get(""repositories.{}"".format(m.group(1)))
                    if repo is None:
                        raise ValueError(
                            ""There is no {} repository defined"".format(m.group(1))
                        )

                    value = repo

                self.line(str(value))
            else:
                values = self.unique_config_values
                if setting_key not in values:
                    raise ValueError(""There is no {} setting."".format(setting_key))

                value = config.get(setting_key)

                if not isinstance(value, basestring):
                    value = json.dumps(value)

                self.line(value)

            return 0

        values = self.argument(""value"")

        unique_config_values = self.unique_config_values
        if setting_key in unique_config_values:
            if self.option(""unset""):
                return config_source.remove_property(setting_key)

            return self._handle_single_value(
                config_source, setting_key, unique_config_values[setting_key], values
            )

        # handle repositories
        m = re.match(r""^repos?(?:itories)?(?:\.(.+))?"", self.argument(""key""))
        if m:
            if not m.group(1):
                raise ValueError(""You cannot remove the [repositories] section"")

            if self.option(""unset""):
                repo = config.get(""repositories.{}"".format(m.group(1)))
                if repo is None:
                    raise ValueError(
                        ""There is no {} repository defined"".format(m.group(1))
                    )

                config_source.remove_property(""repositories.{}"".format(m.group(1)))

                return 0

            if len(values) == 1:
                url = values[0]

                config_source.add_property(
                    ""repositories.{}.url"".format(m.group(1)), url
                )

                return 0

            raise ValueError(
                ""You must pass the url. ""
                ""Example: poetry config repositories.foo https://bar.com""
            )

        # handle auth
        m = re.match(r""^(http-basic|pypi-token)\.(.+)"", self.argument(""key""))
        if m:
            if self.option(""unset""):
                keyring_repository_password_del(config, m.group(2))
                auth_config_source.remove_property(
                    ""{}.{}"".format(m.group(1), m.group(2))
                )

                return 0

            if m.group(1) == ""http-basic"":
                if len(values) == 1:
                    username = values[0]
                    # Only username, so we prompt for password
                    password = self.secret(""Password:"")
                elif len(values) != 2:
                    raise ValueError(
                        ""Expected one or two arguments ""
                        ""(username, password), got {}"".format(len(values))
                    )
                else:
                    username = values[0]
                    password = values[1]

                property_value = dict(username=username)
                try:
                    keyring_repository_password_set(m.group(2), username, password)
                except RuntimeError:
                    property_value.update(password=password)

                auth_config_source.add_property(
                    ""{}.{}"".format(m.group(1), m.group(2)), property_value
                )
            elif m.group(1) == ""pypi-token"":
                if len(values) != 1:
                    raise ValueError(
                        ""Expected only one argument (token), got {}"".format(len(values))
                    )

                token = values[0]

                auth_config_source.add_property(
                    ""{}.{}"".format(m.group(1), m.group(2)), token
                )

            return 0

        raise ValueError(""Setting {} does not exist"".format(self.argument(""key"")))
","    def handle(self):
        from poetry.config.config import Config
        from poetry.config.config_source import ConfigSource
        from poetry.locations import CONFIG_DIR
        from poetry.utils._compat import Path
        from poetry.utils._compat import basestring
        from poetry.utils.toml_file import TomlFile

        config = Config()
        config_file = TomlFile(Path(CONFIG_DIR) / ""config.toml"")
        config_source = ConfigSource(config_file)
        if config_source.file.exists():
            config.merge(config_source.file.read())

        auth_config_file = TomlFile(Path(CONFIG_DIR) / ""auth.toml"")
        auth_config_source = ConfigSource(auth_config_file, auth_config=True)

        local_config_file = TomlFile(self.poetry.file.parent / ""poetry.toml"")
        if local_config_file.exists():
            config.merge(local_config_file.read())

        if self.option(""local""):
            config_source = ConfigSource(local_config_file)

        if not config_file.exists():
            config_file.path.parent.mkdir(parents=True, exist_ok=True)
            config_file.touch(mode=0o0600)

        if self.option(""list""):
            self._list_configuration(config.all(), config.raw())

            return 0

        setting_key = self.argument(""key"")
        if not setting_key:
            return 0

        if self.argument(""value"") and self.option(""unset""):
            raise RuntimeError(""You can not combine a setting value with --unset"")

        # show the value if no value is provided
        if not self.argument(""value"") and not self.option(""unset""):
            m = re.match(r""^repos?(?:itories)?(?:\.(.+))?"", self.argument(""key""))
            if m:
                if not m.group(1):
                    value = {}
                    if config.get(""repositories"") is not None:
                        value = config.get(""repositories"")
                else:
                    repo = config.get(""repositories.{}"".format(m.group(1)))
                    if repo is None:
                        raise ValueError(
                            ""There is no {} repository defined"".format(m.group(1))
                        )

                    value = repo

                self.line(str(value))
            else:
                values = self.unique_config_values
                if setting_key not in values:
                    raise ValueError(""There is no {} setting."".format(setting_key))

                value = config.get(setting_key)

                if not isinstance(value, basestring):
                    value = json.dumps(value)

                self.line(value)

            return 0

        values = self.argument(""value"")

        unique_config_values = self.unique_config_values
        if setting_key in unique_config_values:
            if self.option(""unset""):
                return config_source.remove_property(setting_key)

            return self._handle_single_value(
                config_source, setting_key, unique_config_values[setting_key], values
            )

        # handle repositories
        m = re.match(r""^repos?(?:itories)?(?:\.(.+))?"", self.argument(""key""))
        if m:
            if not m.group(1):
                raise ValueError(""You cannot remove the [repositories] section"")

            if self.option(""unset""):
                repo = config.get(""repositories.{}"".format(m.group(1)))
                if repo is None:
                    raise ValueError(
                        ""There is no {} repository defined"".format(m.group(1))
                    )

                config_source.remove_property(""repositories.{}"".format(m.group(1)))

                return 0

            if len(values) == 1:
                url = values[0]

                config_source.add_property(
                    ""repositories.{}.url"".format(m.group(1)), url
                )

                return 0

            raise ValueError(
                ""You must pass the url. ""
                ""Example: poetry config repositories.foo https://bar.com""
            )

        # handle auth
        m = re.match(r""^(http-basic|pypi-token)\.(.+)"", self.argument(""key""))
        if m:
            if self.option(""unset""):
                keyring_repository_password_del(config, m.group(2))
                auth_config_source.remove_property(
                    ""{}.{}"".format(m.group(1), m.group(2))
                )

                return 0

            if m.group(1) == ""http-basic"":
                if len(values) == 1:
                    username = values[0]
                    # Only username, so we prompt for password
                    password = self.secret(""Password:"")
                elif len(values) != 2:
                    raise ValueError(
                        ""Expected one or two arguments ""
                        ""(username, password), got {}"".format(len(values))
                    )
                else:
                    username = values[0]
                    password = values[1]

                property_value = dict(username=username)
                try:
                    keyring_repository_password_set(m.group(2), username, password)
                except RuntimeError:
                    property_value.update(password=password)

                auth_config_source.add_property(
                    ""{}.{}"".format(m.group(1), m.group(2)), property_value
                )
            elif m.group(1) == ""pypi-token"":
                if len(values) != 1:
                    raise ValueError(
                        ""Expected only one argument (token), got {}"".format(len(values))
                    )

                token = values[0]

                auth_config_source.add_property(
                    ""{}.{}"".format(m.group(1), m.group(2)), token
                )

            return 0

        raise ValueError(""Setting {} does not exist"".format(self.argument(""key"")))
",https://github.com/python-poetry/poetry/issues/1179,"CWE-706: Use of Incorrectly-Resolved Name or Reference, CWE-248: Uncaught Exception",the situation is unhandled where a file corresponding to a variable config_source does not exist ,poetry/console/commands/config.py,ConfigCommand.handle,[12],"root@58ef10ebedba:/build# poetry --version
Poetry version 1.0.0a4
root@58ef10ebedba:/build# poetry config -vvv settings.virtualenvs.in-project true

[FileNotFoundError]
[Errno 2] No such file or directory: '/root/.config/pypoetry/config.toml'

Traceback (most recent call last):
File ""/usr/local/lib/python3.7/dist-packages/clikit/console_application.py"", line 131, in run
status_code = command.handle(parsed_args, io)
File ""/usr/local/lib/python3.7/dist-packages/clikit/api/command/command.py"", line 112, in handle
status_code = self._do_handle(args, io)
File ""/usr/local/lib/python3.7/dist-packages/clikit/api/command/command.py"", line 160, in _do_handle
return getattr(handler, handler_method)(args, io, self)
File ""/usr/local/lib/python3.7/dist-packages/cleo/commands/command.py"", line 92, in wrap_handle
return self.handle()
File ""/usr/local/lib/python3.7/dist-packages/poetry/console/commands/config.py"", line 152, in handle
setting_key, unique_config_values[setting_key], values
File ""/usr/local/lib/python3.7/dist-packages/poetry/console/commands/config.py"", line 240, in _handle_single_value
self._settings_config.add_property(key, normalizer(value))
File ""/usr/local/lib/python3.7/dist-packages/poetry/config.py"", line 65, in add_property
self.dump()
File ""/usr/local/lib/python3.7/dist-packages/poetry/config.py"", line 101, in dump
fd = os.open(str(self._file), os.O_WRONLY | os.O_CREAT, mode)",FileNotFoundError,buggy_snippets_files/7161b2992de47bba98984444083cc968c76c3309c9dde6e0a1b465d8e9faa8c3_before_merge.py,buggy_snippets_files/7161b2992de47bba98984444083cc968c76c3309c9dde6e0a1b465d8e9faa8c3_after_merge.py
18124,"def get_data():
	return frappe._dict({
		""dashboards"": get_dashboards(),
		""charts"": get_charts(),
	})","def get_data():
	data = frappe._dict({
		""dashboards"": [],
		""charts"": []
	})
	company = get_company_for_dashboards()
	if company:
		company_doc = frappe.get_doc(""Company"", company)
		data.dashboards = get_dashboards()
		data.charts = get_charts(company_doc)
	return data
",https://github.com/frappe/erpnext/issues/21669,"CWE-665: Improper Initialization,  CWE-248: Uncaught Exception","In the call of get_charts() function a frappe.exceptions.DoesNotExistError error occurs where get_company_for_dashboards() returns None, i.e. if company is not set. The sutuation is not handled where frappe.exceptions.DoesNotExistError occurs in the get_charts function",erpnext/accounts/dashboard_fixtures.py,get_data,[4],"Migrating test.localhost
Executing execute:frappe.reload_doc('core', 'doctype', 'doctype_action', force=True) #2019-09-23 in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.248s
Executing execute:frappe.reload_doc('core', 'doctype', 'doctype_link', force=True) #2019-09-23 in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.107s
Executing frappe.patches.v12_0.change_existing_dashboard_chart_filters in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.047s
Executing execute:frappe.delete_doc(""Test Runner"") in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.017s
Executing execute:frappe.db.set_default('desktop:home_page', 'workspace') in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.081s
Executing frappe.patches.v13_0.website_theme_custom_scss in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.364s
Executing frappe.patches.v13_0.set_existing_dashboard_charts_as_public in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.366s
Executing frappe.patches.v13_0.set_path_for_homepage_in_web_page_view in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.155s
Executing frappe.patches.v13_0.migrate_translation_column_data in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.181s
Executing frappe.patches.v13_0.set_read_times in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.286s
Executing frappe.patches.v13_0.remove_web_view in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.013s
Executing erpnext.patches.v12_0.update_is_cancelled_field in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.017s
Executing erpnext.patches.v11_0.rename_duplicate_item_code_values in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.012s
Executing execute:frappe.delete_doc_if_exists(""Report"", ""Loan Repayment"") in test.localhost (_fcadc07dbdbc21db)
Success: Done in 1.58s
Executing erpnext.patches.v12_0.set_automatically_process_deferred_accounting_in_accounts_settings in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.268s
Executing erpnext.patches.v12_0.set_production_capacity_in_workstation in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.242s
Executing erpnext.patches.v12_0.set_employee_preferred_emails in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.072s
Executing erpnext.patches.v12_0.set_against_blanket_order_in_sales_and_purchase_order in test.localhost (_fcadc07dbdbc21db)
Success: Done in 1.073s
Executing erpnext.patches.v12_0.set_cost_center_in_child_table_of_expense_claim in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.189s
Executing erpnext.patches.v12_0.add_eway_bill_in_delivery_note in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.088s
Executing erpnext.patches.v12_0.set_lead_title_field in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.581s
Executing erpnext.patches.v12_0.set_published_in_hub_tracked_item in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.374s
Executing erpnext.patches.v12_0.set_job_offer_applicant_email in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.258s
Executing erpnext.patches.v12_0.move_bank_account_swift_number_to_bank in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.594s
Executing erpnext.patches.v12_0.rename_bank_reconciliation in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.057s
Executing erpnext.patches.v12_0.set_purchase_receipt_delivery_note_detail in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.919s
Executing erpnext.patches.v12_0.add_permission_in_lower_deduction in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.016s
Executing erpnext.patches.v12_0.rename_account_type_doctype in test.localhost (_fcadc07dbdbc21db)
Success: Done in 1.138s
Executing erpnext.patches.v12_0.update_healthcare_refactored_changes in test.localhost (_fcadc07dbdbc21db)
Success: Done in 3.16s
Executing erpnext.patches.v12_0.set_total_batch_quantity in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.251s
Executing erpnext.patches.v12_0.set_updated_purpose_in_pick_list in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.264s
Executing erpnext.patches.v12_0.set_default_payroll_based_on in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.205s
Executing erpnext.patches.v12_0.update_end_date_and_status_in_email_campaign in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.019s
Executing erpnext.patches.v13_0.move_tax_slabs_from_payroll_period_to_income_tax_slab #123 in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.725s
Executing erpnext.patches.v12_0.fix_quotation_expired_status in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.016s
Executing erpnext.patches.v12_0.update_appointment_reminder_scheduler_entry in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.013s
Executing erpnext.patches.v12_0.retain_permission_rules_for_video_doctype in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.388s
Executing erpnext.patches.v13_0.patch_to_fix_reverse_linking_in_additional_salary_encashment_and_incentive in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.881s
Executing execute:frappe.delete_doc_if_exists(""Page"", ""appointment-analytic"") in test.localhost (_fcadc07dbdbc21db)
Success: Done in 0.165s
Updating DocTypes for frappe        : [========================================] 100%
Updating DocTypes for erpnext       : [========================================] 100%
Updating Dashboard for frappe
Updating Dashboard for erpnext
Traceback (most recent call last):
File ""/home/frappe/frappe-bench/commands/auto_migrate.py"", line 115, in <module>
main()
File ""/home/frappe/frappe-bench/commands/auto_migrate.py"", line 109, in main
migrate_sites(maintenance_mode=True)
File ""/home/frappe/frappe-bench/commands/migrate.py"", line 41, in migrate_sites
migrate()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/migrate.py"", line 58, in migrate
sync_dashboards()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/utils/dashboard.py"", line 86, in sync_dashboards
config = get_config(app_name, module_name)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/utils/dashboard.py"", line 110, in get_config
return frappe._dict(module_dashboards.get_data())
File ""/home/frappe/frappe-bench/apps/erpnext/erpnext/accounts/dashboard_fixtures.py"", line 11, in get_data
""charts"": get_charts(),
File ""/home/frappe/frappe-bench/apps/erpnext/erpnext/accounts/dashboard_fixtures.py"", line 28, in get_charts
company = frappe.get_doc(""Company"", get_company_for_dashboards())
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 772, in get_doc
doc = frappe.model.document.get_doc(*args, **kwargs)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/model/document.py"", line 71, in get_doc
return controller(*args, **kwargs)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/model/document.py"", line 106, in __init__
self.load_from_db()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/model/document.py"", line 149, in load_from_db
frappe.throw(_(""{0} {1} not found"").format(_(self.doctype), self.name), frappe.DoesNotExistError)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 397, in throw
msgprint(msg, raise_exception=exc, title=title, indicator='red', is_minimizable=is_minimizable)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 376, in msgprint
_raise_exception()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 327, in _raise_exception
raise raise_exception(msg)
frappe.exceptions.DoesNotExistError: Company None not found",frappe.exceptions.DoesNotExistError,buggy_snippets_files/0d980170b04de1d0a90ffd37d3c8162bc40a7e17b763cdba7a87e1e4d9df14bd_before_merge.py,buggy_snippets_files/0d980170b04de1d0a90ffd37d3c8162bc40a7e17b763cdba7a87e1e4d9df14bd_after_merge.py
18624,"def delete_lead_addresses(company_name):
	""""""Delete addresses to which leads are linked""""""
	leads = frappe.get_all(""Lead"", filters={""company"": company_name})
	leads = [ ""'%s'""%row.get(""name"") for row in leads ]
	addresses = []
	if leads:
		addresses = frappe.db.sql_list(""""""select parent from `tabDynamic Link` where link_name 
			in ({leads})"""""".format(leads="","".join(leads)))

		if addresses:
			addresses = [""'%s'""%addr for addr in addresses]

			frappe.db.sql(""""""delete from tabAddress where name in ({addresses}) and 
				name not in (select distinct dl1.parent from `tabDynamic Link` dl1 
				inner join `tabDynamic Link` dl2 on dl1.parent=dl2.parent 
				and dl1.link_doctype<>dl2.link_doctype)"""""".format(addresses="","".join(addresses)))

			frappe.db.sql(""""""delete from `tabDynamic Link` where link_doctype='Lead' 
				and parenttype='Address' and link_name in ({leads})"""""".format(leads="","".join(leads)))

		frappe.db.sql(""""""update tabCustomer set lead_name=NULL where lead_name in ({leads})"""""".format(leads="","".join(leads)))
","def delete_lead_addresses(company_name):
	""""""Delete addresses to which leads are linked""""""
	leads = frappe.get_all(""Lead"", filters={""company"": company_name})
	leads = [ ""'%s'""%row.get(""name"") for row in leads ]
	addresses = []
	if leads:
		addresses = frappe.db.sql_list(""""""select parent from `tabDynamic Link` where link_name 
			in ({leads})"""""".format(leads="","".join(leads)))

		if addresses:
			addresses = [""'%s'""%frappe.db.escape(addr) for addr in addresses]

			frappe.db.sql(""""""delete from tabAddress where name in ({addresses}) and 
				name not in (select distinct dl1.parent from `tabDynamic Link` dl1 
				inner join `tabDynamic Link` dl2 on dl1.parent=dl2.parent 
				and dl1.link_doctype<>dl2.link_doctype)"""""".format(addresses="","".join(addresses)))

			frappe.db.sql(""""""delete from `tabDynamic Link` where link_doctype='Lead' 
				and parenttype='Address' and link_name in ({leads})"""""".format(leads="","".join(leads)))

		frappe.db.sql(""""""update tabCustomer set lead_name=NULL where lead_name in ({leads})"""""".format(leads="","".join(leads)))
",https://github.com/frappe/erpnext/issues/13424,"		
CWE-94: Improper Control of Generation of Code ('Code Injection'),  CWE-248: Uncaught Exception",when sql query is run in the expression frappe.db.sql db escape as single quote in address causes error. The situation is unhandled when the adress has incorrect syntax,erpnext/setup/doctype/company/delete_company_transactions.py,delete_lead_addresses,"[11, 16]","Traceback (most recent call last):
File ""/home/frappe/frappe-bench/apps/frappe/frappe/app.py"", line 62, in application
response = frappe.handler.handle()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/handler.py"", line 22, in handle
data = execute_cmd(cmd)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/handler.py"", line 53, in execute_cmd
return frappe.call(method, **frappe.form_dict)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 939, in call
return fn(*args, **newargs)
File ""/home/frappe/frappe-bench/apps/erpnext/erpnext/setup/doctype/company/delete_company_transactions.py"", line 21, in delete_company_transactions
delete_lead_addresses(company_name)
File ""/home/frappe/frappe-bench/apps/erpnext/erpnext/setup/doctype/company/delete_company_transactions.py"", line 93, in delete_lead_addresses
and dl1.link_doctype<>dl2.link_doctype)"""""".format(addresses="","".join(addresses)))
File ""/home/frappe/frappe-bench/apps/frappe/frappe/database.py"", line 176, in sql
self._cursor.execute(query)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/cursors.py"", line 165, in execute
result = self._query(query)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/cursors.py"", line 321, in _query
conn.query(q)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/connections.py"", line 860, in query
self._affected_rows = self._read_query_result(unbuffered=unbuffered)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/connections.py"", line 1061, in _read_query_result
result.read()
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/connections.py"", line 1349, in read
first_packet = self.connection._read_packet()
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/connections.py"", line 1018, in _read_packet
packet.check_error()
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/connections.py"", line 384, in check_error
err.raise_mysql_exception(self._data)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/pymysql/err.py"", line 107, in raise_mysql_exception
raise errorclass(errno, errval)
ProgrammingError: (1064, u""You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 's Seafood-Billing','New Wave Seafood-Billing','Sea Well Seafood, Inc.-Billing','' at line 1"")",ProgrammingError,buggy_snippets_files/500ba87304997df9a1fe8a7a99a750fca76977348815f5b8ccb9185e36380428_before_merge.py,buggy_snippets_files/500ba87304997df9a1fe8a7a99a750fca76977348815f5b8ccb9185e36380428_after_merge.py
18768,"def execute():
	language = frappe.get_single(""System Settings"").language

	if language and language.startswith('en'): return

	frappe.local.lang = language

	all_domains = frappe.get_hooks(""domains"")

	for domain in all_domains:
		translated_domain = _(domain, lang=language)
		if frappe.db.exists(""Domain"", translated_domain):
			frappe.rename_doc(""Domain"", translated_domain, domain, ignore_permissions=True, merge=True)

	domain_settings = frappe.get_single(""Domain Settings"")
	active_domains = [d.domain for d in domain_settings.active_domains]
	
	try:
		for domain in active_domains:
			domain = frappe.get_doc(""Domain"", domain)
			domain.setup_domain()

			if int(frappe.db.get_single_value('System Settings', 'setup_complete')):
				domain.setup_sidebar_items()
				domain.setup_desktop_icons()
				domain.set_default_portal_role()
	except frappe.LinkValidationError:
		pass","def execute():
	language = frappe.get_single(""System Settings"").language

	if language and language.startswith('en'): return

	frappe.local.lang = language

	all_domains = frappe.get_hooks(""domains"")

	for domain in all_domains:
		translated_domain = _(domain, lang=language)
		if frappe.db.exists(""Domain"", translated_domain):
			#if domain already exists merged translated_domain and domain
			merge = False
			if frappe.db.exists(""Domain"", domain):
				merge=True

			frappe.rename_doc(""Domain"", translated_domain, domain, ignore_permissions=True, merge=merge)

	domain_settings = frappe.get_single(""Domain Settings"")
	active_domains = [d.domain for d in domain_settings.active_domains]
	
	try:
		for domain in active_domains:
			domain = frappe.get_doc(""Domain"", domain)
			domain.setup_domain()

			if int(frappe.db.get_single_value('System Settings', 'setup_complete')):
				domain.setup_sidebar_items()
				domain.setup_desktop_icons()
				domain.set_default_portal_role()
	except frappe.LinkValidationError:
		pass",https://github.com/frappe/erpnext/issues/11879,"CWE-236: Improper Handling of Undefined Parameters, CWE-248: Uncaught Exception","when domain does not exist the expression frappe.rename_doc(""Domain"", translated_domain, domain, ignore_permissions=True, merge=True) causes an error during merging. Such a situation is not handled ",erpnext/patches/v9_2/rename_translated_domains_in_en.py,execute,"[12,13]","nadivia@ID16595:/home/frappe/frappe-bench$ sudo bench update
INFO:bench.utils:updating bench
INFO:bench.utils:git pull
Already up-to-date.
INFO:bench.utils:./env/bin/pip install Pillow
Requirement already satisfied: Pillow in ./env/lib/python2.7/site-packages
Requirement already satisfied: olefile in ./env/lib/python2.7/site-packages (from Pillow)
INFO:bench.app:pulling frappe
INFO:bench.utils:git pull  upstream master
Da https://github.com/frappe/frappe
* branch            master     -> FETCH_HEAD
Already up-to-date.
INFO:bench.utils:find . -name ""*.pyc"" -delete
INFO:bench.app:pulling erpnext
INFO:bench.utils:git pull  upstream master
Da https://github.com/frappe/erpnext
* branch            master     -> FETCH_HEAD
Already up-to-date.
INFO:bench.utils:find . -name ""*.pyc"" -delete
Updating Python libraries...
INFO:bench.utils:./env/bin/pip install --upgrade pip
Requirement already up-to-date: pip in ./env/lib/python2.7/site-packages
INFO:bench.utils:./env/bin/pip install -q -r /home/frappe/.bench/requirements.txt
INFO:bench.utils:./env/bin/pip install -q -r ./apps/erpnext/requirements.txt
INFO:bench.utils:./env/bin/pip install -q -r ./apps/frappe/requirements.txt
Updating node libraries...
INFO:bench.utils:npm install
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules/chokidar/node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.3: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})
npm WARN frappe@ No description
Backing up sites...
Patching sites...
Migrating site1.local
Executing erpnext.patches.v9_2.rename_translated_domains_in_en in site1.local (1bd3e0294da19198)
Traceback (most recent call last):
File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
""__main__"", fname, loader, pkg_name)
File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
exec code in run_globals
File ""/home/frappe/frappe-bench/apps/frappe/frappe/utils/bench_helper.py"", line 94, in <module>
main()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/utils/bench_helper.py"", line 18, in main
click.Group(commands=commands)(prog_name='bench')
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/click/core.py"", line 722, in __call__
return self.main(*args, **kwargs)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/click/core.py"", line 697, in main
rv = self.invoke(ctx)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/click/core.py"", line 1066, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/click/core.py"", line 1066, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/click/core.py"", line 895, in invoke
return ctx.invoke(self.callback, **ctx.params)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/click/core.py"", line 535, in invoke
return callback(*args, **kwargs)
File ""/home/frappe/frappe-bench/env/local/lib/python2.7/site-packages/click/decorators.py"", line 17, in new_func
return f(get_current_context(), *args, **kwargs)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/commands/__init__.py"", line 24, in _func
ret = f(frappe._dict(ctx.obj), *args, **kwargs)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/commands/site.py"", line 217, in migrate
migrate(context.verbose, rebuild_website=rebuild_website)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/migrate.py"", line 31, in migrate
frappe.modules.patch_handler.run_all()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/modules/patch_handler.py"", line 29, in run_all
if not run_single(patchmodule = patch):
File ""/home/frappe/frappe-bench/apps/frappe/frappe/modules/patch_handler.py"", line 63, in run_single
return execute_patch(patchmodule, method, methodargs)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/modules/patch_handler.py"", line 83, in execute_patch
frappe.get_attr(patchmodule.split()[0] + "".execute"")()
File ""/home/frappe/frappe-bench/apps/erpnext/erpnext/patches/v9_2/rename_translated_domains_in_en.py"", line 16, in execute
frappe.rename_doc(""Domain"", translated_domain, domain, ignore_permissions=True, merge=True)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 696, in rename_doc
return rename_doc(*args, **kwargs)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/model/rename_doc.py"", line 39, in rename_doc
new = validate_rename(doctype, new, meta, merge, force, ignore_permissions)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/model/rename_doc.py"", line 117, in validate_rename
frappe.msgprint(_(""{0} {1} does not exist, select a new target to merge"").format(doctype, new), raise_exception=1)
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 309, in msgprint
_raise_exception()
File ""/home/frappe/frappe-bench/apps/frappe/frappe/__init__.py"", line 284, in _raise_exception
raise ValidationError(encode(msg))
frappe.exceptions.ValidationError: Domain Manufacturing non esiste, selezionare un nuovo obiettivo da unire",frappe.exceptions.ValidationError,buggy_snippets_files/1742369205658524af1179e5a19859080bfad5c934344bdb32e5d2823ae40879_before_merge.py,buggy_snippets_files/1742369205658524af1179e5a19859080bfad5c934344bdb32e5d2823ae40879_after_merge.py
18839,"	def set_actual_dates(self):
		self.actual_start_date = None
		self.actual_end_date = None
		if self.get(""operations""):
			self.actual_start_date = min([d.actual_start_time for d in self.get(""operations"") if d.actual_start_time])
			self.actual_end_date = max([d.actual_end_time for d in self.get(""operations"") if d.actual_end_time])
","	def set_actual_dates(self):
		self.actual_start_date = None
		self.actual_end_date = None
		if self.get(""operations""):
			actual_start_dates = [d.actual_start_time for d in self.get(""operations"") if d.actual_start_time]
			if actual_start_dates:
				self.actual_start_date = min(actual_start_dates)

			actual_end_dates = [d.actual_end_time for d in self.get(""operations"") if d.actual_end_time]
			if actual_end_dates:
				self.actual_end_date = max(actual_end_dates)
",https://github.com/frappe/erpnext/issues/11039,"CWE-682: Incorrect Calculation, CWE-248: Uncaught Exception","the situation is unhandled where either [d.actual_start_time for d in self.get(""operations"") if d.actual_start_time] or [d.actual_end_time for d in self.get(""operations"") if d.actual_end_time] list is empty",erpnext/manufacturing/doctype/production_order/production_order.py,ProductionOrder.set_actual_dates,"[5,6]","Traceback (most recent call last):
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/app.py"", line 57, in application
response = frappe.handler.handle()
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/handler.py"", line 22, in handle
data = execute_cmd(cmd)
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/handler.py"", line 53, in execute_cmd
return frappe.call(method, **frappe.form_dict)
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/__init__.py"", line 923, in call
return fn(*args, **newargs)
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/desk/form/save.py"", line 45, in cancel
doc.cancel()
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 752, in cancel
self._cancel()
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 742, in _cancel
self.save()
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 231, in save
return self._save(*args, **kwargs)
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 282, in _save
self.run_post_save_methods()
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 801, in run_post_save_methods
self.run_method(""on_cancel"")
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 672, in run_method
out = Document.hook(fn)(self, *args, **kwargs)
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 893, in composer
return composed(self, method, *args, **kwargs)
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 876, in runner
add_to_return_value(self, fn(self, *args, **kwargs))
File ""/home/frappe/benches/bench-2017-10-02/apps/frappe/frappe/model/document.py"", line 666, in
fn = lambda self, *args, **kwargs: getattr(self, method)(*args, **kwargs)
File ""/home/frappe/benches/bench-2017-10-02/apps/erpnext/erpnext/projects/doctype/timesheet/timesheet.py"", line 100, in on_cancel
self.update_production_order(None)
File ""/home/frappe/benches/bench-2017-10-02/apps/erpnext/erpnext/projects/doctype/timesheet/timesheet.py"", line 147, in update_production_order
pro.set_actual_dates()
File ""/home/frappe/benches/bench-2017-10-02/apps/erpnext/erpnext/manufacturing/doctype/production_order/production_order.py"", line 373, in set_actual_dates
self.actual_start_date = min([d.actual_start_time for d in self.get(""operations"") if d.actual_start_time])
ValueError: min() arg is an empty sequence",ValueError,buggy_snippets_files/4d2f07b3c2af2ee365338b89b049ac109665842be941e9bb12ad7ddf4db24e9e_before_merge.py,buggy_snippets_files/4d2f07b3c2af2ee365338b89b049ac109665842be941e9bb12ad7ddf4db24e9e_after_merge.py
19102,"def reorder_item():
	"""""" Reorder item if stock reaches reorder level""""""
	# if initial setup not completed, return
	if not frappe.db.sql(""select name from `tabFiscal Year` limit 1""):
		return

	if cint(frappe.db.get_value('Stock Settings', None, 'auto_indent')):
		return _reorder_item()
","def reorder_item():
	"""""" Reorder item if stock reaches reorder level""""""
	# if initial setup not completed, return
	if not (frappe.db.a_row_exists(""Company"") and frappe.db.a_row_exists(""Fiscal Year"")):
		return

	if cint(frappe.db.get_value('Stock Settings', None, 'auto_indent')):
		return _reorder_item()
",https://github.com/frappe/erpnext/issues/3365,"CWE-754: Improper Check for Unusual or Exceptional Conditions, CWE-248: Uncaught Exception","insufficient exceptional situation check: only the case where frappe.db.sql(""select name from `tabFiscal Year` limit 1"") query results are empty is processed. But if no company record is available, an IndexError occurs if no company record exists",erpnext/stock/reorder_item.py,reorder_item,[4],"Traceback (most recent call last):
File ""/home/frappe/press/benches/1505261338/env/lib/python2.7/site-packages/celery/app/trace.py"", line 240, in trace_task
R = retval = fun(*args, **kwargs)
File ""/home/frappe/press/benches/1505261338/env/lib/python2.7/site-packages/celery/app/trace.py"", line 438, in __protected_call__
return self.run(*args, **kwargs)
File ""/home/frappe/press/benches/1505261338/apps/frappe/frappe/tasks.py"", line 77, in scheduler_task
frappe.get_attr(handler)()
File ""/home/frappe/press/benches/1505261338/apps/erpnext/erpnext/stock/reorder_item.py"", line 15, in reorder_item
return _reorder_item()
File ""/home/frappe/press/benches/1505261338/apps/erpnext/erpnext/stock/reorder_item.py"", line 24, in _reorder_item
frappe.db.sql(""""""select name from tabCompany limit 1"""""")[0][0])
IndexError: tuple index out of range",IndexError,buggy_snippets_files/bcaa1e196dc45aec9a21b34fbe600fc922d95fe9f6131de9b46a0d46669b7bd9_before_merge.py,buggy_snippets_files/bcaa1e196dc45aec9a21b34fbe600fc922d95fe9f6131de9b46a0d46669b7bd9_after_merge.py
19231,"def log(name: str, log_path: Union[str, bool] = intelmq.DEFAULT_LOGGING_PATH,
        log_level: str = intelmq.DEFAULT_LOGGING_LEVEL,
        stream: Optional[object] = None, syslog: Union[bool, str, list, tuple] = None,
        log_format_stream: str = LOG_FORMAT_STREAM,
        logging_level_stream: Optional[str] = None,
        log_max_size: Optional[int] = 0, log_max_copies: Optional[int] = None):
    """"""
    Returns a logger instance logging to file and sys.stderr or other stream.
    The warnings module will log to the same handlers.

    Parameters:
        name: filename for logfile or string preceding lines in stream
        log_path: Path to log directory, defaults to DEFAULT_LOGGING_PATH
            If False, nothing is logged to files.
        log_level: default is %r
        stream: By default (None), stdout and stderr will be used depending on the level.
            If False, stream output is not used.
            For everything else, the argument is used as stream output.
        syslog:
            If False (default), FileHandler will be used. Otherwise either a list/
            tuple with address and UDP port are expected, e.g. `[""localhost"", 514]`
            or a string with device name, e.g. `""/dev/log""`.
        log_format_stream:
            The log format used for streaming output. Default: LOG_FORMAT_STREAM
        logging_level_stream:
            The logging level for stream (console) output.
            By default the same as log_level.

    Returns:
        logger: An instance of logging.Logger

    See also:
        LOG_FORMAT: Default log format for file handler
        LOG_FORMAT_STREAM: Default log format for stream handler
        LOG_FORMAT_SYSLOG: Default log format for syslog
    """""" % intelmq.DEFAULT_LOGGING_LEVEL
    logging.captureWarnings(True)
    warnings_logger = logging.getLogger(""py.warnings"")
    # set the name of the warnings logger to the bot neme, see #1184
    warnings_logger.name = name

    logger = logging.getLogger(name)
    logger.setLevel(log_level)

    if not logging_level_stream:
        logging_level_stream = log_level

    if log_path and not syslog:
        handler = RotatingFileHandler(""%s/%s.log"" % (log_path, name),
                                      maxBytes=log_max_size,
                                      backupCount=log_max_copies)
        handler.setLevel(log_level)
        handler.setFormatter(logging.Formatter(LOG_FORMAT))
    elif syslog:
        if type(syslog) is tuple or type(syslog) is list:
            handler = logging.handlers.SysLogHandler(address=tuple(syslog))
        else:
            handler = logging.handlers.SysLogHandler(address=syslog)
        handler.setLevel(log_level)
        handler.setFormatter(logging.Formatter(LOG_FORMAT_SYSLOG))

    if log_path or syslog:
        logger.addHandler(handler)
        warnings_logger.addHandler(handler)

    if stream or stream is None:
        console_formatter = logging.Formatter(log_format_stream)
        if stream is None:
            console_handler = StreamHandler()
        else:
            console_handler = logging.StreamHandler(stream)
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        warnings_logger.addHandler(console_handler)
        console_handler.setLevel(logging_level_stream)

    return logger
","def log(name: str, log_path: Union[str, bool] = intelmq.DEFAULT_LOGGING_PATH,
        log_level: str = intelmq.DEFAULT_LOGGING_LEVEL,
        stream: Optional[object] = None, syslog: Union[bool, str, list, tuple] = None,
        log_format_stream: str = LOG_FORMAT_STREAM,
        logging_level_stream: Optional[str] = None,
        log_max_size: Optional[int] = 0, log_max_copies: Optional[int] = None):
    """"""
    Returns a logger instance logging to file and sys.stderr or other stream.
    The warnings module will log to the same handlers.

    Parameters:
        name: filename for logfile or string preceding lines in stream
        log_path: Path to log directory, defaults to DEFAULT_LOGGING_PATH
            If False, nothing is logged to files.
        log_level: default is %r
        stream: By default (None), stdout and stderr will be used depending on the level.
            If False, stream output is not used.
            For everything else, the argument is used as stream output.
        syslog:
            If False (default), FileHandler will be used. Otherwise either a list/
            tuple with address and UDP port are expected, e.g. `[""localhost"", 514]`
            or a string with device name, e.g. `""/dev/log""`.
        log_format_stream:
            The log format used for streaming output. Default: LOG_FORMAT_STREAM
        logging_level_stream:
            The logging level for stream (console) output.
            By default the same as log_level.
        log_max_size:
            The maximum size of the logfile. 0 means no restriction.
        log_max_copies:
            Maximum number of logfiles to keep.

    Returns:
        logger: An instance of logging.Logger

    See also:
        LOG_FORMAT: Default log format for file handler
        LOG_FORMAT_STREAM: Default log format for stream handler
        LOG_FORMAT_SYSLOG: Default log format for syslog
    """""" % intelmq.DEFAULT_LOGGING_LEVEL
    logging.captureWarnings(True)
    warnings_logger = logging.getLogger(""py.warnings"")
    # set the name of the warnings logger to the bot neme, see #1184
    warnings_logger.name = name

    logger = logging.getLogger(name)
    logger.setLevel(log_level)

    if not logging_level_stream:
        logging_level_stream = log_level

    if log_path and not syslog:
        handler = RotatingFileHandler(""%s/%s.log"" % (log_path, name),
                                      maxBytes=log_max_size if log_max_size else 0,
                                      backupCount=log_max_copies)
        handler.setLevel(log_level)
        handler.setFormatter(logging.Formatter(LOG_FORMAT))
    elif syslog:
        if type(syslog) is tuple or type(syslog) is list:
            handler = logging.handlers.SysLogHandler(address=tuple(syslog))
        else:
            handler = logging.handlers.SysLogHandler(address=syslog)
        handler.setLevel(log_level)
        handler.setFormatter(logging.Formatter(LOG_FORMAT_SYSLOG))

    if log_path or syslog:
        logger.addHandler(handler)
        warnings_logger.addHandler(handler)

    if stream or stream is None:
        console_formatter = logging.Formatter(log_format_stream)
        if stream is None:
            console_handler = StreamHandler()
        else:
            console_handler = logging.StreamHandler(stream)
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        warnings_logger.addHandler(console_handler)
        console_handler.setLevel(logging_level_stream)

    return logger
",https://github.com/certtools/intelmq/issues/1778,CWE-754: Improper Check for Unusual or Exceptional Conditions,the case is unhandled where log_max_size is None,intelmq/lib/utils.py,log,[50],"Traceback (most recent call last):
File ""/usr/bin/intelmqctl"", line 11, in <module>
load_entry_point('intelmq==2.3.0', 'console_scripts', 'intelmqctl')()
File ""/usr/lib/python3/dist-packages/intelmq/bin/intelmqctl.py"", line 1898, in main
x = IntelMQController(interactive=True)
File ""/usr/lib/python3/dist-packages/intelmq/bin/intelmqctl.py"", line 718, in __init__
log_max_copies=getattr(self.parameters, ""logging_max_copies"", None))
File ""/usr/lib/python3/dist-packages/intelmq/lib/utils.py"", line 356, in log
backupCount=log_max_copies)
File ""/usr/lib/python3.7/logging/handlers.py"", line 145, in __init__
if maxBytes > 0:
TypeError: '>' not supported between instances of 'NoneType' and 'int'",TypeError,buggy_snippets_files/7dab220cd4ebde82397f24ce39d227880e5e76e41c267489586ce9adbdb25b7e_before_merge.py,buggy_snippets_files/7dab220cd4ebde82397f24ce39d227880e5e76e41c267489586ce9adbdb25b7e_after_merge.py
19233,"    def start(self, starting: bool = True, error_on_pipeline: bool = True,
              error_on_message: bool = False, source_pipeline: Optional[str] = None,
              destination_pipeline: Optional[str] = None):

        self.__source_pipeline = source_pipeline
        self.__destination_pipeline = destination_pipeline

        while True:
            try:
                if not starting and (error_on_pipeline or error_on_message):
                    self.logger.info('Bot will continue in %s seconds.',
                                     self.error_retry_delay)
                    time.sleep(self.error_retry_delay)

                starting = False
                error_on_message = False
                message_to_dump = None

                if error_on_pipeline:
                    try:
                        self.__connect_pipelines()
                    except Exception as exc:
                        raise exceptions.PipelineError(exc)
                    else:
                        error_on_pipeline = False

                self.__handle_sighup()
                self.process()
                self.__error_retries_counter = 0  # reset counter

            except exceptions.PipelineError as exc:
                error_on_pipeline = True

                if self.error_log_exception:
                    self.logger.exception('Pipeline failed.')
                else:
                    self.logger.error(utils.error_message_from_exc(exc))
                    self.logger.error('Pipeline failed.')
                self.__disconnect_pipelines()

            except exceptions.DecodingError as exc:
                self.logger.exception('Could not decode message from pipeline. No retries useful.')

                # ensure that we do not re-process the faulty message
                self.__error_retries_counter = self.error_max_retries + 1
                error_on_message = sys.exc_info()

                message_to_dump = exc.object

            except Exception as exc:
                # in case of serious system issues, exit immediately
                if isinstance(exc, MemoryError):
                    self.logger.exception('Out of memory. Exit immediately. Reason: %r.' % exc.args[0])
                    self.stop()
                elif isinstance(exc, (IOError, OSError)) and exc.errno == 28:
                    self.logger.exception('Out of disk space. Exit immediately.')
                    self.stop()

                error_on_message = sys.exc_info()

                if self.error_log_exception:
                    self.logger.exception(""Bot has found a problem."")
                else:
                    self.logger.error(utils.error_message_from_exc(exc))
                    self.logger.error(""Bot has found a problem."")

                if self.error_log_message:
                    # Print full message if explicitly requested by config
                    self.logger.info(""Current Message(event): %r."",
                                     self.__current_message)

                # In case of permanent failures, stop now
                if isinstance(exc, exceptions.ConfigurationError):
                    self.stop()

            except KeyboardInterrupt:
                self.logger.info(""Received KeyboardInterrupt."")
                self.stop(exitcode=0)

            finally:
                do_rate_limit = False

                if error_on_message or error_on_pipeline:
                    self.__message_counter[""failure""] += 1
                    self.__error_retries_counter += 1

                    # reached the maximum number of retries
                    if (self.__error_retries_counter >
                            self.error_max_retries):

                        if error_on_message:

                            if self.error_dump_message:
                                error_traceback = traceback.format_exception(*error_on_message)
                                self._dump_message(error_traceback,
                                                   message=message_to_dump if message_to_dump else self.__current_message)
                            else:
                                warnings.warn(""Message will be removed from the pipeline and not dumped to the disk. ""
                                              ""Set `error_dump_message` to true to save the message on disk. ""
                                              ""This warning is only shown once in the runtime of a bot."")
                            if self.__destination_queues and '_on_error' in self.__destination_queues:
                                self.send_message(self.__current_message, path='_on_error')

                            if message_to_dump or self.__current_message:
                                self.acknowledge_message()

                            # when bot acknowledge the message,
                            # don't need to wait again
                            error_on_message = False

                        # run_mode: scheduled
                        if self.run_mode == 'scheduled':
                            self.logger.info('Shutting down scheduled bot.')
                            self.stop(exitcode=0)

                        # error_procedure: stop
                        elif self.error_procedure == ""stop"":
                            self.stop()

                        # error_procedure: pass
                        elif not error_on_pipeline:
                            self.__error_retries_counter = 0  # reset counter
                            do_rate_limit = True
                        # error_procedure: pass and pipeline problem
                        else:
                            # retry forever, see https://github.com/certtools/intelmq/issues/1333
                            # https://lists.cert.at/pipermail/intelmq-users/2018-October/000085.html
                            pass
                else:
                    self.__message_counter[""success""] += 1
                    do_rate_limit = True

                    # no errors, check for run mode: scheduled
                    if self.run_mode == 'scheduled':
                        self.logger.info('Shutting down scheduled bot.')
                        self.stop(exitcode=0)

                if getattr(self, 'testing', False):
                    self.logger.debug('Testing environment detected, returning now.')
                    return

                # Do rate_limit at the end on success and after the retries
                # counter has been reset: https://github.com/certtools/intelmq/issues/1431
                if do_rate_limit:
                    if self.rate_limit and self.run_mode != 'scheduled':
                        self.__sleep()
                    if self.collector_empty_process and self.run_mode != 'scheduled':
                        self.__sleep(1, log=False)

            self.__stats()
            self.__handle_sighup()
","    def start(self, starting: bool = True, error_on_pipeline: bool = True,
              error_on_message: bool = False, source_pipeline: Optional[str] = None,
              destination_pipeline: Optional[str] = None):

        self.__source_pipeline = source_pipeline
        self.__destination_pipeline = destination_pipeline

        while True:
            try:
                if not starting and (error_on_pipeline or error_on_message):
                    self.logger.info('Bot will continue in %s seconds.',
                                     self.error_retry_delay)
                    time.sleep(self.error_retry_delay)

                starting = False
                error_on_message = False
                message_to_dump = None

                if error_on_pipeline:
                    try:
                        self.__connect_pipelines()
                    except Exception as exc:
                        raise exceptions.PipelineError(exc)
                    else:
                        error_on_pipeline = False

                self.__handle_sighup()
                self.process()
                self.__error_retries_counter = 0  # reset counter

            except exceptions.PipelineError as exc:
                error_on_pipeline = True

                if self.error_log_exception:
                    self.logger.exception('Pipeline failed.')
                else:
                    self.logger.error(utils.error_message_from_exc(exc))
                    self.logger.error('Pipeline failed.')
                self.__disconnect_pipelines()

            except exceptions.DecodingError as exc:
                self.logger.exception('Could not decode message from pipeline. No retries useful.')

                # ensure that we do not re-process the faulty message
                self.__error_retries_counter = self.error_max_retries + 1
                error_on_message = sys.exc_info()

                message_to_dump = exc.object

            except exceptions.InvalidValue as exc:
                self.logger.exception('Found an invalid value that violates the harmonization rules.')

                # ensure that we do not re-process the faulty message
                self.__error_retries_counter = self.error_max_retries + 1
                error_on_message = sys.exc_info()

                message_to_dump = exc.object

            except Exception as exc:
                # in case of serious system issues, exit immediately
                if isinstance(exc, MemoryError):
                    self.logger.exception('Out of memory. Exit immediately. Reason: %r.' % exc.args[0])
                    self.stop()
                elif isinstance(exc, (IOError, OSError)) and exc.errno == 28:
                    self.logger.exception('Out of disk space. Exit immediately.')
                    self.stop()

                error_on_message = sys.exc_info()

                if self.error_log_exception:
                    self.logger.exception(""Bot has found a problem."")
                else:
                    self.logger.error(utils.error_message_from_exc(exc))
                    self.logger.error(""Bot has found a problem."")

                if self.error_log_message:
                    # Print full message if explicitly requested by config
                    self.logger.info(""Current Message(event): %r."",
                                     self.__current_message)

                # In case of permanent failures, stop now
                if isinstance(exc, exceptions.ConfigurationError):
                    self.stop()

            except KeyboardInterrupt:
                self.logger.info(""Received KeyboardInterrupt."")
                self.stop(exitcode=0)

            finally:
                do_rate_limit = False

                if error_on_message or error_on_pipeline:
                    self.__message_counter[""failure""] += 1
                    self.__error_retries_counter += 1

                    # reached the maximum number of retries
                    if (self.__error_retries_counter >
                            self.error_max_retries):

                        if error_on_message:

                            if self.error_dump_message:
                                error_traceback = traceback.format_exception(*error_on_message)
                                self._dump_message(error_traceback,
                                                   message=message_to_dump if message_to_dump else self.__current_message)
                            else:
                                warnings.warn(""Message will be removed from the pipeline and not dumped to the disk. ""
                                              ""Set `error_dump_message` to true to save the message on disk. ""
                                              ""This warning is only shown once in the runtime of a bot."")
                            if self.__destination_queues and '_on_error' in self.__destination_queues:
                                self.send_message(self.__current_message, path='_on_error')

                            if message_to_dump or self.__current_message:
                                self.acknowledge_message()

                            # when bot acknowledge the message,
                            # don't need to wait again
                            error_on_message = False

                        # run_mode: scheduled
                        if self.run_mode == 'scheduled':
                            self.logger.info('Shutting down scheduled bot.')
                            self.stop(exitcode=0)

                        # error_procedure: stop
                        elif self.error_procedure == ""stop"":
                            self.stop()

                        # error_procedure: pass
                        elif not error_on_pipeline:
                            self.__error_retries_counter = 0  # reset counter
                            do_rate_limit = True
                        # error_procedure: pass and pipeline problem
                        else:
                            # retry forever, see https://github.com/certtools/intelmq/issues/1333
                            # https://lists.cert.at/pipermail/intelmq-users/2018-October/000085.html
                            pass
                else:
                    self.__message_counter[""success""] += 1
                    do_rate_limit = True

                    # no errors, check for run mode: scheduled
                    if self.run_mode == 'scheduled':
                        self.logger.info('Shutting down scheduled bot.')
                        self.stop(exitcode=0)

                if getattr(self, 'testing', False):
                    self.logger.debug('Testing environment detected, returning now.')
                    return

                # Do rate_limit at the end on success and after the retries
                # counter has been reset: https://github.com/certtools/intelmq/issues/1431
                if do_rate_limit:
                    if self.rate_limit and self.run_mode != 'scheduled':
                        self.__sleep()
                    if self.collector_empty_process and self.run_mode != 'scheduled':
                        self.__sleep(1, log=False)

            self.__stats()
            self.__handle_sighup()
",https://github.com/certtools/intelmq/issues/1765,"CWE-755: Improper Handling of Exceptional Conditions, CWE-248: Uncaught Exception","exceptional situation with except exceptions.InvalidValue for source.asn=0  is not correctly handled, looping over raising two exceptions. Instead, it should give a dump and more helpful message in the add function and pass by to work further in the while True loop",intelmq/lib/bot.py,Bot.start,"[31, 78]","abusix-expert-cz: Bot has found a problem.
Traceback (most recent call last):
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 269, in start
self.process()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/bots/experts/abusix/expert.py"", line 26, in process
event = self.receive_message()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 606, in receive_message
self.__current_message = libmessage.MessageFactory.unserialize(message,
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 72, in unserialize
return MessageFactory.from_dict(message, harmonization=harmonization,
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 54, in from_dict
return class_reference(message, auto=True, harmonization=harmonization)
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 539, in __init__
super().__init__(template, auto, harmonization)
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 127, in __init__
self.add(key, value, sanitize=True)
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 249, in add
raise exceptions.InvalidValue(key, old_value)
intelmq.lib.exceptions.InvalidValue: invalid value 0 (<class 'int'>) for key 'source.asn'
abusix-expert-cz: Current Message(event): None.
abusix-expert-cz: Bot will continue in 15 seconds.
abusix-expert-cz: Pipeline failed.
Traceback (most recent call last):
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 269, in start
self.process()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/bots/experts/abusix/expert.py"", line 26, in process
event = self.receive_message()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 591, in receive_message
message = self.__source_pipeline.receive()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/pipeline.py"", line 122, in receive
raise exceptions.PipelineError(""There's already a message, first ""
intelmq.lib.exceptions.PipelineError: pipeline failed - ""There's already a message, first acknowledge the existing one.""
abusix-expert-cz: Bot will continue in 15 seconds.",intelmq.lib.exceptions.PipelineError,buggy_snippets_files/2d60746445321689d150259a7347fc19db7b4765952ab163568ffa4021584e31_before_merge.py,buggy_snippets_files/2d60746445321689d150259a7347fc19db7b4765952ab163568ffa4021584e31_after_merge.py
19235,"    def add(self, key: str, value: str, sanitize: bool = True,
            overwrite: Optional[bool] = None, ignore: Sequence = (),
            raise_failure: bool = True) -> Optional[bool]:
        """"""
        Add a value for the key (after sanitation).

        Parameters:
            key: Key as defined in the harmonization
            value: A valid value as defined in the harmonization
                If the value is None or in _IGNORED_VALUES the value will be ignored.
                If the value is ignored, the key exists and overwrite is True, the key
                is deleted.
            sanitize: Sanitation of harmonization type will be called before validation
                (default: True)
            overwrite: Overwrite an existing value if it already exists (default: None)
                If True, overwrite an existing value
                If False, do not overwrite an existing value
                If None, raise intelmq.exceptions.KeyExists for an existing value
            raise_failure: If a intelmq.lib.exceptions.InvalidValue should be raised for
                invalid values (default: True). If false, the return parameter will be
                False in case of invalid values.

        Returns:
            * True if the value has been added.
            * False if the value is invalid and raise_failure is False or the value existed
                and has not been overwritten.
            * None if the value has been ignored.

        Raises:
            intelmq.lib.exceptions.KeyExists: If key exists and won't be overwritten explicitly.
            intelmq.lib.exceptions.InvalidKey: if key is invalid.
            intelmq.lib.exceptions.InvalidArgument: if ignore is not list or tuple.
            intelmq.lib.exceptions.InvalidValue: If value is not valid for the given key and
                raise_failure is True.
        """"""
        if overwrite is None and key in self:
            raise exceptions.KeyExists(key)
        if overwrite is False and key in self:
            return False

        if value is None or value in self._IGNORED_VALUES:
            if overwrite and key in self:
                del self[key]
            return

        if not self.__is_valid_key(key):
            raise exceptions.InvalidKey(key)

        try:
            if value in ignore:
                return
        except TypeError:
            raise exceptions.InvalidArgument('ignore',
                                             got=type(ignore),
                                             expected='list or tuple')

        if sanitize and not key == '__type':
            old_value = value
            value = self.__sanitize_value(key, value)
            if value is None:
                if raise_failure:
                    raise exceptions.InvalidValue(key, old_value)
                else:
                    return False

        valid_value = self.__is_valid_value(key, value)
        if not valid_value[0]:
            if raise_failure:
                raise exceptions.InvalidValue(key, value, reason=valid_value[1])
            else:
                return False

        class_name, subitem = self.__get_type_config(key)
        if class_name and class_name['type'] == 'JSONDict' and not subitem:
            # for backwards compatibility allow setting the extra field as string
            if overwrite and key in self:
                del self[key]
            for extrakey, extravalue in json.loads(value).items():
                # For extra we must not ignore empty or invalid values because of backwards compatibility issues #1335
                if key != 'extra' and hasattr(extravalue, '__len__'):
                    if not len(extravalue):  # ignore empty values
                        continue
                if key != 'extra' and extravalue in self._IGNORED_VALUES:
                    continue
                super().__setitem__('{}.{}'.format(key, extrakey),
                                    extravalue)
        else:
            super().__setitem__(key, value)
        return True
","    def add(self, key: str, value: str, sanitize: bool = True,
            overwrite: Optional[bool] = None, ignore: Sequence = (),
            raise_failure: bool = True) -> Optional[bool]:
        """"""
        Add a value for the key (after sanitation).

        Parameters:
            key: Key as defined in the harmonization
            value: A valid value as defined in the harmonization
                If the value is None or in _IGNORED_VALUES the value will be ignored.
                If the value is ignored, the key exists and overwrite is True, the key
                is deleted.
            sanitize: Sanitation of harmonization type will be called before validation
                (default: True)
            overwrite: Overwrite an existing value if it already exists (default: None)
                If True, overwrite an existing value
                If False, do not overwrite an existing value
                If None, raise intelmq.exceptions.KeyExists for an existing value
            raise_failure: If a intelmq.lib.exceptions.InvalidValue should be raised for
                invalid values (default: True). If false, the return parameter will be
                False in case of invalid values.

        Returns:
            * True if the value has been added.
            * False if the value is invalid and raise_failure is False or the value existed
                and has not been overwritten.
            * None if the value has been ignored.

        Raises:
            intelmq.lib.exceptions.KeyExists: If key exists and won't be overwritten explicitly.
            intelmq.lib.exceptions.InvalidKey: if key is invalid.
            intelmq.lib.exceptions.InvalidArgument: if ignore is not list or tuple.
            intelmq.lib.exceptions.InvalidValue: If value is not valid for the given key and
                raise_failure is True.
        """"""
        if overwrite is None and key in self:
            raise exceptions.KeyExists(key)
        if overwrite is False and key in self:
            return False

        if value is None or value in self._IGNORED_VALUES:
            if overwrite and key in self:
                del self[key]
            return

        if not self.__is_valid_key(key):
            raise exceptions.InvalidKey(key)

        try:
            if value in ignore:
                return
        except TypeError:
            raise exceptions.InvalidArgument('ignore',
                                             got=type(ignore),
                                             expected='list or tuple')

        if sanitize and not key == '__type':
            old_value = value
            value = self.__sanitize_value(key, value)
            if value is None:
                if raise_failure:
                    raise exceptions.InvalidValue(key, old_value, object=bytes(json.dumps(self.iterable), 'utf-8'))
                else:
                    return False

        valid_value = self.__is_valid_value(key, value)
        if not valid_value[0]:
            if raise_failure:
                raise exceptions.InvalidValue(key, value, reason=valid_value[1], object=bytes(json.dumps(self.iterable), 'utf-8'))
            else:
                return False

        class_name, subitem = self.__get_type_config(key)
        if class_name and class_name['type'] == 'JSONDict' and not subitem:
            # for backwards compatibility allow setting the extra field as string
            if overwrite and key in self:
                del self[key]
            for extrakey, extravalue in json.loads(value).items():
                # For extra we must not ignore empty or invalid values because of backwards compatibility issues #1335
                if key != 'extra' and hasattr(extravalue, '__len__'):
                    if not len(extravalue):  # ignore empty values
                        continue
                if key != 'extra' and extravalue in self._IGNORED_VALUES:
                    continue
                super().__setitem__('{}.{}'.format(key, extrakey),
                                    extravalue)
        else:
            super().__setitem__(key, value)
        return True
",https://github.com/certtools/intelmq/issues/1765,"CWE-755: Improper Handling of Exceptional Conditions, CWE-248: Uncaught Exception","exceptional situation with except exceptions.InvalidValue for source.asn=0  is not correctly handled in the start function, looping over raising two exceptions. Instead, it should give a dump and more helpful message in the add function and pass by to work further in the while True loop",intelmq/lib/message.py,Message.add,[62; 69],"abusix-expert-cz: Bot has found a problem.
Traceback (most recent call last):
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 269, in start
self.process()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/bots/experts/abusix/expert.py"", line 26, in process
event = self.receive_message()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 606, in receive_message
self.__current_message = libmessage.MessageFactory.unserialize(message,
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 72, in unserialize
return MessageFactory.from_dict(message, harmonization=harmonization,
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 54, in from_dict
return class_reference(message, auto=True, harmonization=harmonization)
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 539, in __init__
super().__init__(template, auto, harmonization)
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 127, in __init__
self.add(key, value, sanitize=True)
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/message.py"", line 249, in add
raise exceptions.InvalidValue(key, old_value)
intelmq.lib.exceptions.InvalidValue: invalid value 0 (<class 'int'>) for key 'source.asn'
abusix-expert-cz: Current Message(event): None.
abusix-expert-cz: Bot will continue in 15 seconds.
abusix-expert-cz: Pipeline failed.
Traceback (most recent call last):
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 269, in start
self.process()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/bots/experts/abusix/expert.py"", line 26, in process
event = self.receive_message()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/bot.py"", line 591, in receive_message
message = self.__source_pipeline.receive()
File ""/usr/local/lib/python3.8/dist-packages/intelmq/lib/pipeline.py"", line 122, in receive
raise exceptions.PipelineError(""There's already a message, first ""
intelmq.lib.exceptions.PipelineError: pipeline failed - ""There's already a message, first acknowledge the existing one.""
abusix-expert-cz: Bot will continue in 15 seconds.",intelmq.lib.exceptions.PipelineError,buggy_snippets_files/8b47fd03681d6dbff43155014dcdc515b702669c3c7154b54c0a273dde3c156d_before_merge.py,buggy_snippets_files/8b47fd03681d6dbff43155014dcdc515b702669c3c7154b54c0a273dde3c156d_after_merge.py
19251,"    def parse_line(self, row, report):
        event = self.new_event(report)

        for keygroup, value, required in zip(self.columns, row, self.columns_required):
            keys = keygroup.split('|') if '|' in keygroup else [keygroup, ]
            for key in keys:
                if isinstance(value, str) and not value:  # empty string is never valid
                    break
                regex = self.column_regex_search.get(key, None)
                if regex:
                    search = re.search(regex, value)
                    if search:
                        value = search.group(0)
                    else:
                        value = None

                if key in [""__IGNORE__"", """"]:
                    break

                if key in self.data_type:
                    value = DATA_CONVERSIONS[self.data_type[key]](value)

                if key in [""time.source"", ""time.destination""]:
                    value = TIME_CONVERSIONS[self.time_format](value)
                elif key.endswith('.url'):
                    if not value:
                        continue
                    if '://' not in value:
                        value = self.parameters.default_url_protocol + value
                elif key in [""classification.type""] and self.type_translation:
                    if value in self.type_translation:
                        value = self.type_translation[value]
                    elif not hasattr(self.parameters, 'type'):
                        continue
                if event.add(key, value, raise_failure=False):
                    break
            else:
                # if the value sill remains unadded we need to inform if the key is needed
                if required:
                    raise InvalidValue(key, value)

        if hasattr(self.parameters, 'type')\
                and ""classification.type"" not in event:
            event.add('classification.type', self.parameters.type)
        event.add(""raw"", self.recover_line(row))
        yield event
","    def parse_line(self, row, report):
        event = self.new_event(report)

        for keygroup, value, required in zip(self.columns, row, self.columns_required):
            keys = keygroup.split('|') if '|' in keygroup else [keygroup, ]
            for key in keys:
                if isinstance(value, str) and not value:  # empty string is never valid
                    break
                regex = self.column_regex_search.get(key, None)
                if regex:
                    search = re.search(regex, value)
                    if search:
                        value = search.group(0)
                    else:
                        value = None

                if key in [""__IGNORE__"", """"]:
                    break

                if key in self.data_type:
                    value = DATA_CONVERSIONS[self.data_type[key]](value)

                if key in [""time.source"", ""time.destination""]:
                    value = TIME_CONVERSIONS[self.time_format](value)
                elif key.endswith('.url'):
                    if not value:
                        continue
                    if '://' not in value:
                        value = self.parameters.default_url_protocol + value
                elif key in [""classification.type""] and self.type_translation:
                    if value in self.type_translation:
                        value = self.type_translation[value]
                    elif not hasattr(self.parameters, 'type'):
                        continue
                if event.add(key, value, raise_failure=False) is not False:
                    break
            else:
                # if the value sill remains unadded we need to inform if the key is needed
                if required:
                    raise InvalidValue(key, value)

        if hasattr(self.parameters, 'type')\
                and ""classification.type"" not in event:
            event.add('classification.type', self.parameters.type)
        event.add(""raw"", self.recover_line(row))
        yield event
",https://github.com/certtools/intelmq/issues/1651,"CWE-253: Incorrect Check of Function Return Value,  CWE-248: Uncaught Exception","inproper handling of ignorable character value - in the event.add(key, value, raise_failure=False) expression. This expression results in None. This None value should be processed in the same way as in the case where event.add(key, value, raise_failure=False) gives True",intelmq/bots/parsers/generic/parser_csv.py,GenericCsvParserBot.parse_line,[35],"Traceback (most recent call last):
File ""/usr/lib/python3/dist-packages/intelmq/lib/bot.py"", line 978, in process
events = list(filter(bool, value))
File ""/usr/lib/python3/dist-packages/intelmq/bots/parsers/generic/parser_csv.py"", line 130, in parse_line
raise InvalidValue(key, value)
intelmq.lib.exceptions.InvalidValue: invalid value '-' (<class 'str'>) for key 'extra.cert_orgunit'",intelmq.lib.exceptions.InvalidValue,buggy_snippets_files/5a90712c5a7b33747534583ada9379ac24cef74131a1f3c95ac307633a1d2e39_before_merge.py,buggy_snippets_files/5a90712c5a7b33747534583ada9379ac24cef74131a1f3c95ac307633a1d2e39_after_merge.py
19262,"    def process_message(self, uid, message):
        seen = False

        for attach in message.attachments:
            if not attach:
                continue

            attach_filename = attach['filename']
            if attach_filename.startswith('""'):  # for imbox versions older than 0.9.5, see also above
                attach_filename = attach_filename[1:-1]

            if re.search(self.parameters.attach_regex, attach_filename):

                self.logger.debug(""Found suitable attachment %s."", attach_filename)

                report = self.new_report()

                if self.extract_files:
                    raw_reports = unzip(attach['content'].read(), self.extract_files,
                                        return_names=True, logger=self.logger)
                else:
                    raw_reports = ((None, attach['content'].read()), )

                for file_name, raw_report in raw_reports:
                    report = self.new_report()
                    report.add(""raw"", raw_report)
                    if file_name:
                        report.add(""extra.file_name"", file_name)
                    report[""extra.email_subject""] = message.subject
                    report[""extra.email_from""] = ','.join(x['email'] for x in message.sent_from)
                    report[""extra.email_message_id""] = message.message_id
                    self.send_message(report)

                # Only mark read if message relevant to this instance,
                # so other instances watching this mailbox will still
                # check it.
                seen = True
        self.logger.info(""Email report read."")
        return seen
","    def process_message(self, uid, message):
        seen = False

        for attach in message.attachments:
            if not attach:
                continue

            try:
                attach_filename = attach['filename']
            except KeyError:
                # https://github.com/certtools/intelmq/issues/1538
                self.logger.debug('Skipping attachment because of missing filename.')
                continue
            if attach_filename.startswith('""'):  # for imbox versions older than 0.9.5, see also above
                attach_filename = attach_filename[1:-1]

            if re.search(self.parameters.attach_regex, attach_filename):

                self.logger.debug(""Found suitable attachment %s."", attach_filename)

                report = self.new_report()

                if self.extract_files:
                    raw_reports = unzip(attach['content'].read(), self.extract_files,
                                        return_names=True, logger=self.logger)
                else:
                    raw_reports = ((None, attach['content'].read()), )

                for file_name, raw_report in raw_reports:
                    report = self.new_report()
                    report.add(""raw"", raw_report)
                    if file_name:
                        report.add(""extra.file_name"", file_name)
                    report[""extra.email_subject""] = message.subject
                    report[""extra.email_from""] = ','.join(x['email'] for x in message.sent_from)
                    report[""extra.email_message_id""] = message.message_id
                    self.send_message(report)

                # Only mark read if message relevant to this instance,
                # so other instances watching this mailbox will still
                # check it.
                seen = True
        self.logger.info(""Email report read."")
        return seen
",https://github.com/certtools/intelmq/issues/1538,CWE-248: Uncaught Exception,the situation is not handled where 'filename' key value is not present in attach dict,intelmq/bots/collectors/mail/collector_mail_attach.py,MailAttachCollectorBot.process_message,[8],"mail-collector: Bot has found a problem.
Traceback (most recent call last):
File ""/home/sebastianw/intelmq/intelmq/lib/bot.py"", line 267, in start
self.process()
File ""/home/sebastianw/intelmq/intelmq/bots/collectors/mail/lib.py"", line 53, in process
if self.process_message(uid, message):
File ""/home/sebastianw/intelmq/intelmq/bots/collectors/mail/collector_mail_attach.py"", line 30, in process_message
attach_filename = attach['filename']
KeyError: 'filename'",KeyError,buggy_snippets_files/16987bcf4f10681b21a6b8057d0a85b659b6b1bbe9cd482757cc87bfc3a21ab0_before_merge.py,buggy_snippets_files/16987bcf4f10681b21a6b8057d0a85b659b6b1bbe9cd482757cc87bfc3a21ab0_after_merge.py
19296,"    def parse(self, report):
        if self.mode == 'fixed':
            return self.parse_csv_dict(report)

        # Set config to parse report
        self.report_name = report.get('extra.file_name')
        filename_search = self.__is_filename_regex.search(self.report_name)

        if not filename_search:
            raise ValueError(""Report's 'extra.file_name' {!r} is not valid."".format(self.report_name))
        else:
            self.report_name = filename_search.group(1)
            self.logger.debug(""Detected report's file name: {!r}."".format(self.report_name))
            retval = config.get_feed_by_filename(self.report_name)

            if not retval:
                raise ValueError('Could not get a config for {!r}, check the documentation.'
                                 ''.format(self.report_name))
            self.feedname, self.sparser_config = retval

        # Set default csv parse function
        return self.parse_csv_dict(report)
","    def parse(self, report):
        if self.mode == 'fixed':
            return self.parse_csv_dict(report)

        # Set config to parse report
        self.report_name = report.get('extra.file_name')
        if not self.report_name:
            raise ValueError(""No feedname given as parameter and the ""
                             ""processed report has no 'extra.file_name'. ""
                             ""Ensure that at least one is given. ""
                             ""Also have a look at the documentation of the bot."")
        filename_search = self.__is_filename_regex.search(self.report_name)

        if not filename_search:
            raise ValueError(""Report's 'extra.file_name' {!r} is not valid."".format(self.report_name))
        else:
            self.report_name = filename_search.group(1)
            self.logger.debug(""Detected report's file name: {!r}."".format(self.report_name))
            retval = config.get_feed_by_filename(self.report_name)

            if not retval:
                raise ValueError('Could not get a config for {!r}, check the documentation.'
                                 ''.format(self.report_name))
            self.feedname, self.sparser_config = retval

        # Set default csv parse function
        return self.parse_csv_dict(report)
",https://github.com/certtools/intelmq/issues/1507,CWE-754: Improper Check for Unusual or Exceptional Conditions,the situation is not handled where self.report_name is None,intelmq/bots/parsers/shadowserver/parser.py,ShadowserverParserBot.parse,,"Bot has found a problem.
Traceback (most recent call last):
File ""/usr/local/lib/python3.6/dist-packages/intelmq/lib/bot.py"", line 267, in start
self.process()
File ""/usr/local/lib/python3.6/dist-packages/intelmq/lib/bot.py"", line 942, in process
for line in self.parse(report):
File ""/usr/local/lib/python3.6/dist-packages/intelmq/bots/parsers/shadowserver/parser.py"", line 65, in parse
filename_search = self.__is_filename_regex.search(self.report_name)
TypeError: expected string or bytes-like object",TypeError,buggy_snippets_files/4f69d0c76cf63079c91567a0b6e68161b26769948cd20965615bde8b29ea8547_before_merge.py,buggy_snippets_files/4f69d0c76cf63079c91567a0b6e68161b26769948cd20965615bde8b29ea8547_after_merge.py
19325,"    def process(self):
        event = self.receive_message()
        event.set_default_value()

        csvfile = io.StringIO()
        writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames,
                                quoting=csv.QUOTE_MINIMAL, delimiter=str("";""),
                                extrasaction='ignore', lineterminator='\n')
        writer.writeheader()
        writer.writerow(event)
        attachment = csvfile.getvalue()

        if self.http_verify_cert and self.smtp_class == smtplib.SMTP_SSL:
            kwargs = {'context': ssl.create_default_context()}
        else:
            kwargs = {}

        with self.smtp_class(self.parameters.smtp_host, self.parameters.smtp_port,
                             **kwargs) as smtp:
            if self.starttls:
                if self.http_verify_cert:
                    smtp.starttls(context=ssl.create_default_context())
                else:
                    smtp.starttls()
            if self.username and self.password:
                smtp.auth(smtp.auth_plain, user=self.username, password=self.password)
            msg = MIMEMultipart()
            if self.parameters.text:
                msg.attach(MIMEText(self.parameters.text.format(ev=event)))
            msg.attach(MIMEText(attachment, 'csv'))
            msg['Subject'] = self.parameters.subject.format(ev=event)
            msg['From'] = self.parameters.mail_from.format(ev=event)
            msg['To'] = self.parameters.mail_to.format(ev=event)
            smtp.send_message(msg, from_addr=self.parameters.mail_from,
                              to_addrs=self.parameters.mail_to.format(ev=event))

        self.acknowledge_message()
","    def process(self):
        event = self.receive_message()
        event.set_default_value()

        csvfile = io.StringIO()
        writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames,
                                quoting=csv.QUOTE_MINIMAL, delimiter=str("";""),
                                extrasaction='ignore', lineterminator='\n')
        writer.writeheader()
        writer.writerow(event)
        attachment = csvfile.getvalue()

        if self.http_verify_cert and self.smtp_class == smtplib.SMTP_SSL:
            kwargs = {'context': ssl.create_default_context()}
        else:
            kwargs = {}

        with self.smtp_class(self.parameters.smtp_host, self.parameters.smtp_port,
                             **kwargs) as smtp:
            if self.starttls:
                if self.http_verify_cert:
                    smtp.starttls(context=ssl.create_default_context())
                else:
                    smtp.starttls()
            if self.username and self.password:
                smtp.login(user=self.username, password=self.password)
            msg = MIMEMultipart()
            if self.parameters.text:
                msg.attach(MIMEText(self.parameters.text.format(ev=event)))
            msg.attach(MIMEText(attachment, 'csv'))
            msg['Subject'] = self.parameters.subject.format(ev=event)
            msg['From'] = self.parameters.mail_from.format(ev=event)
            msg['To'] = self.parameters.mail_to.format(ev=event)
            recipients = [recipient.format(ev=event)
                          for recipient
                          in self.parameters.mail_to.split(',')]
            smtp.send_message(msg, from_addr=self.parameters.mail_from,
                              to_addrs=recipients)

        self.acknowledge_message()
",https://github.com/certtools/intelmq/issues/1464,"CWE-236: Improper Handling of Undefined Parameters, CWE-166: Improper Handling of Missing Special Element",unrecognized smtp.auth function argument user,intelmq/bots/outputs/smtp/output.py,SMTPOutputBot.process,[26],"Traceback (most recent call last):
File ""/usr/lib/python3.6/site-packages/intelmq/lib/bot.py"", line 264, in start
self.process()
File ""/usr/lib/python3.6/site-packages/intelmq/bots/outputs/smtp/output.py"", line 52, in process
smtp.auth(smtp.auth_plain, user=self.username, password=self.password)
TypeError: auth() got an unexpected keyword argument 'user'",TypeError,buggy_snippets_files/70cdd17b259538a2ca5dc9c0d4f933e4dab5d5c577a778b2bdd09a2bcfae65b4_before_merge.py,buggy_snippets_files/70cdd17b259538a2ca5dc9c0d4f933e4dab5d5c577a778b2bdd09a2bcfae65b4_after_merge.py
19338,"    def _process(self, dryrun, msg, show):
        if msg:
            msg = MessageFactory.serialize(self.arg2msg(msg))
            if not self.instance._Bot__source_pipeline:
                # is None if source pipeline does not exist
                self.instance._Bot__source_pipeline = Pipeline(None)
            self.instance._Bot__source_pipeline.receive = lambda: msg
            self.instance._Bot__source_pipeline.acknowledge = lambda: None
            self.instance.logger.info("" * Message from cli will be used when processing."")

        if dryrun:
            self.instance.send_message = lambda msg, path=""_default"": self.instance.logger.info(
                ""DRYRUN: Message would be sent now{}!"".format("" to the {} path"".format(path) if (path != ""_default"") else """"))
            self.instance.acknowledge_message = lambda: self.instance.logger.info(""DRYRUN: Message would be acknowledged now!"")
            self.instance.logger.info("" * Dryrun only, no message will be really sent through."")

        if show:
            fn = self.instance.send_message
            self.instance.send_message = lambda msg=None, path=""_default"": [self.pprint(msg or ""No message generated""),
                                                                            fn(msg, path=path)]

        self.instance.logger.info(""Processing..."")
        self.instance.process()
","    def _process(self, dryrun, msg, show):
        if msg:
            msg = MessageFactory.serialize(self.arg2msg(msg))
            if not self.instance._Bot__source_pipeline:
                # is None if source pipeline does not exist
                self.instance._Bot__source_pipeline = Pipeline(None)
            self.instance._Bot__source_pipeline.receive = lambda *args, **kwargs: msg
            self.instance._Bot__source_pipeline.acknowledge = lambda *args, **kwargs: None
            self.instance.logger.info("" * Message from cli will be used when processing."")

        if dryrun:
            self.instance.send_message = lambda *args, **kwargs: self.instance.logger.info(
                ""DRYRUN: Message would be sent now to %r!"",
                kwargs.get('path', ""_default""))
            self.instance.acknowledge_message = lambda *args, **kwargs: self.instance.logger.info(
                ""DRYRUN: Message would be acknowledged now!"")
            self.instance.logger.info("" * Dryrun only, no message will be really sent through."")

        if show:
            fn = self.instance.send_message
            self.instance.send_message = lambda *args, **kwargs: [self.pprint(args or ""No message generated""),
                                                                  fn(*args, **kwargs)]

        self.instance.logger.info(""Processing..."")
        self.instance.process()
",https://github.com/certtools/intelmq/issues/1453,CWE-665: Improper Initialization,"send_message method of self.instance is improperly initialized, not accounting for possible keyword argument path_permissive, which appears within the call self.instance.process()",intelmq/lib/bot_debugger.py,BotDebugger._process,"[12,13]","intelmqctl run Filter-Expert process --show-sent --msg ' { ""source.ip"": ""93.184.216.34"", ""source.geolocation.cc"": ""UU"", ""time.observation"": ""2015-01-01T00:00:00+00:00"" }'


Traceback (most recent call last):
File ""/bin/intelmqctl"", line 11, in <module>
load_entry_point('intelmq==2.0.1', 'console_scripts', 'intelmqctl')()
File ""/usr/lib/python3.6/site-packages/intelmq/bin/intelmqctl.py"", line 1710, in main
return x.run()
File ""/usr/lib/python3.6/site-packages/intelmq/bin/intelmqctl.py"", line 948, in run
retval, results = args.func(**args_dict)
File ""/usr/lib/python3.6/site-packages/intelmq/bin/intelmqctl.py"", line 955, in bot_run
return self.bot_process_manager.bot_run(**kwargs), None
File ""/usr/lib/python3.6/site-packages/intelmq/bin/intelmqctl.py"", line 162, in bot_run
loglevel=loglevel)
File ""/usr/lib/python3.6/site-packages/intelmq/lib/bot_debugger.py"", line 60, in __init__
self._process(dryrun, msg, show)
File ""/usr/lib/python3.6/site-packages/intelmq/lib/bot_debugger.py"", line 142, in _process
self.instance.process()
File ""/usr/lib/python3.6/site-packages/intelmq/bots/experts/filter/expert.py"", line 125, in process
path_permissive=True)
TypeError: <lambda>() got an unexpected keyword argument 'path_permissive'",TypeError,buggy_snippets_files/1acf6ed981d85fcf546a417275beb9b526f746d24408e0901a5458b19d8d5b34_before_merge.py,buggy_snippets_files/1acf6ed981d85fcf546a417275beb9b526f746d24408e0901a5458b19d8d5b34_after_merge.py
19356,"def create_parser():
    parser = argparse.ArgumentParser(
        prog='download_mapping',
        description='Downloads malware family mapping and converts it to modify syntax.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
You can specify additional rules to be added to the file by using:
-e ""^some-expression$"" -i ""some-identifier""
and multiple ones:
-e ""^some-expression$"" -i ""some-identifier"" -e ""^other-expression$"" -i ""other-identifier""
'''
    )

    parser.add_argument('filename', nargs='?',
                        help='The filename to write the converted mapping to. If not given, printed to stdout.')
    parser.add_argument('--url', '-u',
                        default=URL,
                        help='The URL to download the mapping from.')
    parser.add_argument('--add-default', '-d',
                        help='Add a default rule to use the malware name as identifier.',
                        const=True, action='store_const')
    parser.add_argument('--expression', '-e',
                        nargs=1, action='append',
                        help='Expression for an additional rule.')
    parser.add_argument('--identifier', '-i',
                        nargs=1, action='append',
                        help='Identifier for an additional rule.')
    parser.add_argument('-m', '--include-malpedia',
                        default=False, action='store_true',
                        help='Include malpedia data (CC BY-NC-SA 3.0), '
                             'see https://malpedia.caad.fkie.fraunhofer.de/usage/tos from %s'
                             '' % URL_MALPEDIA)
    return parser
","def create_parser():
    parser = argparse.ArgumentParser(
        prog='download_mapping',
        description='Downloads malware family mapping and converts it to modify syntax.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
You can specify additional rules to be added to the file by using:
-e ""^some-expression$"" -i ""some-identifier""
and multiple ones:
-e ""^some-expression$"" -i ""some-identifier"" -e ""^other-expression$"" -i ""other-identifier""
'''
    )

    parser.add_argument('filename', nargs='?',
                        help='The filename to write the converted mapping to. If not given, printed to stdout.')
    parser.add_argument('--url', '-u',
                        default=URL,
                        help='The URL to download the mapping from.')
    parser.add_argument('--add-default', '-d',
                        help='Add a default rule to use the malware name as identifier.',
                        const=True, action='store_const')
    parser.add_argument('--expression', '-e',
                        nargs=1, action='append',
                        help='Expression for an additional rule.',
                        default=[])
    parser.add_argument('--identifier', '-i',
                        nargs=1, action='append',
                        help='Identifier for an additional rule.',
                        default=[])
    parser.add_argument('-m', '--include-malpedia',
                        default=False, action='store_true',
                        help='Include malpedia data (CC BY-NC-SA 3.0), '
                             'see https://malpedia.caad.fkie.fraunhofer.de/usage/tos from %s'
                             '' % URL_MALPEDIA)
    return parser
",https://github.com/certtools/intelmq/issues/1427,"CWE-665: Improper Initialization,  CWE-248: Uncaught Exception","when processing arguments from command line, there two types of command line arguments called expressions and identifiers. When one of such terms is omitted in the command line call of the download_mapping.py module, there is inconsistency in the main  function of the  download_mapping.py module in the expression zip(args.expression, args.identifier) because args.identifier is not iterable",contrib/malware_name_mapping/download_mapping.py,create_parser,"[22, 27]","/opt/dev_intelmq# ./contrib/malware_name_mapping/download_mapping.py /opt/intelmq/var/lib/bots/modify/malware-family-names.conf
Traceback (most recent call last):
File ""./contrib/malware_name_mapping/download_mapping.py"", line 99, in <module>
params=zip(args.expression, args.identifier),
TypeError: zip argument #1 must support iteration
root@intelmq-001:/opt/dev_intelmq#",TypeError,buggy_snippets_files/d0cd691bab0322b97bd29d5e6ba225e5b684a0bbfb2a79b589b7bfe5a6da565c_before_merge.py,buggy_snippets_files/d0cd691bab0322b97bd29d5e6ba225e5b684a0bbfb2a79b589b7bfe5a6da565c_after_merge.py
19390,"    def process(self):
        event = self.receive_message()
        procedure = Procedure.CONTINUE
        for rule in self.sieve.rules:
            procedure = self.process_rule(rule, event)
            if procedure == Procedure.KEEP:
                self.logger.debug('Stop processing based on rule at %s: %s.', self.get_linecol(rule), event)
                break
            elif procedure == Procedure.DROP:
                self.logger.debug('Dropped event based on rule at %s: %s.', self.get_linecol(rule), event)
                break

        # forwarding decision
        if procedure != Procedure.DROP:
            path = getattr(event, ""path"", ""_default"")
            self.send_message(event, path=path)

        self.acknowledge_message()
","    def process(self):
        event = self.receive_message()
        procedure = Procedure.CONTINUE
        if self.sieve:  # empty rules file results in empty string
            for rule in self.sieve.rules:
                procedure = self.process_rule(rule, event)
                if procedure == Procedure.KEEP:
                    self.logger.debug('Stop processing based on rule at %s: %s.', self.get_linecol(rule), event)
                    break
                elif procedure == Procedure.DROP:
                    self.logger.debug('Dropped event based on rule at %s: %s.', self.get_linecol(rule), event)
                    break

        # forwarding decision
        if procedure != Procedure.DROP:
            path = getattr(event, ""path"", ""_default"")
            self.send_message(event, path=path)

        self.acknowledge_message()
",https://github.com/certtools/intelmq/issues/1343,"CWE-241: Improper Handling of Unexpected Data Type, CWE-232: Improper Handling of Undefined Values, CWE-248: Uncaught Exception",the situation is unhandled where self.sieve is None,intelmq/bots/experts/sieve/expert.py,SieveExpertBot.process,[4],"Traceback (most recent call last):
File ""/usr/lib/python3/dist-packages/intelmq/lib/bot.py"", line 167, in start
self.process()
File ""/usr/lib/python3/dist-packages/intelmq/bots/experts/sieve/expert.py"", line 86, in process
for rule in self.sieve.rules:
AttributeError: 'str' object has no attribute 'rules'",AttributeError,buggy_snippets_files/ad08b64a0e9cad3565867dab53148cb0c373fd3694be546511b3a331a64f7d8a_before_merge.py,buggy_snippets_files/ad08b64a0e9cad3565867dab53148cb0c373fd3694be546511b3a331a64f7d8a_after_merge.py
19405,"    def process(self):
        ''' Stop the Bot if cannot connect to AMQP Server after the defined connection attempts '''

        # self.connection and self.channel can be None
        if getattr(self.connection, 'is_closed', None) or getattr(self.channel, 'is_closed', None):
                self.connect_server()

        event = self.receive_message()

        if not self.keep_raw_field:
            del event['raw']

        try:
            if not self.channel.basic_publish(exchange=self.exchange,
                                              routing_key=self.routing_key,
                                              body=event.to_json(),
                                              properties=self.properties,
                                              mandatory=True):
                if self.require_confirmation:
                    raise ValueError('Message sent but not confirmed.')
                else:
                    self.logger.info('Message sent but not confirmed.')
        except (pika.exceptions.ChannelError, pika.exceptions.AMQPChannelError,
                pika.exceptions.NackError):
            self.logger.exception('Error publishing the message.')
        else:
            self.acknowledge_message()
","    def process(self):
        ''' Stop the Bot if cannot connect to AMQP Server after the defined connection attempts '''

        # self.connection and self.channel can be None
        if getattr(self.connection, 'is_closed', None) or getattr(self.channel, 'is_closed', None):
                self.connect_server()

        event = self.receive_message()

        if not self.keep_raw_field:
            del event['raw']

        try:
            if not self.channel.basic_publish(exchange=self.exchange,
                                              routing_key=self.routing_key,
                                              # replace unicode characters when encoding (#1296)
                                              body=event.to_json().encode(errors='backslashreplace'),
                                              properties=self.properties,
                                              mandatory=True):
                if self.require_confirmation:
                    raise ValueError('Message sent but not confirmed.')
                else:
                    self.logger.info('Message sent but not confirmed.')
        except (pika.exceptions.ChannelError, pika.exceptions.AMQPChannelError,
                pika.exceptions.NackError):
            self.logger.exception('Error publishing the message.')
        else:
            self.acknowledge_message()
",https://github.com/certtools/intelmq/issues/1296,"CWE-232: Improper Handling of Undefined Values, CWE-248: Uncaught Exception","the situation is unhandled where body, the unicode (utf-8) string in basic_publish function in pika/channel.py module, contains unencodable characters like '\udbef', which are surrogate. Those surrogates are usually escaped using \. This body is passed in the body parameter in the call of self.channel.basic_publish",intelmq/bots/outputs/amqptopic/output.py,AMQPTopicBot.process,"[14, 18]","Traceback (most recent call last):
File ""/usr/lib/python3/dist-packages/intelmq/lib/bot.py"", line 167, in start
self.process()
File ""/usr/lib/python3/dist-packages/intelmq/bots/outputs/amqptopic/output.py"", line 81, in process
mandatory=True):
File ""/usr/lib/python3/dist-packages/pika/adapters/blocking_connection.py"", line 1978, in basic_publish
mandatory, immediate)
File ""/usr/lib/python3/dist-packages/pika/adapters/blocking_connection.py"", line 2026, in publish
immediate=immediate)
File ""/usr/lib/python3/dist-packages/pika/channel.py"", line 332, in basic_publish
body = body.encode('utf-8')
UnicodeEncodeError: 'utf-8' codec can't encode character '\udbef' in position 215395: surrogates not allowed",UnicodeEncodeError,buggy_snippets_files/742ae5bc753a6bbf1c937357b0c507a36827f05accd2b90caacad0a2b24ab782_before_merge.py,buggy_snippets_files/742ae5bc753a6bbf1c937357b0c507a36827f05accd2b90caacad0a2b24ab782_after_merge.py
19428,"def generate_reports(report_template: Report, infile: BinaryIO, chunk_size: Optional[int],
                     copy_header_line: bool) -> Generator[Report, None, None]:
    """"""Generate reports from a template and input file, optionally split into chunks.

    If chunk_size is None, a single report is generated with the entire
    contents of infile as the raw data. Otherwise chunk_size should be
    an integer giving the maximum number of bytes in a chunk. The data
    read from infile is then split into chunks of this size at newline
    characters (see read_delimited_chunks). For each of the chunks, this
    function yields a copy of the report_template with that chunk as the
    value of the raw attribute.

    When splitting the data into chunks, if copy_header_line is true,
    the first line the file is read before chunking and then prepended
    to each of the chunks. This is particularly useful when splitting
    CSV files.

    The infile should be a file-like object. generate_reports uses only
    two methods, readline and read, with readline only called once and
    only if copy_header_line is true. Both methods should return bytes
    objects.

    Params:
        report_template: report used as template for all yielded copies
        infile: stream to read from
        chunk_size: maximum size of each chunk
        copy_header_line: copy the first line of the infile to each chunk

    Yields:
        report: a Report object holding the chunk in the raw field
    """"""
    if chunk_size is None:
        report = report_template.copy()
        report.add(""raw"", infile.read(), overwrite=True)
        yield report
    else:
        header = b""""
        if copy_header_line:
            header = infile.readline()
        for chunk in read_delimited_chunks(infile, chunk_size):
            report = report_template.copy()
            report.add(""raw"", header + chunk, overwrite=True)
            yield report
","def generate_reports(report_template: Report, infile: BinaryIO, chunk_size: Optional[int],
                     copy_header_line: bool) -> Generator[Report, None, None]:
    """"""Generate reports from a template and input file, optionally split into chunks.

    If chunk_size is None, a single report is generated with the entire
    contents of infile as the raw data. Otherwise chunk_size should be
    an integer giving the maximum number of bytes in a chunk. The data
    read from infile is then split into chunks of this size at newline
    characters (see read_delimited_chunks). For each of the chunks, this
    function yields a copy of the report_template with that chunk as the
    value of the raw attribute.

    When splitting the data into chunks, if copy_header_line is true,
    the first line the file is read before chunking and then prepended
    to each of the chunks. This is particularly useful when splitting
    CSV files.

    The infile should be a file-like object. generate_reports uses only
    two methods, readline and read, with readline only called once and
    only if copy_header_line is true. Both methods should return bytes
    objects.

    Params:
        report_template: report used as template for all yielded copies
        infile: stream to read from
        chunk_size: maximum size of each chunk
        copy_header_line: copy the first line of the infile to each chunk

    Yields:
        report: a Report object holding the chunk in the raw field
    """"""
    if chunk_size is None:
        report = report_template.copy()
        data = infile.read()
        if data:
            report.add(""raw"", data, overwrite=True)
            yield report
    else:
        header = b""""
        if copy_header_line:
            header = infile.readline()
        for chunk in read_delimited_chunks(infile, chunk_size):
            report = report_template.copy()
            report.add(""raw"", header + chunk, overwrite=True)
            yield report
",https://github.com/certtools/intelmq/issues/1244,"CWE-232: Improper Handling of Undefined Values, CWE-248: Uncaught Exception","if infile.read() is empty (contains no data), the error exception is raised when processing empty line in the add method. Namely, '' is invalid value. The case is unhandled where infile.read() contains only empty string",intelmq/lib/splitreports.py,generate_reports,[34],"2018-05-29 09:48:20,659 - ***-collector - ERROR - Bot has found a problem.
Traceback (most recent call last):
File ""/usr/lib/python3/dist-packages/intelmq/lib/bot.py"", line 145, in start
self.process()
File ""/usr/lib/python3/dist-packages/intelmq/bots/collectors/file/collector_file.py"", line 66, in process
self.parameters.chunk_replicate_header):
File ""/usr/lib/python3/dist-packages/intelmq/lib/splitreports.py"", line 125, in generate_reports
report.add(""raw"", infile.read(), force=True)
File ""/usr/lib/python3/dist-packages/intelmq/lib/message.py"", line 212, in add
raise exceptions.InvalidValue(key, value, reason=valid_value[1])
intelmq.lib.exceptions.InvalidValue: invalid value '' (<class 'str'>) for key 'raw' is_valid returned False.",intelmq.lib.exceptions.InvalidValue,buggy_snippets_files/f3b20a5647a11b3f8ba5721c7a502616e940a4642a7447e2fe77485c7ac49210_before_merge.py,buggy_snippets_files/f3b20a5647a11b3f8ba5721c7a502616e940a4642a7447e2fe77485c7ac49210_after_merge.py
19441,"    def process(self):
        mailbox = self.connect_mailbox()
        emails = mailbox.messages(folder=self.parameters.folder, unread=True)

        if emails:
            for uid, message in emails:

                if (self.parameters.subject_regex and
                        not re.search(self.parameters.subject_regex,
                                      re.sub(r""\r\n\s"", "" "", message.subject))):
                    continue

                erroneous = False  # If errors occured this will be set to true.

                for body in message.body['plain']:
                    match = re.search(self.parameters.url_regex, str(body))
                    if match:
                        url = match.group()
                        # strip leading and trailing spaces, newlines and
                        # carriage returns
                        url = url.strip()

                        self.logger.info(""Downloading report from %r."", url)
                        timeoutretries = 0
                        resp = None
                        while timeoutretries < self.http_timeout_max_tries and resp is None:
                            try:
                                resp = requests.get(url=url,
                                                    auth=self.auth, proxies=self.proxy,
                                                    headers=self.http_header,
                                                    verify=self.http_verify_cert,
                                                    cert=self.ssl_client_cert,
                                                    timeout=self.http_timeout_sec)

                            except requests.exceptions.Timeout:
                                timeoutretries += 1
                                self.logger.warn(""Timeout whilst downloading the report."")

                        if resp is None and timeoutretries >= self.http_timeout_max_tries:
                            self.logger.error(""Request timed out %i times in a row. "" %
                                              timeoutretries)
                            erroneous = True
                            # The download timed out too often, leave the Loop.
                            continue

                        if resp.status_code // 100 != 2:
                            raise ValueError('HTTP response status code was {}.'
                                             ''.format(resp.status_code))

                        self.logger.info(""Report downloaded."")

                        template = self.new_report()

                        for report in generate_reports(template, io.BytesIO(resp.content),
                                                       self.chunk_size,
                                                       self.chunk_replicate_header):
                            self.send_message(report)

                        # Only mark read if message relevant to this instance,
                        # so other instances watching this mailbox will still
                        # check it.
                        try:
                            mailbox.mark_seen(uid)
                        except imaplib.abort:
                            # Disconnect, see https://github.com/certtools/intelmq/issues/852
                            mailbox = self.connect_mailbox()
                            mailbox.mark_seen(uid)

                if not erroneous:
                    self.logger.info(""Email report read."")
                else:
                    self.logger.error(""Email report read with errors, the report was not processed."")

        mailbox.logout()
","    def process(self):
        mailbox = self.connect_mailbox()
        emails = mailbox.messages(folder=self.parameters.folder, unread=True)

        if emails:
            for uid, message in emails:

                if (self.parameters.subject_regex and
                        not re.search(self.parameters.subject_regex,
                                      re.sub(r""\r\n\s"", "" "", message.subject))):
                    continue

                erroneous = False  # If errors occured this will be set to true.

                for body in message.body['plain']:
                    match = re.search(self.parameters.url_regex, str(body))
                    if match:
                        url = match.group()
                        # strip leading and trailing spaces, newlines and
                        # carriage returns
                        url = url.strip()

                        self.logger.info(""Downloading report from %r."", url)
                        timeoutretries = 0
                        resp = None
                        while timeoutretries < self.http_timeout_max_tries and resp is None:
                            try:
                                resp = requests.get(url=url,
                                                    auth=self.auth, proxies=self.proxy,
                                                    headers=self.http_header,
                                                    verify=self.http_verify_cert,
                                                    cert=self.ssl_client_cert,
                                                    timeout=self.http_timeout_sec)

                            except requests.exceptions.Timeout:
                                timeoutretries += 1
                                self.logger.warn(""Timeout whilst downloading the report."")

                        if resp is None and timeoutretries >= self.http_timeout_max_tries:
                            self.logger.error(""Request timed out %i times in a row. "" %
                                              timeoutretries)
                            erroneous = True
                            # The download timed out too often, leave the Loop.
                            continue

                        if resp.status_code // 100 != 2:
                            raise ValueError('HTTP response status code was {}.'
                                             ''.format(resp.status_code))
                        if not resp.content:
                            self.logger.warning('Got empty reponse from server.')
                        else:
                            self.logger.info(""Report downloaded."")

                            template = self.new_report()

                            for report in generate_reports(template, io.BytesIO(resp.content),
                                                           self.chunk_size,
                                                           self.chunk_replicate_header):
                                self.send_message(report)

                        # Only mark read if message relevant to this instance,
                        # so other instances watching this mailbox will still
                        # check it.
                        try:
                            mailbox.mark_seen(uid)
                        except imaplib.abort:
                            # Disconnect, see https://github.com/certtools/intelmq/issues/852
                            mailbox = self.connect_mailbox()
                            mailbox.mark_seen(uid)

                if not erroneous:
                    self.logger.info(""Email report read."")
                else:
                    self.logger.error(""Email report read with errors, the report was not processed."")

        mailbox.logout()
",https://github.com/certtools/intelmq/issues/988,"CWE-232: Improper Handling of Undefined Values, CWE-248: Uncaught Exception","the case is unhandled where resp.content is None, being a part of response from requests function. It causes an error exception with invalid value error for empty infile variable in the generate_reports function",intelmq/bots/collectors/mail/collector_mail_url.py,MailURLCollectorBot.process,"[54,56]","2017-05-30 23:00:01,926 - shadowserver-url-collector - ERROR - Bot has found a problem.
Traceback (most recent call last):
File ""/opt/intelmq/venv/lib/python3.5/site-packages/intelmq-1.0.0.dev7-py3.5.egg/intelmq/lib/bot.py"", line 150, in start
self.process()
File ""/opt/intelmq/venv/lib/python3.5/site-packages/intelmq-1.0.0.dev7-py3.5.egg/intelmq/bots/collectors/mail/collector_mail_url.py"", line 87, in process
self.chunk_replicate_header):
File ""/opt/intelmq/venv/lib/python3.5/site-packages/intelmq-1.0.0.dev7-py3.5.egg/intelmq/lib/splitreports.py"", line 152, in generate_reports
report.add(""raw"", infile.read(), force=True)
File ""/opt/intelmq/venv/lib/python3.5/site-packages/intelmq-1.0.0.dev7-py3.5.egg/intelmq/lib/message.py"", line 212, in add
raise exceptions.InvalidValue(key, value, reason=valid_value[1])
intelmq.lib.exceptions.InvalidValue: invalid value '' (<class 'str'>) for key 'raw' is_valid returned False.",intelmq.lib.exceptions.InvalidValue,buggy_snippets_files/470e00ffbb4a0ff54b77402d2a0479dc4b7c15c89fadd4a03381ce7d3615ea2d_before_merge.py,buggy_snippets_files/470e00ffbb4a0ff54b77402d2a0479dc4b7c15c89fadd4a03381ce7d3615ea2d_after_merge.py
19458,"    def acknowledge_message(self):
        self.__source_pipeline.acknowledge()
","    def acknowledge_message(self):
        """"""
        Acknowledges that the last message has been processed, if any.

        For bots without source pipeline (collectors), this is a no-op.
        """"""
        if self.__source_pipeline:
            self.__source_pipeline.acknowledge()
",https://github.com/certtools/intelmq/issues/1181,"CWE-232: Improper Handling of Undefined Values, CWE-248: Uncaught Exception",the case is unhandled where self.__source_pipeline is None,intelmq/lib/bot.py,Bot.acknowledge_message,[2],"Traceback (most recent call last):
File ""/usr/local/bin/intelmqctl"", line 9, in <module>
load_entry_point('intelmq==1.1.0a1', 'console_scripts', 'intelmqctl')()
File ""/home/sebastian/dev/intelmq/intelmq/bin/intelmqctl.py"", line 1101, in main
return x.run()
File ""/home/sebastian/dev/intelmq/intelmq/bin/intelmqctl.py"", line 562, in run
retval, results = args.func(**args_dict)
File ""/home/sebastian/dev/intelmq/intelmq/bin/intelmqctl.py"", line 569, in bot_run
return self.bot_process_manager.bot_run(**kwargs), None
File ""/home/sebastian/dev/intelmq/intelmq/bin/intelmqctl.py"", line 145, in bot_run
console_type, dryrun, message_action_kind, msg)
File ""/home/sebastian/dev/intelmq/intelmq/lib/bot_debugger.py"", line 49, in __init__
self.instance.start()
File ""/home/sebastian/dev/intelmq/intelmq/lib/bot.py"", line 225, in start
self.acknowledge_message()
File ""/home/sebastian/dev/intelmq/intelmq/lib/bot.py"", line 393, in acknowledge_message
self.__source_pipeline.acknowledge()
AttributeError: 'NoneType' object has no attribute 'acknowledge'",AttributeError,buggy_snippets_files/5db7d0ff0ecc6a10e70dd58beec5bbd9abbbb597bf567abc34cddfcf3e6acae5_before_merge.py,buggy_snippets_files/5db7d0ff0ecc6a10e70dd58beec5bbd9abbbb597bf567abc34cddfcf3e6acae5_after_merge.py
19459,"    @staticmethod
    def sanitize(value):
        value = value.strip('.')
        if value:
            return value.encode('idna').decode().lower()
","    @staticmethod
    def sanitize(value):
        value = value.strip('.')
        if value:
            try:
                return value.encode('idna').decode().lower()
            except UnicodeError:
                return
",https://github.com/certtools/intelmq/issues/1175,CWE-248: Uncaught Exception,when value contains characters like &?{} (they are possible in strings with URLs) it can not be encoded with idna codec,intelmq/lib/harmonization.py,FQDN.sanitize,[5],"Traceback (most recent call last):
File "".../intelmq/intelmq/lib/bot.py"", line 153, in start
self.process()
File "".../intelmq/intelmq/bots/parsers/spamhaus/parser_cert.py"",
line 59, in process
event.add('destination.fqdn', row_splitted[5], raise_failure=False)
File "".../intelmq/intelmq/lib/message.py"", line 194, in add
value = self.__sanitize_value(key, value)
File "".../intelmq/intelmq/lib/message.py"", line 281, in __sanitize_value
return class_reference().sanitize(value)
File "".../intelmq/intelmq/lib/harmonization.py"", line 381, in sanitize
return value.encode('idna').decode().lower()
UnicodeError: encoding with 'idna' codec failed (UnicodeError: label empty or too long)",UnicodeError,buggy_snippets_files/e7b1a01c7adf59d0dc7ee7854ec848c59b38e4ec47c7ffb61fc232f4215393d0_before_merge.py,buggy_snippets_files/e7b1a01c7adf59d0dc7ee7854ec848c59b38e4ec47c7ffb61fc232f4215393d0_after_merge.py
19489,"    def process(self):
        """"""
        The Ransomware Tracker has comments in it.
        The IP address field can also have more than one address.
        The ASN and Country code are being ignored, an expert parser can get those added.
        """"""

        report = self.receive_message()
        raw_report = utils.base64_decode(report.get(""raw""))

        for row in csv.reader(io.StringIO(raw_report)):
            if row[0].startswith('#'):
                continue

            if '|' in row[7]:
                for ipaddr in row[7].split('|'):
                    new_row = '""' + row[0] + '"",""' + row[1] + '"",""' + row[2] + '"",""' + row[3] \
                              + '"",""' + row[4] + '"",""' + row[5] + '"",""' + row[6] + '"",""' + ipaddr \
                              + '"",""' + row[8] + '"",""' + row[9] + '""'

                    for nrow in csv.reader(io.StringIO(new_row)):
                        ev = Event(report)
                        ev.add('classification.identifier', nrow[2].lower())
                        ev.add('classification.type', 'c&c')
                        ev.add('time.source', nrow[0] + ' UTC', overwrite=True)
                        ev.add('status', nrow[5])
                        if nrow[7] != '0.0.0.0':
                            ev.add('source.ip', nrow[7])
                        ev.add('raw', ','.join(nrow))
                        ev.add('source.fqdn', nrow[3], raise_failure=False)
                        ev.add('source.url', nrow[4], raise_failure=False)
                        self.send_message(ev)
            else:
                event = Event(report)
                event.add('classification.identifier', row[2].lower())
                event.add('classification.type', 'c&c')
                event.add('time.source', row[0] + ' UTC')
                event.add('status', row[5])
                event.add('raw', ','.join(row))
                event.add('source.ip', row[7], raise_failure=False)
                event.add('source.fqdn', row[3], raise_failure=False)
                event.add('source.url', row[4], raise_failure=False)
                self.send_message(event)
        self.acknowledge_message()
","    def process(self):
        """"""
        The Ransomware Tracker has comments in it.
        The IP address field can also have more than one address.
        The ASN and Country code are being ignored, an expert parser can get those added.
        """"""

        report = self.receive_message()
        raw_report = utils.base64_decode(report.get(""raw""))
        raw_report = raw_report.translate({0: None})

        for row in csv.reader(io.StringIO(raw_report)):
            if row[0].startswith('#'):
                continue

            if '|' in row[7]:
                for ipaddr in row[7].split('|'):
                    new_row = '""' + row[0] + '"",""' + row[1] + '"",""' + row[2] + '"",""' + row[3] \
                              + '"",""' + row[4] + '"",""' + row[5] + '"",""' + row[6] + '"",""' + ipaddr \
                              + '"",""' + row[8] + '"",""' + row[9] + '""'

                    for nrow in csv.reader(io.StringIO(new_row)):
                        ev = Event(report)
                        ev.add('classification.identifier', nrow[2].lower())
                        ev.add('classification.type', 'c&c')
                        ev.add('time.source', nrow[0] + ' UTC', overwrite=True)
                        ev.add('status', nrow[5])
                        if nrow[7] != '0.0.0.0':
                            ev.add('source.ip', nrow[7])
                        ev.add('raw', ','.join(nrow))
                        ev.add('source.fqdn', nrow[3], raise_failure=False)
                        ev.add('source.url', nrow[4], raise_failure=False)
                        self.send_message(ev)
            else:
                event = Event(report)
                event.add('classification.identifier', row[2].lower())
                event.add('classification.type', 'c&c')
                event.add('time.source', row[0] + ' UTC')
                event.add('status', row[5])
                event.add('raw', ','.join(row))
                event.add('source.ip', row[7], raise_failure=False)
                event.add('source.fqdn', row[3], raise_failure=False)
                event.add('source.url', row[4], raise_failure=False)
                self.send_message(event)
        self.acknowledge_message()
",https://github.com/certtools/intelmq/issues/967,"CWE-167: Improper Handling of Additional Special Element, CWE-248: Uncaught Exception","when processing a variable io.StringIO(raw_report) NULL bytes can occur, which cause a processing error",intelmq/bots/parsers/abusech/parser_ransomware.py,AbuseCHRansomwaretrackerParserBot.process,[11],"Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""/usr/lib64/python3.4/csv.py"", line 110, in __next__
row = next(self.reader)
_csv.Error: line contains NULL byte",_csv.Error,buggy_snippets_files/8c0492f59f5597fa8aac72238ddbd5887e282d1284894f585e11bb807e383768_before_merge.py,buggy_snippets_files/8c0492f59f5597fa8aac72238ddbd5887e282d1284894f585e11bb807e383768_after_merge.py
19586,"    @staticmethod
    def sanitize(value):
        value = value.rstrip('.')
        if value:
            return value.encode('idna').decode().lower()
","    @staticmethod
    def sanitize(value):
        value = value.strip('.')
        if value:
            return value.encode('idna').decode().lower()
",https://github.com/certtools/intelmq/issues/1022,"CWE-237: Improper Handling of Structural Elements, CWE-248: Uncaught Exception","when calling sanitize from self.__sanitize_value from event.add(intelmqkey, value) it appears that value contains leading '.' characters, which causes an error with encoding using idna codec",intelmq/lib/harmonization.py,FQDN.sanitize,[3],"'source_queue': 'shadowserver-parser-dns-open-resolvers-queue',
'traceback': ['Traceback (most recent call last):',
'  File ""/usr/lib/python3.4/encodings/idna.py"", line 165, in '
'encode',
'    raise UnicodeError(""label empty or too long"")',
'UnicodeError: label empty or too long',
'',
'The above exception was the direct cause of the following '
'exception:',
'',
'Traceback (most recent call last):',
'  File ""/usr/lib/python3/dist-packages/intelmq/lib/bot.py"", '
'line 606, in process',
'    events = list(filter(bool, self.parse_line(line,
report)))',
'  File '
'""/usr/lib/python3/dist-packages/intelmq/bots/parsers/shadowserver/parser.py"", '
'line 150, in parse_line',
'    event.add(intelmqkey, value)',
'  File '
'""/usr/lib/python3/dist-packages/intelmq/lib/message.py"",
line '
'202, in add',
'    value = self.__sanitize_value(key, value)',
'  File '
'""/usr/lib/python3/dist-packages/intelmq/lib/message.py"",
line '
'298, in __sanitize_value',
'    return class_reference().sanitize(value)',
'  File '
'""/usr/lib/python3/dist-packages/intelmq/lib/harmonization.py"", '
'line 374, in sanitize',
""    return value.encode('idna').decode().lower()"",
""UnicodeError: encoding with 'idna' codec failed
(UnicodeError: ""
'label empty or too long)']}",UnicodeError,buggy_snippets_files/c92dfc0e75e0913b0bf2155936af58a2db86824e8e0c06c441502114780cec09_before_merge.py,buggy_snippets_files/c92dfc0e75e0913b0bf2155936af58a2db86824e8e0c06c441502114780cec09_after_merge.py
20894,"    def __init__(self, execer, ctx, **kwargs):
        super().__init__()
        self.execer = execer
        self.ctx = ctx
        if kwargs.get('completer', True):
            self.completer = Completer()
        self.buffer = []
        self.need_more_lines = False
        self.mlprompt = None
        if HAS_PYGMENTS:
            env = builtins.__xonsh_env__
            self.styler = XonshStyle(env.get('XONSH_COLOR_STYLE'))
        else:
            self.styler = None
","    def __init__(self, execer, ctx, **kwargs):
        super().__init__()
        self.execer = execer
        self.ctx = ctx
        self.completer = Completer() if kwargs.get('completer', True) else None
        self.buffer = []
        self.need_more_lines = False
        self.mlprompt = None
        if HAS_PYGMENTS:
            env = builtins.__xonsh_env__
            self.styler = XonshStyle(env.get('XONSH_COLOR_STYLE'))
        else:
            self.styler = None
",https://github.com/xonsh/xonsh/issues/1070,CWE-232: Improper Handling of Undefined Values,"when working with the xonsh shell, a PromptToolkitShell class object can be created (see traceback). PromptToolkitShell is descendant of BaseShell class. In the __init__method of the PromptToolkitShell class, first,  __init__ method of the base class is called. In the __init__method of the BaseShell class the situation is possible where completer attribute is absent. But the expression self.pt_completer = PromptToolkitCompleter(self.completer, self.ctx, self) is executed either way, raising AttributeError",xonsh/base_shell.py,BaseShell.__init__,"[5, 6]","cryzed@arch ~ $ xonsh update-aur-gits.xsh
Traceback (most recent call last):
File ""/usr/bin/xonsh"", line 9, in <module>
load_entry_point('xonsh==0.3.2', 'console_scripts', 'xonsh')()
File ""/usr/lib/python3.5/site-packages/xonsh/main.py"", line 228, in main
args = premain(argv)
File ""/usr/lib/python3.5/site-packages/xonsh/main.py"", line 215, in premain
shell = builtins.__xonsh_shell__ = Shell(**shell_kwargs)
File ""/usr/lib/python3.5/site-packages/xonsh/shell.py"", line 72, in __init__
ctx=self.ctx, **kwargs)
File ""/usr/lib/python3.5/site-packages/xonsh/ptk/shell.py"", line 33, in __init__
self.pt_completer = PromptToolkitCompleter(self.completer, self.ctx)
AttributeError: 'PromptToolkitShell' object has no attribute 'completer'",AttributeError,buggy_snippets_files/7db66dd328748f06bacba2467021692d35a214f8e192f54b5dadb58b59ab33ec_before_merge.py,buggy_snippets_files/7db66dd328748f06bacba2467021692d35a214f8e192f54b5dadb58b59ab33ec_after_merge.py
20902,"def cd(args, stdin=None):
    """"""Changes the directory.

    If no directory is specified (i.e. if `args` is None) then this
    changes to the current user's home directory.
    """"""
    env = builtins.__xonsh_env__
    oldpwd = env.get('OLDPWD', None)
    cwd = env['PWD']

    if len(args) == 0:
        d = os.path.expanduser('~')
    elif len(args) == 1:
        d = os.path.expanduser(args[0])
        if not os.path.isdir(d):
            if d == '-':
                if oldpwd is not None:
                    d = oldpwd
                else:
                    return '', 'cd: no previous directory stored\n', 1
            elif d.startswith('-'):
                try:
                    num = int(d[1:])
                except ValueError:
                    return '', 'cd: Invalid destination: {0}\n'.format(d), 1
                if num == 0:
                    return None, None, 0
                elif num < 0:
                    return '', 'cd: Invalid destination: {0}\n'.format(d), 1
                elif num > len(DIRSTACK):
                    e = 'cd: Too few elements in dirstack ({0} elements)\n'
                    return '', e.format(len(DIRSTACK)), 1
                else:
                    d = DIRSTACK[num - 1]
            else:
                d = _try_cdpath(d)
    else:
        return '', 'cd takes 0 or 1 arguments, not {0}\n'.format(len(args)), 1
    if not os.path.exists(d):
        return '', 'cd: no such file or directory: {0}\n'.format(d), 1
    if not os.path.isdir(d):
        return '', 'cd: {0} is not a directory\n'.format(d), 1
    if not os.access(d, os.X_OK):
        return '', 'cd: permission denied: {0}\n'.format(d), 1
    if ON_WINDOWS and (d[0] == d[1]) and (d[0] in (os.sep, os.altsep)) \
            and _unc_check_enabled() and (not env.get('AUTO_PUSHD')):
        return '', ""cd: can't cd to UNC path on Windows, unless $AUTO_PUSHD set or reg entry "" \
               + r'HKCU\SOFTWARE\MICROSOFT\Command Processor\DisableUNCCheck:DWORD = 1' + '\n', 1

    # now, push the directory onto the dirstack if AUTO_PUSHD is set
    if cwd is not None and env.get('AUTO_PUSHD'):
        pushd(['-n', '-q', cwd])
        if ON_WINDOWS and (d[0] == d[1]) and (d[0] in (os.sep, os.altsep)):
            d = _unc_map_temp_drive(d)
    _change_working_directory(d)
    return None, None, 0
","def cd(args, stdin=None):
    """"""Changes the directory.

    If no directory is specified (i.e. if `args` is None) then this
    changes to the current user's home directory.
    """"""
    env = builtins.__xonsh_env__
    oldpwd = env.get('OLDPWD', None)
    cwd = env['PWD']

    if len(args) == 0:
        d = os.path.expanduser('~')
    elif len(args) == 1:
        d = os.path.expanduser(args[0])
        if not os.path.isdir(d):
            if d == '-':
                if oldpwd is not None:
                    d = oldpwd
                else:
                    return '', 'cd: no previous directory stored\n', 1
            elif d.startswith('-'):
                try:
                    num = int(d[1:])
                except ValueError:
                    return '', 'cd: Invalid destination: {0}\n'.format(d), 1
                if num == 0:
                    return None, None, 0
                elif num < 0:
                    return '', 'cd: Invalid destination: {0}\n'.format(d), 1
                elif num > len(DIRSTACK):
                    e = 'cd: Too few elements in dirstack ({0} elements)\n'
                    return '', e.format(len(DIRSTACK)), 1
                else:
                    d = DIRSTACK[num - 1]
            else:
                d = _try_cdpath(d)
    else:
        return '', 'cd takes 0 or 1 arguments, not {0}\n'.format(len(args)), 1
    if not os.path.exists(d):
        return '', 'cd: no such file or directory: {0}\n'.format(d), 1
    if not os.path.isdir(d):
        return '', 'cd: {0} is not a directory\n'.format(d), 1
    if not os.access(d, os.X_OK):
        return '', 'cd: permission denied: {0}\n'.format(d), 1
    if ON_WINDOWS and len(d) > 1 and (d[0] == d[1]) and (d[0] in (os.sep, os.altsep)) \
            and _unc_check_enabled() and (not env.get('AUTO_PUSHD')):
        return '', ""cd: can't cd to UNC path on Windows, unless $AUTO_PUSHD set or reg entry "" \
               + r'HKCU\SOFTWARE\MICROSOFT\Command Processor\DisableUNCCheck:DWORD = 1' + '\n', 1

    # now, push the directory onto the dirstack if AUTO_PUSHD is set
    if cwd is not None and env.get('AUTO_PUSHD'):
        pushd(['-n', '-q', cwd])
        if ON_WINDOWS and (d[0] == d[1]) and (d[0] in (os.sep, os.altsep)):
            d = _unc_map_temp_drive(d)
    _change_working_directory(d)
    return None, None, 0
",https://github.com/xonsh/xonsh/issues/1667,"CWE-126: Buffer Over-read, CWE-248: Uncaught Exception","the xonsh emulates command line and Python in a single shell. The cd function implements the corresponding cd shell command. If d, a variable, representing cd arguments, contains only a single character, the expression ON_WINDOWS and (d[0] == d[1]) and (d[0] in (os.sep, os.altsep)) \
            and _unc_check_enabled() and (not env.get('AUTO_PUSHD')) becomes invalid",xonsh/dirstack.py,cd,"[45, 46]","Traceback (most recent call last):
File ""C:\Users\James\AppData\Local\Programs\Python\Python35\lib\site-packages\xonsh\proc.py"", line 354, in wrapped_simple_command
r = f(args, i)
File ""C:\Users\James\AppData\Local\Programs\Python\Python35\lib\site-packages\xontrib\avox.py"", line 149, in cd_handler
rtn = _old_cd(args, stdin)
File ""C:\Users\James\AppData\Local\Programs\Python\Python35\lib\site-packages\xonsh\dirstack.py"", line 212, in cd
if ON_WINDOWS and (d[0] == d[1]) and (d[0] in (os.sep, os.altsep)) \
IndexError: string index out of range",IndexError,buggy_snippets_files/9d0af10ad6d3dfabd82d86e961261bed6cff4e2feaa50bc682e65394fad00104_before_merge.py,buggy_snippets_files/9d0af10ad6d3dfabd82d86e961261bed6cff4e2feaa50bc682e65394fad00104_after_merge.py
20957,"    def find_spec(self, fullname, path, target=None):
        """"""Finds the spec for a xonsh module if it exists.""""""
        dot = '.'
        spec = None
        path = sys.path if path is None else path
        if dot not in fullname and dot not in path:
            path = [dot] + path
        name = fullname.rsplit(dot, 1)[-1]
        fname = name + '.xsh'
        for p in path:
            if not isinstance(p, str):
                continue
            if not os.path.isdir(p):
                continue
            if fname not in (x.name for x in scandir(p)):
                continue
            spec = ModuleSpec(fullname, self)
            self._filenames[fullname] = os.path.join(p, fname)
            break
        return spec
","    def find_spec(self, fullname, path, target=None):
        """"""Finds the spec for a xonsh module if it exists.""""""
        dot = '.'
        spec = None
        path = sys.path if path is None else path
        if dot not in fullname and dot not in path:
            path = [dot] + path
        name = fullname.rsplit(dot, 1)[-1]
        fname = name + '.xsh'
        for p in path:
            if not isinstance(p, str):
                continue
            if not os.path.isdir(p) or not os.access(p, os.R_OK):
                continue
            if fname not in (x.name for x in scandir(p)):
                continue
            spec = ModuleSpec(fullname, self)
            self._filenames[fullname] = os.path.join(p, fname)
            break
        return spec
",https://github.com/xonsh/xonsh/issues/1533,"CWE-274: Improper Handling of Insufficient Privileges, CWE-248: Uncaught Exception",the case is unhandled where a directory marked by p variable is unaccessible (insufficient rights etc),xonsh/imphooks.py,XonshImportHook.find_spec,[13],"(C:\Anaconda3) C:\Users\kpr>xonsh
Traceback (most recent call last):
File ""C:\Anaconda3\Scripts\xonsh-script.py"", line 5, in <module>
sys.exit(xonsh.main.main())
File ""C:\Anaconda3\lib\site-packages\xonsh\main.py"", line 209, in main
args = premain(argv)
File ""C:\Anaconda3\lib\site-packages\xonsh\main.py"", line 201, in premain
setup_win_unicode_console(env.get('WIN_UNICODE_CONSOLE', True))
File ""C:\Anaconda3\lib\site-packages\xonsh\tools.py"", line 1091, in setup_win_unicode_console
import win_unicode_console
File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
File ""<frozen importlib._bootstrap>"", line 954, in _find_and_load_unlocked
File ""<frozen importlib._bootstrap>"", line 896, in _find_spec
File ""C:\Anaconda3\lib\site-packages\xonsh\imphooks.py"", line 53, in find_spec
if fname not in (x.name for x in scandir(p)):
PermissionError: [WinError 5] Access is denied: '.'",PermissionError,buggy_snippets_files/9d1bc9761be1c7cdc8d98cbf201dcb397e1fd54b09bad828635e6a32736b563d_before_merge.py,buggy_snippets_files/9d1bc9761be1c7cdc8d98cbf201dcb397e1fd54b09bad828635e6a32736b563d_after_merge.py
