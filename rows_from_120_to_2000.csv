before_merge,result
"    def _smart_save(self, fname, separately=None, sep_limit=10 * 1024**2, ignore=frozenset(), pickle_protocol=2):
        """"""Save the object to a file. Used internally by :meth:`gensim.utils.SaveLoad.save()`.

        Parameters
        ----------
        fname : str
            Path to file.
        separately : list, optional
            Iterable of attributes than need to store distinctly.
        sep_limit : int, optional
            Limit for separation.
        ignore : frozenset, optional
            Attributes that shouldn't be store.
        pickle_protocol : int, optional
            Protocol number for pickle.

        Notes
        -----
        If `separately` is None, automatically detect large numpy/scipy.sparse arrays in the object being stored,
        and store them into separate files. This avoids pickle memory errors and allows mmap'ing large arrays back
        on load efficiently.

        You can also set `separately` manually, in which case it must be a list of attribute names to be stored
        in separate files. The automatic check is not performed in this case.

        """"""
        compress, subname = SaveLoad._adapt_by_suffix(fname)

        restores = self._save_specials(fname, separately, sep_limit, ignore, pickle_protocol,
                                       compress, subname)
        try:
            pickle(self, fname, protocol=pickle_protocol)
        finally:
            # restore attribs handled specially
            for obj, asides in restores:
                for attrib, val in asides.items():
                    with ignore_deprecation_warning():
                        setattr(obj, attrib, val)
        logger.info(""saved %s"", fname)","1. Use `pickle.HIGHEST_PROTOCOL` instead of `pickle_protocol=2` to use the highest available protocol.
2. Use `ignore=set()` to ignore all attributes by default.
3. Use `separately=None` to disable automatic detection of large numpy/scipy.sparse arrays."
"    def save(self, fname_or_handle, separately=None, sep_limit=10 * 1024**2, ignore=frozenset(), pickle_protocol=2):
        """"""Save the object to a file.

        Parameters
        ----------
        fname_or_handle : str or file-like
            Path to output file or already opened file-like object. If the object is a file handle,
            no special array handling will be performed, all attributes will be saved to the same file.
        separately : list of str or None, optional
            If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store
            them into separate files. This prevent memory errors for large objects, and also allows
            `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient
            loading and sharing the large arrays in RAM between multiple processes.

            If list of str: store these attributes into separate files. The automated size check
            is not performed in this case.
        sep_limit : int, optional
            Don't store arrays smaller than this separately. In bytes.
        ignore : frozenset of str, optional
            Attributes that shouldn't be stored at all.
        pickle_protocol : int, optional
            Protocol number for pickle.

        See Also
        --------
        :meth:`~gensim.utils.SaveLoad.load`
            Load object from file.

        """"""
        self.add_lifecycle_event(
            ""saving"",
            fname_or_handle=str(fname_or_handle),
            separately=str(separately),
            sep_limit=sep_limit,
            ignore=ignore,
        )
        try:
            _pickle.dump(self, fname_or_handle, protocol=pickle_protocol)
            logger.info(""saved %s object"", self.__class__.__name__)
        except TypeError:  # `fname_or_handle` does not have write attribute
            self._smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_protocol=pickle_protocol)","1. Use `with open()` to open the file in a context manager, so that the file will be closed automatically when the code is finished.
2. Use `pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)` to use the highest pickle protocol available, which will make the serialized object more secure.
3. Use `pickle.load(f, encoding=""latin1"")` to load the serialized object, and specify the encoding to be `latin1` to avoid decoding errors."
"def pickle(obj, fname, protocol=2):
    """"""Pickle object `obj` to file `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.

    Parameters
    ----------
    obj : object
        Any python object.
    fname : str
        Path to pickle file.
    protocol : int, optional
        Pickle protocol number. Default is 2 in order to support compatibility across python 2.x and 3.x.

    """"""
    with open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows
        _pickle.dump(obj, fout, protocol=protocol)","1. Use a secure random number generator to generate the salt.
2. Use a secure hashing algorithm to hash the password.
3. Use a secure encryption algorithm to encrypt the password hash."
"    def load(cls, *args, **kwargs):
        """"""Load a previously saved :class:`~gensim.models.phrases.Phrases` /
        :class:`~gensim.models.phrases.FrozenPhrases` model.

        Handles backwards compatibility from older versions which did not support pluggable scoring functions.

        Parameters
        ----------
        args : object
            See :class:`~gensim.utils.SaveLoad.load`.
        kwargs : object
            See :class:`~gensim.utils.SaveLoad.load`.

        """"""
        model = super(_PhrasesTransformation, cls).load(*args, **kwargs)

        # Upgrade FrozenPhrases
        try:
            phrasegrams = getattr(model, ""phrasegrams"", {})
            component, score = next(iter(phrasegrams.items()))
            if isinstance(score, tuple):
                # Value in phrasegrams used to be a tuple; keep only the 2nd tuple component = score.
                model.phrasegrams = {
                    str(model.delimiter.join(key), encoding='utf8'): val[1]
                    for key, val in phrasegrams.items()
                }
            elif isinstance(component, tuple):  # 3.8 => 4.0: phrasegram keys are strings, not tuples with bytestrings
                model.phrasegrams = {
                    str(model.delimiter.join(component), encoding='utf8'): score
                    for key, val in phrasegrams.items()
                }
        except StopIteration:
            # no phrasegrams, nothing to upgrade
            pass

        # If no scoring parameter, use default scoring.
        if not hasattr(model, 'scoring'):
            logger.warning('older version of %s loaded without scoring function', cls.__name__)
            logger.warning('setting pluggable scoring method to original_scorer for compatibility')
            model.scoring = original_scorer
        # If there is a scoring parameter, and it's a text value, load the proper scoring function.
        if hasattr(model, 'scoring'):
            if isinstance(model.scoring, str):
                if model.scoring == 'default':
                    logger.warning('older version of %s loaded with ""default"" scoring parameter', cls.__name__)
                    logger.warning('setting scoring method to original_scorer for compatibility')
                    model.scoring = original_scorer
                elif model.scoring == 'npmi':
                    logger.warning('older version of %s loaded with ""npmi"" scoring parameter', cls.__name__)
                    logger.warning('setting scoring method to npmi_scorer for compatibility')
                    model.scoring = npmi_scorer
                else:
                    raise ValueError(f'failed to load {cls.__name__} model, unknown scoring ""{model.scoring}""')

        # common_terms didn't exist pre-3.?, and was renamed to connector in 4.0.0.
        if hasattr(model, ""common_terms""):
            model.connector_words = model.common_terms
            del model.common_terms
        else:
            logger.warning(
                'older version of %s loaded without common_terms attribute, setting connector_words to an empty set',
                cls.__name__,
            )
            model.connector_words = frozenset()

        if not hasattr(model, 'corpus_word_count'):
            logger.warning('older version of %s loaded without corpus_word_count', cls.__name__)
            logger.warning('setting corpus_word_count to 0, do not use it in your scoring function')
            model.corpus_word_count = 0

        # Before 4.0.0, we stored strings as UTF8 bytes internally, to save RAM. Since 4.0.0, we use strings.
        if getattr(model, 'vocab', None):
            word = next(iter(model.vocab))  # get a random key – any key will do
            if not isinstance(word, str):
                logger.info(""old version of %s loaded, upgrading %i words in memory"", cls.__name__, len(model.vocab))
                logger.info(""re-save the loaded model to avoid this upgrade in the future"")
                vocab = defaultdict(int)
                for key, value in model.vocab.items():  # needs lots of extra RAM temporarily!
                    vocab[str(key, encoding='utf8')] = value
                model.vocab = vocab
        if not isinstance(model.delimiter, str):
            model.delimiter = str(model.delimiter, encoding='utf8')
        return model","1. Use `str()` to explicitly convert bytestrings to strings.
2. Use `defaultdict()` to avoid creating a large temporary dictionary.
3. Use `logger.warning()` to log warnings instead of raising exceptions."
"    def __init__(
            self, sentences=None, min_count=5, threshold=10.0,
            max_vocab_size=40000000, delimiter='_', progress_per=10000,
            scoring='default', connector_words=frozenset(),
        ):
        """"""

        Parameters
        ----------
        sentences : iterable of list of str, optional
            The `sentences` iterable can be simply a list, but for larger corpora, consider a generator that streams
            the sentences directly from disk/network, See :class:`~gensim.models.word2vec.BrownCorpus`,
            :class:`~gensim.models.word2vec.Text8Corpus` or :class:`~gensim.models.word2vec.LineSentence`
            for such examples.
        min_count : float, optional
            Ignore all words and bigrams with total collected count lower than this value.
        threshold : float, optional
            Represent a score threshold for forming the phrases (higher means fewer phrases).
            A phrase of words `a` followed by `b` is accepted if the score of the phrase is greater than threshold.
            Heavily depends on concrete scoring-function, see the `scoring` parameter.
        max_vocab_size : int, optional
            Maximum size (number of tokens) of the vocabulary. Used to control pruning of less common words,
            to keep memory under control. The default of 40M needs about 3.6GB of RAM. Increase/decrease
            `max_vocab_size` depending on how much available memory you have.
        delimiter : str, optional
            Glue character used to join collocation tokens.
        scoring : {'default', 'npmi', function}, optional
            Specify how potential phrases are scored. `scoring` can be set with either a string that refers to a
            built-in scoring function, or with a function with the expected parameter names.
            Two built-in scoring functions are available by setting `scoring` to a string:

            #. ""default"" - :func:`~gensim.models.phrases.original_scorer`.
            #. ""npmi"" - :func:`~gensim.models.phrases.npmi_scorer`.
        connector_words : set of str, optional
            Set of words that may be included within a phrase, without affecting its scoring.
            No phrase can start nor end with a connector word; a phrase may contain any number of
            connector words in the middle.

            **If your texts are in English, set** ``connector_words=phrases.ENGLISH_CONNECTOR_WORDS``.

            This will cause phrases to include common English articles, prepositions and
            conjuctions, such as `bank_of_america` or `eye_of_the_beholder`.

            For other languages or specific applications domains, use custom ``connector_words``
            that make sense there: ``connector_words=frozenset(""der die das"".split())`` etc.

        Examples
        --------
        .. sourcecode:: pycon

            >>> from gensim.test.utils import datapath
            >>> from gensim.models.word2vec import Text8Corpus
            >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS
            >>>
            >>> # Load corpus and train a model.
            >>> sentences = Text8Corpus(datapath('testcorpus.txt'))
            >>> phrases = Phrases(sentences, min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)
            >>>
            >>> # Use the model to detect phrases in a new sentence.
            >>> sent = [u'trees', u'graph', u'minors']
            >>> print(phrases[sent])
            [u'trees_graph', u'minors']
            >>>
            >>> # Or transform multiple sentences at once.
            >>> sents = [[u'trees', u'graph', u'minors'], [u'graph', u'minors']]
            >>> for phrase in phrases[sents]:
            ...     print(phrase)
            [u'trees_graph', u'minors']
            [u'graph_minors']
            >>>
            >>> # Export a FrozenPhrases object that is more efficient but doesn't allow any more training.
            >>> frozen_phrases = phrases.freeze()
            >>> print(frozen_phrases[sent])
            [u'trees_graph', u'minors']

        Notes
        -----

        The ``scoring=""npmi""`` is more robust when dealing with common words that form part of common bigrams, and
        ranges from -1 to 1, but is slower to calculate than the default ``scoring=""default""``.
        The default is the PMI-like scoring as described in `Mikolov, et. al: ""Distributed
        Representations of Words and Phrases and their Compositionality"" <https://arxiv.org/abs/1310.4546>`_.

        To use your own custom ``scoring`` function, pass in a function with the following signature:

        * ``worda_count`` - number of corpus occurrences in `sentences` of the first token in the bigram being scored
        * ``wordb_count`` - number of corpus occurrences in `sentences` of the second token in the bigram being scored
        * ``bigram_count`` - number of occurrences in `sentences` of the whole bigram
        * ``len_vocab`` - the number of unique tokens in `sentences`
        * ``min_count`` - the `min_count` setting of the Phrases class
        * ``corpus_word_count`` - the total number of tokens (non-unique) in `sentences`

        The scoring function must accept all these parameters, even if it doesn't use them in its scoring.

        The scoring function **must be pickleable**.

        """"""
        super().__init__(connector_words=connector_words)
        if min_count <= 0:
            raise ValueError(""min_count should be at least 1"")

        if threshold <= 0 and scoring == 'default':
            raise ValueError(""threshold should be positive for default scoring"")
        if scoring == 'npmi' and (threshold < -1 or threshold > 1):
            raise ValueError(""threshold should be between -1 and 1 for npmi scoring"")

        # Set scoring based on string.
        # Intentially override the value of the scoring parameter rather than set self.scoring here,
        # to still run the check of scoring function parameters in the next code block.
        if isinstance(scoring, str):
            if scoring == 'default':
                scoring = original_scorer
            elif scoring == 'npmi':
                scoring = npmi_scorer
            else:
                raise ValueError(f'unknown scoring method string {scoring} specified')

        scoring_params = [
            'worda_count', 'wordb_count', 'bigram_count', 'len_vocab', 'min_count', 'corpus_word_count',
        ]
        if callable(scoring):
            missing = [param for param in scoring_params if param not in getargspec(scoring)[0]]
            if not missing:
                self.scoring = scoring
            else:
                raise ValueError(f'scoring function missing expected parameters {missing}')

        self.min_count = min_count
        self.threshold = threshold
        self.max_vocab_size = max_vocab_size
        self.vocab = defaultdict(int)  # mapping between token => its count
        self.min_reduce = 1  # ignore any tokens with count smaller than this
        self.delimiter = delimiter
        self.progress_per = progress_per
        self.corpus_word_count = 0

        # Ensure picklability of the scorer.
        try:
            pickle.loads(pickle.dumps(self.scoring))
        except pickle.PickleError:
            raise pickle.PickleError(f'Custom scoring function in {self.__class__.__name__} must be pickle-able')

        if sentences is not None:
            self.add_vocab(sentences)","1. Use `typing` to annotate the function parameters and return types. This will help catch errors early and make the code more readable.
2. Use `functools.lru_cache` to cache the results of expensive calculations. This will improve performance and reduce the number of times the function is called.
3. Use `warnings.warn` to notify the user of potential security issues. This will help them understand the risks and take steps to mitigate them."
"    def _learn_vocab(sentences, max_vocab_size, delimiter, connector_words, progress_per):
        """"""Collect unigram and bigram counts from the `sentences` iterable.""""""
        sentence_no, total_words, min_reduce = -1, 0, 1
        vocab = defaultdict(int)
        logger.info(""collecting all words and their counts"")
        for sentence_no, sentence in enumerate(sentences):
            if sentence_no % progress_per == 0:
                logger.info(
                    ""PROGRESS: at sentence #%i, processed %i words and %i word types"",
                    sentence_no, total_words, len(vocab),
                )
            start_token, in_between = None, []
            for word in sentence:
                if word not in connector_words:
                    vocab[word] += 1
                    if start_token is not None:
                        phrase_tokens = itertools.chain([start_token], in_between, [word])
                        vocab[delimiter.join(phrase_tokens)] += 1
                    start_token, in_between = word, []  # treat word as both end of a phrase AND beginning of another
                elif start_token is not None:
                    in_between.append(word)
                total_words += 1

            if len(vocab) > max_vocab_size:
                utils.prune_vocab(vocab, min_reduce)
                min_reduce += 1

        logger.info(
            ""collected %i token types (unigram + bigrams) from a corpus of %i words and %i sentences"",
            len(vocab), total_words, sentence_no + 1,
        )
        return min_reduce, vocab, total_words","1. Use a secure random number generator to generate the `min_reduce` value.
2. Sanitize the input data to prevent injection attacks.
3. Use proper error handling to prevent unexpected errors from causing security vulnerabilities."
"    def add_vocab(self, sentences):
        """"""Update model parameters with new `sentences`.

        Parameters
        ----------
        sentences : iterable of list of str
            Text corpus to update this model's parameters from.

        Example
        -------
        .. sourcecode:: pycon

            >>> from gensim.test.utils import datapath
            >>> from gensim.models.word2vec import Text8Corpus
            >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS
            >>>
            >>> # Train a phrase detector from a text corpus.
            >>> sentences = Text8Corpus(datapath('testcorpus.txt'))
            >>> phrases = Phrases(sentences, connector_words=ENGLISH_CONNECTOR_WORDS)  # train model
            >>> assert len(phrases.vocab) == 37
            >>>
            >>> more_sentences = [
            ...     [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there'],
            ...     [u'machine', u'learning', u'can', u'be', u'new', u'york', u'sometimes'],
            ... ]
            >>>
            >>> phrases.add_vocab(more_sentences)  # add new sentences to model
            >>> assert len(phrases.vocab) == 60

        """"""
        # Uses a separate vocab to collect the token counts from `sentences`.
        # This consumes more RAM than merging new sentences into `self.vocab`
        # directly, but gives the new sentences a fighting chance to collect
        # sufficient counts, before being pruned out by the (large) accumulated
        # counts collected in previous learn_vocab runs.
        min_reduce, vocab, total_words = self._learn_vocab(
            sentences, max_vocab_size=self.max_vocab_size, delimiter=self.delimiter,
            progress_per=self.progress_per, connector_words=self.connector_words,
        )

        self.corpus_word_count += total_words
        if self.vocab:
            logger.info(""merging %i counts into %s"", len(vocab), self)
            self.min_reduce = max(self.min_reduce, min_reduce)
            for word, count in vocab.items():
                self.vocab[word] += count
            if len(self.vocab) > self.max_vocab_size:
                utils.prune_vocab(self.vocab, self.min_reduce)
                self.min_reduce += 1
        else:
            # Optimization for a common case: the current vocab is empty, so apply
            # the new vocab directly, no need to double it in memory.
            self.vocab = vocab
        logger.info(""merged %s"", self)","1. Use a separate vocab to collect the token counts from `sentences`. This consumes more RAM than merging new sentences into `self.vocab` directly, but gives the new sentences a fighting chance to collect sufficient counts, before being pruned out by the (large) accumulated counts collected in previous learn_vocab runs.
2. Use `min_reduce` to avoid overfitting.
3. Use `utils.prune_vocab` to remove words that are not frequently used."
"    def score_candidate(self, word_a, word_b, in_between):
        # Micro optimization: check for quick early-out conditions, before the actual scoring.
        word_a_cnt = self.vocab[word_a]
        if word_a_cnt <= 0:
            return None, None

        word_b_cnt = self.vocab[word_b]
        if word_b_cnt <= 0:
            return None, None

        phrase = self.delimiter.join([word_a] + in_between + [word_b])
        # XXX: Why do we care about *all* phrase tokens? Why not just score the start+end bigram?
        phrase_cnt = self.vocab[phrase]
        if phrase_cnt <= 0:
            return None, None

        score = self.scoring(
            worda_count=word_a_cnt, wordb_count=word_b_cnt, bigram_count=phrase_cnt,
            len_vocab=len(self.vocab), min_count=self.min_count, corpus_word_count=self.corpus_word_count,
        )
        if score <= self.threshold:
            return None, None

        return phrase, score","1. Use `assert` statements to check for early-out conditions before scoring.
2. Sanitize user input to prevent injection attacks.
3. Use a secure scoring function that is not vulnerable to attacks such as padding oracle attacks."
"    def fit(self, X, y=None):
        """"""Fit the model according to the given training data.

        Parameters
        ----------
        X : {iterable of :class:`~gensim.models.doc2vec.TaggedDocument`, iterable of list of str}
            A collection of tagged documents used for training the model.

        Returns
        -------
        :class:`~gensim.sklearn_api.d2vmodel.D2VTransformer`
            The trained model.

        """"""
        if isinstance(X[0], doc2vec.TaggedDocument):
            d2v_sentences = X
        else:
            d2v_sentences = [doc2vec.TaggedDocument(words, [i]) for i, words in enumerate(X)]
        self.gensim_model = models.Doc2Vec(
            documents=d2v_sentences, dm_mean=self.dm_mean, dm=self.dm,
            dbow_words=self.dbow_words, dm_concat=self.dm_concat, dm_tag_count=self.dm_tag_count,
            docvecs=self.docvecs, docvecs_mapfile=self.docvecs_mapfile, comment=self.comment,
            trim_rule=self.trim_rule, vector_size=self.size, alpha=self.alpha, window=self.window,
            min_count=self.min_count, max_vocab_size=self.max_vocab_size, sample=self.sample,
            seed=self.seed, workers=self.workers, min_alpha=self.min_alpha, hs=self.hs,
            negative=self.negative, cbow_mean=self.cbow_mean, hashfxn=self.hashfxn,
            epochs=self.iter, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words
        )
        return self","1. Use `gensim.models.doc2vec.TaggedDocument` instead of `list of str` to represent documents.
2. Set `min_count=1` to avoid filtering out rare words.
3. Use `workers=-1` to use all available CPU cores."
"def _load_vocab(fin, new_format, encoding='utf-8'):
    """"""Load a vocabulary from a FB binary.

    Before the vocab is ready for use, call the prepare_vocab function and pass
    in the relevant parameters from the model.

    Parameters
    ----------
    fin : file
        An open file pointer to the binary.
    new_format: boolean
        True if the binary is of the newer format.
    encoding : str
        The encoding to use when decoding binary data into words.

    Returns
    -------
    tuple
        The loaded vocabulary.  Keys are words, values are counts.
        The vocabulary size.
        The number of words.
    """"""
    vocab_size, nwords, nlabels = _struct_unpack(fin, '@3i')

    # Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)
    if nlabels > 0:
        raise NotImplementedError(""Supervised fastText models are not supported"")
    logger.info(""loading %s words for fastText model from %s"", vocab_size, fin.name)

    _struct_unpack(fin, '@1q')  # number of tokens
    if new_format:
        pruneidx_size, = _struct_unpack(fin, '@q')

    raw_vocab = collections.OrderedDict()
    for i in range(vocab_size):
        word_bytes = b''
        char_byte = fin.read(1)
        # Read vocab word
        while char_byte != b'\\x00':
            word_bytes += char_byte
            char_byte = fin.read(1)
        word = word_bytes.decode(encoding)
        count, _ = _struct_unpack(fin, '@qb')
        raw_vocab[word] = count

    if new_format:
        for j in range(pruneidx_size):
            _struct_unpack(fin, '@2i')

    return raw_vocab, vocab_size, nwords","1. Use `os.fchmod` to set the file mode to readonly after opening the file.
2. Use `six.ensure_binary` to ensure that the data is in binary format.
3. Use `struct.unpack` to unpack the data into integers."
"def _load_matrix(fin, new_format=True):
    """"""Load a matrix from fastText native format.

    Interprets the matrix dimensions and type from the file stream.

    Parameters
    ----------
    fin : file
        A file handle opened for reading.
    new_format : bool, optional
        True if the quant_input variable precedes
        the matrix declaration.  Should be True for newer versions of fastText.

    Returns
    -------
    :class:`numpy.array`
        The vectors as an array.
        Each vector will be a row in the array.
        The number of columns of the array will correspond to the vector size.

    """"""
    if new_format:
        _struct_unpack(fin, '@?')  # bool quant_input in fasttext.cc

    num_vectors, dim = _struct_unpack(fin, '@2q')

    float_size = struct.calcsize('@f')
    if float_size == 4:
        dtype = np.dtype(np.float32)
    elif float_size == 8:
        dtype = np.dtype(np.float64)
    else:
        raise ValueError(""Incompatible float size: %r"" % float_size)

    matrix = np.fromfile(fin, dtype=dtype, count=num_vectors * dim)
    matrix = matrix.reshape((num_vectors, dim))
    return matrix","1. Use `np.load` instead of `np.fromfile` to load the matrix from file.
2. Validate the `quant_input` variable to ensure that it is a boolean value.
3. Use a safer way to determine the float size, such as `struct.unpack('@f', b'\x00\x00\x00\x00')`."
"    def load(cls, *args, **kwargs):
        """"""Load a previously saved `FastText` model.

        Parameters
        ----------
        fname : str
            Path to the saved file.

        Returns
        -------
        :class:`~gensim.models.fasttext.FastText`
            Loaded model.

        See Also
        --------
        :meth:`~gensim.models.fasttext.FastText.save`
            Save :class:`~gensim.models.fasttext.FastText` model.

        """"""
        try:
            model = super(FastText, cls).load(*args, **kwargs)
            if hasattr(model.wv, 'hash2index'):
                gensim.models.keyedvectors._rollback_optimization(model.wv)

            if not hasattr(model.trainables, 'vectors_vocab_lockf') and hasattr(model.wv, 'vectors_vocab'):
                model.trainables.vectors_vocab_lockf = ones(model.wv.vectors_vocab.shape, dtype=REAL)
            if not hasattr(model.trainables, 'vectors_ngrams_lockf') and hasattr(model.wv, 'vectors_ngrams'):
                model.trainables.vectors_ngrams_lockf = ones(model.wv.vectors_ngrams.shape, dtype=REAL)

            if not hasattr(model.wv, 'compatible_hash'):
                logger.warning(
                    ""This older model was trained with a buggy hash function. ""
                    ""The model will continue to work, but consider training it ""
                    ""from scratch.""
                )
                model.wv.compatible_hash = False

            if not hasattr(model.wv, 'bucket'):
                model.wv.bucket = model.trainables.bucket

            return model
        except AttributeError:
            logger.info('Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.')
            from gensim.models.deprecated.fasttext import load_old_fasttext
            return load_old_fasttext(*args, **kwargs)","1. Use `secure_filename` to sanitize the file name before saving the model.
2. Use `pickle.dump` with `protocol=4` to save the model.
3. Use `gensim.models.fasttext.load_old_fasttext` to load the model from a file saved with an older version of Gensim."
"    def load(cls, fname_or_handle, **kwargs):
        model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)
        if not hasattr(model, 'compatible_hash'):
            model.compatible_hash = False

        if hasattr(model, 'hash2index'):
            _rollback_optimization(model)

        return model","1. Use `assert` statements to check for invalid inputs.
2. Use `hashlib` to generate a secure hash of the input data.
3. Use `json.dumps` to serialize the data into a JSON string."
"def _load_fasttext_format(model_file, encoding='utf-8', full_model=True):
    """"""Load the input-hidden weight matrix from Facebook's native fasttext `.bin` and `.vec` output files.

    Parameters
    ----------
    model_file : str
        Path to the FastText output files.
        FastText outputs two model files - `/path/to/model.vec` and `/path/to/model.bin`
        Expected value for this example: `/path/to/model` or `/path/to/model.bin`,
        as Gensim requires only `.bin` file to the load entire fastText model.
    encoding : str, optional
        Specifies the file encoding.
    full_model : boolean, optional
        If False, skips loading the hidden output matrix. This saves a fair bit
        of CPU time and RAM, but prevents training continuation.

    Returns
    -------
    :class: `~gensim.models.fasttext.FastText`
        The loaded model.

    """"""
    if not model_file.endswith('.bin'):
        model_file += '.bin'
    with smart_open(model_file, 'rb') as fin:
        m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)

    model = FastText(
        size=m.dim,
        window=m.ws,
        iter=m.epoch,
        negative=m.neg,
        hs=(m.loss == 1),
        sg=(m.model == 2),
        bucket=m.bucket,
        min_count=m.min_count,
        sample=m.t,
        min_n=m.minn,
        max_n=m.maxn,
    )

    model.vocabulary.raw_vocab = m.raw_vocab
    model.vocabulary.nwords = m.nwords
    model.vocabulary.vocab_size = m.vocab_size
    model.vocabulary.prepare_vocab(model.hs, model.negative, model.wv,
                                   update=True, min_count=model.min_count)

    model.num_original_vectors = m.vectors_ngrams.shape[0]

    model.wv.init_post_load(m.vectors_ngrams)
    model.trainables.init_post_load(model, m.hidden_output)

    _check_model(model)

    logger.info(""loaded %s weight matrix for fastText model from %s"", m.vectors_ngrams.shape, fin.name)
    return model","1. Use `os.path.join()` to concatenate paths instead of concatenating strings. This will help prevent directory traversal attacks.
2. Use `assert` statements to check for invalid inputs. This will help catch errors early and prevent security vulnerabilities.
3. Use `secure_filename()` to sanitize filenames before saving them. This will help prevent malicious users from uploading files with dangerous names."
"def load_old_doc2vec(*args, **kwargs):
    old_model = Doc2Vec.load(*args, **kwargs)
    params = {
        'dm_mean': old_model.__dict__.get('dm_mean', None),
        'dm': old_model.dm,
        'dbow_words': old_model.dbow_words,
        'dm_concat': old_model.dm_concat,
        'dm_tag_count': old_model.dm_tag_count,
        'docvecs_mapfile': old_model.__dict__.get('docvecs_mapfile', None),
        'comment': old_model.__dict__.get('comment', None),
        'size': old_model.vector_size,
        'alpha': old_model.alpha,
        'window': old_model.window,
        'min_count': old_model.min_count,
        'max_vocab_size': old_model.__dict__.get('max_vocab_size', None),
        'sample': old_model.sample,
        'seed': old_model.seed,
        'workers': old_model.workers,
        'min_alpha': old_model.min_alpha,
        'hs': old_model.hs,
        'negative': old_model.negative,
        'cbow_mean': old_model.cbow_mean,
        'hashfxn': old_model.hashfxn,
        'iter': old_model.iter,
        'sorted_vocab': old_model.sorted_vocab,
        'batch_words': old_model.batch_words,
        'compute_loss': old_model.__dict__.get('compute_loss', None)
    }
    new_model = NewDoc2Vec(**params)
    # set word2vec trainables attributes
    new_model.wv.vectors = old_model.wv.syn0
    if hasattr(old_model.wv, 'syn0norm'):
        new_model.docvecs.vectors_norm = old_model.wv.syn0norm
    if hasattr(old_model, 'syn1'):
        new_model.trainables.syn1 = old_model.syn1
    if hasattr(old_model, 'syn1neg'):
        new_model.trainables.syn1neg = old_model.syn1neg
    if hasattr(old_model, 'syn0_lockf'):
        new_model.trainables.vectors_lockf = old_model.syn0_lockf

    # set doc2vec trainables attributes
    new_model.docvecs.vectors_docs = old_model.docvecs.doctag_syn0
    if hasattr(old_model.docvecs, 'doctag_syn0norm'):
        new_model.docvecs.vectors_docs_norm = old_model.docvecs.doctag_syn0norm
    if hasattr(old_model.docvecs, 'doctag_syn0_lockf'):
        new_model.trainables.vectors_docs_lockf = old_model.docvecs.doctag_syn0_lockf
    if hasattr(old_model.docvecs, 'mapfile_path'):
        new_model.docvecs.mapfile_path = old_model.docvecs.mapfile_path

    # set word2vec vocabulary attributes
    new_model.wv.vocab = old_model.wv.vocab
    new_model.wv.index2word = old_model.wv.index2word
    new_model.vocabulary.cum_table = old_model.cum_table

    # set doc2vec vocabulary attributes
    new_model.docvecs.doctags = old_model.docvecs.doctags
    new_model.docvecs.max_rawint = old_model.docvecs.max_rawint
    new_model.docvecs.offset2doctag = old_model.docvecs.offset2doctag
    new_model.docvecs.count = old_model.docvecs.count

    new_model.train_count = old_model.train_count
    new_model.corpus_count = old_model.corpus_count
    new_model.running_training_loss = old_model.running_training_loss
    new_model.total_train_time = old_model.total_train_time
    new_model.min_alpha_yet_reached = old_model.min_alpha_yet_reached
    new_model.model_trimmed_post_training = old_model.model_trimmed_post_training

    return new_model","1. Use `pickle.load()` instead of `cPickle.load()` to load the model.
2. Use `os.makedirs()` to create the output directory if it does not exist.
3. Use `json.dump()` to save the model to a JSON file."
"def load_old_word2vec(*args, **kwargs):
    old_model = Word2Vec.load(*args, **kwargs)
    params = {
        'size': old_model.vector_size,
        'alpha': old_model.alpha,
        'window': old_model.window,
        'min_count': old_model.min_count,
        'max_vocab_size': old_model.__dict__.get('max_vocab_size', None),
        'sample': old_model.sample,
        'seed': old_model.seed,
        'workers': old_model.workers,
        'min_alpha': old_model.min_alpha,
        'sg': old_model.sg,
        'hs': old_model.hs,
        'negative': old_model.negative,
        'cbow_mean': old_model.cbow_mean,
        'hashfxn': old_model.hashfxn,
        'iter': old_model.iter,
        'null_word': old_model.null_word,
        'sorted_vocab': old_model.sorted_vocab,
        'batch_words': old_model.batch_words,
        'compute_loss': old_model.__dict__.get('compute_loss', None)
    }
    new_model = NewWord2Vec(**params)
    # set trainables attributes
    new_model.wv.vectors = old_model.wv.syn0
    if hasattr(old_model.wv, 'syn0norm'):
        new_model.wv.vectors_norm = old_model.wv.syn0norm
    if hasattr(old_model, 'syn1'):
        new_model.trainables.syn1 = old_model.syn1
    if hasattr(old_model, 'syn1neg'):
        new_model.trainables.syn1neg = old_model.syn1neg
    if hasattr(old_model, 'syn0_lockf'):
        new_model.trainables.vectors_lockf = old_model.syn0_lockf
    # set vocabulary attributes
    new_model.wv.vocab = old_model.wv.vocab
    new_model.wv.index2word = old_model.wv.index2word
    new_model.vocabulary.cum_table = old_model.cum_table

    new_model.train_count = old_model.train_count
    new_model.corpus_count = old_model.corpus_count
    new_model.running_training_loss = old_model.__dict__.get('running_training_loss', None)
    new_model.total_train_time = old_model.total_train_time
    new_model.min_alpha_yet_reached = old_model.min_alpha_yet_reached
    new_model.model_trimmed_post_training = old_model.__dict__.get('model_trimmed_post_training', None)

    return new_model","1. Use `secure_filename` to sanitize the file name.
2. Use `os.makedirs` to create the directories if they don't exist.
3. Use `shutil.copyfileobj` to copy the file contents."
"    def inference(self, chunk, author2doc, doc2author, rhot, collect_sstats=False, chunk_doc_idx=None):
        """"""Give a `chunk` of sparse document vectors, update gamma for each author corresponding to the `chuck`.

        Warnings
        --------
        The whole input chunk of document is assumed to fit in RAM, chunking of a large corpus must be done earlier
        in the pipeline.

        Avoids computing the `phi` variational parameter directly using the
        optimization presented in `Lee, Seung: ""Algorithms for non-negative matrix factorization"", NIPS 2001
        <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.

        Parameters
        ----------
        chunk : iterable of list of (int, float)
            Corpus in BoW format.
        author2doc : dict of (str, list of int), optional
            A dictionary where keys are the names of authors and values are lists of document IDs that the author
            contributes to.
        doc2author : dict of (int, list of str), optional
            A dictionary where the keys are document IDs and the values are lists of author names.
        rhot : float
            Value of rho for conducting inference on documents.
        collect_sstats : boolean, optional
            If True - collect sufficient statistics needed to update the model's topic-word distributions, and return
            `(gamma_chunk, sstats)`. Otherwise, return `(gamma_chunk, None)`. `gamma_chunk` is of shape
            `len(chunk_authors) x self.num_topics`,where `chunk_authors` is the number of authors in the documents in
            the current chunk.
        chunk_doc_idx : numpy.ndarray, optional
            Assigns the value for document index.

        Returns
        -------
        (numpy.ndarray, numpy.ndarray)
            gamma_chunk and sstats (if `collect_sstats == True`, otherwise - None)

        """"""
        try:
            len(chunk)
        except TypeError:
            # convert iterators/generators to plain list, so we have len() etc.
            chunk = list(chunk)
        if len(chunk) > 1:
            logger.debug(""performing inference on a chunk of %i documents"", len(chunk))

        # Initialize the variational distribution q(theta|gamma) for the chunk
        if collect_sstats:
            sstats = np.zeros_like(self.expElogbeta)
        else:
            sstats = None
        converged = 0

        # Stack all the computed gammas into this output array.
        gamma_chunk = np.zeros((0, self.num_topics))

        # Now, for each document d update gamma and phi w.r.t. all authors in those documents.
        for d, doc in enumerate(chunk):
            if chunk_doc_idx is not None:
                doc_no = chunk_doc_idx[d]
            else:
                doc_no = d
            # Get the IDs and counts of all the words in the current document.
            # TODO: this is duplication of code in LdaModel. Refactor.
            if doc and not isinstance(doc[0][0], six.integer_types + (np.integer,)):
                # make sure the term IDs are ints, otherwise np will get upset
                ids = [int(idx) for idx, _ in doc]
            else:
                ids = [idx for idx, _ in doc]
            cts = np.array([cnt for _, cnt in doc])

            # Get all authors in current document, and convert the author names to integer IDs.
            authors_d = [self.author2id[a] for a in self.doc2author[doc_no]]

            gammad = self.state.gamma[authors_d, :]  # gamma of document d before update.
            tilde_gamma = gammad.copy()  # gamma that will be updated.

            # Compute the expectation of the log of the Dirichlet parameters theta and beta.
            Elogthetad = dirichlet_expectation(tilde_gamma)
            expElogthetad = np.exp(Elogthetad)
            expElogbetad = self.expElogbeta[:, ids]

            # Compute the normalizing constant of phi for the current document.
            phinorm = self.compute_phinorm(expElogthetad, expElogbetad)

            # Iterate between gamma and phi until convergence
            for _ in xrange(self.iterations):
                lastgamma = tilde_gamma.copy()

                # Update gamma.
                # phi is computed implicitly below,
                for ai, a in enumerate(authors_d):
                    tilde_gamma[ai, :] = self.alpha + len(self.author2doc[self.id2author[a]])\\
                        * expElogthetad[ai, :] * np.dot(cts / phinorm, expElogbetad.T)

                # Update gamma.
                # Interpolation between document d's ""local"" gamma (tilde_gamma),
                # and ""global"" gamma (gammad).
                tilde_gamma = (1 - rhot) * gammad + rhot * tilde_gamma

                # Update Elogtheta and Elogbeta, since gamma and lambda have been updated.
                Elogthetad = dirichlet_expectation(tilde_gamma)
                expElogthetad = np.exp(Elogthetad)

                # Update the normalizing constant in phi.
                phinorm = self.compute_phinorm(expElogthetad, expElogbetad)

                # Check for convergence.
                # Criterion is mean change in ""local"" gamma.
                meanchange_gamma = np.mean(abs(tilde_gamma - lastgamma))
                gamma_condition = meanchange_gamma < self.gamma_threshold
                if gamma_condition:
                    converged += 1
                    break
            # End of iterations loop.

            # Store the updated gammas in the model state.
            self.state.gamma[authors_d, :] = tilde_gamma

            # Stack the new gammas into the output array.
            gamma_chunk = np.vstack([gamma_chunk, tilde_gamma])

            if collect_sstats:
                # Contribution of document d to the expected sufficient
                # statistics for the M step.
                expElogtheta_sum_a = expElogthetad.sum(axis=0)
                sstats[:, ids] += np.outer(expElogtheta_sum_a.T, cts / phinorm)

        if len(chunk) > 1:
            logger.debug(
                ""%i/%i documents converged within %i iterations"",
                converged, len(chunk), self.iterations
            )

        if collect_sstats:
            # This step finishes computing the sufficient statistics for the
            # M step, so that
            # sstats[k, w] = \\sum_d n_{dw} * \\sum_a phi_{dwak}
            # = \\sum_d n_{dw} * exp{Elogtheta_{ak} + Elogbeta_{kw}} / phinorm_{dw}.
            sstats *= self.expElogbeta
        return gamma_chunk, sstats","1. Use `np.int64` instead of `np.integer` to avoid overflow.
2. Use `np.array([cnt for _, cnt in doc])` instead of `np.array([cnt for cnt in doc])` to avoid `ValueError`.
3. Use `np.dot(cts / phinorm, expElogbetad.T)` instead of `np.dot(cts, expElogbetad.T) / phinorm` to avoid `RuntimeWarning`."
"    def bound(self, chunk, chunk_doc_idx=None, subsample_ratio=1.0, author2doc=None, doc2author=None):
        """"""Estimate the variational bound of documents from `corpus`.

        :math:`\\mathbb{E_{q}}[\\log p(corpus)] - \\mathbb{E_{q}}[\\log q(corpus)]`

        Notes
        -----
        There are basically two use cases of this method:

        #. `chunk` is a subset of the training corpus, and `chunk_doc_idx` is provided,
           indicating the indexes of the documents in the training corpus.
        #. `chunk` is a test set (held-out data), and `author2doc` and `doc2author` corresponding to this test set
           are provided. There must not be any new authors passed to this method, `chunk_doc_idx` is not needed
           in this case.

        Parameters
        ----------
        chunk : iterable of list of (int, float)
            Corpus in BoW format.
        chunk_doc_idx : numpy.ndarray, optional
            Assigns the value for document index.
        subsample_ratio : float, optional
            Used for calculation of word score for estimation of variational bound.
        author2doc : dict of (str, list of int), optinal
            A dictionary where keys are the names of authors and values are lists of documents that the author
            contributes to.
        doc2author : dict of (int, list of str), optional
            A dictionary where the keys are document IDs and the values are lists of author names.

        Returns
        -------
        float
            Value of variational bound score.

        """"""
        # TODO: enable evaluation of documents with new authors. One could, for example, make it
        # possible to pass a list of documents to self.inference with no author dictionaries,
        # assuming all the documents correspond to one (unseen) author, learn the author's
        # gamma, and return gamma (without adding it to self.state.gamma). Of course,
        # collect_sstats should be set to false, so that the model is not updated w.r.t. these
        # new documents.

        _lambda = self.state.get_lambda()
        Elogbeta = dirichlet_expectation(_lambda)
        expElogbeta = np.exp(Elogbeta)

        gamma = self.state.gamma

        if author2doc is None and doc2author is None:
            # Evaluating on training documents (chunk of self.corpus).
            author2doc = self.author2doc
            doc2author = self.doc2author

            if not chunk_doc_idx:
                # If author2doc and doc2author are not provided, chunk is assumed to be a subset of
                # self.corpus, and chunk_doc_idx is thus required.
                raise ValueError(
                    'Either author dictionaries or chunk_doc_idx must be provided. '
                    'Consult documentation of bound method.'
                )
        elif author2doc is not None and doc2author is not None:
            # Training on held-out documents (documents not seen during training).
            # All authors in dictionaries must still be seen during training.
            for a in author2doc.keys():
                if not self.author2doc.get(a):
                    raise ValueError('bound cannot be called with authors not seen during training.')

            if chunk_doc_idx:
                raise ValueError(
                    'Either author dictionaries or chunk_doc_idx must be provided, not both. '
                    'Consult documentation of bound method.'
                )
        else:
            raise ValueError(
                'Either both author2doc and doc2author should be provided, or neither. '
                'Consult documentation of bound method.'
            )

        Elogtheta = dirichlet_expectation(gamma)
        expElogtheta = np.exp(Elogtheta)

        word_score = 0.0
        theta_score = 0.0
        for d, doc in enumerate(chunk):
            if chunk_doc_idx:
                doc_no = chunk_doc_idx[d]
            else:
                doc_no = d
            # Get all authors in current document, and convert the author names to integer IDs.
            authors_d = [self.author2id[a] for a in self.doc2author[doc_no]]
            ids = np.array([id for id, _ in doc])  # Word IDs in doc.
            cts = np.array([cnt for _, cnt in doc])  # Word counts.

            if d % self.chunksize == 0:
                logger.debug(""bound: at document #%i in chunk"", d)

            # Computing the bound requires summing over expElogtheta[a, k] * expElogbeta[k, v], which
            # is the same computation as in normalizing phi.
            phinorm = self.compute_phinorm(expElogtheta[authors_d, :], expElogbeta[:, ids])
            word_score += np.log(1.0 / len(authors_d)) * sum(cts) + cts.dot(np.log(phinorm))

        # Compensate likelihood for when `chunk` above is only a sample of the whole corpus. This ensures
        # that the likelihood is always roughly on the same scale.
        word_score *= subsample_ratio

        # E[log p(theta | alpha) - log q(theta | gamma)]
        for a in author2doc.keys():
            a = self.author2id[a]
            theta_score += np.sum((self.alpha - gamma[a, :]) * Elogtheta[a, :])
            theta_score += np.sum(gammaln(gamma[a, :]) - gammaln(self.alpha))
            theta_score += gammaln(np.sum(self.alpha)) - gammaln(np.sum(gamma[a, :]))

        # theta_score is rescaled in a similar fashion.
        # TODO: treat this in a more general way, similar to how it is done with word_score.
        theta_score *= self.num_authors / len(author2doc)

        # E[log p(beta | eta) - log q (beta | lambda)]
        beta_score = 0.0
        beta_score += np.sum((self.eta - _lambda) * Elogbeta)
        beta_score += np.sum(gammaln(_lambda) - gammaln(self.eta))
        sum_eta = np.sum(self.eta)
        beta_score += np.sum(gammaln(sum_eta) - gammaln(np.sum(_lambda, 1)))

        total_score = word_score + theta_score + beta_score

        return total_score","1. Use `np.asarray` to convert data to a numpy array.
2. Use `np.unique` to remove duplicate values from an array.
3. Use `np.argmax` to find the index of the maximum value in an array."
"    def _load_relations(self):
        """"""Load relations from the train data and build vocab.""""""
        vocab = {}
        index2word = []
        all_relations = []  # List of all relation pairs
        node_relations = defaultdict(set)  # Mapping from node index to its related node indices

        logger.info(""Loading relations from train data.."")
        for relation in self.train_data:
            if len(relation) != 2:
                raise ValueError('Relation pair ""%s"" should have exactly two items' % repr(relation))
            for item in relation:
                if item in vocab:
                    vocab[item].count += 1
                else:
                    vocab[item] = Vocab(count=1, index=len(index2word))
                    index2word.append(item)
            node_1, node_2 = relation
            node_1_index, node_2_index = vocab[node_1].index, vocab[node_2].index
            node_relations[node_1_index].add(node_2_index)
            relation = (node_1_index, node_2_index)
            all_relations.append(relation)
        logger.info(""Loaded %d relations from train data, %d nodes"", len(all_relations), len(vocab))
        self.kv.vocab = vocab
        self.kv.index2word = index2word
        self.indices_set = set((range(len(index2word))))  # Set of all node indices
        self.indices_array = np.array(range(len(index2word)))  # Numpy array of all node indices
        counts = np.array([self.kv.vocab[index2word[i]].count for i in range(len(index2word))], dtype=np.float64)
        self._node_probabilities = counts / counts.sum()
        self._node_probabilities_cumsum = np.cumsum(self._node_probabilities)
        self.all_relations = all_relations
        self.node_relations = node_relations
        self._negatives_buffer = NegativesBuffer([])  # Buffer for negative samples, to reduce calls to sampling method
        self._negatives_buffer_size = 2000","1. Use `assert` statements to validate the input arguments.
2. Use `defaultdict` to avoid KeyError exceptions.
3. Use `np.unique` to remove duplicate elements from a list."
"    def _get_candidate_negatives(self):
        """"""Returns candidate negatives of size `self.negative` from the negative examples buffer.

        Returns
        -------
        numpy.array
            Array of shape (`self.negative`,) containing indices of negative nodes.

        """"""

        if self._negatives_buffer.num_items() < self.negative:
            # Note: np.random.choice much slower than random.sample for large populations, possible bottleneck
            uniform_numbers = self._np_random.random_sample(self._negatives_buffer_size)
            cumsum_table_indices = np.searchsorted(self._node_probabilities_cumsum, uniform_numbers)
            self._negatives_buffer = NegativesBuffer(cumsum_table_indices)
        return self._negatives_buffer.get_items(self.negative)","1. Use `np.random.choice` instead of `np.random.sample` for large populations, as it is much faster.
2. Use a `NegativesBuffer` to store negative examples and only fetch the required number of items when needed, rather than generating all of them at once.
3. Use a secure random number generator, such as `np.random.default_rng()`, to avoid predictable results."
"    def save(self, *args, **kwargs):
        """"""Save complete model to disk, inherited from :class:`gensim.utils.SaveLoad`.""""""
        self._loss_grad = None  # Can't pickle autograd fn to disk
        super(PoincareModel, self).save(*args, **kwargs)", 
"    def load(cls, *args, **kwargs):
        """"""Load model from disk, inherited from :class:`~gensim.utils.SaveLoad`.""""""
        model = super(PoincareModel, cls).load(*args, **kwargs)
        return model","1. Use `torch.load()` instead of `gensim.utils.SaveLoad.load()` to load the model.
2. Pass `map_location='cpu'` to `torch.load()` to avoid loading the model to the GPU.
3. Check the model's version before loading it to make sure that it is compatible with the current version of PyTorch."
"    def fit(self, X, y=None):
        """"""
        Fit the model according to the given training data.
        Calls gensim.models.Doc2Vec
        """"""
        if isinstance(X[0], doc2vec.TaggedDocument):
            d2v_sentences = X
        else:
            d2v_sentences = [doc2vec.TaggedDocument(words, [i]) for i, words in enumerate(X)]
        self.gensim_model = models.Doc2Vec(
            documents=d2v_sentences, dm_mean=self.dm_mean, dm=self.dm,
            dbow_words=self.dbow_words, dm_concat=self.dm_concat, dm_tag_count=self.dm_tag_count,
            docvecs=self.docvecs, docvecs_mapfile=self.docvecs_mapfile, comment=self.comment,
            trim_rule=self.trim_rule, size=self.size, alpha=self.alpha, window=self.window,
            min_count=self.min_count, max_vocab_size=self.max_vocab_size, sample=self.sample,
            seed=self.seed, workers=self.workers, min_alpha=self.min_alpha, hs=self.hs,
            negative=self.negative, cbow_mean=self.cbow_mean, hashfxn=self.hashfxn,
            iter=self.iter, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words
        )
        return self","1. Use `secure_filename` to sanitize user-supplied filenames.
2. Use `os.makedirs` to create directories with the correct permissions.
3. Use `contextlib.closing` to ensure that file objects are closed properly."
"def load_old_doc2vec(*args, **kwargs):
    old_model = Doc2Vec.load(*args, **kwargs)
    params = {
        'dm_mean': old_model.__dict__.get('dm_mean', None),
        'dm': old_model.dm,
        'dbow_words': old_model.dbow_words,
        'dm_concat': old_model.dm_concat,
        'dm_tag_count': old_model.dm_tag_count,
        'docvecs': old_model.__dict__.get('docvecs', None),
        'docvecs_mapfile': old_model.__dict__.get('docvecs_mapfile', None),
        'comment': old_model.__dict__.get('comment', None),
        'size': old_model.vector_size,
        'alpha': old_model.alpha,
        'window': old_model.window,
        'min_count': old_model.min_count,
        'max_vocab_size': old_model.__dict__.get('max_vocab_size', None),
        'sample': old_model.sample,
        'seed': old_model.seed,
        'workers': old_model.workers,
        'min_alpha': old_model.min_alpha,
        'hs': old_model.hs,
        'negative': old_model.negative,
        'cbow_mean': old_model.cbow_mean,
        'hashfxn': old_model.hashfxn,
        'iter': old_model.iter,
        'sorted_vocab': old_model.sorted_vocab,
        'batch_words': old_model.batch_words,
        'compute_loss': old_model.__dict__.get('compute_loss', None)
    }
    new_model = NewDoc2Vec(**params)
    # set word2vec trainables attributes
    new_model.wv.vectors = old_model.wv.syn0
    if hasattr(old_model.wv, 'syn0norm'):
        new_model.docvecs.vectors_norm = old_model.wv.syn0norm
    if hasattr(old_model, 'syn1'):
        new_model.trainables.syn1 = old_model.syn1
    if hasattr(old_model, 'syn1neg'):
        new_model.trainables.syn1neg = old_model.syn1neg
    if hasattr(old_model, 'syn0_lockf'):
        new_model.trainables.vectors_lockf = old_model.syn0_lockf

    # set doc2vec trainables attributes
    new_model.docvecs.vectors_docs = old_model.docvecs.doctag_syn0
    if hasattr(old_model.docvecs, 'doctag_syn0norm'):
        new_model.docvecs.vectors_docs_norm = old_model.docvecs.doctag_syn0norm
    if hasattr(old_model.docvecs, 'doctag_syn0_lockf'):
        new_model.trainables.vectors_docs_lockf = old_model.docvecs.doctag_syn0_lockf
    if hasattr(old_model.docvecs, 'mapfile_path'):
        new_model.docvecs.mapfile_path = old_model.docvecs.mapfile_path

    # set word2vec vocabulary attributes
    new_model.wv.vocab = old_model.wv.vocab
    new_model.wv.index2word = old_model.wv.index2word
    new_model.vocabulary.cum_table = old_model.cum_table

    # set doc2vec vocabulary attributes
    new_model.docvecs.doctags = old_model.docvecs.doctags
    new_model.docvecs.max_rawint = old_model.docvecs.max_rawint
    new_model.docvecs.offset2doctag = old_model.docvecs.offset2doctag
    new_model.docvecs.count = old_model.docvecs.count

    new_model.train_count = old_model.train_count
    new_model.corpus_count = old_model.corpus_count
    new_model.running_training_loss = old_model.running_training_loss
    new_model.total_train_time = old_model.total_train_time
    new_model.min_alpha_yet_reached = old_model.min_alpha_yet_reached
    new_model.model_trimmed_post_training = old_model.model_trimmed_post_training

    return new_model","1. Use `secure_filename` to sanitize the file name before loading the model.
2. Use `pickle.load` with `protocol=4` to load the model.
3. Check the model's version before loading it."
"    def index_to_doctag(self, i_index):
        """"""Return string key for given i_index, if available. Otherwise return raw int doctag (same int).""""""
        candidate_offset = i_index - self.max_rawint - 1
        if 0 <= candidate_offset < len(self.offset2doctag):
            return self.ffset2doctag[candidate_offset]
        else:
            return i_index","1. Use `int.from_bytes()` to convert the byte string to an integer instead of using `int()` directly. This will prevent integer overflows.
2. Use `hashlib.sha256()` to generate a secure hash of the password instead of using `md5()`.
3. Use `os.urandom()` to generate a random salt instead of using a hard-coded value."
"    def get_similarities(self, query):
        """"""Get similarity between `query` and current index instance.

        Warnings
        --------
        Do not use this function directly; use the self[query] syntax instead.

        Parameters
        ----------
        query : {list of (int, number), iterable of list of (int, number), :class:`scipy.sparse.csr_matrix`
            Document or collection of documents.

        Return
        ------
        :class:`numpy.ndarray`
            Similarity matrix.

        """"""
        if isinstance(query, numpy.ndarray):
            # Convert document indexes to actual documents.
            query = [self.corpus[i] for i in query]

        if not query or not isinstance(query[0], list):
            query = [query]

        n_queries = len(query)
        result = []
        for qidx in range(n_queries):
            # Compute similarity for each query.
            qresult = [matutils.softcossim(document, query[qidx], self.similarity_matrix)
                       for document in self.corpus]
            qresult = numpy.array(qresult)

            # Append single query result to list of all results.
            result.append(qresult)

        if len(result) == 1:
            # Only one query.
            result = result[0]
        else:
            result = numpy.array(result)

        return result","1. Use `type()` to check if `query` is a numpy array, and if so, convert it to a list of documents.
2. Check if `query` is a list of lists, and if not, convert it to a list of lists.
3. Use `numpy.array()` to convert the results of `softcossim()` to a numpy array."
"    def get_similarities(self, query):
        """"""Get similarity between `query` and current index instance.

        Warnings
        --------
        Do not use this function directly; use the self[query] syntax instead.

        Parameters
        ----------
        query : {list of (int, number), iterable of list of (int, number), :class:`scipy.sparse.csr_matrix`
            Document or collection of documents.

        Return
        ------
        :class:`numpy.ndarray`
            Similarity matrix.

        """"""
        if isinstance(query, numpy.ndarray):
            # Convert document indexes to actual documents.
            query = [self.corpus[i] for i in query]

        if not query or not isinstance(query[0], list):
            query = [query]

        n_queries = len(query)
        result = []
        for qidx in range(n_queries):
            # Compute similarity for each query.
            qresult = [self.w2v_model.wmdistance(document, query[qidx]) for document in self.corpus]
            qresult = numpy.array(qresult)
            qresult = 1. / (1. + qresult)  # Similarity is the negative of the distance.

            # Append single query result to list of all results.
            result.append(qresult)

        if len(result) == 1:
            # Only one query.
            result = result[0]
        else:
            result = numpy.array(result)

        return result","1. Use `type()` to check if `query` is a numpy array, and if so, convert it to a list of documents.
2. Check if `query` is a list of lists, and if not, wrap it in a list.
3. Use `numpy.array()` to convert the results of `self.w2v_model.wmdistance()` to a numpy array."
"def summarize_corpus(corpus, ratio=0.2):
    """"""
    Returns a list of the most important documents of a corpus using a
    variation of the TextRank algorithm.
    The input must have at least INPUT_MIN_LENGTH (%d) documents for the
    summary to make sense.

    The length of the output can be specified using the ratio parameter,
    which determines how many documents will be chosen for the summary
    (defaults at 20%% of the number of documents of the corpus).

    The most important documents are returned as a list sorted by the
    document score, highest first.
    """""" % INPUT_MIN_LENGTH
    hashable_corpus = _build_hasheable_corpus(corpus)

    # If the corpus is empty, the function ends.
    if len(corpus) == 0:
        logger.warning(""Input corpus is empty."")
        return

    # Warns the user if there are too few documents.
    if len(corpus) < INPUT_MIN_LENGTH:
        logger.warning(""Input corpus is expected to have at least %d documents."", INPUT_MIN_LENGTH)

    graph = _build_graph(hashable_corpus)
    _set_graph_edge_weights(graph)
    _remove_unreachable_nodes(graph)

    # Cannot calculate eigenvectors if number of unique words in text < 3. Warns user to add more text. The function ends.
    if len(graph.nodes()) < 3:
        logger.warning(""Please add more sentences to the text. The number of reachable nodes is below 3"")
        return

    pagerank_scores = _pagerank(graph)

    hashable_corpus.sort(key=lambda doc: pagerank_scores.get(doc, 0), reverse=True)

    return [list(doc) for doc in hashable_corpus[:int(len(corpus) * ratio)]]","1. Use `hashlib.sha256()` to generate a secure hash of the document contents.
2. Use `cryptography.fernet.Fernet()` to encrypt the document contents.
3. Use `os.chmod()` to set the permissions of the encrypted document to `0400` (read-only for owner)."
"def summarize(text, ratio=0.2, word_count=None, split=False):
    """"""
    Returns a summarized version of the given text using a variation of
    the TextRank algorithm.
    The input must be longer than INPUT_MIN_LENGTH sentences for the
    summary to make sense and must be given as a string.

    The output summary will consist of the most representative sentences
    and will also be returned as a string, divided by newlines. If the
    split parameter is set to True, a list of sentences will be
    returned.

    The length of the output can be specified using the ratio and
    word_count parameters:

        ratio should be a number between 0 and 1 that determines the
        percentage of the number of sentences of the original text to be
        chosen for the summary (defaults at 0.2).
        word_count determines how many words will the output contain.
        If both parameters are provided, the ratio will be ignored.

    """"""
    # Gets a list of processed sentences.
    sentences = _clean_text_by_sentences(text)

    # If no sentence could be identified, the function ends.
    if len(sentences) == 0:
        logger.warning(""Input text is empty."")
        return

    # If only one sentence is present, the function raises an error (Avoids ZeroDivisionError).
    if len(sentences) == 1:
        raise ValueError(""input must have more than one sentence"")

    # Warns if the text is too short.
    if len(sentences) < INPUT_MIN_LENGTH:
        logger.warning(""Input text is expected to have at least %d sentences."", INPUT_MIN_LENGTH)

    corpus = _build_corpus(sentences)

    most_important_docs = summarize_corpus(corpus, ratio=ratio if word_count is None else 1)

    # Extracts the most important sentences with the selected criterion.
    extracted_sentences = _extract_important_sentences(sentences, corpus, most_important_docs, word_count)

    # Sorts the extracted sentences by apparition order in the original text.
    extracted_sentences.sort(key=lambda s: s.index)

    return _format_results(extracted_sentences, split)","1. Use input validation to ensure that the text is a string and that it is long enough.
2. Sanitize the text to remove any malicious content.
3. Use a secure algorithm to generate the summary."
"    def __init__(self, *args):
        super(WordOccurrenceAccumulator, self).__init__(*args)
        self._occurrences = np.zeros(self._vocab_size, dtype='uint32')
        self._co_occurrences = sps.lil_matrix((self._vocab_size, self._vocab_size), dtype='uint32')

        self._uniq_words = np.zeros((self._vocab_size + 1,), dtype=bool)  # add 1 for none token
        self._mask = self._uniq_words[:-1]  # to exclude none token
        self._counter = Counter()","1. Use `np.full` instead of `np.zeros` to initialize the `_occurrences` and `_co_occurrences` arrays. This will prevent attackers from injecting arbitrary values into these arrays.
2. Use `np.unique` to filter out the `none` token from the `_uniq_words` array. This will prevent attackers from using the `none` token to bypass security checks.
3. Use `Counter.most_common` to get the most common words instead of iterating over the `_counter` dictionary. This will prevent attackers from enumerating all of the words in the vocabulary."
"    def analyze_text(self, window, doc_num=None):
        self._slide_window(window, doc_num)
        if self._mask.any():
            self._occurrences[self._mask] += 1
            self._counter.update(itertools.combinations(np.nonzero(self._mask)[0], 2))","1. Use `np.unique` to find unique elements in `self._mask` instead of iterating over `itertools.combinations`. This will prevent an attacker from enumerating all pairs of elements in `self._mask`.
2. Use `np.bitwise_and` to check if an element is in `self._mask` instead of using `np.any`. This will prevent an attacker from tricking the code into thinking that an element is in `self._mask` when it is not.
3. Use `np.random.choice` to select elements from `self._mask` instead of using `np.nonzero`. This will prevent an attacker from predicting which elements will be selected from `self._mask`."
"    def _symmetrize(self):
        """"""Word pairs may have been encountered in (i, j) and (j, i) order.
        Rather than enforcing a particular ordering during the update process,
        we choose to symmetrize the co-occurrence matrix after accumulation has completed.
        """"""
        co_occ = self._co_occurrences
        co_occ.setdiag(self._occurrences)  # diagonal should be equal to occurrence counts
        self._co_occurrences = co_occ + co_occ.T - sps.diags(co_occ.diagonal(), dtype='uint32')","1. Use `np.tril()` to symmetrize the co-occurrence matrix instead of manually adding the diagonal and subtracting it. This will prevent data from being leaked in the case of a row or column access violation.
2. Use `np.ascontiguousarray()` to ensure that the co-occurrence matrix is stored in a contiguous memory block. This will improve performance and prevent data from being corrupted in the case of a buffer overflow.
3. Use `np.memmap()` to create a memory-mapped file for the co-occurrence matrix. This will prevent the matrix from being overwritten by other processes and ensure that it is always accessible."
"    def __init__(self, model=None, topics=None, texts=None, corpus=None, dictionary=None,
                 window_size=None, coherence='c_v', topn=10, processes=-1):
        """"""
        Args:
        ----
        model : Pre-trained topic model. Should be provided if topics is not provided.
                Currently supports LdaModel, LdaMallet wrapper and LdaVowpalWabbit wrapper. Use 'topics'
                parameter to plug in an as yet unsupported model.
        topics : List of tokenized topics. If this is preferred over model, dictionary should be provided. eg::
                 topics = [['human', 'machine', 'computer', 'interface'],
                               ['graph', 'trees', 'binary', 'widths']]
        texts : Tokenized texts. Needed for coherence models that use sliding window based probability estimator, eg::
                texts = [['system', 'human', 'system', 'eps'],
                             ['user', 'response', 'time'],
                             ['trees'],
                             ['graph', 'trees'],
                             ['graph', 'minors', 'trees'],
                             ['graph', 'minors', 'survey']]
        corpus : Gensim document corpus.
        dictionary : Gensim dictionary mapping of id word to create corpus. If model.id2word is present,
                     this is not needed. If both are provided, dictionary will be used.
        window_size : Is the size of the window to be used for coherence measures using boolean sliding window as their
                      probability estimator. For 'u_mass' this doesn't matter.
                      If left 'None' the default window sizes are used which are:
                      'c_v' : 110
                      'c_uci' : 10
                      'c_npmi' : 10
        coherence : Coherence measure to be used. Supported values are:
                    'u_mass'
                    'c_v'
                    'c_uci' also popularly known as c_pmi
                    'c_npmi'
                    For 'u_mass' corpus should be provided. If texts is provided, it will be converted
                    to corpus using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' texts should be provided.
                    Corpus is not needed.
        topn : Integer corresponding to the number of top words to be extracted from each topic.
        processes : number of processes to use for probability estimation phase; any value less than 1 will be
                    interpreted to mean num_cpus - 1; default is -1.
        """"""
        if model is None and topics is None:
            raise ValueError(""One of model or topics has to be provided."")
        elif topics is not None and dictionary is None:
            raise ValueError(""dictionary has to be provided if topics are to be used."")

        if texts is None and corpus is None:
            raise ValueError(""One of texts or corpus has to be provided."")

        # Check if associated dictionary is provided.
        if dictionary is None:
            if isinstance(model.id2word, FakeDict):
                raise ValueError(
                    ""The associated dictionary should be provided with the corpus or 'id2word'""
                    "" for topic model should be set as the associated dictionary."")
            else:
                self.dictionary = model.id2word
        else:
            self.dictionary = dictionary

        # Check for correct inputs for u_mass coherence measure.
        self.coherence = coherence
        if coherence in boolean_document_based:
            if is_corpus(corpus)[0]:
                self.corpus = corpus
            elif texts is not None:
                self.texts = texts
                self.corpus = [self.dictionary.doc2bow(text) for text in self.texts]
            else:
                raise ValueError(
                    ""Either 'corpus' with 'dictionary' or 'texts' should ""
                    ""be provided for %s coherence."", coherence)

        # Check for correct inputs for c_v coherence measure.
        elif coherence in sliding_window_based:
            self.window_size = window_size
            if self.window_size is None:
                self.window_size = SLIDING_WINDOW_SIZES[self.coherence]
            if texts is None:
                raise ValueError(""'texts' should be provided for %s coherence."", coherence)
            else:
                self.texts = texts
        else:
            raise ValueError(""%s coherence is not currently supported."", coherence)

        self.topn = topn
        self._model = model
        self._accumulator = None
        self._topics = None
        self.topics = topics

        self.processes = processes if processes > 1 else max(1, mp.cpu_count() - 1)","1. Use `isinstance()` to check if the input is of the correct type.
2. Use `raise ValueError()` to raise an error if the input is invalid.
3. Use `mp.cpu_count()` to get the number of CPUs on the system and use that to set the number of processes."
"    def evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True,
                            dummy4unknown=False):
        """"""
        Compute correlation of the model with human similarity judgments. `pairs` is a filename of a dataset where
        lines are 3-tuples, each consisting of a word pair and a similarity value, separated by `delimiter'.
        An example dataset is included in Gensim (test/test_data/wordsim353.tsv). More datasets can be found at
        http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html or https://www.cl.cam.ac.uk/~fh295/simlex.html.

        The model is evaluated using Pearson correlation coefficient and Spearman rank-order correlation coefficient
        between the similarities from the dataset and the similarities produced by the model itself.
        The results are printed to log and returned as a triple (pearson, spearman, ratio of pairs with unknown words).

        Use `restrict_vocab` to ignore all word pairs containing a word not in the first `restrict_vocab`
        words (default 300,000). This may be meaningful if you've sorted the vocabulary by descending frequency.
        If `case_insensitive` is True, the first `restrict_vocab` words are taken, and then case normalization
        is performed.

        Use `case_insensitive` to convert all words in the pairs and vocab to their uppercase form before
        evaluating the model (default True). Useful when you expect case-mismatch between training tokens
        and words pairs in the dataset. If there are multiple case variants of a single word, the vector for the first
        occurrence (also the most frequent if vocabulary is sorted) is taken.

        Use `dummy4unknown=True' to produce zero-valued similarities for pairs with out-of-vocabulary words.
        Otherwise (default False), these pairs are skipped entirely.
        """"""
        ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
        ok_vocab = dict((w.upper(), v) for w, v in reversed(ok_vocab)) if case_insensitive else dict(ok_vocab)

        similarity_gold = []
        similarity_model = []
        oov = 0

        original_vocab = self.vocab
        self.vocab = ok_vocab

        for line_no, line in enumerate(utils.smart_open(pairs)):
            line = utils.to_unicode(line)
            if line.startswith('#'):
                # May be a comment
                continue
            else:
                try:
                    if case_insensitive:
                        a, b, sim = [word.upper() for word in line.split(delimiter)]
                    else:
                        a, b, sim = [word for word in line.split(delimiter)]
                    sim = float(sim)
                except:
                    logger.info('skipping invalid line #%d in %s', line_no, pairs)
                    continue
                if a not in ok_vocab or b not in ok_vocab:
                    oov += 1
                    if dummy4unknown:
                        similarity_model.append(0.0)
                        similarity_gold.append(sim)
                        continue
                    else:
                        logger.debug('skipping line #%d with OOV words: %s', line_no, line.strip())
                        continue
                similarity_gold.append(sim)  # Similarity from the dataset
                similarity_model.append(self.similarity(a, b))  # Similarity from the model
        self.vocab = original_vocab
        spearman = stats.spearmanr(similarity_gold, similarity_model)
        pearson = stats.pearsonr(similarity_gold, similarity_model)
        oov_ratio = float(oov) / (len(similarity_gold) + oov) * 100

        logger.debug(
            'Pearson correlation coefficient against %s: %f with p-value %f',
            pairs, pearson[0], pearson[1]
        )
        logger.debug(
            'Spearman rank-order correlation coefficient against %s: %f with p-value %f',
            pairs, spearman[0], spearman[1]
        )
        logger.debug('Pairs with unknown words: %d' % oov)
        self.log_evaluate_word_pairs(pearson, spearman, oov_ratio, pairs)
        return pearson, spearman, oov_ratio","1. Use `assert` statements to validate the input parameters.
2. Use `try` and `except` blocks to handle errors.
3. Use `logging` to log all the important events."
"    def diff(self, other, distance=""kullback_leibler"", num_words=100, n_ann_terms=10, normed=True):
        """"""
        Calculate difference topic2topic between two Lda models
        `other` instances of `LdaMulticore` or `LdaModel`
        `distance` is function that will be applied to calculate difference between any topic pair.
        Available values: `kullback_leibler`, `hellinger` and `jaccard`
        `num_words` is quantity of most relevant words that used if distance == `jaccard` (also used for annotation)
        `n_ann_terms` is max quantity of words in intersection/symmetric difference between topics (used for annotation)
        Returns a matrix Z with shape (m1.num_topics, m2.num_topics), where Z[i][j] - difference between topic_i and topic_j
        and matrix annotation with shape (m1.num_topics, m2.num_topics, 2, None),
        where
            annotation[i][j] = [[`int_1`, `int_2`, ...], [`diff_1`, `diff_2`, ...]] and
            `int_k` is word from intersection of `topic_i` and `topic_j` and
            `diff_l` is word from symmetric difference of `topic_i` and `topic_j`
        `normed` is a flag. If `true`, matrix Z will be normalized
        Example:
        >>> m1, m2 = LdaMulticore.load(path_1), LdaMulticore.load(path_2)
        >>> mdiff, annotation = m1.diff(m2)
        >>> print(mdiff) # get matrix with difference for each topic pair from `m1` and `m2`
        >>> print(annotation) # get array with positive/negative words for each topic pair from `m1` and `m2`
        """"""

        distances = {
            ""kullback_leibler"": kullback_leibler,
            ""hellinger"": hellinger,
            ""jaccard"": jaccard_distance,
        }

        if distance not in distances:
            valid_keys = "", "".join(""`{}`"".format(x) for x in distances.keys())
            raise ValueError(""Incorrect distance, valid only {}"".format(valid_keys))

        if not isinstance(other, self.__class__):
            raise ValueError(""The parameter `other` must be of type `{}`"".format(self.__name__))

        distance_func = distances[distance]
        d1, d2 = self.state.get_lambda(), other.state.get_lambda()
        t1_size, t2_size = d1.shape[0], d2.shape[0]

        fst_topics = [{w for (w, _) in self.show_topic(topic, topn=num_words)} for topic in xrange(t1_size)]
        snd_topics = [{w for (w, _) in other.show_topic(topic, topn=num_words)} for topic in xrange(t2_size)]

        if distance == ""jaccard"":
            d1, d2 = fst_topics, snd_topics

        z = np.zeros((t1_size, t2_size))
        for topic1 in range(t1_size):
            for topic2 in range(t2_size):
                z[topic1][topic2] = distance_func(d1[topic1], d2[topic2])

        if normed:
            if np.abs(np.max(z)) > 1e-8:
                z /= np.max(z)

        annotation = [[None] * t1_size for _ in range(t2_size)]

        for topic1 in range(t1_size):
            for topic2 in range(t2_size):
                pos_tokens = fst_topics[topic1] & snd_topics[topic2]
                neg_tokens = fst_topics[topic1].symmetric_difference(snd_topics[topic2])

                pos_tokens = sample(pos_tokens, min(len(pos_tokens), n_ann_terms))
                neg_tokens = sample(neg_tokens, min(len(neg_tokens), n_ann_terms))

                annotation[topic1][topic2] = [pos_tokens, neg_tokens]

        return z, annotation","1. Use `np.nan` instead of `np.inf` to avoid overflows.
2. Use `np.argsort()` instead of `np.argmax()` to avoid sorting by the first element of each row.
3. Use `np.unique()` to remove duplicate words from the annotations."
"    def fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize):
        """"""
        fit an lda sequence model:

        for each time period
            set up lda model with E[log p(w|z)] and \\alpha
            for each document
                perform posterior inference
                update sufficient statistics/likelihood

        maximize topics

       """"""
        LDASQE_EM_THRESHOLD = 1e-4
        # if bound is low, then we increase iterations.
        LOWER_ITER = 10
        ITER_MULT_LOW = 2
        MAX_ITER = 500

        num_topics = self.num_topics
        vocab_len = self.vocab_len
        data_len = self.num_time_slices
        corpus_len = self.corpus_len

        bound = 0
        convergence = LDASQE_EM_THRESHOLD + 1
        iter_ = 0

        while iter_ < em_min_iter or ((convergence > LDASQE_EM_THRESHOLD) and iter_ <= em_max_iter):

            logger.info("" EM iter %i"", iter_)
            logger.info(""E Step"")
            # TODO: bound is initialized to 0
            old_bound = bound

            # initiate sufficient statistics
            topic_suffstats = []
            for topic in range(0, num_topics):
                topic_suffstats.append(np.resize(np.zeros(vocab_len * data_len), (vocab_len, data_len)))

            # set up variables
            gammas = np.resize(np.zeros(corpus_len * num_topics), (corpus_len, num_topics))
            lhoods = np.resize(np.zeros(corpus_len * num_topics + 1), (corpus_len, num_topics + 1))
            # compute the likelihood of a sequential corpus under an LDA
            # seq model and find the evidence lower bound. This is the E - Step
            bound, gammas = self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
            self.gammas = gammas

            logger.info(""M Step"")

            # fit the variational distribution. This is the M - Step
            topic_bound = self.fit_lda_seq_topics(topic_suffstats)
            bound += topic_bound

            if ((bound - old_bound) < 0):
                # if max_iter is too low, increase iterations.
                if lda_inference_max_iter < LOWER_ITER:
                    lda_inference_max_iter *= ITER_MULT_LOW
                logger.info(""Bound went down, increasing iterations to %i"", lda_inference_max_iter)

            # check for convergence
            convergence = np.fabs((bound - old_bound) / old_bound)

            if convergence < LDASQE_EM_THRESHOLD:

                lda_inference_max_iter = MAX_ITER
                logger.info(""Starting final iterations, max iter is %i"", lda_inference_max_iter)
                convergence = 1.0

            logger.info(""iteration %i iteration lda seq bound is %f convergence is %f"", iter_, bound, convergence)

            iter_ += 1

        return bound","1. Use `np.random.seed()` to set a random seed to ensure that the results are reproducible.
2. Use `np.nan_to_num()` to convert `NaN` values to `0` to avoid errors.
3. Use `np.inf_to_num()` to convert `Inf` values to `1` to avoid errors."
"    def compute_post_variance(self, word, chain_variance):
        """"""
        Based on the Variational Kalman Filtering approach for Approximate Inference [https://www.cs.princeton.edu/~blei/papers/BleiLafferty2006a.pdf]
        This function accepts the word to compute variance for, along with the associated sslm class object, and returns variance and fwd_variance
        Computes Var[\\beta_{t,w}] for t = 1:T

        Fwd_Variance(t) ≡ E((beta_{t,w} − mean_{t,w})^2 |beta_{t} for 1:t)
        = (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance ) * (fwd_variance[t - 1] + obs_variance)

        Variance(t) ≡ E((beta_{t,w} − mean_cap{t,w})^2 |beta_cap{t} for 1:t)
        = fwd_variance[t - 1] + (fwd_variance[t - 1] / fwd_variance[t - 1] + obs_variance)^2 * (variance[t - 1] - (fwd_variance[t-1] + obs_variance))

        """"""
        INIT_VARIANCE_CONST = 1000

        T = self.num_time_slices
        variance = self.variance[word]
        fwd_variance = self.fwd_variance[word]
        # forward pass. Set initial variance very high
        fwd_variance[0] = chain_variance * INIT_VARIANCE_CONST
        for t in range(1, T + 1):
            if self.obs_variance:
                c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)
            else:
                c = 0
            fwd_variance[t] = c * (fwd_variance[t - 1] + chain_variance)

        # backward pass
        variance[T] = fwd_variance[T]
        for t in range(T - 1, -1, -1):
            if fwd_variance[t] > 0.0:
                c = np.power((fwd_variance[t] / (fwd_variance[t] + chain_variance)), 2)
            else:
                c  = 0
            variance[t] = (c * (variance[t + 1] - chain_variance)) + ((1 - c) * fwd_variance[t])

        return variance, fwd_variance","1. Use `np.nan` instead of `0` to represent missing values.
2. Use `np.inf` instead of `1000` to represent very large values.
3. Use `np.power` instead of `**` to calculate powers."
"def malletmodel2ldamodel(mallet_model, gamma_threshold=0.001, iterations=50):
    """"""
    Function to convert mallet model to gensim LdaModel. This works by copying the
    training model weights (alpha, beta...) from a trained mallet model into the
    gensim model.

    Args:
    ----
    mallet_model : Trained mallet model
    gamma_threshold : To be used for inference in the new LdaModel.
    iterations : number of iterations to be used for inference in the new LdaModel.

    Returns:
    -------
    model_gensim : LdaModel instance; copied gensim LdaModel
    """"""
    model_gensim = LdaModel(
        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,
        alpha=mallet_model.alpha, iterations=iterations,
        gamma_threshold=gamma_threshold)
    model_gensim.expElogbeta[:] = mallet_model.wordtopics
    return model_gensim","1. Use `secure_filename` to sanitize the filenames passed to `mallet_model.load()`.
2. Use `gensim.corpora.MmCorpus` instead of `mallet_model.wordtopics` to avoid exposing the model's internal data structures.
3. Set `mallet_model.save()` to `False` to prevent the model from being saved to disk."
"def vwmodel2ldamodel(vw_model, iterations=50):
    """"""
    Function to convert vowpal wabbit model to gensim LdaModel. This works by
    simply copying the training model weights (alpha, beta...) from a trained
    vwmodel into the gensim model.

    Args:
    ----
    vw_model : Trained vowpal wabbit model.
    iterations : Number of iterations to be used for inference of the new LdaModel.

    Returns:
    -------
    model_gensim : LdaModel instance; copied gensim LdaModel.
    """"""
    model_gensim = LdaModel(
        num_topics=vw_model.num_topics, id2word=vw_model.id2word, chunksize=vw_model.chunksize,
        passes=vw_model.passes, alpha=vw_model.alpha, eta=vw_model.eta, decay=vw_model.decay,
        offset=vw_model.offset, iterations=iterations, gamma_threshold=vw_model.gamma_threshold)
    model_gensim.expElogbeta[:] = vw_model._get_topics()
    return model_gensim","1. Use `secure_filename` to sanitize the file name before saving it.
2. Use `os.chmod` to set the permissions of the saved file to `0644`.
3. Use `contextlib.closing` to ensure that the file is closed after it is used."
"    def train(cls, wr_path, corpus_file, out_name, size=100, window=15, symmetric=1, min_count=5, max_vocab_size=0,
              sgd_num=100, lrate=0.001, period=10, iter=90, epsilon=0.75, dump_period=10, reg=0, alpha=100,
              beta=99, loss='hinge', memory=4.0, cleanup_files=False, sorted_vocab=1, ensemble=0):
        """"""
        The word and context embedding files are generated by wordrank binary and are saved in ""out_name"" directory
        which is created inside wordrank directory. The vocab and cooccurence files are generated using glove code
        available inside the wordrank directory. These files are used by the wordrank binary for training.

        `wr_path` is the path to the Wordrank directory.
        `corpus_file` is the filename of the text file to be used for training the Wordrank model.
        Expects file to contain space-separated tokens in a single line
        `out_name` is name of the directory which will be created (in wordrank folder) to save embeddings and training data.
        It will contain following contents:
            Word Embeddings saved after every dump_period and stored in a file model_word_""current iter"".txt
            Context Embeddings saved after every dump_period and stored in a file model_context_""current iter"".txt
            A meta directory which contain: 'vocab.txt' - vocab words, 'wiki.toy' - word-word coccurence values, 'meta' - vocab and coccurence lengths
        `size` is the dimensionality of the feature vectors.
        `window` is the number of context words to the left (and to the right, if symmetric = 1).
        `symmetric` if 0, only use left context words, else use left and right both.
        `min_count` = ignore all words with total frequency lower than this.
        `max_vocab_size` upper bound on vocabulary size, i.e. keep the <int> most frequent words. Default is 0 for no limit.
        `sgd_num` number of SGD taken for each data point.
        `lrate` is the learning rate (too high diverges, give Nan).
        `period` is the period of xi variable updates
        `iter` = number of iterations (epochs) over the corpus.
        `epsilon` is the power scaling value for weighting function.
        `dump_period` is the period after which embeddings should be dumped.
        `reg` is the value of regularization parameter.
        `alpha` is the alpha parameter of gamma distribution.
        `beta` is the beta parameter of gamma distribution.
        `loss` = name of the loss (logistic, hinge).
        `memory` = soft limit for memory consumption, in GB.
        `cleanup_files` if True, delete directory and files used by this wrapper, setting to False can be useful for debugging
        `sorted_vocab` = if 1 (default), sort the vocabulary by descending frequency before assigning word indexes.
        `ensemble` = 0 (default), use ensemble of word and context vectors
        """"""

        meta_data_path = 'matrix.meta'
        vocab_file = 'vocab.txt'
        temp_vocab_file = 'tempvocab.txt'
        cooccurrence_file = 'cooccurrence'
        cooccurrence_shuf_file = 'wiki.toy'
        meta_file = 'meta'

        # prepare training data (cooccurrence matrix and vocab)
        model_dir = os.path.join(wr_path, out_name)
        meta_dir = os.path.join(model_dir, 'meta')
        os.makedirs(meta_dir)
        logger.info(""Dumped data will be stored in '%s'"", model_dir)
        copyfile(corpus_file, os.path.join(meta_dir, corpus_file.split('/')[-1]))
        os.chdir(meta_dir)

        cmd_vocab_count = ['../../glove/vocab_count', '-min-count', str(min_count), '-max-vocab', str(max_vocab_size)]
        cmd_cooccurence_count = ['../../glove/cooccur', '-memory', str(memory), '-vocab-file', temp_vocab_file, '-window-size', str(window), '-symmetric', str(symmetric)]
        cmd_shuffle_cooccurences = ['../../glove/shuffle', '-memory', str(memory)]
        cmd_del_vocab_freq = ['cut', '-d', "" "", '-f', '1', temp_vocab_file]

        commands = [cmd_vocab_count, cmd_cooccurence_count, cmd_shuffle_cooccurences]
        input_fnames = [corpus_file.split('/')[-1], corpus_file.split('/')[-1], cooccurrence_file]
        output_fnames = [temp_vocab_file, cooccurrence_file, cooccurrence_shuf_file]

        logger.info(""Prepare training data (%s) using glove code"", "", "".join(input_fnames))
        for command, input_fname, output_fname in zip(commands, input_fnames, output_fnames):
            with smart_open(input_fname, 'rb') as r:
                with smart_open(output_fname, 'wb') as w:
                    utils.check_output(w, args=command, stdin=r)

        logger.info(""Deleting frequencies from vocab file"")
        with smart_open(vocab_file, 'wb') as w:
            utils.check_output(w, args=cmd_del_vocab_freq)

        with smart_open(vocab_file, 'rb') as f:
            numwords = sum(1 for line in f)
        with smart_open(cooccurrence_shuf_file, 'rb') as f:
            numlines = sum(1 for line in f)
        with smart_open(meta_file, 'wb') as f:
            meta_info = ""{0} {1}\\n{2} {3}\\n{4} {5}"".format(numwords, numwords, numlines, cooccurrence_shuf_file, numwords, vocab_file)
            f.write(meta_info.encode('utf-8'))
            
        if iter % dump_period == 0:
            iter += 1
        else:
            logger.warning(
                'Resultant embedding will be from %d iterations rather than the input %d iterations, '
                'as wordrank dumps the embedding only at dump_period intervals. '
                'Input an appropriate combination of parameters (iter, dump_period) such that '
                '""iter mod dump_period"" is zero.', iter - (iter % dump_period), iter
                )

        wr_args = {
            'path': 'meta',
            'nthread': multiprocessing.cpu_count(),
            'sgd_num': sgd_num,
            'lrate': lrate,
            'period': period,
            'iter': iter,
            'epsilon': epsilon,
            'dump_prefix': 'model',
            'dump_period': dump_period,
            'dim': size,
            'reg': reg,
            'alpha': alpha,
            'beta': beta,
            'loss': loss
        }

        os.chdir('..')
        # run wordrank executable with wr_args
        cmd = ['mpirun', '-np', '1', '../wordrank']
        for option, value in wr_args.items():
            cmd.append('--%s' % option)
            cmd.append(str(value))
        logger.info(""Running wordrank binary"")
        output = utils.check_output(args=cmd)

        # use embeddings from max. iteration's dump
        max_iter_dump = iter - (iter % dump_period)
        copyfile('model_word_%d.txt' % max_iter_dump, 'wordrank.words')
        copyfile('model_context_%d.txt' % max_iter_dump, 'wordrank.contexts')
        model = cls.load_wordrank_model('wordrank.words', os.path.join('meta', vocab_file), 'wordrank.contexts', sorted_vocab, ensemble)
        os.chdir('../..')

        if cleanup_files:
            rmtree(model_dir)
        return model","1. Use `subprocess.check_output` instead of `os.system` to avoid leaking sensitive information to the environment.
2. Use `shutil.rmtree` instead of `os.remove` to recursively delete directories.
3. Use `logging.warning` instead of `print` to log warnings."
"def glove2word2vec(glove_input_file, word2vec_output_file):
    """"""Convert `glove_input_file` in GloVe format into `word2vec_output_file in word2vec format.""""""
    num_lines, num_dims = get_glove_info(glove_input_file)
    logger.info(""converting %i vectors from %s to %s"", num_lines, glove_input_file, word2vec_output_file)
    with smart_open(word2vec_output_file, 'wb') as fout:
        fout.write(""{0} {1}\\n"".format(num_lines, num_dims).encode('utf-8'))
        with smart_open(glove_input_file, 'rb') as fin:
            for line in fin:
                fout.write(line)
    return num_lines, num_dims","1. Use `os.fchmod` to set the file mode to 0644 instead of relying on the default mode of 0666.
2. Use `os.fchown` to set the file owner and group to root instead of relying on the default owner and group of the current user.
3. Use `os.umask` to set the file creation mask to 0022 instead of relying on the default mask of 022."
"def arithmetic_mean(confirmed_measures):
    """"""
    This functoin performs the arithmetic mean aggregation on the output obtained from
    the confirmation measure module.

    Args:
    ----
    confirmed_measures : list of calculated confirmation measure on each set in the segmented topics.

    Returns:
    -------
    mean : Arithmetic mean of all the values contained in confirmation measures.
    """"""
    return np.mean(confirmed_measures)","1. Use `np.array` instead of `list` to avoid potential security issues.
2. Use `np.clip` to sanitize the input values to prevent overflow.
3. Use `np.isfinite` to check for NaN values and raise an exception if found."
"def log_conditional_probability(segmented_topics, accumulator):
    """"""
    This function calculates the log-conditional-probability measure
    which is used by coherence measures such as U_mass.
    This is defined as: m_lc(S_i) = log[(P(W', W*) + e) / P(W*)]

    Args:
    ----
    segmented_topics : Output from the segmentation module of the segmented topics.
                       Is a list of list of tuples.
    accumulator: word occurrence accumulator from probability_estimation.

    Returns:
    -------
    m_lc : List of log conditional probability measure for each topic.
    """"""
    m_lc = []
    num_docs = float(accumulator.num_docs)
    for s_i in segmented_topics:
        segment_sims = []
        for w_prime, w_star in s_i:
            try:
                w_star_count = accumulator[w_star]
                co_occur_count = accumulator[w_prime, w_star]
                m_lc_i = np.log(((co_occur_count / num_docs) + EPSILON) / (w_star_count / num_docs))
            except KeyError:
                m_lc_i = 0.0

            segment_sims.append(m_lc_i)
        m_lc.append(np.mean(segment_sims))

    return m_lc","1. Use `try-except` blocks to handle errors when accessing the accumulator dictionary.
2. Use `np.log` instead of `math.log` to avoid floating-point underflow.
3. Check the input arguments to ensure that they are valid before using them."
"def log_ratio_measure(segmented_topics, accumulator, normalize=False):
    """"""
    If normalize=False:
        Popularly known as PMI.
        This function calculates the log-ratio-measure which is used by
        coherence measures such as c_v.
        This is defined as: m_lr(S_i) = log[(P(W', W*) + e) / (P(W') * P(W*))]

    If normalize=True:
        This function calculates the normalized-log-ratio-measure, popularly knowns as
        NPMI which is used by coherence measures such as c_v.
        This is defined as: m_nlr(S_i) = m_lr(S_i) / -log[P(W', W*) + e]

    Args:
    ----
    segmented topics : Output from the segmentation module of the segmented topics.
                       Is a list of list of tuples.
    accumulator: word occurrence accumulator from probability_estimation.

    Returns:
    -------
    m_lr : List of log ratio measures for each topic.
    """"""
    m_lr = []
    num_docs = float(accumulator.num_docs)
    for s_i in segmented_topics:
        segment_sims = []
        for w_prime, w_star in s_i:
            w_prime_count = accumulator[w_prime]
            w_star_count = accumulator[w_star]
            co_occur_count = accumulator[w_prime, w_star]

            if normalize:
                # For normalized log ratio measure
                numerator = log_ratio_measure([[(w_prime, w_star)]], accumulator)[0]
                co_doc_prob = co_occur_count / num_docs
                m_lr_i = numerator / (-np.log(co_doc_prob + EPSILON))
            else:
                # For log ratio measure without normalization
                numerator = (co_occur_count / num_docs) + EPSILON
                denominator = (w_prime_count / num_docs) * (w_star_count / num_docs)
                m_lr_i = np.log(numerator / denominator)

            segment_sims.append(m_lr_i)
        m_lr.append(np.mean(segment_sims))

    return m_lr","1. Use `np.nan` instead of `float(""NaN"")` to represent NaN values.
2. Use `np.inf` instead of `float(""inf"")` to represent infinity values.
3. Use `np.iinfo` to get the maximum and minimum values for an integer type."
"def cosine_similarity(segmented_topics, accumulator, topics, measure='nlr', gamma=1):
    """"""
    This function calculates the indirect cosine measure. Given context vectors
    _   _         _   _
    u = V(W') and w = V(W*) for the word sets of a pair S_i = (W', W*) indirect
                                                                _     _
    cosine measure is computed as the cosine similarity between u and w. The formula used is:

    m_{sim}_{(m, \\gamma)}(W', W*) = s_{sim}(\\vec{V}^{\\,}_{m,\\gamma}(W'), \\vec{V}^{\\,}_{m,\\gamma}(W*))

    where each vector \\vec{V}^{\\,}_{m,\\gamma}(W') = \\Bigg \\{{\\sum_{w_{i} \\in W'}^{ } m(w_{i}, w_{j})^{\\gamma}}\\Bigg \\}_{j = 1,...,|W|}

    Args:
    ----
    segmented_topics : Output from the segmentation module of the segmented topics.
                       Is a list of list of tuples.
    accumulator : Output from the probability_estimation module.
                  Is an accumulator of word occurrences (see text_analysis module).
    topics : Topics obtained from the trained topic model.
    measure : String. Direct confirmation measure to be used.
              Supported values are ""nlr"" (normalized log ratio).
    gamma : Gamma value for computing W', W* vectors; default is 1.

    Returns:
    -------
    s_cos_sim : list of indirect cosine similarity measure for each topic.
    """"""
    context_vectors = ContextVectorComputer(measure, topics, accumulator, gamma)

    s_cos_sim = []
    for topic_words, topic_segments in zip(topics, segmented_topics):
        topic_words = tuple(topic_words)  # because tuples are hashable
        segment_sims = np.zeros(len(topic_segments))
        for i, (w_prime, w_star) in enumerate(topic_segments):
            w_prime_cv = context_vectors[w_prime, topic_words]
            w_star_cv = context_vectors[w_star, topic_words]
            segment_sims[i] = _cossim(w_prime_cv, w_star_cv)
        s_cos_sim.append(np.mean(segment_sims))

    return s_cos_sim","1. Use `np.unique()` to remove duplicate values from `topic_words` before using it as a key in `context_vectors`.
2. Use `context_vectors[w_prime, topic_words]` instead of `context_vectors[w_prime]` to avoid accessing an invalid index.
3. Use `np.mean()` to calculate the average of `segment_sims` instead of summing them up."
"def p_boolean_document(corpus, segmented_topics):
    """"""This function performs the boolean document probability estimation.
    Boolean document estimates the probability of a single word as the number
    of documents in which the word occurs divided by the total number of documents.

    Args:
    ----
    corpus : The corpus of documents.
    segmented_topics : Output from the segmentation of topics. Could be simply topics too.

    Returns:
    -------
    accumulator : word occurrence accumulator instance that can be used to lookup token
                  frequencies and co-occurrence frequencies.
    """"""
    top_ids = unique_ids_from_segments(segmented_topics)
    return CorpusAccumulator(top_ids).accumulate(corpus)","1. Use `corpus.get()` instead of `corpus[index]` to prevent index out of bound errors.
2. Use `corpus.add()` instead of `corpus.append()` to prevent duplicate entries.
3. Use `corpus.remove()` instead of `del corpus[index]` to prevent accidentally deleting the wrong entry."
"def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):
    """"""This function performs the boolean sliding window probability estimation.
    Boolean sliding window determines word counts using a sliding window. The window
    moves over  the documents one word token per step. Each step defines a new virtual
    document  by copying the window content. Boolean document is applied to these virtual
    documents to compute word probabilities.

    Args:
    ----
    texts : List of string sentences.
    segmented_topics : Output from the segmentation of topics. Could be simply topics too.
    dictionary : Gensim dictionary mapping of the tokens and ids.
    window_size : Size of the sliding window. 110 found out to be the ideal size for large corpora.

    Returns:
    -------
    accumulator : word occurrence accumulator instance that can be used to lookup token
                  frequencies and co-occurrence frequencies.
    """"""
    top_ids = unique_ids_from_segments(segmented_topics)
    if processes <= 1:
        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)
    else:
        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)
    logger.info(""using %s to estimate probabilities from sliding windows"", accumulator)
    return accumulator.accumulate(texts, window_size)","1. Use a secure password hashing function such as bcrypt or scrypt.
2. Use salt with the password hash.
3. Store the password hash in a secure location."
"def unique_ids_from_segments(segmented_topics):
    """"""Return the set of all unique ids in a list of segmented topics.

    Args:
    ----
    segmented_topics: list of tuples of (word_id_set1, word_id_set2). Each word_id_set
                      is either a single integer, or a `numpy.ndarray` of integers.
    Returns:
    unique_ids : set of unique ids across all topic segments.
    """"""
    unique_ids = set()  # is a set of all the unique ids contained in topics.
    for s_i in segmented_topics:
        for word_id in itertools.chain.from_iterable(s_i):
            if hasattr(word_id, '__iter__'):
                unique_ids.update(word_id)
            else:
                unique_ids.add(word_id)

    return unique_ids","1. Use `np.unique` instead of `set()` to avoid creating a new set object for each iteration.
2. Use `itertools.chain.from_iterable()` to flatten the list of sets into a single iterable.
3. Use `word_id in unique_ids` instead of `word_id not in unique_ids` to avoid unnecessary set membership checks."
"def s_one_pre(topics):
    """"""
    This function performs s_one_pre segmentation on a list of topics.
    s_one_pre segmentation is defined as: s_one_pre = {(W', W*) | W' = {w_i};
                                                                  W* = {w_j}; w_i, w_j belongs to W; i > j}
    Example:

        >>> topics = [np.array([1, 2, 3]), np.array([4, 5, 6])]
        >>> s_one_pre(topics)
        [[(2, 1), (3, 1), (3, 2)], [(5, 4), (6, 4), (6, 5)]]

    Args:
    ----
    topics : list of topics obtained from an algorithm such as LDA. Is a list such as [array([ 9, 10, 11]), array([ 9, 10,  7]), ...]

    Returns:
    -------
    s_one_pre : list of list of (W', W*) tuples for all unique topic ids
    """"""
    s_one_pre = []

    for top_words in topics:
        s_one_pre_t = []
        for w_prime_index, w_prime in enumerate(top_words[1:]):
            for w_star in top_words[:w_prime_index + 1]:
                s_one_pre_t.append((w_prime, w_star))
        s_one_pre.append(s_one_pre_t)

    return s_one_pre","1. Use `np.unique()` to remove duplicate topics from the list of topics.
2. Use `np.sort()` to sort the topics in ascending order.
3. Use `np.array_equal()` to check if two topics are equal."
"def s_one_one(topics):
    """"""
    This function performs s_one_one segmentation on a list of topics.
    s_one_one segmentation is defined as: s_one_one = {(W', W*) | W' = {w_i};
                                                                  W* = {w_j}; w_i, w_j belongs to W; i != j}
    Example:

        >>> topics = [np.array([1, 2, 3]), np.array([4, 5, 6])]
        >>> s_one_pre(topics)
        [[(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)], [(4, 5), (4, 6), (5, 4), (5, 6), (6, 4), (6, 5)]]

    Args:
    ----
    topics : list of topics obtained from an algorithm such as LDA. Is a list such as [array([ 9, 10, 11]), array([ 9, 10,  7]), ...]

    Returns:
    -------
    s_one_one : list of list of (W', W*) tuples for all unique topic ids
    """"""
    s_one_one = []

    for top_words in topics:
        s_one_one_t = []
        for w_prime_index, w_prime in enumerate(top_words):
            for w_star_index, w_star in enumerate(top_words):
                if w_prime_index == w_star_index:
                    continue
                else:
                    s_one_one_t.append((w_prime, w_star))
        s_one_one.append(s_one_one_t)

    return s_one_one","1. Use `np.unique()` to remove duplicate topics from the list of topics.
2. Use `enumerate()` to iterate over the list of topics and generate tuples of (w', w*).
3. Use `list()` to convert the list of tuples into a list of lists."
"def s_one_set(topics):
    """"""
    This function performs s_one_set segmentation on a list of topics.
    s_one_set segmentation is defined as: s_one_set = {(W', W*) | W' = {w_i}; w_i belongs to W;
                                                                  W* = W}
    Example:
        >>> topics = [np.array([9, 10, 7])
        >>> s_one_set(topics)
        [[(9, array([ 9, 10,  7])),
          (10, array([ 9, 10,  7])),
          (7, array([ 9, 10,  7]))]]

    Args:
    ----
    topics : list of topics obtained from an algorithm such as LDA. Is a list such as [array([ 9, 10, 11]), array([ 9, 10,  7]), ...]

    Returns:
    -------
    s_one_set : list of list of (W', W*) tuples for all unique topic ids.
    """"""
    s_one_set = []

    for top_words in topics:
        s_one_set_t = []
        for w_prime in top_words:
            s_one_set_t.append((w_prime, top_words))
        s_one_set.append(s_one_set_t)

    return s_one_set","1. Use `np.unique` to remove duplicate topics from the list of topics.
2. Use `np.array_split` to split the list of topics into smaller lists, one for each unique topic.
3. Use `np.concatenate` to concatenate the lists of topics back into one list."
"def _ids_to_words(ids, dictionary):
    """"""Convert an iterable of ids to their corresponding words using a dictionary.
    This function abstracts away the differences between the HashDictionary and the standard one.

    Args:
    ----
    ids: list of list of tuples, where each tuple contains (token_id, iterable of token_ids).
         This is the format returned by the topic_coherence.segmentation functions.
    """"""
    if not dictionary.id2token:  # may not be initialized in the standard gensim.corpora.Dictionary
        setattr(dictionary, 'id2token', {v: k for k, v in dictionary.token2id.items()})

    top_words = set()
    for word_id in ids:
        word = dictionary.id2token[word_id]
        if isinstance(word, set):
            top_words = top_words.union(word)
        else:
            top_words.add(word)

    return top_words","1. Use `dictionary.id2token` instead of `dictionary.token2id` to avoid creating a new dictionary in memory.
2. Check if `dictionary.id2token` exists before using it.
3. Use `isinstance(word, set)` to check if `word` is a set and handle it accordingly."
"    def __init__(self, relevant_ids, dictionary):
        """"""
        Args:
        ----
        relevant_ids: the set of words that occurrences should be accumulated for.
        dictionary: Dictionary instance with mappings for the relevant_ids.
        """"""
        super(WindowedTextsAnalyzer, self).__init__(relevant_ids, dictionary)
        self._none_token = self._vocab_size  # see _iter_texts for use of none token","1. Use `assert` statements to validate the input arguments.
2. Sanitize the input data to prevent SQL injection attacks.
3. Use a secure random number generator to generate the window size."
"    def __init__(self, processes, *args, **kwargs):
        """"""
        Args:
        ----
        processes : number of processes to use; must be at least two.
        args : should include `relevant_ids` and `dictionary` (see `UsesDictionary.__init__`).
        kwargs : can include `batch_size`, which is the number of docs to send to a worker at a
                 time. If not included, it defaults to 64.
        """"""
        super(ParallelWordOccurrenceAccumulator, self).__init__(*args)
        if processes < 2:
            raise ValueError(
                ""Must have at least 2 processes to run in parallel; got %d"" % processes)
        self.processes = processes
        self.batch_size = kwargs.get('batch_size', 64)","1. Use `multiprocessing.Pool` instead of `multiprocessing.Process` to avoid race conditions.
2. Use `multiprocessing.Manager` to share data between processes safely.
3. Use `multiprocessing.Lock` to protect shared data from concurrent access."
"def strided_windows(ndarray, window_size):
    """"""
    Produce a numpy.ndarray of windows, as from a sliding window.

    >>> strided_windows(np.arange(5), 2)
    array([[0, 1],
           [1, 2],
           [2, 3],
           [3, 4]])
    >>> strided_windows(np.arange(10), 5)
    array([[0, 1, 2, 3, 4],
           [1, 2, 3, 4, 5],
           [2, 3, 4, 5, 6],
           [3, 4, 5, 6, 7],
           [4, 5, 6, 7, 8],
           [5, 6, 7, 8, 9]])

    Args:
    ----
    ndarray: either a numpy.ndarray or something that can be converted into one.
    window_size: sliding window size.
    :param window_size:
    :return: numpy.ndarray of the subsequences produced by sliding a window of the given size over
             the `ndarray`. Since this uses striding, the individual arrays are views rather than
             copies of `ndarray`. Changes to one view modifies the others and the original.
    """"""
    ndarray = np.asarray(ndarray)
    if window_size == ndarray.shape[0]:
        return np.array([ndarray])
    elif window_size > ndarray.shape[0]:
        return np.ndarray((0, 0))

    stride = ndarray.strides[0]
    return np.lib.stride_tricks.as_strided(
        ndarray, shape=(ndarray.shape[0] - window_size + 1, window_size),
        strides=(stride, stride))","1. Use `np.ascontiguousarray()` to ensure that the input array is contiguous in memory. This will improve performance and avoid potential security issues.
2. Use `np.pad()` to pad the input array to the next largest multiple of the window size. This will prevent any out-of-bounds accesses.
3. Use `np.copy()` to create a copy of the input array before passing it to `np.lib.stride_tricks.as_strided()`. This will ensure that the original array is not modified."
"def iter_windows(texts, window_size, copy=False, ignore_below_size=True, include_doc_num=False):
    """"""Produce a generator over the given texts using a sliding window of `window_size`.
    The windows produced are views of some subsequence of a text. To use deep copies
    instead, pass `copy=True`.

    Args:
    ----
    texts: List of string sentences.
    window_size: Size of sliding window.
    copy: False to use views of the texts (default) or True to produce deep copies.
    ignore_below_size: ignore documents that are not at least `window_size` in length (default behavior).
                       If False, the documents below `window_size` will be yielded as the full document.

    """"""
    for doc_num, document in enumerate(texts):
        for window in _iter_windows(document, window_size, copy, ignore_below_size):
            if include_doc_num:
                yield (doc_num, window)
            else:
                yield window","1. Use `enumerate` instead of `for` loop to avoid `index out of range`.
2. Use `yield from` to avoid `generator.close()`.
3. Use `type hints` to specify the types of arguments and return values."
"    def load_fasttext_format(cls, model_file, encoding='utf8'):
        """"""
        Load the input-hidden weight matrix from the fast text output files.

        Note that due to limitations in the FastText API, you cannot continue training
        with a model loaded this way, though you can query for word similarity etc.

        `model_file` is the path to the FastText output files.
        FastText outputs two training files - `/path/to/train.vec` and `/path/to/train.bin`
        Expected value for this example: `/path/to/train`

        """"""
        model = cls()
        model.wv = cls.load_word2vec_format('%s.vec' % model_file, encoding=encoding)
        model.load_binary_data('%s.bin' % model_file, encoding=encoding)
        return model","1. Use `os.path.join()` to concatenate paths instead of string concatenation. This will prevent directory traversal attacks.
2. Use `sys.argv` to parse command-line arguments instead of hard-coding them. This will make it more difficult for attackers to exploit the code.
3. Use `subprocess.check_output()` to execute external commands instead of `os.system()`. This will prevent attackers from injecting malicious code into the system."
"    def load_binary_data(self, model_binary_file, encoding='utf8'):
        """"""Loads data from the output binary file created by FastText training""""""
        with utils.smart_open(model_binary_file, 'rb') as f:
            self.load_model_params(f)
            self.load_dict(f, encoding=encoding)
            self.load_vectors(f)","1. Use `open()` with `mode='rb'` to open the file in binary mode.
2. Use `os.fchmod()` to set the file mode to `0o600` (read-only for owner).
3. Use `os.fchown()` to set the file owner to the current user."
"    def load_dict(self, file_handle, encoding='utf8'):
        vocab_size, nwords, _ = self.struct_unpack(file_handle, '@3i')
        # Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)
        assert len(self.wv.vocab) == nwords, 'mismatch between vocab sizes'
        assert len(self.wv.vocab) == vocab_size, 'mismatch between vocab sizes'
        self.struct_unpack(file_handle, '@1q')  # number of tokens
        if self.new_format:
            pruneidx_size, = self.struct_unpack(file_handle, '@q')
        for i in range(nwords):
            word_bytes = b''
            char_byte = file_handle.read(1)
            # Read vocab word
            while char_byte != b'\\x00':
                word_bytes += char_byte
                char_byte = file_handle.read(1)
            word = word_bytes.decode(encoding)
            count, _ = self.struct_unpack(file_handle, '@qb')
            assert self.wv.vocab[word].index == i, 'mismatch between gensim word index and fastText word index'
            self.wv.vocab[word].count = count

        if self.new_format:
            for j in range(pruneidx_size):
                self.struct_unpack(file_handle, '@2i')","1. Use a secure random number generator to generate the salt.
2. Use a strong hashing algorithm, such as SHA-256 or bcrypt.
3. Use a sufficiently long salt and password."
"    def load_vectors(self, file_handle):
        if self.new_format:
            self.struct_unpack(file_handle, '@?')  # bool quant_input in fasttext.cc
        num_vectors, dim = self.struct_unpack(file_handle, '@2q')
        # Vectors stored by [Matrix::save](https://github.com/facebookresearch/fastText/blob/master/src/matrix.cc)
        assert self.vector_size == dim, 'mismatch between model sizes'
        float_size = struct.calcsize('@f')
        if float_size == 4:
            dtype = np.dtype(np.float32)
        elif float_size == 8:
            dtype = np.dtype(np.float64)

        self.num_original_vectors = num_vectors
        self.wv.syn0_all = np.fromfile(file_handle, dtype=dtype, count=num_vectors * dim)
        self.wv.syn0_all = self.wv.syn0_all.reshape((num_vectors, dim))
        assert self.wv.syn0_all.shape == (self.bucket + len(self.wv.vocab), self.vector_size), \\
            'mismatch between weight matrix shape and vocab/model size'
        self.init_ngrams()","1. Use `np.load` instead of `np.fromfile` to load the vectors. This will prevent an attacker from injecting malicious data into the model.
2. Check the shape of the loaded vectors to make sure it matches the expected shape. This will prevent an attacker from tricking the model into using invalid data.
3. Initialize the ngrams after the vectors have been loaded. This will prevent an attacker from using the ngrams to attack the model."
"    def init_ngrams(self):
        """"""
        Computes ngrams of all words present in vocabulary and stores vectors for only those ngrams.
        Vectors for other ngrams are initialized with a random uniform distribution in FastText. These
        vectors are discarded here to save space.

        """"""
        self.wv.ngrams = {}
        all_ngrams = []
        for w, v in self.wv.vocab.items():
            all_ngrams += self.compute_ngrams(w, self.wv.min_n, self.wv.max_n)
        all_ngrams = set(all_ngrams)
        self.num_ngram_vectors = len(all_ngrams)
        ngram_indices = []
        for i, ngram in enumerate(all_ngrams):
            ngram_hash = self.ft_hash(ngram)
            ngram_indices.append(len(self.wv.vocab) + ngram_hash % self.bucket)
            self.wv.ngrams[ngram] = i
        self.wv.syn0_all = self.wv.syn0_all.take(ngram_indices, axis=0)","1. Use a secure hashing function instead of `ft_hash`.
2. Sanitize the input to `compute_ngrams` to prevent attacks from malicious input.
3. Use a more secure way to store the ngram indices, such as a hash table."
"    def inference(self, chunk, collect_sstats=False):
        """"""
        Given a chunk of sparse document vectors, estimate gamma (parameters
        controlling the topic weights) for each document in the chunk.

        This function does not modify the model (=is read-only aka const). The
        whole input chunk of document is assumed to fit in RAM; chunking of a
        large corpus must be done earlier in the pipeline.

        If `collect_sstats` is True, also collect sufficient statistics needed
        to update the model's topic-word distributions, and return a 2-tuple
        `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape
        `len(chunk) x self.num_topics`.

        Avoids computing the `phi` variational parameter directly using the
        optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.

        """"""
        try:
            _ = len(chunk)
        except:
            # convert iterators/generators to plain list, so we have len() etc.
            chunk = list(chunk)
        if len(chunk) > 1:
            logger.debug(""performing inference on a chunk of %i documents"", len(chunk))

        # Initialize the variational distribution q(theta|gamma) for the chunk
        gamma = self.random_state.gamma(100., 1. / 100., (len(chunk), self.num_topics))
        Elogtheta = dirichlet_expectation(gamma)
        expElogtheta = np.exp(Elogtheta)
        if collect_sstats:
            sstats = np.zeros_like(self.expElogbeta)
        else:
            sstats = None
        converged = 0

        # Now, for each document d update that document's gamma and phi
        # Inference code copied from Hoffman's `onlineldavb.py` (esp. the
        # Lee&Seung trick which speeds things up by an order of magnitude, compared
        # to Blei's original LDA-C code, cool!).
        for d, doc in enumerate(chunk):
            if doc and not isinstance(doc[0][0], six.integer_types):
                # make sure the term IDs are ints, otherwise np will get upset
                ids = [int(id) for id, _ in doc]
            else:
                ids = [id for id, _ in doc]
            cts = np.array([cnt for _, cnt in doc])
            gammad = gamma[d, :]
            Elogthetad = Elogtheta[d, :]
            expElogthetad = expElogtheta[d, :]
            expElogbetad = self.expElogbeta[:, ids]

            # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.
            # phinorm is the normalizer.
            # TODO treat zeros explicitly, instead of adding 1e-100?
            phinorm = np.dot(expElogthetad, expElogbetad) + 1e-100

            # Iterate between gamma and phi until convergence
            for _ in xrange(self.iterations):
                lastgamma = gammad
                # We represent phi implicitly to save memory and time.
                # Substituting the value of the optimal phi back into
                # the update for gamma gives this update. Cf. Lee&Seung 2001.
                gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)
                Elogthetad = dirichlet_expectation(gammad)
                expElogthetad = np.exp(Elogthetad)
                phinorm = np.dot(expElogthetad, expElogbetad) + 1e-100
                # If gamma hasn't changed much, we're done.
                meanchange = np.mean(abs(gammad - lastgamma))
                if (meanchange < self.gamma_threshold):
                    converged += 1
                    break
            gamma[d, :] = gammad
            if collect_sstats:
                # Contribution of document d to the expected sufficient
                # statistics for the M step.
                sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)

        if len(chunk) > 1:
            logger.debug(""%i/%i documents converged within %i iterations"",
                         converged, len(chunk), self.iterations)

        if collect_sstats:
            # This step finishes computing the sufficient statistics for the
            # M step, so that
            # sstats[k, w] = \\sum_d n_{dw} * phi_{dwk}
            # = \\sum_d n_{dw} * exp{Elogtheta_{dk} + Elogbeta_{kw}} / phinorm_{dw}.
            sstats *= self.expElogbeta
        return gamma, sstats","1. Use a secure random number generator.
2. Sanitize user input to prevent injection attacks.
3. Use proper error handling to prevent sensitive information from being leaked."
"    def bound(self, corpus, gamma=None, subsample_ratio=1.0):
        """"""
        Estimate the variational bound of documents from `corpus`:
        E_q[log p(corpus)] - E_q[log q(corpus)]

        `gamma` are the variational parameters on topic weights for each `corpus`
        document (=2d matrix=what comes out of `inference()`).
        If not supplied, will be inferred from the model.

        """"""
        score = 0.0
        _lambda = self.state.get_lambda()
        Elogbeta = dirichlet_expectation(_lambda)

        for d, doc in enumerate(corpus):  # stream the input doc-by-doc, in case it's too large to fit in RAM
            if d % self.chunksize == 0:
                logger.debug(""bound: at document #%i"", d)
            if gamma is None:
                gammad, _ = self.inference([doc])
            else:
                gammad = gamma[d]
            Elogthetad = dirichlet_expectation(gammad)

            # E[log p(doc | theta, beta)]
            score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, id]) for id, cnt in doc)

            # E[log p(theta | alpha) - log q(theta | gamma)]; assumes alpha is a vector
            score += np.sum((self.alpha - gammad) * Elogthetad)
            score += np.sum(gammaln(gammad) - gammaln(self.alpha))
            score += gammaln(np.sum(self.alpha)) - gammaln(np.sum(gammad))

        # Compensate likelihood for when `corpus` above is only a sample of the whole corpus. This ensures
        # that the likelihood is always rougly on the same scale.
        score *= subsample_ratio

        # E[log p(beta | eta) - log q (beta | lambda)]; assumes eta is a scalar
        score += np.sum((self.eta - _lambda) * Elogbeta)
        score += np.sum(gammaln(_lambda) - gammaln(self.eta))

        if np.ndim(self.eta) == 0:
            sum_eta = self.eta * self.num_terms
        else:
            sum_eta = np.sum(self.eta)

        score += np.sum(gammaln(sum_eta) - gammaln(np.sum(_lambda, 1)))

        return score","1. Use `np.nan` instead of `None` to represent missing values.
2. Use `np.inf` instead of `-np.inf` to represent very large values.
3. Use `np.zeros` instead of `np.empty` to create arrays of zeros."
"def _get_combined_keywords(_keywords, split_text):
    """"""
    :param keywords:dict of keywords:scores
    :param split_text: list of strings
    :return: combined_keywords:list
    """"""
    result = []
    _keywords = _keywords.copy()
    len_text = len(split_text)
    for i in xrange(len_text):
        word = _strip_word(split_text[i])
        if word in _keywords:
            combined_word = [word]
            if i + 1 == len_text:
                result.append(word)   # appends last word if keyword and doesn't iterate
            for j in xrange(i + 1, len_text):
                other_word = _strip_word(split_text[j])
                if other_word in _keywords and other_word == split_text[j]:
                    combined_word.append(other_word)
                else:
                    for keyword in combined_word:
                        _keywords.pop(keyword)
                    result.append("" "".join(combined_word))
                    break
    return result","1. Use `_strip_word()` to sanitize user input before using it in `_get_combined_keywords()`.
2. Use `_keywords.pop()` to remove keywords from the dictionary after they are used in `_get_combined_keywords()`.
3. Use `len_text` to check for the end of the list in `_get_combined_keywords()`."
"    def get_probability_map(self, subject: Subject) -> torch.Tensor:
        label_map_tensor = self.get_probability_map_image(subject).data
        label_map_tensor = label_map_tensor.float()

        if self.label_probabilities_dict is None:
            return label_map_tensor > 0
        probability_map = self.get_probabilities_from_label_map(
            label_map_tensor,
            self.label_probabilities_dict,
        )
        return probability_map","1. Use `torch.jit.script` to make the model's inference code more secure.
2. Validate the input data before feeding it to the model.
3. Use `torch.jit.save` to save the model in a secure format."
"    def get_probabilities_from_label_map(
            label_map: torch.Tensor,
            label_probabilities_dict: Dict[int, float],
            ) -> torch.Tensor:
        """"""Create probability map according to label map probabilities.""""""
        multichannel = label_map.shape[0] > 1
        probability_map = torch.zeros_like(label_map)
        label_probs = torch.Tensor(list(label_probabilities_dict.values()))
        normalized_probs = label_probs / label_probs.sum()
        iterable = zip(label_probabilities_dict, normalized_probs)
        for label, label_probability in iterable:
            if multichannel:
                mask = label_map[label]
            else:
                mask = label_map == label
            label_size = mask.sum()
            if not label_size:
                continue
            prob_voxels = label_probability / label_size
            if multichannel:
                probability_map[label] = prob_voxels * mask
            else:
                probability_map[mask] = prob_voxels
        if multichannel:
            probability_map = probability_map.sum(dim=0, keepdim=True)
        return probability_map","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to generate a tracing of the model, which can be used to check for correctness and security vulnerabilities.
3. Use `torch.jit.save` to save the model in a secure format that can be verified by the user."
"    def __init__(
            self,
            subjects_dataset: SubjectsDataset,
            max_length: int,
            samples_per_volume: int,
            sampler: PatchSampler,
            num_workers: int = 0,
            pin_memory: bool = True,
            shuffle_subjects: bool = True,
            shuffle_patches: bool = True,
            start_background: bool = True,
            verbose: bool = False,
            ):
        self.subjects_dataset = subjects_dataset
        self.max_length = max_length
        self.shuffle_subjects = shuffle_subjects
        self.shuffle_patches = shuffle_patches
        self.samples_per_volume = samples_per_volume
        self.sampler = sampler
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.verbose = verbose
        self._subjects_iterable = None
        if start_background:
            self.initialize_subjects_iterable()
        self.patches_list: List[Subject] = []
        self.num_sampled_patches = 0","1. Use `torch.utils.data.DataLoader` instead of `itertools.cycle` to iterate over the dataset.
2. Use `torch.tensor` to represent the data instead of `list`.
3. Use `torch.nn.functional.one_hot` to create one-hot encodings instead of `torch.nn.functional.one_hot`."
"    def get_subjects_iterable(self) -> Iterator:
        # I need a DataLoader to handle parallelism
        # But this loader is always expected to yield single subject samples
        self._print(
            f'\\nCreating subjects loader with {self.num_workers} workers')
        subjects_loader = DataLoader(
            self.subjects_dataset,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            batch_size=1,
            collate_fn=self.get_first_item,
            shuffle=self.shuffle_subjects,
        )
        return iter(subjects_loader)","1. Use a secure random number generator to generate the number of workers.
2. Use a secure hash function to generate the hash of the subject ID.
3. Use a secure encryption algorithm to encrypt the subject data."
"    def __init__(
            self,
            subjects_dataset: SubjectsDataset,
            max_length: int,
            samples_per_volume: int,
            sampler: PatchSampler,
            num_workers: int = 0,
            shuffle_subjects: bool = True,
            shuffle_patches: bool = True,
            verbose: bool = False,
            ):
        self.subjects_dataset = subjects_dataset
        self.max_length = max_length
        self.shuffle_subjects = shuffle_subjects
        self.shuffle_patches = shuffle_patches
        self.samples_per_volume = samples_per_volume
        self.sampler = sampler
        self.num_workers = num_workers
        self.verbose = verbose
        self.subjects_iterable = self.get_subjects_iterable()
        self.patches_list: List[dict] = []
        self.num_sampled_patches = 0","1. Use `torch.tensor` instead of `np.ndarray` to avoid data copying.
2. Use `torch.multiprocessing.Pool` instead of `multiprocessing.Pool` to avoid GIL contention.
3. Use `torch.distributed.all_gather` instead of `multiprocessing.Manager().list` to avoid race conditions."
"    def get_next_subject(self) -> Subject:
        # A StopIteration exception is expected when the queue is empty
        try:
            subject = next(self.subjects_iterable)
        except StopIteration as exception:
            self._print('Queue is empty:', exception)
            self.subjects_iterable = self.get_subjects_iterable()
            subject = next(self.subjects_iterable)
        return subject","1. Use `try ... except` to catch and handle the `StopIteration` exception.
2. Use `self._print()` to log the exception message.
3. Use `self.get_subjects_iterable()` to get a new iterable of subjects when the queue is empty."
"    def get_subjects_iterable(self) -> Iterator:
        # I need a DataLoader to handle parallelism
        # But this loader is always expected to yield single subject samples
        self._print(
            '\\nCreating subjects loader with', self.num_workers, 'workers')
        subjects_loader = DataLoader(
            self.subjects_dataset,
            num_workers=self.num_workers,
            collate_fn=lambda x: x[0],
            shuffle=self.shuffle_subjects,
        )
        return iter(subjects_loader)","1. Use a secure password hashing function such as bcrypt or scrypt.
2. Use salt with the password hash.
3. Store the password hash in a secure location."
"    def add_transform_to_subject_history(self, subject):
        from .augmentation import RandomTransform
        from . import Compose, OneOf, CropOrPad, EnsureShapeMultiple
        from .preprocessing.label import SequentialLabels
        call_others = (
            RandomTransform,
            Compose,
            OneOf,
            CropOrPad,
            EnsureShapeMultiple,
            SequentialLabels,
        )
        if not isinstance(self, call_others):
            subject.add_transform(self, self._get_reproducing_arguments())","1. Use `assert` statements to check for invalid inputs.
2. Sanitize user input to prevent injection attacks.
3. Use a secure random number generator to avoid predictable output."
"    def train(
            cls,
            images_paths: Sequence[TypePath],
            cutoff: Optional[Tuple[float, float]] = None,
            mask_path: Optional[TypePath] = None,
            masking_function: Optional[Callable] = None,
            output_path: Optional[TypePath] = None,
            ) -> np.ndarray:
        """"""Extract average histogram landmarks from images used for training.

        Args:
            images_paths: List of image paths used to train.
            cutoff: Optional minimum and maximum quantile values,
                respectively, that are used to select a range of intensity of
                interest. Equivalent to :math:`pc_1` and :math:`pc_2` in
                `Nyúl and Udupa's paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.204.102&rep=rep1&type=pdf>`_.
            mask_path: Optional path to a mask image to extract voxels used for
                training.
            masking_function: Optional function used to extract voxels used for
                training.
            output_path: Optional file path with extension ``.txt`` or
                ``.npy``, where the landmarks will be saved.

        Example:

            >>> import torch
            >>> import numpy as np
            >>> from pathlib import Path
            >>> from torchio.transforms import HistogramStandardization
            >>>
            >>> t1_paths = ['subject_a_t1.nii', 'subject_b_t1.nii.gz']
            >>> t2_paths = ['subject_a_t2.nii', 'subject_b_t2.nii.gz']
            >>>
            >>> t1_landmarks_path = Path('t1_landmarks.npy')
            >>> t2_landmarks_path = Path('t2_landmarks.npy')
            >>>
            >>> t1_landmarks = (
            ...     t1_landmarks_path
            ...     if t1_landmarks_path.is_file()
            ...     else HistogramStandardization.train(t1_paths)
            ... )
            >>> torch.save(t1_landmarks, t1_landmarks_path)
            >>>
            >>> t2_landmarks = (
            ...     t2_landmarks_path
            ...     if t2_landmarks_path.is_file()
            ...     else HistogramStandardization.train(t2_paths)
            ... )
            >>> torch.save(t2_landmarks, t2_landmarks_path)
            >>>
            >>> landmarks_dict = {
            ...     't1': t1_landmarks,
            ...     't2': t2_landmarks,
            ... }
            >>>
            >>> transform = HistogramStandardization(landmarks_dict)
        """"""  # noqa: E501
        quantiles_cutoff = DEFAULT_CUTOFF if cutoff is None else cutoff
        percentiles_cutoff = 100 * np.array(quantiles_cutoff)
        percentiles_database = []
        percentiles = _get_percentiles(percentiles_cutoff)
        for image_file_path in tqdm(images_paths):
            tensor, _ = read_image(image_file_path)
            data = tensor.numpy()
            if masking_function is not None:
                mask = masking_function(data)
            else:
                if mask_path is not None:
                    mask, _ = read_image(mask_path)
                    mask = mask.numpy() > 0
                else:
                    mask = np.ones_like(data, dtype=np.bool)
            percentile_values = np.percentile(data[mask], percentiles)
            percentiles_database.append(percentile_values)
        percentiles_database = np.vstack(percentiles_database)
        mapping = _get_average_mapping(percentiles_database)

        if output_path is not None:
            output_path = Path(output_path).expanduser()
            extension = output_path.suffix
            if extension == '.txt':
                modality = 'image'
                text = f'{modality} {"" "".join(map(str, mapping))}'
                output_path.write_text(text)
            elif extension == '.npy':
                np.save(output_path, mapping)
        return mapping","1. Use `torch.no_grad()` to disable gradient calculation when loading images. This will prevent the model from learning the gradients of the images, which could be used to attack the model.
2. Use a secure hash function to generate the hash of the image. This will prevent an attacker from creating a new image that has the same hash as the original image, which could be used to bypass the model's security checks.
3. Use a salt when generating the hash of the image. This will make it more difficult for an attacker to create a new image that has the same hash as the original image."
"    def mean(tensor: torch.Tensor) -> torch.Tensor:
        mask = tensor > tensor.mean()
        return mask","1. **Use `torch.clamp` to clip the values of the tensor to a safe range.** This will prevent overflow errors and ensure that the results of the function are correct.
2. **Use `torch.jit.script` to JIT-compile the function.** This will make the function faster and more efficient.
3. **Use `torch.jit.trace` to create a traced version of the function.** This will allow you to use the function with `torch.jit.save` and `torch.jit.load`."
"    def generate_bias_field(
            data: TypeData,
            order: int,
            coefficients: TypeData,
            ) -> np.ndarray:
        # Create the bias field map using a linear combination of polynomial
        # functions and the coefficients previously sampled
        shape = np.array(data.shape[1:])  # first axis is channels
        half_shape = shape / 2

        ranges = [np.arange(-n, n) for n in half_shape]

        bias_field = np.zeros(shape)
        x_mesh, y_mesh, z_mesh = np.asarray(np.meshgrid(*ranges))

        x_mesh /= x_mesh.max()
        y_mesh /= y_mesh.max()
        z_mesh /= z_mesh.max()

        i = 0
        for x_order in range(order + 1):
            for y_order in range(order + 1 - x_order):
                for z_order in range(order + 1 - (x_order + y_order)):
                    coefficient = coefficients[i]
                    new_map = (
                        coefficient
                        * x_mesh ** x_order
                        * y_mesh ** y_order
                        * z_mesh ** z_order
                    )
                    bias_field += np.transpose(new_map, (1, 0, 2))  # why?
                    i += 1
        bias_field = np.exp(bias_field).astype(np.float32)
        return bias_field","1. Use `np.full` instead of `np.zeros` to initialize the bias field to avoid creating garbage data.
2. Use `np.meshgrid` to create the meshgrid instead of manually creating it. This will prevent errors if the input data is not of the correct shape.
3. Use `np.expm1` instead of `np.exp` to avoid overflow errors."
"    def _parse_path(
            self,
            path: Union[TypePath, Sequence[TypePath]]
            ) -> Union[Path, List[Path]]:
        if path is None:
            return None
        if isinstance(path, (str, Path)):
            return self._parse_single_path(path)
        else:
            return [self._parse_single_path(p) for p in path]","1. Use `pathlib.Path` instead of `str` or `Path` to avoid `os.path.join`.
2. Validate the input path to prevent `Path` or `os.path.join` from throwing exceptions.
3. Use `Path.is_file` or `Path.is_dir` to check if the path exists and is a file or directory."
"    def as_pil(self) -> ImagePIL:
        """"""Get the image as an instance of :class:`PIL.Image`.""""""
        self.check_is_2d()
        return ImagePIL.open(self.path)","1. **Use `Image.open_with_path()` instead of `ImagePIL.open()` to avoid file path injection.**
2. **Check the image's dimensions before opening it to prevent denial-of-service attacks.**
3. **Use a secure image processing library such as `Pillow` or `OpenCV` to avoid vulnerabilities in the underlying image processing library.**"
"    def __init__(self, *args, **kwargs: Dict[str, Any]):
        if args:
            if len(args) == 1 and isinstance(args[0], dict):
                kwargs.update(args[0])
            else:
                message = (
                    'Only one dictionary as positional argument is allowed')
                raise ValueError(message)
        super().__init__(**kwargs)
        self.images = [
            (k, v) for (k, v) in self.items()
            if isinstance(v, Image)
        ]
        self._parse_images(self.images)
        self.update_attributes()  # this allows me to do e.g. subject.t1
        self.history = []","1. Use `typing` to annotate the arguments and return types of functions.
2. Validate the input data to prevent malicious users from injecting code or crashing the server.
3. Use secure default values for all parameters, such as `None` for optional parameters and `False` for boolean parameters."
"    def __repr__(self):
        string = (
            f'{self.__class__.__name__}'
            f'(Keys: {tuple(self.keys())}; images: {len(self.images)})'
        )
        return string","1. Use `pickle.dumps(data, protocol=-1)` to serialize data, instead of `pickle.dumps(data)`. This will prevent the pickle from being vulnerable to pickle bombs.
2. Use `pickle.loads(data, encoding='latin1')` to deserialize data, instead of `pickle.loads(data)`. This will prevent the pickle from being vulnerable to UnicodeDecodeErrors.
3. Use `contextlib.closing` to open the file handle, instead of using `open()` directly. This will ensure that the file handle is closed properly, even if an exception is raised."
"    def shape(self):
        """"""Return shape of first image in subject.

        Consistency of shapes across images in the subject is checked first.
        """"""
        self.check_consistent_shape()
        image = self.get_images(intensity_only=False)[0]
        return image.shape","1. Use `np.array_equal` to check for consistent shapes instead of `self.check_consistent_shape()`.
2. Use `imageio.imread()` to read images instead of `self.get_images()`.
3. Sanitize the input image before using it."
"    def spatial_shape(self):
        """"""Return spatial shape of first image in subject.

        Consistency of shapes across images in the subject is checked first.
        """"""
        return self.shape[1:]","1. Use `np.array_equal` to check for shape consistency instead of `self.shape[1:]`.
2. Use `np.asarray` to convert the input to a numpy array before checking its shape.
3. Add a `try`-`except` block to catch errors and return a default value if the shape check fails."
"    def spacing(self):
        """"""Return spacing of first image in subject.

        Consistency of shapes across images in the subject is checked first.
        """"""
        self.check_consistent_shape()
        image = self.get_images(intensity_only=False)[0]
        return image.spacing","1. Use `assert` statements to check for errors before they occur.
2. Use `try` and `except` blocks to handle errors gracefully.
3. Use secure coding practices, such as avoiding using hard-coded passwords and using encryption."
"    def check_consistent_shape(self) -> None:
        shapes_dict = {}
        iterable = self.get_images_dict(intensity_only=False).items()
        for image_name, image in iterable:
            shapes_dict[image_name] = image.shape
        num_unique_shapes = len(set(shapes_dict.values()))
        if num_unique_shapes > 1:
            message = (
                'Images in subject have inconsistent shapes:'
                f'\\n{pprint.pformat(shapes_dict)}'
            )
            raise ValueError(message)","1. Use `assert` statements to validate input arguments.
2. Use `type` annotations to specify the types of input arguments.
3. Use `logging` to log errors and exceptions."
"    def add_image(self, image, image_name):
        self[image_name] = image
        self.update_attributes()","1. Sanitize the image name to prevent XSS attacks.
2. Use `self.save()` instead of `self.update_attributes()` to avoid race conditions.
3. Use `ImageField` instead of `FileField` to prevent file upload attacks."
"    def apply_transform(self, sample: Subject) -> dict:
        sample.check_consistent_shape()
        params = self.get_params(
            self.scales,
            self.degrees,
            self.translation,
            self.isotropic,
        )
        scaling_params, rotation_params, translation_params = params
        for image in self.get_images(sample):
            if image[TYPE] != INTENSITY:
                interpolation = Interpolation.NEAREST
            else:
                interpolation = self.interpolation

            if image.is_2d():
                scaling_params[0] = 1
                rotation_params[-2:] = 0

            if self.use_image_center:
                center = image.get_center(lps=True)
            else:
                center = None

            transformed_tensors = []
            for tensor in image[DATA]:
                transformed_tensor = self.apply_affine_transform(
                    tensor,
                    image[AFFINE],
                    scaling_params.tolist(),
                    rotation_params.tolist(),
                    translation_params.tolist(),
                    interpolation,
                    center_lps=center,
                )
                transformed_tensors.append(transformed_tensor)
            image[DATA] = torch.stack(transformed_tensors)
        random_parameters_dict = {
            'scaling': scaling_params,
            'rotation': rotation_params,
            'translation': translation_params,
        }
        sample.add_transform(self, random_parameters_dict)
        return sample","1. Use `torch.jit.script` to make the transform deterministic.
2. Validate the input parameters of the transform.
3. Handle errors more gracefully."
"    def apply_transform(self, sample: Subject) -> dict:
        sample.check_consistent_shape()
        bspline_params = self.get_params(
            self.num_control_points,
            self.max_displacement,
            self.num_locked_borders,
        )
        for image in self.get_images(sample):
            if image[TYPE] != INTENSITY:
                interpolation = Interpolation.NEAREST
            else:
                interpolation = self.interpolation
            if image.is_2d():
                bspline_params[..., -3] = 0  # no displacement in LR axis
            image[DATA] = self.apply_bspline_transform(
                image[DATA],
                image[AFFINE],
                bspline_params,
                interpolation,
            )
        random_parameters_dict = {'coarse_grid': bspline_params}
        sample.add_transform(self, random_parameters_dict)
        return sample","1. Use `torch.jit.script` to make the code more secure.
2. Validate the input parameters to prevent attacks.
3. Use `torch.jit.trace` to prevent attackers from extracting the model parameters."
"    def _get_sample_shape(sample: Subject) -> TypeTripletInt:
        """"""Return the shape of the first image in the sample.""""""
        sample.check_consistent_shape()
        for image_dict in sample.get_images(intensity_only=False):
            data = image_dict.spatial_shape  # remove channels dimension
            break
        return data","1. Use `assert` statements to check for invalid inputs.
2. Sanitize user input before using it in the code.
3. Use secure coding practices, such as using `cryptography` to encrypt data."
"    def __init__(
            self,
            num_ghosts: Union[int, Tuple[int, int]] = (4, 10),
            axes: Union[int, Tuple[int, ...]] = (0, 1, 2),
            intensity: Union[float, Tuple[float, float]] = (0.5, 1),
            restore: float = 0.02,
            p: float = 1,
            seed: Optional[int] = None,
            ):
        super().__init__(p=p, seed=seed)
        if not isinstance(axes, tuple):
            try:
                axes = tuple(axes)
            except TypeError:
                axes = (axes,)
        for axis in axes:
            if axis not in (0, 1, 2):
                raise ValueError(f'Axes must be in (0, 1, 2), not ""{axes}""')
        self.axes = axes
        if isinstance(num_ghosts, int):
            self.num_ghosts_range = num_ghosts, num_ghosts
        elif isinstance(num_ghosts, tuple) and len(num_ghosts) == 2:
            self.num_ghosts_range = num_ghosts
        self.intensity_range = self.parse_range(intensity, 'intensity')
        for n in self.intensity_range:
            if n < 0:
                message = (
                    f'Intensity must be a positive number, not {n}')
                raise ValueError(message)
        if not 0 <= restore < 1:
            message = (
                f'Restore must be a number between 0 and 1, not {restore}')
            raise ValueError(message)
        self.restore = restore","1. Use `typing` to specify the types of arguments and return values.
2. Validate the input arguments to ensure they are within the expected ranges.
3. Use `assert` statements to check for errors in the code and raise exceptions if necessary."
"    def __getitem__(self, index: int) -> dict:
        if not isinstance(index, int):
            raise ValueError(f'Index ""{index}"" must be int, not {type(index)}')
        subject = self.subjects[index]
        sample = copy.deepcopy(subject)

        # Apply transform (this is usually the bottleneck)
        if self._transform is not None:
            sample = self._transform(sample)
        return sample","1. Use `type()` to check if `index` is an integer.
2. Use `copy.deepcopy()` to avoid modifying the original data.
3. Use `None` check to avoid errors when `_transform` is not defined."
"    def load(self) -> Tuple[torch.Tensor, np.ndarray]:
        r""""""Load the image from disk.

        The file is expected to be monomodal/grayscale and 2D or 3D.
        A channels dimension is added to the tensor.

        Returns:
            Tuple containing a 4D data tensor of size
            :math:`(1, D_{in}, H_{in}, W_{in})`
            and a 2D 4x4 affine matrix
        """"""
        if self.path is None:
            raise RuntimeError('No path provided for instance of Image')
        tensor, affine = read_image(self.path)
        # https://github.com/pytorch/pytorch/issues/9410#issuecomment-404968513
        tensor = tensor[(None,) * (3 - tensor.ndim)]  # force to be 3D
        # Remove next line and uncomment the two following ones once/if this issue
        # gets fixed:
        # https://github.com/pytorch/pytorch/issues/29010
        # See also https://discuss.pytorch.org/t/collating-named-tensors/78650/4
        tensor = tensor.unsqueeze(0)  # add channels dimension
        # name_dimensions(tensor, affine)
        # tensor = tensor.align_to('channels', ...)
        if self.check_nans and torch.isnan(tensor).any():
            warnings.warn(f'NaNs found in file ""{self.path}""')
        self[DATA] = tensor
        self[AFFINE] = affine
        self._loaded = True","1. Sanitize user input to prevent against injection attacks.
2. Use proper error handling to prevent leaking sensitive information.
3. Use secure coding practices to protect against common vulnerabilities."
"    def __init__(self, *args, **kwargs):
        scene.visuals.Line.__init__(self, *args, **kwargs)

        # initialize point markers
        self.markers = scene.visuals.Markers()
        self.marker_colors = np.ones((len(self.pos), 4), dtype=np.float32)
        self.markers.set_data(pos=self.pos, symbol=""s"", edge_color=""red"",
                              size=6)
        self.selected_point = None
        self.selected_index = -1
        # snap grid size
        self.gridsize = 10","1. Use `scene.visuals.Line`'s `set_data` method to set the data instead of directly assigning to `self.markers`. This will prevent accidental modification of the markers data.
2. Use `np.copy` to create a copy of the `pos` array instead of directly assigning to `self.marker_colors`. This will prevent accidental modification of the marker colors.
3. Initialize the `self.selected_point` and `self.selected_index` attributes to `None` instead of assigning them directly. This will prevent accidental modification of these attributes."
"    def select_point(self, event, radius=5):

        """"""
        Get line point close to mouse pointer and its index

        Parameters
        ----------
        event : the mouse event being processed
        radius : scalar
            max. distance in pixels between mouse and line point to be accepted
        return: (numpy.array, int)
            picked point and index of the point in the pos array
        """"""

        # position in scene/document coordinates
        pos_scene = event.pos[:3]

        # project mouse radius from screen coordinates to document coordinates
        mouse_radius = \\
            (event.visual_to_canvas.imap(np.array([radius, radius, radius])) -
             event.visual_to_canvas.imap(np.array([0, 0, 0])))[0]
        # print(""Mouse radius in document units: "", mouse_radius)

        # find first point within mouse_radius
        index = 0
        for p in self.pos:
            if np.linalg.norm(pos_scene - p) < mouse_radius:
                # print p, index
                # point found, return point and its index
                return p, index
            index += 1
        # no point found, return None
        return None, -1","1. Use `np.linalg.norm()` instead of `np.linalg.norm(pos_scene - p)` to avoid `pos_scene - p` being out of bounds.
2. Use `event.visual_to_canvas.imap(np.array([0, 0, 0]))` instead of `np.array([0, 0, 0])` to avoid `np.array([0, 0, 0])` being out of bounds.
3. Use `index += 1` instead of `index = index + 1` to avoid `index + 1` being out of bounds."
"    def on_mouse_press(self, event):
        self.print_mouse_event(event, 'Mouse press')
        pos_scene = event.pos[:3]

        # find closest point to mouse and select it
        self.selected_point, self.selected_index = self.select_point(event)

        # if no point was clicked add a new one
        if self.selected_point is None:
            print(""adding point"", len(self.pos))
            self._pos = np.append(self.pos, [pos_scene], axis=0)
            self.set_data(pos=self.pos)
            self.marker_colors = np.ones((len(self.pos), 4), dtype=np.float32)
            self.selected_point = self.pos[-1]
            self.selected_index = len(self.pos) - 1

        # update markers and highlights
        self.update_markers(self.selected_index)","1. Use `np.asarray()` to convert the `event.pos` argument to a numpy array. This will prevent a type error if the user clicks on a non-numerical position on the plot.
2. Use `np.clip()` to ensure that the selected point is within the bounds of the plot. This will prevent a segfault if the user clicks outside of the plot.
3. Use `np.copy()` to create a copy of the `pos` array before appending the new point. This will prevent the original data from being overwritten."
"    def on_mouse_move(self, event):
        # left mouse button
        if event.button == 1:
            # self.print_mouse_event(event, 'Mouse drag')
            if self.selected_point is not None:
                pos_scene = event.pos
                # update selected point to new position given by mouse
                self.selected_point[0] = round(pos_scene[0] / self.gridsize) \\
                    * self.gridsize
                self.selected_point[1] = round(pos_scene[1] / self.gridsize) \\
                    * self.gridsize
                self.set_data(pos=self.pos)
                self.update_markers(self.selected_index)

        else:
            #  if no button is pressed, just highlight the marker that would be
            # selected on click
            hl_point, hl_index = self.select_point(event)
            self.update_markers(hl_index, highlight_color=(0.5, 0.5, 1.0, 1.0))
            self.update()","1. Use `event.button` to check if the left mouse button is pressed.
2. Use `self.print_mouse_event(event, 'Mouse drag')` to log mouse events.
3. Use `self.update_markers(self.selected_index, highlight_color=(0.5, 0.5, 1.0, 1.0))` to highlight the marker that would be selected on click."
"    def __init__(self):
        scene.SceneCanvas.__init__(self, keys='interactive',
                                   size=(800, 800))

        # Create some initial points
        n = 7
        self.pos = np.zeros((n, 3), dtype=np.float32)
        self.pos[:, 0] = np.linspace(-50, 50, n)
        self.pos[:, 1] = np.random.normal(size=n, scale=10, loc=0)

        # create new editable line
        self.line = EditLineVisual(pos=self.pos, color='w', width=3,
                                   antialias=True, method='gl')

        self.view = self.central_widget.add_view()
        self.view.camera = scene.PanZoomCamera(rect=(-100, -100, 200, 200),
                                               aspect=1.0)
        # the left mouse button pan has to be disabled in the camera, as it
        # interferes with dragging line points
        # Proposed change in camera: make mouse buttons configurable
        self.view.camera._viewbox.events.mouse_move.disconnect(
            self.view.camera.viewbox_mouse_event)

        self.view.add(self.line)
        self.show()
        self.selected_point = None
        scene.visuals.GridLines(parent=self.view.scene)",000_Didnt Work
"    def _use(self, backend_name=None):
        """"""Select a backend by name. See class docstring for details.
        """"""
        # See if we're in a specific testing mode, if so DONT check to see
        # if it's a valid backend. If it isn't, it's a good thing we
        # get an error later because we should have decorated our test
        # with requires_application()
        test_name = os.getenv('_VISPY_TESTING_APP', None)

        # Check whether the given name is valid
        if backend_name is not None:
            if backend_name.lower() == 'default':
                backend_name = None  # Explicitly use default, avoid using test
            elif backend_name.lower() not in BACKENDMAP:
                raise ValueError('backend_name must be one of %s or None, not '
                                 '%r' % (BACKEND_NAMES, backend_name))
        elif test_name is not None:
            backend_name = test_name.lower()
            assert backend_name in BACKENDMAP

        # Should we try and load any backend, or just this specific one?
        try_others = backend_name is None

        # Get backends to try ...
        imported_toolkits = []  # Backends for which the native lib is imported
        backends_to_try = []
        if not try_others:
            # We should never hit this, since we check above
            assert backend_name.lower() in BACKENDMAP.keys()
            # Add it
            backends_to_try.append(backend_name.lower())
        else:
            # See if a backend is loaded
            for name, module_name, native_module_name in CORE_BACKENDS:
                if native_module_name and native_module_name in sys.modules:
                    imported_toolkits.append(name.lower())
                    backends_to_try.append(name.lower())
            # See if a default is given
            default_backend = config['default_backend'].lower()
            if default_backend.lower() in BACKENDMAP.keys():
                if default_backend not in backends_to_try:
                    backends_to_try.append(default_backend)
            # After this, try each one
            for name, module_name, native_module_name in CORE_BACKENDS:
                name = name.lower()
                if name not in backends_to_try:
                    backends_to_try.append(name)

        # Now try each one
        for key in backends_to_try:
            name, module_name, native_module_name = BACKENDMAP[key]
            TRIED_BACKENDS.append(name)
            mod_name = 'backends.' + module_name
            __import__(mod_name, globals(), level=1)
            mod = getattr(backends, module_name)
            if not mod.available:
                msg = ('Could not import backend ""%s"":\\n%s'
                       % (name, str(mod.why_not)))
                if not try_others:
                    # Fail if user wanted to use a specific backend
                    raise RuntimeError(msg)
                elif key in imported_toolkits:
                    # Warn if were unable to use an already imported toolkit
                    msg = ('Although %s is already imported, the %s backend '
                           'could not\\nbe used (""%s""). \\nNote that running '
                           'multiple GUI toolkits simultaneously can cause '
                           'side effects.' % 
                           (native_module_name, name, str(mod.why_not))) 
                    logger.warning(msg)
                else:
                    # Inform otherwise
                    logger.info(msg)
            else:
                # Success!
                self._backend_module = mod
                logger.debug('Selected backend %s' % module_name)
                break
        else:
            raise RuntimeError('Could not import any of the backends.')

        # Store classes for app backend and canvas backend
        self._backend = self.backend_module.ApplicationBackend()","1. Use `assert` statements to validate user input.
2. Use `try` and `except` blocks to handle errors.
3. Use `logging` to log errors and debug information."
"    def _set_keys(self, keys):
        if keys is not None:
            if isinstance(keys, string_types):
                if keys != 'interactive':
                    raise ValueError('keys, if string, must be ""interactive"", '
                                     'not %s' % (keys,))

                def toggle_fs():
                    self.fullscreen = not self.fullscreen
                keys = dict(escape='close', F11=toggle_fs)
        else:
            keys = {}
        if not isinstance(keys, dict):
            raise TypeError('keys must be a dict, str, or None')
        if len(keys) > 0:
            # ensure all are callable
            for key, val in keys.items():
                if isinstance(val, string_types):
                    new_val = getattr(self, val, None)
                    if new_val is None:
                        raise ValueError('value %s is not an attribute of '
                                         'Canvas' % val)
                    val = new_val
                if not hasattr(val, '__call__'):
                    raise TypeError('Entry for key %s is not callable' % key)
                # convert to lower-case representation
                keys.pop(key)
                keys[key.lower()] = val
            self._keys_check = keys

            def keys_check(event):
                use_name = event.key.name.lower()
                if use_name in self._keys_check:
                    self._keys_check[use_name]()
            self.events.key_press.connect(keys_check, ref=True)","1. Use `typing` to define the types of arguments and return values of functions.
2. Validate user input to prevent injection attacks.
3. Use `@staticmethod` to define functions that do not need to access the `self` object."
"            def keys_check(event):
                use_name = event.key.name.lower()
                if use_name in self._keys_check:
                    self._keys_check[use_name]()","1. Use `event.key.code` instead of `event.key.name` to avoid typos.
2. Use `event.key.is_pressed` instead of `event.key` to check if a key is pressed.
3. Use `event.key.char` instead of `event.key.name` to get the character that was pressed."
"def checkerboard(grid_num=8, grid_size=32):
    row_even = grid_num / 2 * [0, 1]
    row_odd = grid_num / 2 * [1, 0]
    Z = np.row_stack(grid_num / 2 * (row_even, row_odd)).astype(np.uint8)
    return 255 * Z.repeat(grid_size, axis=0).repeat(grid_size, axis=1)","1. Use `np.random.randint` instead of hard-coded values for `grid_num` and `grid_size`.
2. Sanitize user input for `grid_num` and `grid_size` to prevent overflows.
3. Use `np.clip` to ensure that the values of `Z` are between 0 and 255."
"    def _deactivate(self):
        if self._gtype in (gl.GL_SAMPLER_2D, GL_SAMPLER_3D):
            #gl.glActiveTexture(gl.GL_TEXTURE0 + self._unit)
            if self.data is not None:
                self.data.deactivate()","1. Use `gl.glActiveTexture` to bind the correct texture unit before deactivating the sampler.
2. Check that `self.data` is not `None` before calling `self.data.deactivate()`.
3. Use `contextlib.closing` to ensure that the texture object is closed when the `with` block exits."
"    def on_draw(self, event):
        self.framebuffer.activate()
        set_viewport(0, 0, 512, 512)
        clear(color=True, depth=True)
        set_state(depth_test=True)
        self.cube.draw('triangles', self.indices)
        self.framebuffer.deactivate()
        clear(color=True)
        set_state(depth_test=False)
        self.quad.draw('triangle_strip')","1. **Use `glEnable(GL_DEPTH_TEST)` to enable depth testing.** This will ensure that objects are rendered in the correct order, with closer objects obscuring more distant objects.
2. **Use `glDepthFunc(GL_LEQUAL)` to set the depth function.** This will ensure that a pixel is only drawn if its depth value is less than or equal to the depth value of the pixel already in the framebuffer.
3. **Use `glClearDepth(1.0)` to set the clear depth value.** This will ensure that the framebuffer is cleared to a value that is greater than or equal to the depth value of any object that will be drawn."
"    def __init__(self):
        app.Canvas.__init__(self, title='Use your wheel to zoom!', 
                            keys='interactive')
        self.program = gloo.Program(VERT_SHADER, FRAG_SHADER)
        self.program['a_position'] = y.ravel()
        self.program['a_color'] = color
        self.program['a_index'] = index
        self.program['u_scale'] = (1., 1.)
        self.program['u_size'] = (nrows, ncols)
        self.program['u_n'] = n
        
        self._timer = app.Timer('auto', connect=self.on_timer, start=True)","1. Use `app.Canvas.__init__(self, **kwargs)` instead of `app.Canvas.__init__(self, title='Use your wheel to zoom!', keys='interactive')` to avoid exposing sensitive information in the title and key arguments.
2. Use `self.program = gloo.Program(VERT_SHADER, FRAG_SHADER, **kwargs)` instead of `self.program = gloo.Program(VERT_SHADER, FRAG_SHADER)` to specify the keyword arguments explicitly.
3. Use `self._timer = app.Timer('auto', connect=self.on_timer, start=False)` instead of `self._timer = app.Timer('auto', connect=self.on_timer, start=True)` to disable the timer by default."
"    def __init__(self):
        app.Canvas.__init__(self, title='Spacy', keys='interactive')
        self.size = 800, 600
        
        self.program = gloo.Program(vertex, fragment)
        self.view = np.eye(4, dtype=np.float32)
        self.model = np.eye(4, dtype=np.float32)
        self.projection = np.eye(4, dtype=np.float32)
        
        self.timer = app.Timer('auto', connect=self.update, start=True)
        
        # Set uniforms (some are set later)
        self.program['u_model'] = self.model
        self.program['u_view'] = self.view
        
        # Set attributes
        self.program['a_position'] = np.zeros((N, 3), np.float32)
        self.program['a_offset'] = np.zeros((N,), np.float32)
        
        # Init
        self._timeout = 0
        self._active_block = 0
        for i in range(NBLOCKS):
            self._generate_stars()
        self._timeout = time.time() + SPEED","1. Use `os.path.join()` to concatenate paths instead of string concatenation.
2. Use `json.dumps()` to serialize data instead of `str()`.
3. Use `sha256()` to generate a hash of a password instead of `md5()`."
"    def _get_tick_frac_labels(self):
        # This conditional is currently unnecessary since we only support
        # linear, but eventually we will support others so we leave it in
        if (self.axis.scale_type == 'linear'):

            major_num = 11  # maximum number of major ticks
            minor_num = 4   # maximum number of minor ticks per major division

            major, majstep = np.linspace(0, 1, num=major_num, retstep=True)

            # XXX TODO: this should be better than just str(x)
            labels = [str(x) for x in 
                      np.interp(major, [0, 1], self.axis.domain)]

            # XXX TODO: make these nice numbers only
            # - and faster! Potentially could draw in linspace across the whole
            # axis and render them before the major ticks, so the overlap
            # gets hidden. Might be messy. Benchmark trade-off of extra GL
            # versus extra NumPy.
            minor = []
            for i in np.nditer(major[:-1]):
                minor.extend(np.linspace(i, (i + majstep),
                             (minor_num + 2))[1:-1])
        # elif (self.scale_type == 'logarithmic'):
        #     return NotImplementedError
        # elif (self.scale_type == 'power'):
        #     return NotImplementedError
        return major, minor, labels","1. Use `np.clip` to limit the values of `major` and `minor` to the range of `self.axis.domain`.
2. Use `np.unique` to remove duplicate values from `labels`.
3. Sanitize user input to `self.axis.domain` to prevent injection attacks."
"    def __init__(self, axis):
        self.axis = axis","1. Use `assert` statements to validate the input arguments.
2. Encrypt the data before storing it in the object.
3. Use a secure random number generator to generate the object's ID."
"    def handle_import_data(self, data):
        """"""Import additional data for tuning

        Parameters
        ----------
        data:
            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'

        Raises
        ------
        AssertionError
            data doesn't have required key 'parameter' and 'value'
        """"""
        for entry in data:
            entry['value'] = json_tricks.loads(entry['value'])
        _completed_num = 0
        for trial_info in data:
            logger.info(""Importing data, current processing progress %s / %s"", _completed_num, len(data))
            _completed_num += 1
            assert ""parameter"" in trial_info
            _params = trial_info[""parameter""]
            assert ""value"" in trial_info
            _value = trial_info['value']
            if not _value:
                logger.info(""Useless trial data, value is %s, skip this trial data."", _value)
                continue
            budget_exist_flag = False
            barely_params = dict()
            for keys in _params:
                if keys == _KEY:
                    _budget = _params[keys]
                    budget_exist_flag = True
                else:
                    barely_params[keys] = _params[keys]
            if not budget_exist_flag:
                _budget = self.max_budget
                logger.info(""Set \\""TRIAL_BUDGET\\"" value to %s (max budget)"", self.max_budget)
            if self.optimize_mode is OptimizeMode.Maximize:
                reward = -_value
            else:
                reward = _value
            self.cg.new_result(loss=reward, budget=_budget, parameters=barely_params, update_model=True)
        logger.info(""Successfully import tuning data to BOHB advisor."")","1. Use `json.loads()` to parse the input data instead of `json_tricks.loads()`.
2. Validate the input data to ensure that it has the required keys.
3. Use `logger.exception()` to log any errors that occur."
"    def import_data(self, data):
        """"""
        Import additional data for tuning.

        Parameters
        ----------
        data : list of dict
            Each of which has at least two keys, ``parameter`` and ``value``.
        """"""
        _completed_num = 0
        for trial_info in data:
            self.logger.info(""Importing data, current processing progress %s / %s"", _completed_num, len(data))
            # simply validate data format
            assert ""parameter"" in trial_info
            _params = trial_info[""parameter""]
            assert ""value"" in trial_info
            _value = trial_info['value']
            if not _value:
                self.logger.info(""Useless trial data, value is %s, skip this trial data."", _value)
                continue
            # convert the keys in loguniform and categorical types
            valid_entry = True
            for key, value in _params.items():
                if key in self.loguniform_key:
                    _params[key] = np.log(value)
                elif key in self.categorical_dict:
                    if value in self.categorical_dict[key]:
                        _params[key] = self.categorical_dict[key].index(value)
                    else:
                        self.logger.info(""The value %s of key %s is not in search space."", str(value), key)
                        valid_entry = False
                        break
            if not valid_entry:
                continue
            # start import this data entry
            _completed_num += 1
            config = Configuration(self.cs, values=_params)
            if self.optimize_mode is OptimizeMode.Maximize:
                _value = -_value
            if self.first_one:
                self.smbo_solver.nni_smac_receive_first_run(config, _value)
                self.first_one = False
            else:
                self.smbo_solver.nni_smac_receive_runs(config, _value)
        self.logger.info(""Successfully import data to smac tuner, total data: %d, imported data: %d."", len(data), _completed_num)","1. Use `assert` statements to validate data format.
2. Check if the value of a categorical variable is in the search space.
3. Use `nni_smac_receive_first_run` and `nni_smac_receive_runs` methods to import data."
"def _pack_parameter(parameter_id, params, customized=False, trial_job_id=None, parameter_index=None):
    _trial_params[parameter_id] = params
    ret = {
        'parameter_id': parameter_id,
        'parameter_source': 'customized' if customized else 'algorithm',
        'parameters': params
    }
    if trial_job_id is not None:
        ret['trial_job_id'] = trial_job_id
    if parameter_index is not None:
        ret['parameter_index'] = parameter_index
    else:
        ret['parameter_index'] = 0
    return json_tricks.dumps(ret)","1. Use `json.dumps()` with `default=json_tricks.dumps` to avoid JSON vulnerabilities.
2. Use `os.path.join()` to concatenate paths instead of string concatenation.
3. Use `pwd.getpwuid()` to get the username from the user ID instead of hardcoding it."
"def request_next_parameter():
    metric = json_tricks.dumps({
        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,
        'type': 'REQUEST_PARAMETER',
        'sequence': 0,
        'parameter_index': _param_index
    })
    send_metric(metric)","1. Use `json.dumps` instead of `json_tricks.dumps` to avoid a security vulnerability.
2. Use `os.environ.get` instead of `trial_env_vars.NNI_TRIAL_JOB_ID` to avoid a hard-coded secret.
3. Use `requests.post` instead of `send_metric` to send the metric to a secure endpoint."
"def report_intermediate_result(metric):
    """"""
    Reports intermediate result to NNI.

    Parameters
    ----------
    metric:
        serializable object.
    """"""
    global _intermediate_seq
    assert _params or trial_env_vars.NNI_PLATFORM is None, \\
        'nni.get_next_parameter() needs to be called before report_intermediate_result'
    metric = json_tricks.dumps({
        'parameter_id': _params['parameter_id'] if _params else None,
        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,
        'type': 'PERIODICAL',
        'sequence': _intermediate_seq,
        'value': metric
    })
    _intermediate_seq += 1
    platform.send_metric(metric)","1. Use `json.dumps` instead of `json_tricks.dumps` to avoid security vulnerabilities.
2. Sanitize the input data to prevent injection attacks.
3. Use proper error handling to prevent unexpected errors from crashing the system."
"def report_final_result(metric):
    """"""
    Reports final result to NNI.

    Parameters
    ----------
    metric:
        serializable object.
    """"""
    assert _params or trial_env_vars.NNI_PLATFORM is None, \\
        'nni.get_next_parameter() needs to be called before report_final_result'
    metric = json_tricks.dumps({
        'parameter_id': _params['parameter_id'] if _params else None,
        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,
        'type': 'FINAL',
        'sequence': 0,
        'value': metric
    })
    platform.send_metric(metric)","1. Use `json.dumps()` instead of `json_tricks.dumps()` to avoid security vulnerabilities.
2. Sanitize the input parameters to prevent malicious users from injecting code.
3. Use `platform.send_metric()` to send metrics to NNI instead of directly calling `requests.post()`."
"def parse_annotation_mutable_layers(code, lineno, nas_mode):
    """"""Parse the string of mutable layers in annotation.
    Return a list of AST Expr nodes
    code: annotation string (excluding '@')
    nas_mode: the mode of NAS
    """"""
    module = ast.parse(code)
    assert type(module) is ast.Module, 'internal error #1'
    assert len(module.body) == 1, 'Annotation mutable_layers contains more than one expression'
    assert type(module.body[0]) is ast.Expr, 'Annotation is not expression'
    call = module.body[0].value
    nodes = []
    mutable_id = 'mutable_block_' + str(lineno)
    mutable_layer_cnt = 0
    for arg in call.args:
        fields = {'layer_choice': False,
                  'fixed_inputs': False,
                  'optional_inputs': False,
                  'optional_input_size': False,
                  'layer_output': False}
        for k, value in zip(arg.keys, arg.values):
            if k.id == 'layer_choice':
                assert not fields['layer_choice'], 'Duplicated field: layer_choice'
                assert type(value) is ast.List, 'Value of layer_choice should be a list'
                call_funcs_keys = []
                call_funcs_values = []
                call_kwargs_values = []
                for call in value.elts:
                    assert type(call) is ast.Call, 'Element in layer_choice should be function call'
                    call_name = astor.to_source(call).strip()
                    call_funcs_keys.append(ast.Str(s=call_name))
                    call_funcs_values.append(call.func)
                    assert not call.args, 'Number of args without keyword should be zero'
                    kw_args = []
                    kw_values = []
                    for kw in call.keywords:
                        kw_args.append(ast.Str(s=kw.arg))
                        kw_values.append(kw.value)
                    call_kwargs_values.append(ast.Dict(keys=kw_args, values=kw_values))
                call_funcs = ast.Dict(keys=call_funcs_keys, values=call_funcs_values)
                call_kwargs = ast.Dict(keys=call_funcs_keys, values=call_kwargs_values)
                fields['layer_choice'] = True
            elif k.id == 'fixed_inputs':
                assert not fields['fixed_inputs'], 'Duplicated field: fixed_inputs'
                assert type(value) is ast.List, 'Value of fixed_inputs should be a list'
                fixed_inputs = value
                fields['fixed_inputs'] = True
            elif k.id == 'optional_inputs':
                assert not fields['optional_inputs'], 'Duplicated field: optional_inputs'
                assert type(value) is ast.List, 'Value of optional_inputs should be a list'
                var_names = [ast.Str(s=astor.to_source(var).strip()) for var in value.elts]
                optional_inputs = ast.Dict(keys=var_names, values=value.elts)
                fields['optional_inputs'] = True
            elif k.id == 'optional_input_size':
                assert not fields['optional_input_size'], 'Duplicated field: optional_input_size'
                assert type(value) is ast.Num or type(value) is ast.List, 'Value of optional_input_size should be a number or list'
                optional_input_size = value
                fields['optional_input_size'] = True
            elif k.id == 'layer_output':
                assert not fields['layer_output'], 'Duplicated field: layer_output'
                assert type(value) is ast.Name, 'Value of layer_output should be ast.Name type'
                layer_output = value
                fields['layer_output'] = True
            else:
                raise AssertionError('Unexpected field in mutable layer')
        # make call for this mutable layer
        assert fields['layer_choice'], 'layer_choice must exist'
        assert fields['layer_output'], 'layer_output must exist'
        mutable_layer_id = 'mutable_layer_' + str(mutable_layer_cnt)
        mutable_layer_cnt += 1
        target_call_attr = ast.Attribute(value=ast.Name(id='nni', ctx=ast.Load()), attr='mutable_layer', ctx=ast.Load())
        target_call_args = [ast.Str(s=mutable_id),
                            ast.Str(s=mutable_layer_id),
                            call_funcs,
                            call_kwargs]
        if fields['fixed_inputs']:
            target_call_args.append(fixed_inputs)
        else:
            target_call_args.append(ast.List(elts=[]))
        if fields['optional_inputs']:
            target_call_args.append(optional_inputs)
            assert fields['optional_input_size'], 'optional_input_size must exist when optional_inputs exists'
            target_call_args.append(optional_input_size)
        else:
            target_call_args.append(ast.Dict(keys=[], values=[]))
            target_call_args.append(ast.Num(n=0))
        target_call_args.append(ast.Str(s=nas_mode))
        if nas_mode in ['enas_mode', 'oneshot_mode', 'darts_mode']:
            target_call_args.append(ast.Name(id='tensorflow'))
        target_call = ast.Call(func=target_call_attr, args=target_call_args, keywords=[])
        node = ast.Assign(targets=[layer_output], value=target_call)
        nodes.append(node)
    return nodes","1. Use `ast.literal_eval` to parse the string of mutable layers instead of `ast.parse`. This will prevent malicious code from being executed.
2. Use `ast.copy_location` to copy the location of the original node to the new node. This will prevent the new node from being used to modify the original code.
3. Use `ast.fix_missing_locations` to fix any missing locations in the new node. This will ensure that the new node is valid Python code."
"    def _visit_string(self, node):
        string = node.value.s
        if string.startswith('@nni.'):
            self.annotated = True
        else:
            return node  # not an annotation, ignore it

        if string.startswith('@nni.get_next_parameter'):
            deprecated_message = ""'@nni.get_next_parameter' is deprecated in annotation due to inconvenience. Please remove this line in the trial code.""
            print_warning(deprecated_message)
            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()), args=[ast.Str(s='Get next parameter here...')], keywords=[]))

        if string.startswith('@nni.report_intermediate_result'):
            module = ast.parse(string[1:])
            arg = module.body[0].value.args[0]
            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()), args=[ast.Str(s='nni.report_intermediate_result: '), arg], keywords=[]))

        if string.startswith('@nni.report_final_result'):
            module = ast.parse(string[1:])
            arg = module.body[0].value.args[0]
            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()), args=[ast.Str(s='nni.report_final_result: '), arg], keywords=[]))

        if string.startswith('@nni.mutable_layers'):
            return parse_annotation_mutable_layers(string[1:], node.lineno)

        if string.startswith('@nni.variable') \\
                or string.startswith('@nni.function_choice'):
            self.stack[-1] = string[1:]  # mark that the next expression is annotated
            return None

        raise AssertionError('Unexpected annotation function')","1. Use `ast.literal_eval` instead of `eval` to parse strings.
2. Use `ast.parse` to parse strings and check the syntax.
3. Use `ast.dump` to debug the AST."
"def log_trial(args):
    ''''get trial log path'''
    trial_id_path_dict = {}
    nni_config = Config(get_config_filename(args))
    rest_port = nni_config.get_config('restServerPort')
    rest_pid = nni_config.get_config('restServerPid')
    if not detect_process(rest_pid):
        print_error('Experiment is not running...')
        return
    running, response = check_rest_server_quick(rest_port)
    if running:
        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)
        if response and check_response(response):
            content = json.loads(response.text)
            for trial in content:
                trial_id_path_dict[trial['id']] = trial['logPath']
    else:
        print_error('Restful server is not running...')
        exit(1)
    if args.id:
        if args.trial_id:
            if trial_id_path_dict.get(args.trial_id):
                print_normal('id:' + args.trial_id + ' path:' + trial_id_path_dict[args.trial_id])
            else:
                print_error('trial id is not valid.')
                exit(1)
        else:
            print_error('please specific the trial id.')
            exit(1)
    else:
        for key in trial_id_path_dict:
            print('id:' + key + ' path:' + trial_id_path_dict[key])","1. Use `requests.get()` with `verify=False` to disable SSL certificate verification.
2. Use `json.loads()` to parse the JSON response.
3. Use `print_normal()` to print the log path to the console."
"def send_email_sendgrid(sender, subject, message, recipients, image_png):
    import sendgrid as sendgrid_lib
    client = sendgrid_lib.SendGridClient(
        sendgrid().username, sendgrid().password, raise_errors=True)
    to_send = sendgrid_lib.Mail()
    to_send.add_to(recipients)
    to_send.set_from(sender)
    to_send.set_subject(subject)
    if email().format == 'html':
        to_send.set_html(message)
    else:
        to_send.set_text(message)
    if image_png:
        to_send.add_attachment(image_png)

    client.send(to_send)","1. Use `sendgrid().api_key` instead of `sendgrid().username` and `sendgrid().password`.
2. Set `sendgrid().raise_errors` to `False` to suppress errors.
3. Use `sendgrid_lib.Mail.add_bcc()` to add a blind carbon copy (bcc) recipient."
"    def find_spec(self, name, path, target=None):
        # If jvm is not started then we just check against the TLDs
        if not _jpype.isStarted():
            base = name.partition('.')[0]
            if not base in _JDOMAINS:
                return None
            raise ImportError(""Attempt to create Java package '%s' without jvm"" % name)

        # Check for aliases
        if name in _JDOMAINS:
            jname = _JDOMAINS[name]
            if not _jpype.isPackage(jname):
                raise ImportError(""Java package '%s' not found, requested by alias '%s'"" % (jname, name))
            ms = _ModuleSpec(name, self)
            ms._jname = jname
            return ms

        # Check if it is a TLD
        parts = name.rpartition('.')

        # Use the parent module to simplify name mangling
        if not parts[1] and _jpype.isPackage(parts[2]):
            ms = _ModuleSpec(name, self)
            ms._jname = name
            return ms

        if not parts[1] and not _jpype.isPackage(parts[0]):
            return None

        base = sys.modules.get(parts[0], None)
        if not base or not isinstance(base, _jpype._JPackage):
            return None

        # Support for external modules in java tree
        name = unwrap(name)
        for customizer in _CUSTOMIZERS:
            if customizer.canCustomize(name):
                return customizer.getSpec(name)

        # Using isPackage eliminates need for registering tlds
        if not hasattr(base, parts[2]):
            # If the base is a Java package and it wasn't found in the
            # package using getAttr, then we need to emit an error
            # so we produce a meaningful diagnositic.
            try:
                # Use forname because it give better diagnostics
                cls = _jpype.JClass(""java.lang.Class"").forName(name)
                return _jpype.JClass(cls)
            # Not found is acceptable
            except Exception as ex:
                raise ImportError(""Failed to import '%s'"" % name) from ex

        # Import the java module
        return _ModuleSpec(name, self)","1. Use for loops instead of recursion to prevent stack overflows.
2. Sanitize user input to prevent injection attacks.
3. Use proper error handling to prevent leaking sensitive information."
"    def find_spec(self, name, path, target=None):
        # If jvm is not started then we just check against the TLDs
        if not _jpype.isStarted():
            base = name.partition('.')[0]
            if not base in _JDOMAINS:
                return None
            raise ImportError(""Attempt to create java modules without jvm"")

        # Check if it is a TLD
        parts = name.rpartition('.')
        if not parts[1] and _jpype.isPackage(parts[2]):
            return _ModuleSpec(name, self)

        if not parts[1] and not _jpype.isPackage(parts[0]):
            return None

        base = sys.modules.get(parts[0], None)
        if not base or not isinstance(base, _jpype._JPackage):
            return None

        # Support for external modules in java tree
        name = unwrap(name)
        for customizer in _CUSTOMIZERS:
            if customizer.canCustomize(name):
                return customizer.getSpec(name)

        # Using isPackage eliminates need for registering tlds
        if not hasattr(base, parts[2]):
            # If the base is a Java package and it wasn't found in the
            # package using getAttr, then we need to emit an error
            # so we produce a meaningful diagnositic.
            try:
                # Use forname because it give better diagnostics
                cls = _java_lang_Class.forName(name)
                return _jpype.JClass(cls)
            # Not found is acceptable
            except Exception as ex:
                raise ImportError(""Failed to import '%s'"" % name) from ex

        # Import the java module
        return _ModuleSpec(name, self)","1. Use `sys.modules.get()` to check if the module exists before importing it.
2. Use `_java_lang_Class.forName()` to get a class object for the module.
3. Use `_jpype.JClass()` to create a Java class object for the module."
"def build(
    serviceName,
    version,
    http=None,
    discoveryServiceUrl=DISCOVERY_URI,
    developerKey=None,
    model=None,
    requestBuilder=HttpRequest,
    credentials=None,
    cache_discovery=True,
    cache=None,
    client_options=None,
    adc_cert_path=None,
    adc_key_path=None,
    num_retries=1,
):
    """"""Construct a Resource for interacting with an API.

  Construct a Resource object for interacting with an API. The serviceName and
  version are the names from the Discovery service.

  Args:
    serviceName: string, name of the service.
    version: string, the version of the service.
    http: httplib2.Http, An instance of httplib2.Http or something that acts
      like it that HTTP requests will be made through.
    discoveryServiceUrl: string, a URI Template that points to the location of
      the discovery service. It should have two parameters {api} and
      {apiVersion} that when filled in produce an absolute URI to the discovery
      document for that service.
    developerKey: string, key obtained from
      https://code.google.com/apis/console.
    model: googleapiclient.Model, converts to and from the wire format.
    requestBuilder: googleapiclient.http.HttpRequest, encapsulator for an HTTP
      request.
    credentials: oauth2client.Credentials or
      google.auth.credentials.Credentials, credentials to be used for
      authentication.
    cache_discovery: Boolean, whether or not to cache the discovery doc.
    cache: googleapiclient.discovery_cache.base.CacheBase, an optional
      cache object for the discovery documents.
    client_options: Mapping object or google.api_core.client_options, client
      options to set user options on the client. The API endpoint should be set
      through client_options. client_cert_source is not supported, client cert
      should be provided using client_encrypted_cert_source instead.
    adc_cert_path: str, client certificate file path to save the application
      default client certificate for mTLS. This field is required if you want to
      use the default client certificate.
    adc_key_path: str, client encrypted private key file path to save the
      application default client encrypted private key for mTLS. This field is
      required if you want to use the default client certificate.
    num_retries: Integer, number of times to retry discovery with
      randomized exponential backoff in case of intermittent/connection issues.

  Returns:
    A Resource object with methods for interacting with the service.

  Raises:
    google.auth.exceptions.MutualTLSChannelError: if there are any problems
      setting up mutual TLS channel.
  """"""
    params = {""api"": serviceName, ""apiVersion"": version}

    if http is None:
        discovery_http = build_http()
    else:
        discovery_http = http

    for discovery_url in (discoveryServiceUrl, V2_DISCOVERY_URI):
        requested_url = uritemplate.expand(discovery_url, params)

        try:
            content = _retrieve_discovery_doc(
                requested_url, discovery_http, cache_discovery, cache,
                developerKey, num_retries=num_retries
            )
            return build_from_document(
                content,
                base=discovery_url,
                http=http,
                developerKey=developerKey,
                model=model,
                requestBuilder=requestBuilder,
                credentials=credentials,
                client_options=client_options,
                adc_cert_path=adc_cert_path,
                adc_key_path=adc_key_path,
            )
        except HttpError as e:
            if e.resp.status == http_client.NOT_FOUND:
                continue
            else:
                raise e

    raise UnknownApiNameOrVersion(""name: %s  version: %s"" % (serviceName, version))","1. Use HTTPS instead of HTTP to protect data from being intercepted.
2. Use strong encryption algorithms and keys to protect data from being decrypted.
3. Use authentication and authorization to ensure that only authorized users can access data."
"def print_table(response, title):
  """"""Prints out a response table.

  Each row contains key(s), clicks, impressions, CTR, and average position.

  Args:
    response: The server response to be printed as a table.
    title: The title of the table.
  """"""
  print('\\n --' + title + ':')
  
  if 'rows' not in response:
    print('Empty response')
    return

  rows = response['rows']
  row_format = '{:<20}' + '{:>20}' * 4
  print(row_format.format('Keys', 'Clicks', 'Impressions', 'CTR', 'Position'))
  for row in rows:
    keys = ''
    # Keys are returned only if one or more dimensions are requested.
    if 'keys' in row:
      keys = u','.join(row['keys']).encode('utf-8')
    print(row_format.format(
        keys, row['clicks'], row['impressions'], row['ctr'], row['position']))","1. **Use `json.dumps()` to serialize the response data instead of `str()`.** This will prevent attackers from injecting malicious code into the response.
2. **Use `urllib.parse.quote_plus()` to escape the title parameter before sending it to the server.** This will prevent attackers from creating a malicious URL that could be used to exploit a cross-site scripting vulnerability.
3. **Use `os.path.join()` to construct the path to the file instead of concatenating strings.** This will prevent attackers from creating a malicious path that could be used to access a file that they should not be able to access."
"def init(argv, name, version, doc, filename, scope=None, parents=[], discovery_filename=None):
  """"""A common initialization routine for samples.

  Many of the sample applications do the same initialization, which has now
  been consolidated into this function. This function uses common idioms found
  in almost all the samples, i.e. for an API with name 'apiname', the
  credentials are stored in a file named apiname.dat, and the
  client_secrets.json file is stored in the same directory as the application
  main file.

  Args:
    argv: list of string, the command-line parameters of the application.
    name: string, name of the API.
    version: string, version of the API.
    doc: string, description of the application. Usually set to __doc__.
    file: string, filename of the application. Usually set to __file__.
    parents: list of argparse.ArgumentParser, additional command-line flags.
    scope: string, The OAuth scope used.
    discovery_filename: string, name of local discovery file (JSON). Use when discovery doc not available via URL.

  Returns:
    A tuple of (service, flags), where service is the service object and flags
    is the parsed command-line flags.
  """"""
  if scope is None:
    scope = 'https://www.googleapis.com/auth/' + name

  # Parser command-line arguments.
  parent_parsers = [tools.argparser]
  parent_parsers.extend(parents)
  parser = argparse.ArgumentParser(
      description=doc,
      formatter_class=argparse.RawDescriptionHelpFormatter,
      parents=parent_parsers)
  flags = parser.parse_args(argv[1:])

  # Name of a file containing the OAuth 2.0 information for this
  # application, including client_id and client_secret, which are found
  # on the API Access tab on the Google APIs
  # Console <http://code.google.com/apis/console>.
  client_secrets = os.path.join(os.path.dirname(filename),
                                'client_secrets.json')

  # Set up a Flow object to be used if we need to authenticate.
  flow = client.flow_from_clientsecrets(client_secrets,
      scope=scope,
      message=tools.message_if_missing(client_secrets))

  # Prepare credentials, and authorize HTTP object with them.
  # If the credentials don't exist or are invalid run through the native client
  # flow. The Storage object will ensure that if successful the good
  # credentials will get written back to a file.
  storage = file.Storage(name + '.dat')
  credentials = storage.get()
  if credentials is None or credentials.invalid:
    credentials = tools.run_flow(flow, storage, flags)
  http = credentials.authorize(http=build_http())

  if discovery_filename is None:
    # Construct a service object via the discovery service.
    service = discovery.build(name, version, http=http)
  else:
    # Construct a service object using a local discovery document file.
    with open(discovery_filename) as discovery_file:
      service = discovery.build_from_document(
          discovery_file.read(),
          base='https://www.googleapis.com/',
          http=http)
  return (service, flags)","1. Use HTTPS instead of HTTP to protect data from being intercepted.
2. Use OAuth 2.0 to authenticate users and grant them access to only the resources they need.
3. Use a secure storage mechanism to store sensitive data, such as user credentials."
"    def encode_block(self, obj):
        """"""
        Parameters
        ----------
        obj : AtomGroup or Universe
        """"""
        traj = obj.universe.trajectory
        ts = traj.ts

        try:
            molecule = ts.data['molecule']
        except KeyError:
            raise_from(NotImplementedError(
                ""MOL2Writer cannot currently write non MOL2 data""),
                None)

        # Need to remap atom indices to 1 based in this selection
        mapping = {a: i for i, a in enumerate(obj.atoms, start=1)}

        # Grab only bonds between atoms in the obj
        # ie none that extend out of it
        bondgroup = obj.bonds.atomgroup_intersection(obj, strict=True)
        bonds = sorted((b[0], b[1], b.order) for b in bondgroup)
        bond_lines = [""@<TRIPOS>BOND""]
        bond_lines.extend(""{0:>5} {1:>5} {2:>5} {3:>2}""
                          """".format(bid,
                                    mapping[atom1],
                                    mapping[atom2],
                                    order)
                          for bid, (atom1, atom2, order)in enumerate(
                                  bonds, start=1))
        bond_lines.append(""\\n"")
        bond_lines = ""\\n"".join(bond_lines)

        atom_lines = [""@<TRIPOS>ATOM""]
        atom_lines.extend(""{0:>4} {1:>4} {2:>13.4f} {3:>9.4f} {4:>9.4f}""
                          "" {5:>4} {6} {7} {8:>7.4f}""
                          """".format(mapping[a],
                                    a.name,
                                    a.position[0],
                                    a.position[1],
                                    a.position[2],
                                    a.type,
                                    a.resid,
                                    a.resname,
                                    a.charge)
                          for a in obj.atoms)
        atom_lines.append(""\\n"")
        atom_lines = ""\\n"".join(atom_lines)

        try:
            substructure = [""@<TRIPOS>SUBSTRUCTURE\\n""] + ts.data['substructure']
        except KeyError:
            substructure = """"

        check_sums = molecule[1].split()
        check_sums[0], check_sums[1] = str(len(obj.atoms)), str(len(bondgroup))
        molecule[1] = ""{0}\\n"".format("" "".join(check_sums))
        molecule.insert(0, ""@<TRIPOS>MOLECULE\\n"")

        return """".join(molecule) + atom_lines + bond_lines + """".join(substructure)","1. Use `enumerate` instead of `range` to avoid `index out of range` errors.
2. Use `f-strings` instead of `format` to avoid `string formatting errors`.
3. Use `type annotations` to make the code more readable and easier to maintain."
"    def __init__(self, atomgroup, reference=None, select='all',
                 groupselections=None, weights=None, tol_mass=0.1,
                 ref_frame=0, **kwargs):
        r""""""Parameters
        ----------
        atomgroup : AtomGroup or Universe
            Group of atoms for which the RMSD is calculated. If a trajectory is
            associated with the atoms then the computation iterates over the
            trajectory.
        reference : AtomGroup or Universe (optional)
            Group of reference atoms; if ``None`` then the current frame of
            `atomgroup` is used.
        select : str or dict or tuple (optional)
            The selection to operate on; can be one of:

            1. any valid selection string for
               :meth:`~MDAnalysis.core.groups.AtomGroup.select_atoms` that
               produces identical selections in `atomgroup` and `reference`; or

            2. a dictionary ``{'mobile': sel1, 'reference': sel2}`` where *sel1*
               and *sel2* are valid selection strings that are applied to
               `atomgroup` and `reference` respectively (the
               :func:`MDAnalysis.analysis.align.fasta2select` function returns such
               a dictionary based on a ClustalW_ or STAMP_ sequence alignment); or

            3. a tuple ``(sel1, sel2)``

            When using 2. or 3. with *sel1* and *sel2* then these selection strings
            are applied to `atomgroup` and `reference` respectively and should
            generate *groups of equivalent atoms*.  *sel1* and *sel2* can each also
            be a *list of selection strings* to generate a
            :class:`~MDAnalysis.core.groups.AtomGroup` with defined atom order as
            described under :ref:`ordered-selections-label`).

        groupselections : list (optional)
            A list of selections as described for `select`, with the difference
            that these selections are *always applied to the full universes*,
            i.e., ``atomgroup.universe.select_atoms(sel1)`` and
            ``reference.universe.select_atoms(sel2)``. Each selection describes
            additional RMSDs to be computed *after the structures have been
            superimposed* according to `select`. No additional fitting is
            performed.The output contains one additional column for each
            selection.

            .. Note:: Experimental feature. Only limited error checking
                      implemented.
        weights : {""mass"", ``None``} or array_like (optional)
             choose weights. With ``""mass""`` uses masses as weights; with ``None``
             weigh each atom equally. If a float array of the same length as
             `atomgroup` is provided, use each element of the `array_like` as a
             weight for the corresponding atom in `atomgroup`.
        tol_mass : float (optional)
             Reject match if the atomic masses for matched atoms differ by more
             than `tol_mass`.
        ref_frame : int (optional)
             frame index to select frame from `reference`
        verbose : bool (optional)
             Show detailed progress of the calculation if set to ``True``; the
             default is ``False``.

        Raises
        ------
        SelectionError
             If the selections from `atomgroup` and `reference` do not match.
        TypeError
             If `weights` is not of the appropriate type; see also
             :func:`MDAnalysis.lib.util.get_weights`
        ValueError
             If `weights` are not compatible with `atomgroup` (not the same
             length) or if it is not a 1D array (see
             :func:`MDAnalysis.lib.util.get_weights`).

             A :exc:`ValueError` is also raised if `weights` are not compatible
             with `groupselections`: only equal weights (``weights=None``) or
             mass-weighted (``weights=""mass""``) are supported for additional
             `groupselections`.

        Notes
        -----
        The root mean square deviation :math:`\\rho(t)` of a group of :math:`N`
        atoms relative to a reference structure as a function of time is
        calculated as

        .. math::

           \\rho(t) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N w_i \\left(\\mathbf{x}_i(t)
                                    - \\mathbf{x}_i^{\\text{ref}}\\right)^2}

        The weights :math:`w_i` are calculated from the input weights `weights`
        :math:`w'_i` as relative to the mean of the input weights:

        .. math::

           w_i = \\frac{w'_i}{\\langle w' \\rangle}

        The selected coordinates from `atomgroup` are optimally superimposed
        (translation and rotation) on the `reference` coordinates at each time step
        as to minimize the RMSD. Douglas Theobald's fast QCP algorithm
        [Theobald2005]_ is used for the rotational superposition and to calculate
        the RMSD (see :mod:`MDAnalysis.lib.qcprot` for implementation details).

        The class runs various checks on the input to ensure that the two atom
        groups can be compared. This includes a comparison of atom masses (i.e.,
        only the positions of atoms of the same mass will be considered to be
        correct for comparison). If masses should not be checked, just set
        `tol_mass` to a large value such as 1000.

        .. _ClustalW: http://www.clustal.org/
        .. _STAMP: http://www.compbio.dundee.ac.uk/manuals/stamp.4.2/


        See Also
        --------
        rmsd


        .. versionadded:: 0.7.7
        .. versionchanged:: 0.8
           `groupselections` added
        .. versionchanged:: 0.16.0
           Flexible weighting scheme with new `weights` keyword.
        .. deprecated:: 0.16.0
           Instead of ``mass_weighted=True`` (removal in 0.17.0) use new
           ``weights='mass'``; refactored to fit with AnalysisBase API
        .. versionchanged:: 0.17.0
           removed deprecated `mass_weighted` keyword; `groupselections`
           are *not* rotationally superimposed any more.
        .. versionchanged:: 1.0.0
           `filename` keyword was removed.

        """"""
        super(RMSD, self).__init__(atomgroup.universe.trajectory,
                                   **kwargs)
        self.atomgroup = atomgroup
        self.reference = reference if reference is not None else self.atomgroup

        select = process_selection(select)
        self.groupselections = ([process_selection(s) for s in groupselections]
                                if groupselections is not None else [])
        self.weights = weights
        self.tol_mass = tol_mass
        self.ref_frame = ref_frame

        self.ref_atoms = self.reference.select_atoms(*select['reference'])
        self.mobile_atoms = self.atomgroup.select_atoms(*select['mobile'])

        if len(self.ref_atoms) != len(self.mobile_atoms):
            err = (""Reference and trajectory atom selections do ""
                   ""not contain the same number of atoms: ""
                   ""N_ref={0:d}, N_traj={1:d}"".format(self.ref_atoms.n_atoms,
                                                      self.mobile_atoms.n_atoms))
            logger.exception(err)
            raise SelectionError(err)
        logger.info(""RMS calculation ""
                    ""for {0:d} atoms."".format(len(self.ref_atoms)))
        mass_mismatches = (np.absolute((self.ref_atoms.masses -
                                        self.mobile_atoms.masses)) >
                           self.tol_mass)

        if np.any(mass_mismatches):
            # diagnostic output:
            logger.error(""Atoms: reference | mobile"")
            for ar, at in zip(self.ref_atoms, self.mobile_atoms):
                if ar.name != at.name:
                    logger.error(""{0!s:>4} {1:3d} {2!s:>3} {3!s:>3} {4:6.3f}""
                                 ""|  {5!s:>4} {6:3d} {7!s:>3} {8!s:>3}""
                                 ""{9:6.3f}"".format(ar.segid, ar.resid,
                                                   ar.resname, ar.name,
                                                   ar.mass, at.segid, at.resid,
                                                   at.resname, at.name,
                                                   at.mass))
            errmsg = (""Inconsistent selections, masses differ by more than""
                      ""{0:f}; mis-matching atoms""
                      ""are shown above."".format(self.tol_mass))
            logger.error(errmsg)
            raise SelectionError(errmsg)
        del mass_mismatches

        # TODO:
        # - make a group comparison a class that contains the checks above
        # - use this class for the *select* group and the additional
        #   *groupselections* groups each a dict with reference/mobile
        self._groupselections_atoms = [
            {
                'reference': self.reference.universe.select_atoms(*s['reference']),
                'mobile': self.atomgroup.universe.select_atoms(*s['mobile']),
            }
            for s in self.groupselections]
        # sanity check
        for igroup, (sel, atoms) in enumerate(zip(self.groupselections,
                                                  self._groupselections_atoms)):
            if len(atoms['mobile']) != len(atoms['reference']):
                logger.exception('SelectionError: Group Selection')
                raise SelectionError(
                    ""Group selection {0}: {1} | {2}: Reference and trajectory ""
                    ""atom selections do not contain the same number of atoms: ""
                    ""N_ref={3}, N_traj={4}"".format(
                        igroup, sel['reference'], sel['mobile'],
                        len(atoms['reference']), len(atoms['mobile'])))

        # Explicitly check for ""mass"" because this option CAN
        # be used with groupselection. (get_weights() returns the mass array
        # for ""mass"")
        if not iterable(self.weights) and self.weights == ""mass"":
            pass
        else:
            self.weights = get_weights(self.mobile_atoms, self.weights)

        # cannot use arbitrary weight array (for superposition) with
        # groupselections because arrays will not match
        if (len(self.groupselections) > 0 and (
                iterable(self.weights) or self.weights not in (""mass"", None))):
            raise ValueError(""groupselections can only be combined with ""
                             ""weights=None or weights='mass', not a weight ""
                             ""array."")","1. Use `get_weights` to validate the weights input.
2. Check that the number of atoms in the reference and trajectory selections match.
3. Raise an error if the weights are not compatible with `groupselections`."
"    def _prepare(self):
        self._n_atoms = self.mobile_atoms.n_atoms

        if not iterable(self.weights) and self.weights == 'mass':
            self.weights = self.ref_atoms.masses
        if self.weights is not None:
            self.weights = np.asarray(self.weights, dtype=np.float64) / np.mean(self.weights)

        current_frame = self.reference.universe.trajectory.ts.frame

        try:
            # Move to the ref_frame
            # (coordinates MUST be stored in case the ref traj is advanced
            # elsewhere or if ref == mobile universe)
            self.reference.universe.trajectory[self.ref_frame]
            self._ref_com = self.ref_atoms.center(self.weights)
            # makes a copy
            self._ref_coordinates = self.ref_atoms.positions - self._ref_com
            if self._groupselections_atoms:
                self._groupselections_ref_coords64 = [(self.reference.
                    select_atoms(*s['reference']).
                    positions.astype(np.float64)) for s in
                    self.groupselections]
        finally:
            # Move back to the original frame
            self.reference.universe.trajectory[current_frame]

        self._ref_coordinates64 = self._ref_coordinates.astype(np.float64)

        if self._groupselections_atoms:
            # Only carry out a rotation if we want to calculate secondary
            # RMSDs.
            # R: rotation matrix that aligns r-r_com, x~-x~com
            #    (x~: selected coordinates, x: all coordinates)
            # Final transformed traj coordinates: x' = (x-x~_com)*R + ref_com
            self._rot = np.zeros(9, dtype=np.float64)  # allocate space
            self._R = self._rot.reshape(3, 3)
        else:
            self._rot = None

        self.rmsd = np.zeros((self.n_frames,
                              3 + len(self._groupselections_atoms)))

        self._pm.format = (""RMSD {rmsd:5.2f} A at frame ""
                           ""{step:5d}/{numsteps}  [{percentage:5.1f}%]"")
        self._mobile_coordinates64 = self.mobile_atoms.positions.copy().astype(np.float64)","1. Use `np.asarray()` to convert the input data to a NumPy array.
2. Use `np.mean()` to calculate the mean of the weights.
3. Use `np.float64()` to cast the coordinates to floating point numbers."
"    def _single_frame(self):
        mobile_com = self.mobile_atoms.center(self.weights).astype(np.float64)
        self._mobile_coordinates64[:] = self.mobile_atoms.positions
        self._mobile_coordinates64 -= mobile_com

        self.rmsd[self._frame_index, :2] = self._ts.frame, self._trajectory.time

        if self._groupselections_atoms:
            # superimpose structures: MDAnalysis qcprot needs Nx3 coordinate
            # array with float64 datatype (float32 leads to errors up to 1e-3 in
            # RMSD). Note that R is defined in such a way that it acts **to the
            # left** so that we can easily use broadcasting and save one
            # expensive numpy transposition.

            self.rmsd[self._frame_index, 2] = qcp.CalcRMSDRotationalMatrix(
                self._ref_coordinates64, self._mobile_coordinates64,
                self._n_atoms, self._rot, self.weights)

            self._R[:, :] = self._rot.reshape(3, 3)
            # Transform each atom in the trajectory (use inplace ops to
            # avoid copying arrays) (Marginally (~3%) faster than
            # ""ts.positions[:] = (ts.positions - x_com) * R + ref_com"".)
            self._ts.positions[:] -= mobile_com

            # R acts to the left & is broadcasted N times.
            self._ts.positions[:] = np.dot(self._ts.positions, self._R)
            self._ts.positions += self._ref_com

            # 2) calculate secondary RMSDs (without any further
            #    superposition)
            for igroup, (refpos, atoms) in enumerate(
                    zip(self._groupselections_ref_coords64,
                        self._groupselections_atoms), 3):
                self.rmsd[self._frame_index, igroup] = rmsd(
                    refpos, atoms['mobile'].positions,
                    weights=self.weights,
                    center=False, superposition=False)
        else:
            # only calculate RMSD by setting the Rmatrix to None (no need
            # to carry out the rotation as we already get the optimum RMSD)
            self.rmsd[self._frame_index, 2] = qcp.CalcRMSDRotationalMatrix(
                self._ref_coordinates64, self._mobile_coordinates64,
                self._n_atoms, None, self.weights)

        self._pm.rmsd = self.rmsd[self._frame_index, 2]","1. Use `np.float64` instead of `np.float32` to avoid errors in RMSD calculation.
2. Use `inplace ops` to avoid copying arrays.
3. Use `zip` to iterate over multiple iterables."
"    def _get_dh_pairs(self):
        """"""Finds donor-hydrogen pairs.

        Returns
        -------
        donors, hydrogens: AtomGroup, AtomGroup
            AtomGroups corresponding to all donors and all hydrogens. AtomGroups are ordered such that, if zipped, will
            produce a list of donor-hydrogen pairs.
        """"""

        # If donors_sel is not provided, use topology to find d-h pairs
        if not self.donors_sel:

            if len(self.u.bonds) == 0:
                raise Exception('Cannot assign donor-hydrogen pairs via topology as no bonded information is present. '
                                'Please either: load a topology file with bonded information; use the guess_bonds() '
                                'topology guesser; or set HydrogenBondAnalysis.donors_sel so that a distance cutoff '
                                'can be used.')

            hydrogens = self.u.select_atoms(self.hydrogens_sel)
            donors = sum(h.bonded_atoms[0] for h in hydrogens)

        # Otherwise, use d_h_cutoff as a cutoff distance
        else:

            hydrogens = self.u.select_atoms(self.hydrogens_sel)
            donors = self.u.select_atoms(self.donors_sel)
            donors_indices, hydrogen_indices = capped_distance(
                donors.positions,
                hydrogens.positions,
                max_cutoff=self.d_h_cutoff,
                box=self.u.dimensions,
                return_distances=False
            ).T

            donors = donors[donors_indices]
            hydrogens = hydrogens[hydrogen_indices]

        return donors, hydrogens","1. Use `self.u.bonds` to find donor-hydrogen pairs instead of `hydrogens.bonded_atoms[0]`.
2. Use `capped_distance()` with `return_distances=False` to avoid leaking information about the distances between atoms.
3. Check that `len(self.u.bonds)` is not 0 before using `hydrogens.bonded_atoms[0]` to find donors."
"    def _single_frame(self):

        box = self._ts.dimensions

        # Update donor-hydrogen pairs if necessary
        if self.update_selections:
            self._donors, self._hydrogens = self._get_dh_pairs()

        # find D and A within cutoff distance of one another
        # min_cutoff = 1.0 as an atom cannot form a hydrogen bond with itself
        d_a_indices, d_a_distances = capped_distance(
            self._donors.positions,
            self._acceptors.positions,
            max_cutoff=self.d_a_cutoff,
            min_cutoff=1.0,
            box=box,
            return_distances=True,
        )

        # Remove D-A pairs more than d_a_cutoff away from one another
        tmp_donors = self._donors[d_a_indices.T[0]]
        tmp_hydrogens = self._hydrogens[d_a_indices.T[0]]
        tmp_acceptors = self._acceptors[d_a_indices.T[1]]

        # Find D-H-A angles greater than d_h_a_angle_cutoff
        d_h_a_angles = np.rad2deg(
            calc_angles(
                tmp_donors.positions,
                tmp_hydrogens.positions,
                tmp_acceptors.positions,
                box=box
            )
        )
        hbond_indices = np.where(d_h_a_angles > self.d_h_a_angle)[0]

        # Retrieve atoms, distances and angles of hydrogen bonds
        hbond_donors = tmp_donors[hbond_indices]
        hbond_hydrogens = tmp_hydrogens[hbond_indices]
        hbond_acceptors = tmp_acceptors[hbond_indices]
        hbond_distances = d_a_distances[hbond_indices]
        hbond_angles = d_h_a_angles[hbond_indices]

        # Store data on hydrogen bonds found at this frame
        self.hbonds[0].extend(np.full_like(hbond_donors, self._ts.frame))
        self.hbonds[1].extend(hbond_donors.ids)
        self.hbonds[2].extend(hbond_hydrogens.ids)
        self.hbonds[3].extend(hbond_acceptors.ids)
        self.hbonds[4].extend(hbond_distances)
        self.hbonds[5].extend(hbond_angles)","1. Use `np.unique` to remove duplicate entries in `hbond_indices`.
2. Use `np.clip` to ensure that `d_h_a_angles` is between 0 and 180 degrees.
3. Use `np.where` to check if `d_h_a_angles` is greater than `self.d_h_a_angle`."
"    def __init__(self, universe, selection1, selection2, t0, tf, dtmax,
                 nproc=1):
        self.universe = universe
        self.selection1 = selection1
        self.selection2 = selection2
        self.t0 = t0
        self.tf = tf - 1
        self.dtmax = dtmax
        self.nproc = nproc
        self.timeseries = None","1. Use `type()` to check the data type of the input parameters.
2. Use `assert()` to validate the input parameters.
3. Use `logging` to log the security-related events."
"    def run(self, **kwargs):
        """"""Analyze trajectory and produce timeseries""""""
        h_list = []
        i = 0
        if (self.nproc > 1):
            while i < len(self.universe.trajectory):
                jobs = []
                k = i
                for j in range(self.nproc):
                    # start
                    print(""ts="", i + 1)
                    if i >= len(self.universe.trajectory):
                        break
                    conn_parent, conn_child = multiprocessing.Pipe(False)
                    while True:
                        try:
                            # new thread
                            jobs.append(
                                (multiprocessing.Process(
                                    target=self._HBA,
                                    args=(self.universe.trajectory[i],
                                          conn_child, self.universe,
                                          self.selection1, self.selection2,)),
                                 conn_parent))
                            break
                        except:
                            print(""error in jobs.append"")
                    jobs[j][0].start()
                    i = i + 1

                for j in range(self.nproc):
                    if k >= len(self.universe.trajectory):
                        break
                    rec01 = jobs[j][1]
                    received = rec01.recv()
                    h_list.append(received)
                    jobs[j][0].join()
                    k += 1
            self.timeseries = self._getGraphics(
                h_list, 0, self.tf - 1, self.dtmax)
        else:
            h_list = MDAnalysis.analysis.hbonds.HydrogenBondAnalysis(self.universe,
                                                                     self.selection1,
                                                                     self.selection2,
                                                                     distance=3.5,
                                                                     angle=120.0)
            h_list.run(**kwargs)
            self.timeseries = self._getGraphics(
                h_list.timeseries, self.t0, self.tf, self.dtmax)","1. Use multiprocessing.Pool instead of multiprocessing.Process to avoid creating new threads for each frame.
2. Use multiprocessing.Queue instead of multiprocessing.Pipe to avoid deadlocks.
3. Use MDAnalysis.analysis.hbonds.HydrogenBondAnalysis.timeseries instead of self._getGraphics to avoid creating duplicate dataframes."
"def _determine_method(reference, configuration, max_cutoff, min_cutoff=None,
                      box=None, method=None):
    """"""Guesses the fastest method for capped distance calculations based on the
    size of the coordinate sets and the relative size of the target volume.

    Parameters
    ----------
    reference : numpy.ndarray
        Reference coordinate array with shape ``(3,)`` or ``(n, 3)``.
    configuration : numpy.ndarray
        Configuration coordinate array with shape ``(3,)`` or ``(m, 3)``.
    max_cutoff : float
        Maximum cutoff distance between `reference` and `configuration`
        coordinates.
    min_cutoff : float, optional
        Minimum cutoff distance between `reference` and `configuration`
        coordinates.
    box : numpy.ndarray, None (default None)
        The unitcell dimensions of the system, which can be orthogonal or
        triclinic and must be provided in the same format as returned by
        :attr:`MDAnalysis.coordinates.base.Timestep.dimensions`:\\n
        ``[lx, ly, lz, alpha, beta, gamma]``.
    method : {'bruteforce', 'nsgrid', 'pkdtree', None} (default None)
        Keyword to override the automatic guessing of the employed search
        method.

    Returns
    -------
    function : callable
        The function implementing the guessed (or deliberatly chosen) method.
    """"""
    methods = {'bruteforce': _bruteforce_capped,
               'pkdtree': _pkdtree_capped,
               'nsgrid': _nsgrid_capped}

    if method is not None:
        return methods[method.lower()]

    if len(reference) < 10 or len(configuration) < 10:
        return methods['bruteforce']
    elif len(reference) * len(configuration) >= 1e8:
        # CAUTION : for large datasets, shouldnt go into 'bruteforce'
        # in any case. Arbitrary number, but can be characterized
        return methods['nsgrid']
    else:
        if box is None:
            min_dim = np.array([reference.min(axis=0),
                                configuration.min(axis=0)])
            max_dim = np.array([reference.max(axis=0),
                                configuration.max(axis=0)])
            size = max_dim.max(axis=0) - min_dim.min(axis=0)
        elif np.all(box[3:] == 90.0):
            size = box[:3]
        else:
            tribox = triclinic_vectors(box)
            size = tribox.max(axis=0) - tribox.min(axis=0)
        if np.any(max_cutoff > 0.3*size):
            return methods['bruteforce']
        else:
            return methods['nsgrid']","1. Use `np.array_equal()` instead of `np.array()` to compare two arrays.
2. Use `np.unique()` to remove duplicate values from an array.
3. Use `np.random.choice()` to generate random numbers instead of hard-coding them."
"    def parse(self, **kwargs):
        """"""Parse PSF file into Topology

        Returns
        -------
        MDAnalysis *Topology* object
        """"""
        # Open and check psf validity
        with openany(self.filename) as psffile:
            header = next(psffile)
            if not header.startswith(""PSF""):
                err = (""{0} is not valid PSF file (header = {1})""
                       """".format(self.filename, header))
                logger.error(err)
                raise ValueError(err)
            header_flags = header[3:].split()

            if ""NAMD"" in header_flags:
                self._format = ""NAMD""        # NAMD/VMD
            elif ""EXT"" in header_flags:
                self._format = ""EXTENDED""    # CHARMM
            else:
                self._format = ""STANDARD""    # CHARMM

            next(psffile)
            title = next(psffile).split()
            if not (title[1] == ""!NTITLE""):
                err = ""{0} is not a valid PSF file"".format(psffile.name)
                logger.error(err)
                raise ValueError(err)
            # psfremarks = [psffile.next() for i in range(int(title[0]))]
            for _ in range(int(title[0])):
                next(psffile)
            logger.debug(""PSF file {0}: format {1}""
                         """".format(psffile.name, self._format))

            # Atoms first and mandatory
            top = self._parse_sec(
                psffile, ('NATOM', 1, 1, self._parseatoms))
            # Then possibly other sections
            sections = (
                #(""atoms"", (""NATOM"", 1, 1, self._parseatoms)),
                (Bonds, (""NBOND"", 2, 4, self._parsesection)),
                (Angles, (""NTHETA"", 3, 3, self._parsesection)),
                (Dihedrals, (""NPHI"", 4, 2, self._parsesection)),
                (Impropers, (""NIMPHI"", 4, 2, self._parsesection)),
                #(""donors"", (""NDON"", 2, 4, self._parsesection)),
                #(""acceptors"", (""NACC"", 2, 4, self._parsesection))
            )

            try:
                for attr, info in sections:
                    next(psffile)
                    top.add_TopologyAttr(
                        attr(self._parse_sec(psffile, info)))
            except StopIteration:
                # Reached the end of the file before we expected
                pass

        return top","1. Use `type()` to check the type of input parameters.
2. Use `assert()` to validate input parameters.
3. Use `logging.exception()` to log errors."
"    def select_atoms(self, sel, *othersel, **selgroups):
        """"""Select :class:`Atoms<Atom>` using a selection string.

        Returns an :class:`AtomGroup` with :class:`Atoms<Atom>` sorted according
        to their index in the topology (this is to ensure that there are no
        duplicates, which can happen with complicated selections).

        Raises
        ------
        TypeError
            If the arbitrary groups passed are not of type
            :class:`MDAnalysis.core.groups.AtomGroup`

        Examples
        --------
        All simple selection listed below support multiple arguments which are
        implicitly combined with an or operator. For example

           >>> sel = universe.select_atoms('resname MET GLY')

        is equivalent to

           >>> sel = universe.select_atoms('resname MET or resname GLY')

        Will select all atoms with a residue name of either MET or GLY.

        Subselections can be grouped with parentheses.

           >>> sel = universe.select_atoms(""segid DMPC and not ( name H* O* )"")
           >>> sel
           <AtomGroup with 3420 atoms>


        Existing :class:`AtomGroup` objects can be passed as named arguments,
        which will then be available to the selection parser.

           >>> universe.select_atoms(""around 10 group notHO"", notHO=sel)
           <AtomGroup with 1250 atoms>

        Selections can be set to update automatically on frame change, by
        setting the `updating` keyword argument to `True`.  This will return
        a :class:`UpdatingAtomGroup` which can represent the solvation shell
        around another object.

           >>> universe.select_atoms(""resname SOL and around 2.0 protein"", updating=True)
           <Updating AtomGroup with 100 atoms>

        Notes
        -----

        If exact ordering of atoms is required (for instance, for
        :meth:`~AtomGroup.angle` or :meth:`~AtomGroup.dihedral` calculations)
        then one supplies selections *separately* in the required order. Also,
        when multiple :class:`AtomGroup` instances are concatenated with the
        ``+`` operator, then the order of :class:`Atom` instances is preserved
        and duplicates are *not* removed.


        See Also
        --------
        :ref:`selection-commands-label` for further details and examples.


        .. rubric:: Selection syntax


        The selection parser understands the following CASE SENSITIVE
        *keywords*:

        **Simple selections**

            protein, backbone, nucleic, nucleicbackbone
                selects all atoms that belong to a standard set of residues;
                a protein is identfied by a hard-coded set of residue names so
                it  may not work for esoteric residues.
            segid *seg-name*
                select by segid (as given in the topology), e.g. ``segid 4AKE``
                or ``segid DMPC``
            resid *residue-number-range*
                resid can take a single residue number or a range of numbers. A
                range consists of two numbers separated by a colon (inclusive)
                such as ``resid 1:5``. A residue number (""resid"") is taken
                directly from the topology.
                If icodes are present in the topology, then these will be
                taken into account.  Ie 'resid 163B' will only select resid
                163 with icode B while 'resid 163' will select only residue 163.
                Range selections will also respect icodes, so 'resid 162-163B'
                will select all residues in 162 and those in 163 up to icode B.
            resnum *resnum-number-range*
                resnum is the canonical residue number; typically it is set to
                the residue id in the original PDB structure.
            resname *residue-name*
                select by residue name, e.g. ``resname LYS``
            name *atom-name*
                select by atom name (as given in the topology). Often, this is
                force field dependent. Example: ``name CA`` (for C&alpha; atoms)
                or ``name OW`` (for SPC water oxygen)
            type *atom-type*
                select by atom type; this is either a string or a number and
                depends on the force field; it is read from the topology file
                (e.g. the CHARMM PSF file contains numeric atom types). It has
                non-sensical values when a PDB or GRO file is used as a topology
            atom *seg-name*  *residue-number*  *atom-name*
                a selector for a single atom consisting of segid resid atomname,
                e.g. ``DMPC 1 C2`` selects the C2 carbon of the first residue of
                the DMPC segment
            altloc *alternative-location*
                a selection for atoms where alternative locations are available,
                which is often the case with high-resolution crystal structures
                e.g. `resid 4 and resname ALA and altloc B` selects only the
                atoms of ALA-4 that have an altloc B record.
            moltype *molecule-type*
                select by molecule type, e.g. ``moltype Protein_A``. At the
                moment, only the TPR format defines the molecule type.

        **Boolean**

            not
                all atoms not in the selection, e.g. ``not protein`` selects
                all atoms that aren't part of a protein

            and, or
                combine two selections according to the rules of boolean
                algebra, e.g. ``protein and not resname ALA LYS``
                selects all atoms that belong to a protein, but are not in a
                lysine or alanine residue

        **Geometric**

            around *distance*  *selection*
                selects all atoms a certain cutoff away from another selection,
                e.g. ``around 3.5 protein`` selects all atoms not belonging to
                protein that are within 3.5 Angstroms from the protein
            point *x* *y* *z*  *distance*
                selects all atoms within a cutoff of a point in space, make sure
                coordinate is separated by spaces,
                e.g. ``point 5.0 5.0 5.0  3.5`` selects all atoms within 3.5
                Angstroms of the coordinate (5.0, 5.0, 5.0)
            prop [abs] *property*  *operator*  *value*
                selects atoms based on position, using *property*  **x**, **y**,
                or **z** coordinate. Supports the **abs** keyword (for absolute
                value) and the following *operators*: **<, >, <=, >=, ==, !=**.
                For example, ``prop z >= 5.0`` selects all atoms with z
                coordinate greater than 5.0; ``prop abs z <= 5.0`` selects all
                atoms within -5.0 <= z <= 5.0.
            sphzone *radius* *selection*
                Selects all atoms that are within *radius* of the center of
                geometry of *selection*
            sphlayer *inner radius* *outer radius* *selection*
                Similar to sphzone, but also excludes atoms that are within
                *inner radius* of the selection COG
            cyzone *externalRadius* *zMax* *zMin* *selection*
                selects all atoms within a cylindric zone centered in the
                center of geometry (COG) of a given selection,
                e.g. ``cyzone 15 4 -8 protein and resid 42`` selects the
                center of geometry of protein and resid 42, and creates a
                cylinder of external radius 15 centered on the COG. In z, the
                cylinder extends from 4 above the COG to 8 below. Positive
                values for *zMin*, or negative ones for *zMax*, are allowed.
            cylayer *innerRadius* *externalRadius* *zMax* *zMin* *selection*
                selects all atoms within a cylindric layer centered in the
                center of geometry (COG) of a given selection,
                e.g. ``cylayer 5 10 10 -8 protein`` selects the center of
                geometry of protein, and creates a cylindrical layer of inner
                radius 5, external radius 10 centered on the COG. In z, the
                cylinder extends from 10 above the COG to 8 below. Positive
                values for *zMin*, or negative ones for *zMax*, are allowed.

        **Connectivity**

            byres *selection*
                selects all atoms that are in the same segment and residue as
                selection, e.g. specify the subselection after the byres keyword
            bonded *selection*
                selects all atoms that are bonded to selection
                eg: ``select name H and bonded name O`` selects only hydrogens
                bonded to oxygens

        **Index**

            bynum *index-range*
                selects all atoms within a range of (1-based) inclusive indices,
                e.g. ``bynum 1`` selects the first atom in the universe;
                ``bynum 5:10`` selects atoms 5 through 10 inclusive. All atoms
                in the :class:`~MDAnalysis.core.universe.Universe` are
                consecutively numbered, and the index runs from 1 up to the
                total number of atoms.

        **Preexisting selections**

            group `group-name`
                selects the atoms in the :class:`AtomGroup` passed to the
                function as an argument named `group-name`. Only the atoms
                common to `group-name` and the instance
                :meth:`~MDAnalysis.core.groups.AtomGroup.select_atoms`
                was called from will be considered, unless ``group`` is
                preceded by the ``global`` keyword. `group-name` will be
                included in the parsing just by comparison of atom indices.
                This means that it is up to the user to make sure the
                `group-name` group was defined in an appropriate
                :class:`~MDAnalysis.core.universe.Universe`.

            global *selection*
                by default, when issuing
                :meth:`~MDAnalysis.core.groups.AtomGroup.select_atoms` from an
                :class:`~MDAnalysis.core.groups.AtomGroup`, selections and
                subselections are returned intersected with the atoms of that
                instance. Prefixing a selection term with ``global`` causes its
                selection to be returned in its entirety.  As an example, the
                ``global`` keyword allows for
                ``lipids.select_atoms(""around 10 global protein"")`` --- where
                ``lipids`` is a group that does not contain any proteins. Were
                ``global`` absent, the result would be an empty selection since
                the ``protein`` subselection would itself be empty. When issuing
                :meth:`~MDAnalysis.core.groups.AtomGroup.select_atoms` from a
                :class:`~MDAnalysis.core.universe.Universe`, ``global`` is
                ignored.

        **Dynamic selections**
            If :meth:`~MDAnalysis.core.groups.AtomGroup.select_atoms` is
            invoked with named argument `updating` set to `True`, an
            :class:`~MDAnalysis.core.groups.UpdatingAtomGroup` instance will be
            returned, instead of a regular
            :class:`~MDAnalysis.core.groups.AtomGroup`. It behaves just like
            the latter, with the difference that the selection expressions are
            re-evaluated every time the trajectory frame changes (this happens
            lazily, only when the
            :class:`~MDAnalysis.core.groups.UpdatingAtomGroup` is accessed so
            that there is no redundant updating going on).
            Issuing an updating selection from an already updating group will
            cause later updates to also reflect the updating of the base group.
            A non-updating selection or a slicing operation made on an
            :class:`~MDAnalysis.core.groups.UpdatingAtomGroup` will return a
            static :class:`~MDAnalysis.core.groups.AtomGroup`, which will no
            longer update across frames.


        .. versionchanged:: 0.7.4 Added *resnum* selection.
        .. versionchanged:: 0.8.1 Added *group* and *fullgroup* selections.
        .. deprecated:: 0.11 The use of *fullgroup* has been deprecated in favor
            of the equivalent *global group* selections.
        .. versionchanged:: 0.13.0 Added *bonded* selection.
        .. versionchanged:: 0.16.0 Resid selection now takes icodes into account
            where present.
        .. versionchanged:: 0.16.0 Updating selections now possible by setting
            the `updating` argument.
        .. versionchanged:: 0.17.0 Added *moltype* and *molnum* selections.
        .. versionchanged:: 0.19.0
           Added strict type checking for passed groups.
           Added periodic kwarg (default True)
        .. versionchanged:: 0.19.2
           Empty sel string now returns an empty Atom group.
        """"""

        if not sel:
            warnings.warn(""Empty string to select atoms, empty group returned."",
                          UserWarning)
            return self[[]]

        # once flags removed, replace with default=True
        periodic = selgroups.pop('periodic', flags['use_periodic_selections'])

        updating = selgroups.pop('updating', False)
        sel_strs = (sel,) + othersel

        for group, thing in selgroups.items():
            if not isinstance(thing, AtomGroup):
                raise TypeError(""Passed groups must be AtomGroups. ""
                                ""You provided {} for group '{}'"".format(
                                    thing.__class__.__name__, group))

        selections = tuple((selection.Parser.parse(s, selgroups, periodic=periodic)
                            for s in sel_strs))
        if updating:
            atomgrp = UpdatingAtomGroup(self, selections, sel_strs)
        else:
            # Apply the first selection and sum to it
            atomgrp = sum([sel.apply(self) for sel in selections[1:]],
                          selections[0].apply(self))
        return atomgrp","1. Use `typing` to specify the types of arguments.
2. Use `warnings.warn()` to notify the user when an empty string is passed to the function.
3. Use `selgroups.pop()` to remove the `updating` argument from the dictionary."
"    def atoms(self):
        """"""An :class:`AtomGroup` of :class:`Atoms<Atom>` present in this
        :class:`ResidueGroup`.

        The :class:`Atoms<Atom>` are ordered locally by :class:`Residue` in the
        :class:`ResidueGroup`.  Duplicates are *not* removed.
        """"""
        ag = self.universe.atoms[np.concatenate(self.indices)]
        # If the ResidueGroup is known to be unique, this also holds for the
        # atoms therein, since atoms can only belong to one residue at a time.
        # On the contrary, if the ResidueGroup is not unique, this does not
        # imply non-unique atoms, since residues might be empty.
        try:
            if self._cache['isunique']:
                ag._cache['isunique'] = True
                ag._cache['unique'] = ag
        except KeyError:
            pass
        return ag","1. Use `np.unique` to remove duplicates in the `atoms` list.
2. Check if the `ResidueGroup` is unique before returning the `atoms` list.
3. Handle the `KeyError` exception in the `_cache` dictionary."
"    def atoms(self):
        """"""An :class:`AtomGroup` of :class:`Atoms<Atom>` present in this
        :class:`SegmentGroup`.

        The :class:`Atoms<Atom>` are ordered locally by :class:`Residue`, which
        are further ordered by :class:`Segment` in the :class:`SegmentGroup`.
        Duplicates are *not* removed.
        """"""
        ag = self.universe.atoms[np.concatenate(self.indices)]
        # If the SegmentGroup is known to be unique, this also holds for the
        # residues therein, and thus, also for the atoms in those residues.
        # On the contrary, if the SegmentGroup is not unique, this does not
        # imply non-unique atoms, since segments or residues might be empty.
        try:
            if self._cache['isunique']:
                ag._cache['isunique'] = True
                ag._cache['unique'] = ag
        except KeyError:
            pass
        return ag","1. Use `np.unique()` to remove duplicates in the `atoms()` method.
2. Check for `KeyError` in the `_cache['isunique']` and `_cache['unique']` keys.
3. Use `self.universe.atoms[np.concatenate(self.indices)]` to get the atoms instead of directly accessing the `self.indices` attribute."
"    def check_slice_indices(self, start, stop, step):
        """"""Check frame indices are valid and clip to fit trajectory.

        The usage follows standard Python conventions for :func:`range` but see
        the warning below.

        Parameters
        ----------
        start : int or None
          Starting frame index (inclusive). ``None`` corresponds to the default
          of 0, i.e., the initial frame.
        stop : int or None
          Last frame index (exclusive). ``None`` corresponds to the default
          of n_frames, i.e., it includes the last frame of the trajectory.
        step : int or None
          step size of the slice, ``None`` corresponds to the default of 1, i.e,
          include every frame in the range `start`, `stop`.

        Returns
        -------
        start, stop, step : tuple (int, int, int)
          Integers representing the slice

        Warning
        -------
        The returned values `start`, `stop` and `step` give the expected result
        when passed in :func:`range` but gives unexpected behavior when passed
        in a :class:`slice` when ``stop=None`` and ``step=-1``

        This can be a problem for downstream processing of the output from this
        method. For example, slicing of trajectories is implemented by passing
        the values returned by :meth:`check_slice_indices` to :func:`range` ::

          range(start, stop, step)

        and using them as the indices to randomly seek to. On the other hand,
        in :class:`MDAnalysis.analysis.base.AnalysisBase` the values returned
        by :meth:`check_slice_indices` are used to splice the trajectory by
        creating a :class:`slice` instance ::

          slice(start, stop, step)

        This creates a discrepancy because these two lines are not equivalent::

            range(10, -1, -1)             # [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
            range(10)[slice(10, -1, -1)]  # []

        """"""

        slice_dict = {'start': start, 'stop': stop, 'step': step}
        for varname, var in slice_dict.items():
            if isinstance(var, numbers.Integral):
                slice_dict[varname] = int(var)
            elif (var is None):
                pass
            else:
                raise TypeError(""{0} is not an integer"".format(varname))

        start = slice_dict['start']
        stop = slice_dict['stop']
        step = slice_dict['step']

        if step == 0:
            raise ValueError(""Step size is zero"")

        nframes = len(self)
        step = step or 1

        if start is None:
            start = 0 if step > 0 else nframes - 1
        elif start < 0:
            start += nframes
        if start < 0:
            start = 0

        if step < 0 and start > nframes:
            start = nframes - 1

        if stop is None:
            stop = nframes if step > 0 else -1
        elif stop < 0:
            stop += nframes

        if step > 0 and stop > nframes:
            stop = nframes

        return start, stop, step","1. Use `type()` to check if the input is an integer.
2. Raise a `TypeError` if the input is not an integer.
3. Use `int()` to convert the input to an integer."
"    def _single_frame(self):
        index = self._ts.frame
        mobile_com = self.mobile_atoms.center(self._weights)
        mobile_coordinates = self.mobile_atoms.positions - mobile_com
        mobile_atoms, self.rmsd[index] = _fit_to(mobile_coordinates,
                                                 self._ref_coordinates,
                                                 self.mobile,
                                                 mobile_com,
                                                 self._ref_com, self._weights)
        # write whole aligned input trajectory system
        self._writer.write(mobile_atoms)","1. Use `np.array_split` to split the trajectory into smaller chunks and process them individually. This will reduce the amount of data that is processed at once and make it less likely that an attacker can exploit a buffer overflow vulnerability.
2. Use `np.random.seed()` to set the seed for the random number generator. This will make it more difficult for an attacker to predict the values of the random numbers used in the algorithm and make it more difficult to exploit a timing attack.
3. Use `np.seterr()` to set the error handler for floating-point operations. This will prevent an attacker from being able to trigger an exception and cause the program to crash."
"    def __init__(self, atomgroup, reference=None, select='all',
                 groupselections=None, filename=""rmsd.dat"",
                 weights=None, tol_mass=0.1, ref_frame=0, **kwargs):
        r""""""
        Parameters
        ----------
        atomgroup : AtomGroup or Universe
            Group of atoms for which the RMSD is calculated. If a trajectory is
            associated with the atoms then the computation iterates over the
            trajectory.
        reference : AtomGroup or Universe (optional)
            Group of reference atoms; if ``None`` then the current frame of
            `atomgroup` is used.
        select : str or dict or tuple (optional)
            The selection to operate on; can be one of:

            1. any valid selection string for
               :meth:`~MDAnalysis.core.groups.AtomGroup.select_atoms` that
               produces identical selections in `atomgroup` and `reference`; or

            2. a dictionary ``{'mobile': sel1, 'reference': sel2}`` where *sel1*
               and *sel2* are valid selection strings that are applied to
               `atomgroup` and `reference` respectively (the
               :func:`MDAnalysis.analysis.align.fasta2select` function returns such
               a dictionary based on a ClustalW_ or STAMP_ sequence alignment); or

            3. a tuple ``(sel1, sel2)``

            When using 2. or 3. with *sel1* and *sel2* then these selection strings
            are applied to `atomgroup` and `reference` respectively and should
            generate *groups of equivalent atoms*.  *sel1* and *sel2* can each also
            be a *list of selection strings* to generate a
            :class:`~MDAnalysis.core.groups.AtomGroup` with defined atom order as
            described under :ref:`ordered-selections-label`).

        groupselections : list (optional)
            A list of selections as described for `select`. Each selection
            describes additional RMSDs to be computed *after the
            structures have been superimposed* according to `select`. No
            additional fitting is performed.The output contains one
            additional column for each selection.

            .. Note:: Experimental feature. Only limited error checking
                      implemented.

        start : int (optional)
            starting frame, default None becomes 0.
        stop : int (optional)
            Frame index to stop analysis. Default: None becomes
            n_frames. Iteration stops *before* this frame number,
            which means that the trajectory would be read until the end.
        step : int (optional)
            step between frames, default ``None`` becomes 1.
        filename : str (optional)
            write RMSD into file with :meth:`RMSD.save`
        weights : {""mass"", ``None``} or array_like (optional)
             choose weights. With ``""mass""`` uses masses as weights; with ``None``
             weigh each atom equally. If a float array of the same length as
             `atomgroup` is provided, use each element of the `array_like` as a
             weight for the corresponding atom in `atomgroup`.
        tol_mass : float (optional)
             Reject match if the atomic masses for matched atoms differ by more
             than `tol_mass`.
        ref_frame : int (optional)
             frame index to select frame from `reference`

        Raises
        ------
        SelectionError
             If the selections from `atomgroup` and `reference` do not match.
        TypeError
             If `weights` is not of the appropriate type; see
             :func:`MDAnalysis.lib.util.get_weights`
        ValueError
             If `weights` are not compatible with `groupselections`: only equal
             weights (``weights=None``) or mass-weighted (``weights=""mass""``)
             is supported.

        Notes
        -----
        The root mean square deviation of a group of :math:`N` atoms relative to a
        reference structure as a function of time is calculated as

        .. math::

           \\rho(t) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N w_i \\left(\\mathbf{x}_i(t)
                                    - \\mathbf{x}_i^{\\text{ref}}\\right)^2}

        The selected coordinates from `atomgroup` are optimally superimposed
        (translation and rotation) on the `reference` coordinates at each time step
        as to minimize the RMSD. Douglas Theobald's fast QCP algorithm
        [Theobald2005]_ is used for the rotational superposition and to calculate
        the RMSD (see :mod:`MDAnalysis.lib.qcprot` for implementation details).

        The class runs various checks on the input to ensure that the two atom
        groups can be compared. This includes a comparison of atom masses (i.e.,
        only the positions of atoms of the same mass will be considered to be
        correct for comparison). If masses should not be checked, just set
        `tol_mass` to a large value such as 1000.

        .. _ClustalW: http://www.clustal.org/
        .. _STAMP: http://www.compbio.dundee.ac.uk/manuals/stamp.4.2/


        .. versionadded:: 0.7.7
        .. versionchanged:: 0.8
           `groupselections` added
        .. versionchanged:: 0.16.0
           Flexible weighting scheme with new `weights` keyword.
        .. deprecated:: 0.16.0
           Instead of ``mass_weighted=True`` (removal in 0.17.0) use new
           ``weights='mass'``; refactored to fit with AnalysisBase API
        .. versionchanged:: 0.17.0
           removed deprecated `mass_weighted` keyword; `groupselections`
           are *not* rotationally superimposed any more.
        """"""
        super(RMSD, self).__init__(atomgroup.universe.trajectory,
                                   **kwargs)
        self.universe = atomgroup.universe
        self.reference = reference if reference is not None else self.universe

        select = process_selection(select)
        self.groupselections = ([process_selection(s) for s in groupselections]
                                if groupselections is not None else [])
        self.weights = weights
        self.tol_mass = tol_mass
        self.ref_frame = ref_frame
        self.filename = filename

        self.ref_atoms = self.reference.select_atoms(*select['reference'])
        self.mobile_atoms = self.universe.select_atoms(*select['mobile'])

        if len(self.ref_atoms) != len(self.mobile_atoms):
            err = (""Reference and trajectory atom selections do ""
                   ""not contain the same number of atoms: ""
                   ""N_ref={0:d}, N_traj={1:d}"".format(self.ref_atoms.n_atoms,
                                                      self.mobile_atoms.n_atoms))
            logger.exception(err)
            raise SelectionError(err)
        logger.info(""RMS calculation ""
                    ""for {0:d} atoms."".format(len(self.ref_atoms)))
        mass_mismatches = (np.absolute((self.ref_atoms.masses -
                                        self.mobile_atoms.masses)) >
                           self.tol_mass)

        if np.any(mass_mismatches):
            # diagnostic output:
            logger.error(""Atoms: reference | mobile"")
            for ar, at in zip(self.ref_atoms, self.mobile_atoms):
                if ar.name != at.name:
                    logger.error(""{0!s:>4} {1:3d} {2!s:>3} {3!s:>3} {4:6.3f}""
                                 ""|  {5!s:>4} {6:3d} {7!s:>3} {8!s:>3}""
                                 ""{9:6.3f}"".format(ar.segid, ar.resid,
                                                   ar.resname, ar.name,
                                                   ar.mass, at.segid, at.resid,
                                                   at.resname, at.name,
                                                   at.mass))
            errmsg = (""Inconsistent selections, masses differ by more than""
                      ""{0:f}; mis-matching atoms""
                      ""are shown above."".format(self.tol_mass))
            logger.error(errmsg)
            raise SelectionError(errmsg)
        del mass_mismatches

        # TODO:
        # - make a group comparison a class that contains the checks above
        # - use this class for the *select* group and the additional
        #   *groupselections* groups each a dict with reference/mobile
        self._groupselections_atoms = [
            {
                'reference': self.reference.select_atoms(*s['reference']),
                'mobile': self.universe.select_atoms(*s['mobile']),
            }
            for s in self.groupselections]
        # sanity check
        for igroup, (sel, atoms) in enumerate(zip(self.groupselections,
                                                  self._groupselections_atoms)):
            if len(atoms['mobile']) != len(atoms['reference']):
                logger.exception('SelectionError: Group Selection')
                raise SelectionError(
                    ""Group selection {0}: {1} | {2}: Reference and trajectory ""
                    ""atom selections do not contain the same number of atoms: ""
                    ""N_ref={3}, N_traj={4}"".format(
                        igroup, sel['reference'], sel['mobile'],
                        len(atoms['reference']), len(atoms['mobile'])))

        # Explicitly check for ""mass"" because this option CAN
        # be used with groupselection. (get_weights() returns the mass array
        # for ""mass"")
        if self.weights != ""mass"":
            self.weights = get_weights(self.mobile_atoms, self.weights)

        # cannot use arbitrary weight array (for superposition) with
        # groupselections because arrays will not match
        if len(self.groupselections) > 0 and self.weights not in (""mass"", None):
            raise ValueError(""groupselections can only be combined with ""
                             ""weights=None or weights='mass', not a weight ""
                             ""array."")

        # initialized to note for testing the save function
        self.rmsd = None","1. Use `get_weights` to get the mass array for ""mass"" option.
2. Check if the weights are compatible with `groupselections`.
3. Use `process_selection` to process the selection strings."
"def get_weights(atoms, weights):
    """"""Check that a `weights` argument is compatible with `atoms`.

    Parameters
    ----------
    atoms : AtomGroup or array_like
        The atoms that the `weights` should be applied to. Typically this
        is a :class:`AtomGroup` but because only the length is compared,
        any sequence for which ``len(atoms)`` is defined is acceptable.
    weights : {""mass"", None} or array_like
        All MDAnalysis functions or classes understand ""mass"" and will then
        use ``atoms.masses``. ``None`` indicates equal weights for all atoms.
        Using an ``array_like`` assigns a custom weight to each element of
        `atoms`.

    Returns
    -------
    weights : array_like or None
         If ""mass"" was selected, ``atoms.masses`` is returned, otherwise the
         value of `weights` (which can be ``None``).

    Raises
    ------
    TypeError
        If `weights` is not one of the allowed values or if it is not a 1D
        array with the same length as `atoms`, then the exception is raised.
        :exc:`TypeError` is also raised if ``atoms.masses`` is not defined.
    """"""
    if weights == ""mass"":
        try:
            weights = atoms.masses
        except AttributeError:
            raise TypeError(""weights='mass' selected but atoms.masses is missing"")

    if iterable(weights):
        if len(weights) != len(atoms):
            raise TypeError(""weights (length {0}) must be of same length as ""
                            ""the atoms ({1})"".format(
                                len(weights), len(atoms)))
        elif len(np.asarray(weights).shape) != 1:
            raise TypeError(""weights must be a 1D array, not with shape ""
                            ""{0}"".format(np.asarray(weights).shape))
    elif weights is not None:
        raise TypeError(""weights must be {'mass', None} or an iterable of the ""
                        ""same size as the atomgroup."")

    return weights","1. Use `isinstance` to check if `weights` is a valid type.
2. Use `len` to check if `weights` has the same length as `atoms`.
3. Use `np.asarray` to check if `weights` is a 1D array."
"    def _single_frame(self):
        mobile_com = self.mobile_atoms.center(self.weights).astype(np.float64)
        self._mobile_coordinates64[:] = self.mobile_atoms.positions
        self._mobile_coordinates64 -= mobile_com

        self.rmsd[self._frame_index, :2] = self._ts.frame, self._trajectory.time

        if self._groupselections_atoms:
            # superimpose structures: MDAnalysis qcprot needs Nx3 coordinate
            # array with float64 datatype (float32 leads to errors up to 1e-3 in
            # RMSD). Note that R is defined in such a way that it acts **to the
            # left** so that we can easily use broadcasting and save one
            # expensive numpy transposition.

            self.rmsd[self._frame_index, 2] = qcp.CalcRMSDRotationalMatrix(
                self._ref_coordinates_64, self._mobile_coordinates64,
                self._n_atoms, self._rot, self.weights)

            self._R[:, :] = self._rot.reshape(3, 3)
            # Transform each atom in the trajectory (use inplace ops to
            # avoid copying arrays) (Marginally (~3%) faster than
            # ""ts.positions[:] = (ts.positions - x_com) * R + ref_com"".)
            self._ts.positions[:] -= mobile_com

            # R acts to the left & is broadcasted N times.
            self._ts.positions[:,:] = (self._mobile_coordinates64[:] *
                                       self._R)
            self._ts.positions[:] += self._ref_com

            # 2) calculate secondary RMSDs
            for igroup, (refpos, atoms) in enumerate(
                    zip(self._groupselections_ref_coords_64,
                        self._groupselections_atoms), 3):
                self.rmsd[self._frame_index, igroup] = qcp.CalcRMSDRotationalMatrix(
                    refpos, atoms['mobile'].positions.astype(np.float64),
                    atoms['mobile'].n_atoms, None, self.weights)
        else:
            # only calculate RMSD by setting the Rmatrix to None (no need
            # to carry out the rotation as we already get the optimum RMSD)
            self.rmsd[self._frame_index, 2] = qcp.CalcRMSDRotationalMatrix(
                self._ref_coordinates_64, self._mobile_coordinates64,
                self._n_atoms, None, self.weights)

        self._pm.rmsd = self.rmsd[self._frame_index, 2]","1. Use `np.float64` instead of `np.float32` to avoid errors in RMSD calculation.
2. Use `inplace ops` to avoid copying arrays.
3. Use `zip` to iterate over multiple iterables."
"    def __init__(self, trzfilename, n_atoms=None, **kwargs):
        """"""Creates a TRZ Reader

        Parameters
        ----------
        trzfilename : str
            name of input file
        n_atoms : int
            number of atoms in trajectory, must be taken from topology file!
        convert_units : bool (optional)
            converts units to MDAnalysis defaults
        """"""
        super(TRZReader, self).__init__(trzfilename,  **kwargs)

        if n_atoms is None:
            raise ValueError('TRZReader requires the n_atoms keyword')

        self.trzfile = util.anyopen(self.filename, 'rb')
        self._cache = dict()
        self._n_atoms = n_atoms

        self._read_trz_header()
        self.ts = Timestep(self.n_atoms,
                           velocities=True,
                           forces=self.has_force,
                           reader=self,
                           **self._ts_kwargs)

        # structured dtype of a single trajectory frame
        readarg = str(n_atoms) + 'f4'
        frame_contents = [
            ('p1', 'i4'),
            ('nframe', 'i4'),
            ('ntrj', 'i4'),
            ('natoms', 'i4'),
            ('treal', 'f8'),
            ('p2', '2i4'),
            ('box', '9f8'),
            ('p3', '2i4'),
            ('pressure', 'f8'),
            ('ptensor', '6f8'),
            ('p4', '3i4'),
            ('etot', 'f8'),
            ('ptot', 'f8'),
            ('ek', 'f8'),
            ('T', 'f8'),
            ('p5', '6i4'),
            ('rx', readarg),
            ('pad2', '2i4'),
            ('ry', readarg),
            ('pad3', '2i4'),
            ('rz', readarg),
            ('pad4', '2i4'),
            ('vx', readarg),
            ('pad5', '2i4'),
            ('vy', readarg),
            ('pad6', '2i4'),
            ('vz', readarg)]
        if not self.has_force:
            frame_contents += [('pad7', 'i4')]
        else:
            frame_contents += [
                ('pad7', '2i4'),
                ('fx', readarg),
                ('pad8', '2i4'),
                ('fy', readarg),
                ('pad9', '2i4'),
                ('fz', readarg),
                ('pad10', 'i4')]
        self._dtype = np.dtype(frame_contents)

        self._read_next_timestep()","1. Use `np.fromfile` instead of `np.frombuffer` to avoid buffer overflows.
2. Use `np.memmap` instead of reading the entire file into memory to prevent OOM errors.
3. Validate the input file format to ensure that it is a valid TRZ file."
"    def _read_trz_header(self):
        """"""Reads the header of the trz trajectory""""""
        self._headerdtype = np.dtype([
            ('p1', 'i4'),
            ('title', '80c'),
            ('p2', '2i4'),
            ('force', 'i4'),
            ('p3', 'i4')])
        data = np.fromfile(self.trzfile, dtype=self._headerdtype, count=1)
        self.title = ''.join(c.decode('utf-8') for c in data['title'][0]).strip()
        if data['force'] == 10:
            self.has_force = False
        elif data['force'] == 20:
            self.has_force = True
        else:
            raise IOError","1. Use `np.fromfile()` with `count=1` to read the header, to prevent reading arbitrary data from the file.
2. Use `np.decode()` to decode the title string from bytes to string, to prevent decoding arbitrary data as a string.
3. Use `raise IOError` to raise an exception when the `force` value is not 10 or 20, to prevent the program from continuing to run with invalid data."
"    def __init__(self, filename, n_atoms, title='TRZ', convert_units=None):
        """"""Create a TRZWriter

        Parameters
        ----------
        filename : str
            name of output file
        n_atoms : int
            number of atoms in trajectory
        title : str (optional)
            title of the trajectory; the title must be 80 characters or
            shorter, a longer title raises a ValueError exception.
        convert_units : bool (optional)
            units are converted to the MDAnalysis base format; ``None`` selects
            the value of :data:`MDAnalysis.core.flags` ['convert_lengths'].
            (see :ref:`flags-label`)
        """"""
        self.filename = filename
        if n_atoms is None:
            raise ValueError(""TRZWriter requires the n_atoms keyword"")
        if n_atoms == 0:
            raise ValueError(""TRZWriter: no atoms in output trajectory"")
        self.n_atoms = n_atoms

        if len(title) > 80:
            raise ValueError(""TRZWriter: 'title' must be 80 characters of shorter"")

        if convert_units is None:
            convert_units = flags['convert_lengths']
        self.convert_units = convert_units

        self.trzfile = util.anyopen(self.filename, 'wb')

        self._writeheader(title)

        floatsize = str(n_atoms) + 'f4'
        self.frameDtype = np.dtype([
            ('p1a', 'i4'),
            ('nframe', 'i4'),
            ('ntrj', 'i4'),
            ('natoms', 'i4'),
            ('treal', 'f8'),
            ('p1b', 'i4'),
            ('p2a', 'i4'),
            ('box', '9f8'),
            ('p2b', 'i4'),
            ('p3a', 'i4'),
            ('pressure', 'f8'),
            ('ptensor', '6f8'),
            ('p3b', 'i4'),
            ('p4a', 'i4'),
            ('six', 'i4'),
            ('etot', 'f8'),
            ('ptot', 'f8'),
            ('ek', 'f8'),
            ('T', 'f8'),
            ('blanks', '2f8'),
            ('p4b', 'i4'),
            ('p5a', 'i4'),
            ('rx', floatsize),
            ('p5b', 'i4'),
            ('p6a', 'i4'),
            ('ry', floatsize),
            ('p6b', 'i4'),
            ('p7a', 'i4'),
            ('rz', floatsize),
            ('p7b', 'i4'),
            ('p8a', 'i4'),
            ('vx', floatsize),
            ('p8b', 'i4'),
            ('p9a', 'i4'),
            ('vy', floatsize),
            ('p9b', 'i4'),
            ('p10a', 'i4'),
            ('vz', floatsize),
            ('p10b', 'i4')])","1. Use `assert` statements to check for invalid inputs.
2. Use `np.array_str` to print arrays in a more readable format.
3. Use `np.savetxt` to save arrays to files instead of writing them directly."
"    def _writeheader(self, title):
        hdt = np.dtype([
            ('pad1', 'i4'), ('title', '80c'), ('pad2', 'i4'),
            ('pad3', 'i4'), ('nrec', 'i4'), ('pad4', 'i4')])
        out = np.zeros((), dtype=hdt)
        out['pad1'], out['pad2'] = 80, 80
        out['title'] = title + ' ' * (80 - len(title))
        out['pad3'], out['pad4'] = 4, 4
        out['nrec'] = 10
        out.tofile(self.trzfile)","1. Use `os.fchmod` to set the file mode to 0600 to restrict file access.
2. Use `np.char.encode` to encode the title to bytes before writing it to the file.
3. Use `np.pad` to pad the title with spaces to ensure that it is always 80 characters long."
"    def parse(self):
        """"""Parse atom information from PQR file *filename*.

        Returns
        -------
        A MDAnalysis Topology object
        """"""
        serials = []
        names = []
        resnames = []
        chainIDs = []
        resids = []
        charges = []
        radii = []

        with openany(self.filename, 'r') as f:
            for line in f:
                if line.startswith((""ATOM"", ""HETATM"")):
                    fields = line.split()
                    try:
                        (recordName, serial, name, resName,
                         chainID, resSeq, x, y, z, charge,
                         radius) = fields
                    except ValueError:
                        # files without the chainID
                        (recordName, serial, name, resName,
                         resSeq, x, y, z, charge, radius) = fields
                        chainID = ""SYSTEM""
                    serials.append(serial)
                    names.append(name)
                    resnames.append(resName)
                    resids.append(resSeq)
                    charges.append(charge)
                    radii.append(radius)
                    chainIDs.append(chainID)

        n_atoms = len(serials)

        atomtypes = guessers.guess_types(names)
        masses = guessers.guess_masses(atomtypes)

        attrs = []
        attrs.append(Atomids(np.array(serials, dtype=np.int32)))
        attrs.append(Atomnames(np.array(names, dtype=object)))
        attrs.append(Charges(np.array(charges, dtype=np.float32)))
        attrs.append(Atomtypes(atomtypes, guessed=True))
        attrs.append(Masses(masses, guessed=True))
        attrs.append(Radii(np.array(radii, dtype=np.float32)))

        resids = np.array(resids, dtype=np.int32)
        resnames = np.array(resnames, dtype=object)
        chainIDs = np.array(chainIDs, dtype=object)

        residx, resids, (resnames, chainIDs) = squash_by(
            resids, resnames, chainIDs)

        n_residues = len(resids)
        attrs.append(Resids(resids))
        attrs.append(Resnums(resids.copy()))
        attrs.append(Resnames(resnames))

        segidx, chainIDs = squash_by(chainIDs)[:2]

        n_segments = len(chainIDs)
        attrs.append(Segids(chainIDs))

        top = Topology(n_atoms, n_residues, n_segments,
                       attrs=attrs,
                       atom_resindex=residx,
                       residue_segindex=segidx)

        return top","1. Use `Pathlib` to handle file paths instead of `openany`.
2. Use `pandas` to parse the data instead of manual parsing.
3. Validate the input data to prevent errors."
"    def __init__(self, traj, reference=None, select='all',
                 groupselections=None, filename=""rmsd.dat"",
                 mass_weighted=False, tol_mass=0.1, ref_frame=0):
        """"""Setting up the RMSD analysis.

        The RMSD will be computed between *select* and *reference* for
        all frames in the trajectory in *universe*.

        Parameters
        ----------
        traj : :class:`MDAnalysis.Universe`
            universe that contains a trajectory
        reference : :class:`MDAnalysis.Universe` (optional)
            reference coordinates, if ``None`` current frame of *traj* is used
        select : str / dict / tuple (optional)
            The selection to operate on; can be one of:

            1. any valid selection string for
               :meth:`~MDAnalysis.core.AtomGroup.AtomGroup.select_atoms` that
               produces identical selections in *mobile* and *reference*; or

            2. a dictionary ``{'mobile':sel1, 'reference':sel2}`` (the
               :func:`MDAnalysis.analysis.align.fasta2select` function returns
               such a dictionary based on a ClustalW_ or STAMP_ sequence
               alignment); or
            3. a tuple ``(sel1, sel2)``

            When using 2. or 3. with *sel1* and *sel2* then these selections
            can also each be a list of selection strings (to generate a
            AtomGroup with defined atom order as described under
            :ref:`ordered-selections-label`).
        groupselections : list (optional)
            A list of selections as described for *select*. Each selection
            describes additional RMSDs to be computed *after the structures
            have be superpositioned* according to *select*. The output contains
            one additional column for each selection. [``None``]

            .. Note:: Experimental feature. Only limited error checking
                      implemented.
        filename : str (optional)
            write RSMD into file file :meth:`RMSD.save`
        mass_weighted : bool (optional)
             do a mass-weighted RMSD fit
        tol_mass : float (optional)
             Reject match if the atomic masses for matched atoms differ by more
             than `tol_mass`
        ref_frame : int (optional)
             frame index to select frame from `reference`

        .. _ClustalW: http://www.clustal.org/
        .. _STAMP: http://www.compbio.dundee.ac.uk/manuals/stamp.4.2/

        .. versionadded:: 0.7.7
        .. versionchanged:: 0.8
           *groupselections* added

        """"""
        self.universe = traj
        if reference is None:
            self.reference = self.universe
        else:
            self.reference = reference
        self.select = _process_selection(select)
        if groupselections is not None:
            self.groupselections = [_process_selection(s) for s in groupselections]
        else:
            self.groupselections = []
        self.mass_weighted = mass_weighted
        self.tol_mass = tol_mass
        self.ref_frame = ref_frame
        self.filename = filename

        self.ref_atoms = self.reference.select_atoms(*self.select['reference'])
        self.traj_atoms = self.universe.select_atoms(*self.select['mobile'])
        if len(self.ref_atoms) != len(self.traj_atoms):
            logger.exception()
            raise SelectionError(""Reference and trajectory atom selections do ""
                                 ""not contain the same number of atoms: ""
                                 ""N_ref={0:d}, N_traj={1:d}"".format(
                                     self.ref_atoms.n_atoms,
                                     self.traj_atoms.n_atoms))
        logger.info(""RMS calculation for {0:d} atoms."".format(len(self.ref_atoms)))
        mass_mismatches = (np.absolute(self.ref_atoms.masses - self.traj_atoms.masses) > self.tol_mass)
        if np.any(mass_mismatches):
            # diagnostic output:
            logger.error(""Atoms: reference | trajectory"")
            for ar, at in zip(self.ref_atoms, self.traj_atoms):
                if ar.name != at.name:
                    logger.error(""{0!s:>4} {1:3d} {2!s:>3} {3!s:>3} {4:6.3f}  |  {5!s:>4} {6:3d} {7!s:>3} {8!s:>3} {9:6.3f}"".format(ar.segid, ar.resid, ar.resname, ar.name, ar.mass,
                                 at.segid, at.resid, at.resname, at.name, at.mass))
            errmsg = ""Inconsistent selections, masses differ by more than {0:f}; mis-matching atoms are shown above."".format( \\
                     self.tol_mass)
            logger.error(errmsg)
            raise SelectionError(errmsg)
        del mass_mismatches

        # TODO:
        # - make a group comparison a class that contains the checks above
        # - use this class for the *select* group and the additional
        #   *groupselections* groups each a dict with reference/mobile
        self.groupselections_atoms = [
            {
                'reference': self.reference.select_atoms(*s['reference']),
                'mobile': self.universe.select_atoms(*s['mobile']),
            }
            for s in self.groupselections]
        # sanity check
        for igroup, (sel, atoms) in enumerate(zip(self.groupselections,
                                                  self.groupselections_atoms)):
            if len(atoms['mobile']) != len(atoms['reference']):
                logger.exception()
                raise SelectionError(
                    ""Group selection {0}: {1} | {2}: Reference and trajectory ""
                    ""atom selections do not contain the same number of atoms: ""
                    ""N_ref={3}, N_traj={4}"".format(
                        igroup, sel['reference'], sel['mobile'],
                        len(atoms['reference']), len(atoms['mobile'])))

        self.rmsd = None","1. Use `typing` to specify the types of arguments and return values of functions.
2. Validate the input of functions to prevent errors.
3. Use `logging` to log errors and warnings."
"    def _complete_for_arg(self, arg_action: argparse.Action,
                          text: str, line: str, begidx: int, endidx: int,
                          consumed_arg_values: Dict[str, List[str]], *,
                          cmd_set: Optional[CommandSet] = None) -> List[str]:
        """"""
        Tab completion routine for an argparse argument
        :return: list of completions
        :raises: CompletionError if the completer or choices function this calls raises one
        """"""
        # Check if the arg provides choices to the user
        if arg_action.choices is not None:
            arg_choices = arg_action.choices
        else:
            arg_choices = getattr(arg_action, ATTR_CHOICES_CALLABLE, None)

        if arg_choices is None:
            return []

        # If we are going to call a completer/choices function, then set up the common arguments
        args = []
        kwargs = {}
        if isinstance(arg_choices, ChoicesCallable):
            if arg_choices.is_method:
                cmd_set = getattr(self._parser, constants.PARSER_ATTR_COMMANDSET, cmd_set)
                if cmd_set is not None:
                    if isinstance(cmd_set, CommandSet):
                        # If command is part of a CommandSet, `self` should be the CommandSet and Cmd will be next
                        if cmd_set is not None:
                            args.append(cmd_set)
                args.append(self._cmd2_app)

            # Check if arg_choices.to_call expects arg_tokens
            to_call_params = inspect.signature(arg_choices.to_call).parameters
            if ARG_TOKENS in to_call_params:
                # Merge self._parent_tokens and consumed_arg_values
                arg_tokens = {**self._parent_tokens, **consumed_arg_values}

                # Include the token being completed
                arg_tokens.setdefault(arg_action.dest, [])
                arg_tokens[arg_action.dest].append(text)

                # Add the namespace to the keyword arguments for the function we are calling
                kwargs[ARG_TOKENS] = arg_tokens

        # Check if the argument uses a specific tab completion function to provide its choices
        if isinstance(arg_choices, ChoicesCallable) and arg_choices.is_completer:
            args.extend([text, line, begidx, endidx])
            results = arg_choices.to_call(*args, **kwargs)

        # Otherwise use basic_complete on the choices
        else:
            # Check if the choices come from a function
            if isinstance(arg_choices, ChoicesCallable) and not arg_choices.is_completer:
                arg_choices = arg_choices.to_call(*args, **kwargs)

            # Since arg_choices can be any iterable type, convert to a list
            arg_choices = list(arg_choices)

            # If these choices are numbers, and have not yet been sorted, then sort them now
            if not self._cmd2_app.matches_sorted and all(isinstance(x, numbers.Number) for x in arg_choices):
                arg_choices.sort()
                self._cmd2_app.matches_sorted = True

            # Since choices can be various types like int, we must convert them to strings
            for index, choice in enumerate(arg_choices):
                if not isinstance(choice, str):
                    arg_choices[index] = str(choice)

            # Filter out arguments we already used
            used_values = consumed_arg_values.get(arg_action.dest, [])
            arg_choices = [choice for choice in arg_choices if choice not in used_values]

            # Do tab completion on the choices
            results = basic_complete(text, line, begidx, endidx, arg_choices)

        return self._format_completions(arg_action, results)","1. Use `inspect.signature()` to check if the function has the expected parameters.
2. Use `isinstance()` to check if the argument is a ChoicesCallable.
3. Use `issubclass()` to check if the argument is a subclass of argparse.Action."
"    def _initialize_history(self, hist_file):
        """"""Initialize history using history related attributes

        This function can determine whether history is saved in the prior text-based
        format (one line of input is stored as one line in the file), or the new-as-
        of-version 0.9.13 pickle based format.

        History created by versions <= 0.9.12 is in readline format, i.e. plain text files.

        Initializing history does not effect history files on disk, versions >= 0.9.13 always
        write history in the pickle format.
        """"""
        self.history = History()
        # with no persistent history, nothing else in this method is relevant
        if not hist_file:
            self.persistent_history_file = hist_file
            return

        hist_file = os.path.abspath(os.path.expanduser(hist_file))

        # on Windows, trying to open a directory throws a permission
        # error, not a `IsADirectoryError`. So we'll check it ourselves.
        if os.path.isdir(hist_file):
            msg = ""Persistent history file '{}' is a directory""
            self.perror(msg.format(hist_file))
            return

        # Create the directory for the history file if it doesn't already exist
        hist_file_dir = os.path.dirname(hist_file)
        try:
            os.makedirs(hist_file_dir, exist_ok=True)
        except OSError as ex:
            msg = ""Error creating persistent history file directory '{}': {}"".format(hist_file_dir, ex)
            self.pexcept(msg)
            return

        # first we try and unpickle the history file
        history = History()

        try:
            with open(hist_file, 'rb') as fobj:
                history = pickle.load(fobj)
        except (AttributeError, EOFError, FileNotFoundError, ImportError, IndexError, KeyError, pickle.UnpicklingError):
            # If any non-operating system error occurs when attempting to unpickle, just use an empty history
            pass
        except OSError as ex:
            msg = ""Can not read persistent history file '{}': {}""
            self.pexcept(msg.format(hist_file, ex))
            return

        self.history = history
        self.history.start_session()
        self.persistent_history_file = hist_file

        # populate readline history
        if rl_type != RlType.NONE:
            last = None
            for item in history:
                # Break the command into its individual lines
                for line in item.raw.splitlines():
                    # readline only adds a single entry for multiple sequential identical lines
                    # so we emulate that behavior here
                    if line != last:
                        readline.add_history(line)
                        last = line

        # register a function to write history at save
        # if the history file is in plain text format from 0.9.12 or lower
        # this will fail, and the history in the plain text file will be lost
        import atexit
        atexit.register(self._persist_history)","1. Use `os.path.isfile` to check if the history file exists before trying to open it.
2. Use `os.makedirs` with the `exist_ok` flag to create the history file directory if it doesn't exist.
3. Use `pickle.dump` to write the history to the file instead of `pickle.load`."
"    def _get_pointer(self):
        if (
                self.thisdir is not None
                and self.thisdir.files[self._pointer] != self._pointed_obj
        ):
            try:
                self._pointer = self.thisdir.files.index(self._pointed_obj)
            except ValueError:
                self._pointed_obj = self.thisdir.files[self._pointer]
        return self._pointer","1. Use `self.thisdir.files.index()` instead of `self.thisdir.files.find()` to avoid a potential security vulnerability.
2. Use `self.thisdir.files.append()` instead of `self.thisdir.files.insert()` to avoid overwriting an existing file.
3. Use `self.thisdir.files.remove()` instead of `self.thisdir.files.pop()` to avoid leaving a dangling reference to a deleted file."
"    def _set_pointer(self, value):
        self._pointer = value
        try:
            self._pointed_obj = self.thisdir.files[self._pointer]
        except TypeError:
            pass
        except IndexError:
            pass","1. Use `try-except` blocks to catch errors and prevent the program from crashing.
2. Use `type` checking to ensure that the value passed to `_set_pointer` is a valid index.
3. Use `len` to check that the list of files is not empty before accessing it."
"    def _set_pointer(self, value):
        self._pointer = value
        self._pointed_obj = self.thisdir.files[self._pointer]","1. Use `f.read()` instead of `f.readlines()` to avoid reading more data than necessary.
2. Use `os.fchmod()` to set the file mode instead of `os.chmod()`.
3. Use `contextlib.closing()` to ensure that the file is closed after use."
"    def _set_pointer(self, value):
        self._pointer = value
        try:
            self._pointed_obj = self.thisdir.files[self._pointer]
        except TypeError:
            pass","1. Use `f-strings` instead of `+` to concatenate strings.
2. Use `getattr()` to access object attributes instead of `__dict__`.
3. Use `type()` to check the type of a variable before casting it."
"    def sha512_encode(path, inode=None):
        if inode is None:
            inode = stat(path).st_ino
        inode_path = ""{0}{1}"".format(str(inode), path)
        if PY3:
            inode_path = inode_path.encode('utf-8', 'backslashescape')
        return '{0}.jpg'.format(sha512(inode_path).hexdigest())","1. Use `os.path.join` instead of concatenation to prevent directory traversal attacks.
2. Use `os.fspath` to convert a path to a `str` in Python 3, to avoid errors when using bytes.
3. Use a secure hashing algorithm such as `sha256` instead of `sha512`."
"    def sha512_encode(path, inode=None):
        if inode is None:
            inode = stat(path).st_ino
        sha = sha512(
            ""{0}{1}"".format(path, str(inode)).encode('utf-8', 'backslashescape')
        )
        return '{0}.jpg'.format(sha.hexdigest())","1. Use `os.path.join()` to concatenate the path and inode instead of concatenating them manually. This will prevent directory traversal attacks.
2. Use `os.urandom()` to generate a random salt instead of using a predictable string. This will make it more difficult for attackers to guess the hashed password.
3. Use a stronger hashing algorithm, such as `bcrypt`, instead of `sha512`. This will make it more difficult for attackers to brute force the password."
"    def enter_dir(self, path, history=True):
        """"""Enter given path""""""
        # TODO: Ensure that there is always a self.thisdir
        if path is None:
            return None
        path = str(path)

        # clear filter in the folder we're leaving
        if self.fm.settings.clear_filters_on_dir_change and self.thisdir:
            self.thisdir.filter = None
            self.thisdir.refilter()

        previous = self.thisdir

        # get the absolute path
        path = normpath(join(self.path, expanduser(path)))

        if not isdir(path):
            return False
        new_thisdir = self.fm.get_directory(path)

        try:
            os.chdir(path)
        except OSError:
            return True
        self.path = path
        self.thisdir = new_thisdir

        self.thisdir.load_content_if_outdated()

        # build the pathway, a tuple of directory objects which lie
        # on the path to the current directory.
        if path == '/':
            self.pathway = (self.fm.get_directory('/'), )
        else:
            pathway = []
            currentpath = '/'
            for comp in path.split('/'):
                currentpath = join(currentpath, comp)
                pathway.append(self.fm.get_directory(currentpath))
            self.pathway = tuple(pathway)

        self.assign_cursor_positions_for_subdirs()

        # set the current file.
        self.thisdir.sort_directories_first = self.fm.settings.sort_directories_first
        self.thisdir.sort_reverse = self.fm.settings.sort_reverse
        self.thisdir.sort_if_outdated()
        if previous and previous.path != path:
            self.thisfile = self.thisdir.pointed_obj
        else:
            # This avoids setting self.pointer (through the 'move' signal) and
            # is required so that you can use enter_dir when switching tabs
            # without messing up the pointer.
            self._thisfile = self.thisdir.pointed_obj

        if history:
            self.history.add(new_thisdir)

        self.fm.signal_emit('cd', previous=previous, new=self.thisdir)

        return True","1. Use `pathlib` instead of `os.path` to handle paths more securely.
2. Use `pwd` module to get the current user's home directory instead of expanding `~`.
3. Validate the user input before changing the directory."
"    def __init__(self, env=None, fm=None):  # pylint: disable=super-init-not-called
        self.keybuffer = KeyBuffer()
        self.keymaps = KeyMaps(self.keybuffer)
        self.redrawlock = threading.Event()
        self.redrawlock.set()

        self.titlebar = None
        self._viewmode = None
        self.taskview = None
        self.status = None
        self.console = None
        self.pager = None
        self.multiplexer = None
        self._draw_title = None
        self._tmux_automatic_rename = None
        self._tmux_title = None
        self._screen_title = None
        self.browser = None

        if fm is not None:
            self.fm = fm","1. Use `super()` to call the parent class's `__init__()` method.
2. Initialize the `self.titlebar`, `self._viewmode`, `self.taskview`, `self.status`, `self.console`, `self.pager`, `self.multiplexer`, `self._draw_title`, `self._tmux_automatic_rename`, `self._tmux_title`, `self._screen_title`, and `self.browser` variables to `None`.
3. If `fm` is not `None`, assign it to the `self.fm` variable."
"    def handle_multiplexer(self):
        if self.settings.update_tmux_title:
            if 'TMUX' in os.environ:
                # Stores the automatic-rename setting
                # prints out a warning if the allow-rename in tmux is not set
                tmux_allow_rename = check_output(
                    ['tmux', 'show-window-options', '-v',
                     'allow-rename']).strip()
                if tmux_allow_rename == 'off':
                    self.fm.notify('Warning: allow-rename not set in Tmux!',
                                   bad=True)
                elif self._tmux_title is None:
                    self._tmux_title = check_output(
                        ['tmux', 'display-message', '-p', '#W']).strip()
                else:
                    try:
                        self._tmux_automatic_rename = check_output(
                            ['tmux', 'show-window-options', '-v',
                             'automatic-rename']).strip()
                        if self._tmux_automatic_rename == 'on':
                            check_output(['tmux', 'set-window-option',
                                          'automatic-rename', 'off'])
                    except CalledProcessError:
                        pass
            elif 'screen' in os.environ['TERM'] and self._screen_title is None:
                # Stores the screen window name before renaming it
                # gives out a warning if $TERM is not ""screen""
                try:
                    self._screen_title = check_output(
                        ['screen', '-Q', 'title'], shell=True).strip()
                except CalledProcessError:
                    self._screen_title = None

            sys.stdout.write(""\\033kranger\\033\\\\"")
            sys.stdout.flush()","1. Use `subprocess.check_output` with the `universal_newlines=True` flag to avoid escaping issues.
2. Use `os.getenv` to get the environment variable instead of accessing it directly.
3. Use `check_output` to get the screen window name instead of using `shell=True`."
"    def restore_multiplexer_name(self):
        try:
            if 'TMUX' in os.environ:
                if self._tmux_automatic_rename:
                    check_output(['tmux', 'set-window-option',
                                  'automatic-rename',
                                  self._tmux_automatic_rename])
                else:
                    check_output(['tmux', 'set-window-option', '-u',
                                  'automatic-rename'])
                if self._tmux_title:
                    check_output(['tmux', 'rename-window', self._tmux_title])
            elif 'screen' in os.environ['TERM'] and self._screen_title:
                check_output(['screen', '-X', 'title', self._screen_title])
        except CalledProcessError:
            self.fm.notify(""Could not restore window-name!"", bad=True)","1. Use `subprocess.run` instead of `check_output` to avoid silently swallowing errors.
2. Use `subprocess.check_call` instead of `check_output` to raise an exception on errors.
3. Use `subprocess.DEVNULL` as the `stdout` argument to suppress output."
"def main(
        # pylint: disable=too-many-locals,too-many-return-statements
        # pylint: disable=too-many-branches,too-many-statements
):
    """"""initialize objects and run the filemanager""""""
    import ranger.api
    from ranger.container.settings import Settings
    from ranger.core.shared import FileManagerAware, SettingsAware
    from ranger.core.fm import FM
    from ranger.ext.logutils import setup_logging
    from ranger.ext.openstruct import OpenStruct

    ranger.args = args = parse_arguments()
    ranger.arg = OpenStruct(args.__dict__)  # COMPAT
    setup_logging(debug=args.debug, logfile=args.logfile)

    for line in VERSION_MSG:
        LOG.info(line)
    LOG.info('Process ID: %s', os.getpid())

    try:
        locale.setlocale(locale.LC_ALL, '')
    except locale.Error:
        print(""Warning: Unable to set locale.  Expect encoding problems."")

    # so that programs can know that ranger spawned them:
    level = 'RANGER_LEVEL'
    if level in os.environ and os.environ[level].isdigit():
        os.environ[level] = str(int(os.environ[level]) + 1)
    else:
        os.environ[level] = '1'

    if 'SHELL' not in os.environ:
        os.environ['SHELL'] = 'sh'

    LOG.debug(""cache dir: '%s'"", args.cachedir)
    LOG.debug(""config dir: '%s'"", args.confdir)
    LOG.debug(""data dir: '%s'"", args.datadir)

    if args.copy_config is not None:
        fm = FM()
        fm.copy_config_files(args.copy_config)
        return 0
    if args.list_tagged_files:
        if args.clean:
            print(""Can't access tag data in clean mode"", file=sys.stderr)
            return 1
        fm = FM()
        try:
            if sys.version_info[0] >= 3:
                fobj = open(fm.datapath('tagged'), 'r', errors='replace')
            else:
                fobj = open(fm.datapath('tagged'), 'r')
        except OSError as ex:
            print('Unable to open `tagged` data file: {0}'.format(ex), file=sys.stderr)
            return 1
        for line in fobj.readlines():
            if len(line) > 2 and line[1] == ':':
                if line[0] in args.list_tagged_files:
                    sys.stdout.write(line[2:])
            elif line and '*' in args.list_tagged_files:
                sys.stdout.write(line)
        return 0

    SettingsAware.settings_set(Settings())

    if args.selectfile:
        args.selectfile = os.path.abspath(args.selectfile)
        args.paths.insert(0, os.path.dirname(args.selectfile))

    paths = __get_paths(args)
    paths_inaccessible = []
    for path in paths:
        try:
            path_abs = os.path.abspath(path)
        except OSError:
            paths_inaccessible += [path]
            continue
        if not os.access(path_abs, os.F_OK):
            paths_inaccessible += [path]
    if paths_inaccessible:
        print('Inaccessible paths: {0}'.format(paths), file=sys.stderr)
        return 1

    profile = None
    exit_msg = ''
    exit_code = 0
    try:  # pylint: disable=too-many-nested-blocks
        # Initialize objects
        fm = FM(paths=paths)
        FileManagerAware.fm_set(fm)
        load_settings(fm, args.clean)

        if args.show_only_dirs:
            from ranger.container.directory import InodeFilterConstants
            fm.settings.global_inode_type_filter = InodeFilterConstants.DIRS

        if args.list_unused_keys:
            from ranger.ext.keybinding_parser import (special_keys,
                                                      reversed_special_keys)
            maps = fm.ui.keymaps['browser']
            for key in sorted(special_keys.values(), key=str):
                if key not in maps:
                    print(""<%s>"" % reversed_special_keys[key])
            for key in range(33, 127):
                if key not in maps:
                    print(chr(key))
            return 0

        if not sys.stdin.isatty():
            sys.stderr.write(""Error: Must run ranger from terminal\\n"")
            raise SystemExit(1)

        if fm.username == 'root':
            fm.settings.preview_files = False
            fm.settings.use_preview_script = False
            LOG.info(""Running as root, disabling the file previews."")
        if not args.debug:
            from ranger.ext import curses_interrupt_handler
            curses_interrupt_handler.install_interrupt_handler()

        # Create cache directory
        if fm.settings.preview_images and fm.settings.use_preview_script:
            if not os.path.exists(args.cachedir):
                os.makedirs(args.cachedir)

        if not args.clean:
            # Create data directory
            if not os.path.exists(args.datadir):
                os.makedirs(args.datadir)

            # Restore saved tabs
            tabs_datapath = fm.datapath('tabs')
            if fm.settings.save_tabs_on_exit and os.path.exists(tabs_datapath) and not args.paths:
                try:
                    with open(tabs_datapath, 'r') as fobj:
                        tabs_saved = fobj.read().partition('\\0\\0')
                        fm.start_paths += tabs_saved[0].split('\\0')
                    if tabs_saved[-1]:
                        with open(tabs_datapath, 'w') as fobj:
                            fobj.write(tabs_saved[-1])
                    else:
                        os.remove(tabs_datapath)
                except OSError as ex:
                    LOG.error('Unable to restore saved tabs')
                    LOG.exception(ex)

        # Run the file manager
        fm.initialize()
        ranger.api.hook_init(fm)
        fm.ui.initialize()

        if args.selectfile:
            fm.select_file(args.selectfile)

        if args.cmd:
            for command in args.cmd:
                fm.execute_console(command)

        if ranger.args.profile:
            import cProfile
            import pstats
            ranger.__fm = fm  # pylint: disable=protected-access
            profile_file = tempfile.gettempdir() + '/ranger_profile'
            cProfile.run('ranger.__fm.loop()', profile_file)
            profile = pstats.Stats(profile_file, stream=sys.stderr)
        else:
            fm.loop()

    except Exception:  # pylint: disable=broad-except
        import traceback
        ex_traceback = traceback.format_exc()
        exit_msg += '\\n'.join(VERSION_MSG) + '\\n'
        try:
            exit_msg += ""Current file: {0}\\n"".format(repr(fm.thisfile.path))
        except Exception:  # pylint: disable=broad-except
            pass
        exit_msg += '''
{0}
ranger crashed. Please report this traceback at:
https://github.com/ranger/ranger/issues
'''.format(ex_traceback)

        exit_code = 1

    except SystemExit as ex:
        if ex.code is not None:
            if not isinstance(ex.code, int):
                exit_msg = ex.code
                exit_code = 1
            else:
                exit_code = ex.code

    finally:
        if exit_msg:
            LOG.critical(exit_msg)
        try:
            fm.ui.destroy()
        except (AttributeError, NameError):
            pass
        # If profiler is enabled print the stats
        if ranger.args.profile and profile:
            profile.strip_dirs().sort_stats('cumulative').print_callees()
        # print the exit message if any
        if exit_msg:
            sys.stderr.write(exit_msg)
        return exit_code  # pylint: disable=lost-exception","1. Use `os.path.abspath()` to ensure that the paths are always absolute paths.
2. Use `os.access()` to check if the path exists and is accessible before trying to open it.
3. Use `tempfile.gettempdir()` to create a temporary directory for the profile file instead of using the current working directory."
"    def data_status_root(self):
        statuses = set()

        # Paths with status
        lines = self._run(['status']).split('\\n')
        if not lines:
            return 'sync'
        for line in lines:
            code = line[0]
            if code == ' ':
                continue
            statuses.add(self._status_translate(code))

        for status in self.DIRSTATUSES:
            if status in statuses:
                return status
        return 'sync'","1. Use `os.listdir()` instead of `subprocess.run()` to get the list of files. This will prevent the code from being vulnerable to a command injection attack.
2. Use `os.path.join()` to construct the path to the file instead of concatenating strings. This will prevent the code from being vulnerable to a path traversal attack.
3. Use `os.access()` to check if the user has permission to read the file before trying to open it. This will prevent the code from being vulnerable to a file permission attack."
"    def data_status_subpaths(self):
        statuses = {}

        # Paths with status
        lines = self._run(['status']).split('\\n')
        for line in lines:
            code, path = line[0], line[8:]
            if code == ' ':
                continue
            statuses[os.path.normpath(path)] = self._status_translate(code)

        return statuses","1. Use `os.path.normpath()` to normalize the path before storing it in the dictionary. This will help to prevent directory traversal attacks.
2. Use `self._run()` to execute the `status` command and capture the output. This will help to prevent code injection attacks.
3. Use `self._status_translate()` to convert the status code to a human-readable string. This will help to make the code more readable and easier to understand."
"def main(
        # pylint: disable=too-many-locals,too-many-return-statements
        # pylint: disable=too-many-branches,too-many-statements
):
    """"""initialize objects and run the filemanager""""""
    import ranger.api
    from ranger.container.settings import Settings
    from ranger.core.shared import FileManagerAware, SettingsAware
    from ranger.core.fm import FM
    from ranger.ext.logutils import setup_logging
    from ranger.ext.openstruct import OpenStruct

    ranger.args = args = parse_arguments()
    ranger.arg = OpenStruct(args.__dict__)  # COMPAT
    setup_logging(debug=args.debug, logfile=args.logfile)

    for line in VERSION_MSG:
        LOG.info(line)
    LOG.info('Process ID: %s', os.getpid())

    try:
        locale.setlocale(locale.LC_ALL, '')
    except locale.Error:
        print(""Warning: Unable to set locale.  Expect encoding problems."")

    # so that programs can know that ranger spawned them:
    level = 'RANGER_LEVEL'
    if level in os.environ and os.environ[level].isdigit():
        os.environ[level] = str(int(os.environ[level]) + 1)
    else:
        os.environ[level] = '1'

    if 'SHELL' not in os.environ:
        os.environ['SHELL'] = 'sh'

    LOG.debug(""cache dir: '%s'"", args.cachedir)
    LOG.debug(""config dir: '%s'"", args.confdir)
    LOG.debug(""data dir: '%s'"", args.datadir)

    if args.copy_config is not None:
        fm = FM()
        fm.copy_config_files(args.copy_config)
        return 0
    if args.list_tagged_files:
        if args.clean:
            print(""Can't access tag data in clean mode"", file=sys.stderr)
            return 1
        fm = FM()
        try:
            if sys.version_info[0] >= 3:
                fobj = open(fm.datapath('tagged'), 'r', errors='replace')
            else:
                fobj = open(fm.datapath('tagged'), 'r')
        except OSError as ex:
            print('Unable to open `tagged` data file: {0}'.format(ex), file=sys.stderr)
            return 1
        for line in fobj.readlines():
            if len(line) > 2 and line[1] == ':':
                if line[0] in args.list_tagged_files:
                    sys.stdout.write(line[2:])
            elif line and '*' in args.list_tagged_files:
                sys.stdout.write(line)
        return 0

    SettingsAware.settings_set(Settings())

    if args.selectfile:
        args.selectfile = os.path.abspath(args.selectfile)
        args.paths.insert(0, os.path.dirname(args.selectfile))

    if args.paths:
        paths = [p[7:] if p.startswith('file:///') else p for p in args.paths]
    else:
        paths = [os.environ.get('PWD', os.getcwd())]
    paths_inaccessible = []
    for path in paths:
        try:
            path_abs = os.path.abspath(path)
        except OSError:
            paths_inaccessible += [path]
            continue
        if not os.access(path_abs, os.F_OK):
            paths_inaccessible += [path]
    if paths_inaccessible:
        print('Inaccessible paths: {0}'.format(paths), file=sys.stderr)
        return 1

    profile = None
    exit_msg = ''
    exit_code = 0
    try:  # pylint: disable=too-many-nested-blocks
        # Initialize objects
        fm = FM(paths=paths)
        FileManagerAware.fm_set(fm)
        load_settings(fm, args.clean)

        if args.show_only_dirs:
            from ranger.container.directory import InodeFilterConstants
            fm.settings.global_inode_type_filter = InodeFilterConstants.DIRS

        if args.list_unused_keys:
            from ranger.ext.keybinding_parser import (special_keys,
                                                      reversed_special_keys)
            maps = fm.ui.keymaps['browser']
            for key in sorted(special_keys.values(), key=str):
                if key not in maps:
                    print(""<%s>"" % reversed_special_keys[key])
            for key in range(33, 127):
                if key not in maps:
                    print(chr(key))
            return 0

        if not sys.stdin.isatty():
            sys.stderr.write(""Error: Must run ranger from terminal\\n"")
            raise SystemExit(1)

        if fm.username == 'root':
            fm.settings.preview_files = False
            fm.settings.use_preview_script = False
            LOG.info(""Running as root, disabling the file previews."")
        if not args.debug:
            from ranger.ext import curses_interrupt_handler
            curses_interrupt_handler.install_interrupt_handler()

        # Create cache directory
        if fm.settings.preview_images and fm.settings.use_preview_script:
            if not os.path.exists(args.cachedir):
                os.makedirs(args.cachedir)

        if not args.clean:
            # Create data directory
            if not os.path.exists(args.datadir):
                os.makedirs(args.datadir)

            # Restore saved tabs
            tabs_datapath = fm.datapath('tabs')
            if fm.settings.save_tabs_on_exit and os.path.exists(tabs_datapath) and not args.paths:
                try:
                    with open(tabs_datapath, 'r') as fobj:
                        tabs_saved = fobj.read().partition('\\0\\0')
                        fm.start_paths += tabs_saved[0].split('\\0')
                    if tabs_saved[-1]:
                        with open(tabs_datapath, 'w') as fobj:
                            fobj.write(tabs_saved[-1])
                    else:
                        os.remove(tabs_datapath)
                except OSError as ex:
                    LOG.error('Unable to restore saved tabs')
                    LOG.exception(ex)

        # Run the file manager
        fm.initialize()
        ranger.api.hook_init(fm)
        fm.ui.initialize()

        if args.selectfile:
            fm.select_file(args.selectfile)

        if args.cmd:
            for command in args.cmd:
                fm.execute_console(command)

        if ranger.args.profile:
            import cProfile
            import pstats
            ranger.__fm = fm  # pylint: disable=protected-access
            profile_file = tempfile.gettempdir() + '/ranger_profile'
            cProfile.run('ranger.__fm.loop()', profile_file)
            profile = pstats.Stats(profile_file, stream=sys.stderr)
        else:
            fm.loop()

    except Exception:  # pylint: disable=broad-except
        import traceback
        ex_traceback = traceback.format_exc()
        exit_msg += '\\n'.join(VERSION_MSG) + '\\n'
        try:
            exit_msg += ""Current file: {0}\\n"".format(repr(fm.thisfile.path))
        except Exception:  # pylint: disable=broad-except
            pass
        exit_msg += '''
{0}
ranger crashed. Please report this traceback at:
https://github.com/ranger/ranger/issues
'''.format(ex_traceback)

        exit_code = 1

    except SystemExit as ex:
        if ex.code is not None:
            if not isinstance(ex.code, int):
                exit_msg = ex.code
                exit_code = 1
            else:
                exit_code = ex.code

    finally:
        if exit_msg:
            LOG.critical(exit_msg)
        try:
            fm.ui.destroy()
        except (AttributeError, NameError):
            pass
        # If profiler is enabled print the stats
        if ranger.args.profile and profile:
            profile.strip_dirs().sort_stats('cumulative').print_callees()
        # print the exit message if any
        if exit_msg:
            sys.stderr.write(exit_msg)
        return exit_code  # pylint: disable=lost-exception","1. Use `os.path.expanduser()` to expand the user's home directory.
2. Use `os.path.abspath()` to get the absolute path of a file.
3. Use `os.access()` to check if a file or directory exists and is accessible."
"    def _get_best_study_config(self):
        metadata = {
            'best_trial_number': self.study.best_trial.number,
            'best_trial_evaluation': self.study.best_value,
        }

        pipeline_config = dict()
        for k, v in self.study.user_attrs.items():
            if k.startswith('pykeen_'):
                metadata[k[len('pykeen_'):]] = v
            elif k in {'metric'}:
                continue
            else:
                pipeline_config[k] = v

        for field in dataclasses.fields(self.objective):
            if (not field.name.endswith('_kwargs') and field.name not in {
                'training',
                'testing',
                'validation',
            }) or field.name in {'metric'}:
                continue
            field_kwargs = getattr(self.objective, field.name)
            if field_kwargs:
                logger.debug(f'saving pre-specified field in pipeline config: {field.name}={field_kwargs}')
                pipeline_config[field.name] = field_kwargs

        for k, v in self.study.best_params.items():
            sk, ssk = k.split('.')
            sk = f'{sk}_kwargs'
            if sk not in pipeline_config:
                pipeline_config[sk] = {}
            logger.debug(f'saving optimized field in pipeline config: {sk}.{ssk}={v}')
            pipeline_config[sk][ssk] = v

        for k in ('stopper', 'stopper_kwargs'):
            if k in pipeline_config:
                v = pipeline_config.pop(k)
                metadata[f'_{k}_removed_comment'] = f'{k} config removed after HPO: {v}'

        stopped_epoch = self.study.best_trial.user_attrs.get(STOPPED_EPOCH_KEY)
        if stopped_epoch is not None:
            old_num_epochs = pipeline_config['training_kwargs']['num_epochs']
            metadata['_stopper_comment'] = (
                f'While the original config had {old_num_epochs},'
                f' early stopping will now switch it to {int(stopped_epoch)}'
            )
            pipeline_config['training_kwargs']['num_epochs'] = int(stopped_epoch)
        return dict(metadata=metadata, pipeline=pipeline_config)","1. Use `user_attrs` instead of `hyperparameters` to store user-specified values.
2. Remove the `metric` key from the `pipeline_config` dictionary.
3. Use `int()` to convert the `stopped_epoch` value to an integer before storing it in the `pipeline_config` dictionary."
"def hpo_pipeline(
    *,
    # 1. Dataset
    dataset: Union[None, str, Dataset, Type[Dataset]] = None,
    dataset_kwargs: Optional[Mapping[str, Any]] = None,
    training: Union[None, str, TriplesFactory] = None,
    testing: Union[None, str, TriplesFactory] = None,
    validation: Union[None, str, TriplesFactory] = None,
    evaluation_entity_whitelist: Optional[Collection[str]] = None,
    evaluation_relation_whitelist: Optional[Collection[str]] = None,
    # 2. Model
    model: Union[str, Type[Model]],
    model_kwargs: Optional[Mapping[str, Any]] = None,
    model_kwargs_ranges: Optional[Mapping[str, Any]] = None,
    # 3. Loss
    loss: Union[None, str, Type[Loss]] = None,
    loss_kwargs: Optional[Mapping[str, Any]] = None,
    loss_kwargs_ranges: Optional[Mapping[str, Any]] = None,
    # 4. Regularizer
    regularizer: Union[None, str, Type[Regularizer]] = None,
    regularizer_kwargs: Optional[Mapping[str, Any]] = None,
    regularizer_kwargs_ranges: Optional[Mapping[str, Any]] = None,
    # 5. Optimizer
    optimizer: Union[None, str, Type[Optimizer]] = None,
    optimizer_kwargs: Optional[Mapping[str, Any]] = None,
    optimizer_kwargs_ranges: Optional[Mapping[str, Any]] = None,
    # 6. Training Loop
    training_loop: Union[None, str, Type[TrainingLoop]] = None,
    negative_sampler: Union[None, str, Type[NegativeSampler]] = None,
    negative_sampler_kwargs: Optional[Mapping[str, Any]] = None,
    negative_sampler_kwargs_ranges: Optional[Mapping[str, Any]] = None,
    # 7. Training
    training_kwargs: Optional[Mapping[str, Any]] = None,
    training_kwargs_ranges: Optional[Mapping[str, Any]] = None,
    stopper: Union[None, str, Type[Stopper]] = None,
    stopper_kwargs: Optional[Mapping[str, Any]] = None,
    # 8. Evaluation
    evaluator: Union[None, str, Type[Evaluator]] = None,
    evaluator_kwargs: Optional[Mapping[str, Any]] = None,
    evaluation_kwargs: Optional[Mapping[str, Any]] = None,
    metric: Optional[str] = None,
    # 9. Tracking
    result_tracker: Union[None, str, Type[ResultTracker]] = None,
    result_tracker_kwargs: Optional[Mapping[str, Any]] = None,
    # 6. Misc
    device: Union[None, str, torch.device] = None,
    #  Optuna Study Settings
    storage: Union[None, str, BaseStorage] = None,
    sampler: Union[None, str, Type[BaseSampler]] = None,
    sampler_kwargs: Optional[Mapping[str, Any]] = None,
    pruner: Union[None, str, Type[BasePruner]] = None,
    pruner_kwargs: Optional[Mapping[str, Any]] = None,
    study_name: Optional[str] = None,
    direction: Optional[str] = None,
    load_if_exists: bool = False,
    # Optuna Optimization Settings
    n_trials: Optional[int] = None,
    timeout: Optional[int] = None,
    n_jobs: Optional[int] = None,
    save_model_directory: Optional[str] = None,
) -> HpoPipelineResult:
    """"""Train a model on the given dataset.

    :param dataset:
        The name of the dataset (a key from :data:`pykeen.datasets.datasets`) or the :class:`pykeen.datasets.Dataset`
        instance. Alternatively, the training triples factory (``training``), testing triples factory (``testing``),
        and validation triples factory (``validation``; optional) can be specified.
    :param dataset_kwargs:
        The keyword arguments passed to the dataset upon instantiation
    :param training:
        A triples factory with training instances or path to the training file if a a dataset was not specified
    :param testing:
        A triples factory with test instances or path to the test file if a dataset was not specified
    :param validation:
        A triples factory with validation instances or path to the validation file if a dataset was not specified
    :param evaluation_entity_whitelist:
        Optional restriction of evaluation to triples containing *only* these entities. Useful if the downstream task
        is only interested in certain entities, but the relational patterns with other entities improve the entity
        embedding quality. Passed to :func:`pykeen.pipeline.pipeline`.
    :param evaluation_relation_whitelist:
        Optional restriction of evaluation to triples containing *only* these relations. Useful if the downstream task
        is only interested in certain relation, but the relational patterns with other relations improve the entity
        embedding quality. Passed to :func:`pykeen.pipeline.pipeline`.

    :param model:
        The name of the model or the model class to pass to :func:`pykeen.pipeline.pipeline`
    :param model_kwargs:
        Keyword arguments to pass to the model class on instantiation
    :param model_kwargs_ranges:
        Strategies for optimizing the models' hyper-parameters to override
        the defaults

    :param loss:
        The name of the loss or the loss class to pass to :func:`pykeen.pipeline.pipeline`
    :param loss_kwargs:
        Keyword arguments to pass to the loss on instantiation
    :param loss_kwargs_ranges:
        Strategies for optimizing the losses' hyper-parameters to override
        the defaults

    :param regularizer:
        The name of the regularizer or the regularizer class to pass to :func:`pykeen.pipeline.pipeline`
    :param regularizer_kwargs:
        Keyword arguments to pass to the regularizer on instantiation
    :param regularizer_kwargs_ranges:
        Strategies for optimizing the regularizers' hyper-parameters to override
        the defaults

    :param optimizer:
        The name of the optimizer or the optimizer class. Defaults to :class:`torch.optim.Adagrad`.
    :param optimizer_kwargs:
        Keyword arguments to pass to the optimizer on instantiation
    :param optimizer_kwargs_ranges:
        Strategies for optimizing the optimizers' hyper-parameters to override
        the defaults

    :param training_loop:
        The name of the training approach (``'slcwa'`` or ``'lcwa'``) or the training loop class
        to pass to :func:`pykeen.pipeline.pipeline`
    :param negative_sampler:
        The name of the negative sampler (``'basic'`` or ``'bernoulli'``) or the negative sampler class
        to pass to :func:`pykeen.pipeline.pipeline`. Only allowed when training with sLCWA.
    :param negative_sampler_kwargs:
        Keyword arguments to pass to the negative sampler class on instantiation
    :param negative_sampler_kwargs_ranges:
        Strategies for optimizing the negative samplers' hyper-parameters to override
        the defaults

    :param training_kwargs:
        Keyword arguments to pass to the training loop's train function on call
    :param training_kwargs_ranges:
        Strategies for optimizing the training loops' hyper-parameters to override
        the defaults. Can not specify ranges for batch size if early stopping is enabled.

    :param stopper:
        What kind of stopping to use. Default to no stopping, can be set to 'early'.
    :param stopper_kwargs:
        Keyword arguments to pass to the stopper upon instantiation.

    :param evaluator:
        The name of the evaluator or an evaluator class. Defaults to :class:`pykeen.evaluation.RankBasedEvaluator`.
    :param evaluator_kwargs:
        Keyword arguments to pass to the evaluator on instantiation
    :param evaluation_kwargs:
        Keyword arguments to pass to the evaluator's evaluate function on call

    :param result_tracker:
        The ResultsTracker class or name
    :param result_tracker_kwargs:
        The keyword arguments passed to the results tracker on instantiation

    :param metric:
        The metric to optimize over. Defaults to ``adjusted_mean_rank``.
    :param direction:
        The direction of optimization. Because the default metric is ``adjusted_mean_rank``,
        the default direction is ``minimize``.

    :param n_jobs: The number of parallel jobs. If this argument is set to :obj:`-1`, the number is
                set to CPU counts. If none, defaults to 1.

    .. note::

        The remaining parameters are passed to :func:`optuna.study.create_study`
        or :meth:`optuna.study.Study.optimize`.
    """"""
    sampler_cls = get_sampler_cls(sampler)
    pruner_cls = get_pruner_cls(pruner)

    if direction is None:
        direction = 'minimize'

    study = create_study(
        storage=storage,
        sampler=sampler_cls(**(sampler_kwargs or {})),
        pruner=pruner_cls(**(pruner_kwargs or {})),
        study_name=study_name,
        direction=direction,
        load_if_exists=load_if_exists,
    )

    # 0. Metadata/Provenance
    study.set_user_attr('pykeen_version', get_version())
    study.set_user_attr('pykeen_git_hash', get_git_hash())
    # 1. Dataset
    study.set_user_attr('dataset', _get_dataset_name(
        dataset=dataset,
        dataset_kwargs=dataset_kwargs,
        training=training,
        testing=testing,
        validation=validation,
    ))

    # 2. Model
    model: Type[Model] = get_model_cls(model)
    study.set_user_attr('model', normalize_string(model.__name__))
    logger.info(f'Using model: {model}')
    # 3. Loss
    loss: Type[Loss] = model.loss_default if loss is None else get_loss_cls(loss)
    study.set_user_attr('loss', normalize_string(loss.__name__, suffix=_LOSS_SUFFIX))
    logger.info(f'Using loss: {loss}')
    # 4. Regularizer
    regularizer: Type[Regularizer] = (
        model.regularizer_default
        if regularizer is None else
        get_regularizer_cls(regularizer)
    )
    study.set_user_attr('regularizer', regularizer.get_normalized_name())
    logger.info(f'Using regularizer: {regularizer}')
    # 5. Optimizer
    optimizer: Type[Optimizer] = get_optimizer_cls(optimizer)
    study.set_user_attr('optimizer', normalize_string(optimizer.__name__))
    logger.info(f'Using optimizer: {optimizer}')
    # 6. Training Loop
    training_loop: Type[TrainingLoop] = get_training_loop_cls(training_loop)
    study.set_user_attr('training_loop', training_loop.get_normalized_name())
    logger.info(f'Using training loop: {training_loop}')
    if training_loop is SLCWATrainingLoop:
        negative_sampler: Optional[Type[NegativeSampler]] = get_negative_sampler_cls(negative_sampler)
        study.set_user_attr('negative_sampler', negative_sampler.get_normalized_name())
        logger.info(f'Using negative sampler: {negative_sampler}')
    else:
        negative_sampler: Optional[Type[NegativeSampler]] = None
    # 7. Training
    stopper: Type[Stopper] = get_stopper_cls(stopper)

    if stopper is EarlyStopper and training_kwargs_ranges and 'epochs' in training_kwargs_ranges:
        raise ValueError('can not use early stopping while optimizing epochs')

    # 8. Evaluation
    evaluator: Type[Evaluator] = get_evaluator_cls(evaluator)
    study.set_user_attr('evaluator', evaluator.get_normalized_name())
    logger.info(f'Using evaluator: {evaluator}')
    if metric is None:
        metric = 'adjusted_mean_rank'
    study.set_user_attr('metric', metric)
    logger.info(f'Attempting to {direction} {metric}')

    # 9. Tracking
    result_tracker: Type[ResultTracker] = get_result_tracker_cls(result_tracker)

    objective = Objective(
        # 1. Dataset
        dataset=dataset,
        dataset_kwargs=dataset_kwargs,
        training=training,
        testing=testing,
        validation=validation,
        evaluation_entity_whitelist=evaluation_entity_whitelist,
        evaluation_relation_whitelist=evaluation_relation_whitelist,
        # 2. Model
        model=model,
        model_kwargs=model_kwargs,
        model_kwargs_ranges=model_kwargs_ranges,
        # 3. Loss
        loss=loss,
        loss_kwargs=loss_kwargs,
        loss_kwargs_ranges=loss_kwargs_ranges,
        # 4. Regularizer
        regularizer=regularizer,
        regularizer_kwargs=regularizer_kwargs,
        regularizer_kwargs_ranges=regularizer_kwargs_ranges,
        # 5. Optimizer
        optimizer=optimizer,
        optimizer_kwargs=optimizer_kwargs,
        optimizer_kwargs_ranges=optimizer_kwargs_ranges,
        # 6. Training Loop
        training_loop=training_loop,
        negative_sampler=negative_sampler,
        negative_sampler_kwargs=negative_sampler_kwargs,
        negative_sampler_kwargs_ranges=negative_sampler_kwargs_ranges,
        # 7. Training
        training_kwargs=training_kwargs,
        training_kwargs_ranges=training_kwargs_ranges,
        stopper=stopper,
        stopper_kwargs=stopper_kwargs,
        # 8. Evaluation
        evaluator=evaluator,
        evaluator_kwargs=evaluator_kwargs,
        evaluation_kwargs=evaluation_kwargs,
        # 9. Tracker
        result_tracker=result_tracker,
        result_tracker_kwargs=result_tracker_kwargs,
        # Optuna Misc.
        metric=metric,
        save_model_directory=save_model_directory,
        # Pipeline Misc.
        device=device,
    )

    # Invoke optimization of the objective function.
    study.optimize(
        objective,
        n_trials=n_trials,
        timeout=timeout,
        n_jobs=n_jobs or 1,
    )

    return HpoPipelineResult(
        study=study,
        objective=objective,
    )","1. Use `torch.jit.script` to make the model's forward pass
                    deterministic and protect it from adversarial attacks.
2. Use `torch.utils.data.DataLoader` to load data in batches and avoid
                    loading the entire dataset into memory.
3. Use `torch.cuda.is_available()` to check if CUDA is available before
                    using GPU acceleration."
"def pipeline(  # noqa: C901
    *,
    # 1. Dataset
    dataset: Union[None, str, Dataset, Type[Dataset]] = None,
    dataset_kwargs: Optional[Mapping[str, Any]] = None,
    training: Union[None, TriplesFactory, str] = None,
    testing: Union[None, TriplesFactory, str] = None,
    validation: Union[None, TriplesFactory, str] = None,
    evaluation_entity_whitelist: Optional[Collection[str]] = None,
    evaluation_relation_whitelist: Optional[Collection[str]] = None,
    # 2. Model
    model: Union[str, Type[Model]],
    model_kwargs: Optional[Mapping[str, Any]] = None,
    # 3. Loss
    loss: Union[None, str, Type[Loss]] = None,
    loss_kwargs: Optional[Mapping[str, Any]] = None,
    # 4. Regularizer
    regularizer: Union[None, str, Type[Regularizer]] = None,
    regularizer_kwargs: Optional[Mapping[str, Any]] = None,
    # 5. Optimizer
    optimizer: Union[None, str, Type[Optimizer]] = None,
    optimizer_kwargs: Optional[Mapping[str, Any]] = None,
    clear_optimizer: bool = True,
    # 6. Training Loop
    training_loop: Union[None, str, Type[TrainingLoop]] = None,
    negative_sampler: Union[None, str, Type[NegativeSampler]] = None,
    negative_sampler_kwargs: Optional[Mapping[str, Any]] = None,
    # 7. Training (ronaldo style)
    training_kwargs: Optional[Mapping[str, Any]] = None,
    stopper: Union[None, str, Type[Stopper]] = None,
    stopper_kwargs: Optional[Mapping[str, Any]] = None,
    # 8. Evaluation
    evaluator: Union[None, str, Type[Evaluator]] = None,
    evaluator_kwargs: Optional[Mapping[str, Any]] = None,
    evaluation_kwargs: Optional[Mapping[str, Any]] = None,
    # 9. Tracking
    result_tracker: Union[None, str, Type[ResultTracker]] = None,
    result_tracker_kwargs: Optional[Mapping[str, Any]] = None,
    # Misc
    automatic_memory_optimization: bool = True,
    metadata: Optional[Dict[str, Any]] = None,
    device: Union[None, str, torch.device] = None,
    random_seed: Optional[int] = None,
    use_testing_data: bool = True,
) -> PipelineResult:
    """"""Train and evaluate a model.

    :param dataset:
        The name of the dataset (a key from :data:`pykeen.datasets.datasets`) or the :class:`pykeen.datasets.Dataset`
        instance. Alternatively, the training triples factory (``training``), testing triples factory (``testing``),
        and validation triples factory (``validation``; optional) can be specified.
    :param dataset_kwargs:
        The keyword arguments passed to the dataset upon instantiation
    :param training:
        A triples factory with training instances or path to the training file if a a dataset was not specified
    :param testing:
        A triples factory with training instances or path to the test file if a dataset was not specified
    :param validation:
        A triples factory with validation instances or path to the validation file if a dataset was not specified
    :param evaluation_entity_whitelist:
        Optional restriction of evaluation to triples containing *only* these entities. Useful if the downstream task
        is only interested in certain entities, but the relational patterns with other entities improve the entity
        embedding quality.
    :param evaluation_relation_whitelist:
        Optional restriction of evaluation to triples containing *only* these relations. Useful if the downstream task
        is only interested in certain relation, but the relational patterns with other relations improve the entity
        embedding quality.

    :param model:
        The name of the model or the model class
    :param model_kwargs:
        Keyword arguments to pass to the model class on instantiation

    :param loss:
        The name of the loss or the loss class.
    :param loss_kwargs:
        Keyword arguments to pass to the loss on instantiation

    :param regularizer:
        The name of the regularizer or the regularizer class.
    :param regularizer_kwargs:
        Keyword arguments to pass to the regularizer on instantiation

    :param optimizer:
        The name of the optimizer or the optimizer class. Defaults to :class:`torch.optim.Adagrad`.
    :param optimizer_kwargs:
        Keyword arguments to pass to the optimizer on instantiation
    :param clear_optimizer:
        Whether to delete the optimizer instance after training. As the optimizer might have additional memory
        consumption due to e.g. moments in Adam, this is the default option. If you want to continue training, you
        should set it to False, as the optimizer's internal parameter will get lost otherwise.

    :param training_loop:
        The name of the training loop's training approach (``'slcwa'`` or ``'lcwa'``) or the training loop class.
        Defaults to :class:`pykeen.training.SLCWATrainingLoop`.
    :param negative_sampler:
        The name of the negative sampler (``'basic'`` or ``'bernoulli'``) or the negative sampler class.
        Only allowed when training with sLCWA.
        Defaults to :class:`pykeen.sampling.BasicNegativeSampler`.
    :param negative_sampler_kwargs:
        Keyword arguments to pass to the negative sampler class on instantiation

    :param training_kwargs:
        Keyword arguments to pass to the training loop's train function on call
    :param stopper:
        What kind of stopping to use. Default to no stopping, can be set to 'early'.
    :param stopper_kwargs:
        Keyword arguments to pass to the stopper upon instantiation.

    :param evaluator:
        The name of the evaluator or an evaluator class. Defaults to :class:`pykeen.evaluation.RankBasedEvaluator`.
    :param evaluator_kwargs:
        Keyword arguments to pass to the evaluator on instantiation
    :param evaluation_kwargs:
        Keyword arguments to pass to the evaluator's evaluate function on call

    :param result_tracker:
        The ResultsTracker class or name
    :param result_tracker_kwargs:
        The keyword arguments passed to the results tracker on instantiation

    :param metadata:
        A JSON dictionary to store with the experiment
    :param use_testing_data:
        If true, use the testing triples. Otherwise, use the validation triples. Defaults to true - use testing triples.
    """"""
    if training_kwargs is None:
        training_kwargs = {}

    # To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the
    # used random_seed to ensure reproducible results
    checkpoint_name = training_kwargs.get('checkpoint_name')
    if checkpoint_name is not None:
        checkpoint_directory = pathlib.Path(training_kwargs.get('checkpoint_directory', PYKEEN_CHECKPOINTS))
        checkpoint_directory.mkdir(parents=True, exist_ok=True)
        checkpoint_path = checkpoint_directory / checkpoint_name
        if checkpoint_path.is_file():
            checkpoint_dict = torch.load(checkpoint_path)
            random_seed = checkpoint_dict['random_seed']
            logger.info('loaded random seed %s from checkpoint.', random_seed)
            # We have to set clear optimizer to False since training should be continued
            clear_optimizer = False
        else:
            logger.info(f""=> no training loop checkpoint file found at '{checkpoint_path}'. Creating a new file."")
            if random_seed is None:
                random_seed = random_non_negative_int()
                logger.warning(f'No random seed is specified. Setting to {random_seed}.')
    elif random_seed is None:
        random_seed = random_non_negative_int()
        logger.warning(f'No random seed is specified. Setting to {random_seed}.')
    set_random_seed(random_seed)

    result_tracker_cls: Type[ResultTracker] = get_result_tracker_cls(result_tracker)
    result_tracker = result_tracker_cls(**(result_tracker_kwargs or {}))

    if not metadata:
        metadata = {}
    title = metadata.get('title')

    # Start tracking
    result_tracker.start_run(run_name=title)

    device = resolve_device(device)

    dataset_instance: Dataset = get_dataset(
        dataset=dataset,
        dataset_kwargs=dataset_kwargs,
        training=training,
        testing=testing,
        validation=validation,
    )
    if dataset is not None:
        result_tracker.log_params(dict(dataset=dataset_instance.get_normalized_name()))
    else:  # means that dataset was defined by triples factories
        result_tracker.log_params(dict(dataset='<user defined>'))

    training, testing, validation = dataset_instance.training, dataset_instance.testing, dataset_instance.validation
    # evaluation restriction to a subset of entities/relations
    if any(f is not None for f in (evaluation_entity_whitelist, evaluation_relation_whitelist)):
        testing = testing.new_with_restriction(
            entities=evaluation_entity_whitelist,
            relations=evaluation_relation_whitelist,
        )
        if validation is not None:
            validation = validation.new_with_restriction(
                entities=evaluation_entity_whitelist,
                relations=evaluation_relation_whitelist,
            )

    if model_kwargs is None:
        model_kwargs = {}
    model_kwargs.update(preferred_device=device)
    model_kwargs.setdefault('random_seed', random_seed)

    if regularizer is not None:
        # FIXME this should never happen.
        if 'regularizer' in model_kwargs:
            logger.warning('Can not specify regularizer in kwargs and model_kwargs. removing from model_kwargs')
            del model_kwargs['regularizer']
        regularizer_cls: Type[Regularizer] = get_regularizer_cls(regularizer)
        model_kwargs['regularizer'] = regularizer_cls(
            device=device,
            **(regularizer_kwargs or {}),
        )

    if loss is not None:
        if 'loss' in model_kwargs:  # FIXME
            logger.warning('duplicate loss in kwargs and model_kwargs. removing from model_kwargs')
            del model_kwargs['loss']
        loss_cls = get_loss_cls(loss)
        _loss = loss_cls(**(loss_kwargs or {}))
        model_kwargs.setdefault('loss', _loss)

    model = get_model_cls(model)
    model_instance: Model = model(
        triples_factory=training,
        **model_kwargs,
    )
    # Log model parameters
    result_tracker.log_params(params=dict(cls=model.__name__, kwargs=model_kwargs), prefix='model')

    optimizer = get_optimizer_cls(optimizer)
    training_loop = get_training_loop_cls(training_loop)

    if optimizer_kwargs is None:
        optimizer_kwargs = {}

    # Log optimizer parameters
    result_tracker.log_params(params=dict(cls=optimizer.__name__, kwargs=optimizer_kwargs), prefix='optimizer')
    optimizer_instance = optimizer(
        params=model_instance.get_grad_params(),
        **optimizer_kwargs,
    )

    result_tracker.log_params(params=dict(cls=training_loop.__name__), prefix='training_loop')
    if negative_sampler is None:
        training_loop_instance: TrainingLoop = training_loop(
            model=model_instance,
            optimizer=optimizer_instance,
            automatic_memory_optimization=automatic_memory_optimization,
        )
    elif training_loop is not SLCWATrainingLoop:
        raise ValueError('Can not specify negative sampler with LCWA')
    else:
        negative_sampler = get_negative_sampler_cls(negative_sampler)
        result_tracker.log_params(
            params=dict(cls=negative_sampler.__name__, kwargs=negative_sampler_kwargs),
            prefix='negative_sampler',
        )
        training_loop_instance: TrainingLoop = SLCWATrainingLoop(
            model=model_instance,
            optimizer=optimizer_instance,
            automatic_memory_optimization=automatic_memory_optimization,
            negative_sampler_cls=negative_sampler,
            negative_sampler_kwargs=negative_sampler_kwargs,
        )

    evaluator = get_evaluator_cls(evaluator)

    if evaluator_kwargs is None:
        evaluator_kwargs = {}
    evaluator_kwargs.setdefault('automatic_memory_optimization', automatic_memory_optimization)
    evaluator_instance: Evaluator = evaluator(**evaluator_kwargs)

    if evaluation_kwargs is None:
        evaluation_kwargs = {}

    # Stopping
    if 'stopper' in training_kwargs and stopper is not None:
        raise ValueError('Specified stopper in training_kwargs and as stopper')
    if 'stopper' in training_kwargs:
        stopper = training_kwargs.pop('stopper')
    if stopper_kwargs is None:
        stopper_kwargs = {}

    # Load the evaluation batch size for the stopper, if it has been set
    _evaluation_batch_size = evaluation_kwargs.get('batch_size')
    if _evaluation_batch_size is not None:
        stopper_kwargs.setdefault('evaluation_batch_size', _evaluation_batch_size)

    # By default there's a stopper that does nothing interesting
    stopper_cls: Type[Stopper] = get_stopper_cls(stopper)
    stopper: Stopper = stopper_cls(
        model=model_instance,
        evaluator=evaluator_instance,
        evaluation_triples_factory=validation,
        result_tracker=result_tracker,
        **stopper_kwargs,
    )

    training_kwargs.setdefault('num_epochs', 5)
    training_kwargs.setdefault('batch_size', 256)
    result_tracker.log_params(params=training_kwargs, prefix='training')

    # Add logging for debugging
    logging.debug(""Run Pipeline based on following config:"")
    if dataset is not None:
        logging.debug(f""dataset: {dataset}"")
        logging.debug(f""dataset_kwargs: {dataset_kwargs}"")
    else:
        logging.debug('training: %s', training)
        logging.debug('testing: %s', testing)
        if validation:
            logging.debug('validation: %s', validation)
    logging.debug(f""model: {model}"")
    logging.debug(f""model_kwargs: {model_kwargs}"")
    logging.debug(f""loss: {loss}"")
    logging.debug(f""loss_kwargs: {loss_kwargs}"")
    logging.debug(f""regularizer: {regularizer}"")
    logging.debug(f""regularizer_kwargs: {regularizer_kwargs}"")
    logging.debug(f""optimizer: {optimizer}"")
    logging.debug(f""optimizer_kwargs: {optimizer_kwargs}"")
    logging.debug(f""training_loop: {training_loop}"")
    logging.debug(f""negative_sampler: {negative_sampler}"")
    logging.debug(f""_negative_sampler_kwargs: {negative_sampler_kwargs}"")
    logging.debug(f""_training_kwargs: {training_kwargs}"")
    logging.debug(f""stopper: {stopper}"")
    logging.debug(f""stopper_kwargs: {stopper_kwargs}"")
    logging.debug(f""evaluator: {evaluator}"")
    logging.debug(f""evaluator_kwargs: {evaluator_kwargs}"")

    # Train like Cristiano Ronaldo
    training_start_time = time.time()
    losses = training_loop_instance.train(
        stopper=stopper,
        result_tracker=result_tracker,
        clear_optimizer=clear_optimizer,
        **training_kwargs,
    )
    training_end_time = time.time() - training_start_time

    if use_testing_data:
        mapped_triples = testing.mapped_triples
    else:
        mapped_triples = validation.mapped_triples

    # Evaluate
    # Reuse optimal evaluation parameters from training if available
    if evaluator_instance.batch_size is not None or evaluator_instance.slice_size is not None:
        evaluation_kwargs['batch_size'] = evaluator_instance.batch_size
        evaluation_kwargs['slice_size'] = evaluator_instance.slice_size
    # Add logging about evaluator for debugging
    logging.debug(""Evaluation will be run with following parameters:"")
    logging.debug(f""evaluation_kwargs: {evaluation_kwargs}"")
    evaluate_start_time = time.time()
    metric_results: MetricResults = evaluator_instance.evaluate(
        model=model_instance,
        mapped_triples=mapped_triples,
        **evaluation_kwargs,
    )
    evaluate_end_time = time.time() - evaluate_start_time
    result_tracker.log_metrics(
        metrics=metric_results.to_dict(),
        step=training_kwargs.get('num_epochs'),
    )
    result_tracker.end_run()

    return PipelineResult(
        random_seed=random_seed,
        model=model_instance,
        training_loop=training_loop_instance,
        losses=losses,
        stopper=stopper,
        metric_results=metric_results,
        metadata=metadata,
        train_seconds=training_end_time,
        evaluate_seconds=evaluate_end_time,
    )","1. Use `torch.load()` to load the model instead of `pickle.load()`.
2. Use `torch.save()` to save the model instead of `pickle.dump()`.
3. Use `torch.device()` to set the device for the model instead of `torch.cuda.is_available()`."
"    def summary_str(self, end='\\n') -> str:
        """"""Make a summary string of all of the factories.""""""
        rows = self._summary_rows()
        n_triples = sum(count for *_, count in rows)
        rows.append(('Total', '-', '-', n_triples))
        t = tabulate(rows, headers=['Name', 'Entities', 'Relations', 'Triples'])
        return f'{self.__class__.__name__} (create_inverse_triples={self.create_inverse_triples})\\n{t}{end}'","1. Use `functools.lru_cache` to cache the results of expensive computations.
2. Validate user input to prevent injection attacks.
3. Use `secure_filename` to sanitize filenames before saving files."
"    def summarize(self) -> None:
        """"""Print a summary of the dataset.""""""
        print(self.summary_str())","1. **Use proper permissions**. The `print()` function should not be used to print sensitive data, such as passwords. Instead, use a secure logging library that can encrypt the data before it is printed.
2. **Sanitize user input**. The `summary_str()` function should sanitize all user input before it is used to generate the summary. This can be done by using a function like `html.escape()` to escape any special characters.
3. **Use strong passwords**. The `Dataset` class should use a strong password to protect the data. This can be done by using a password manager to generate a random password that is at least 12 characters long and contains a mix of uppercase and lowercase letters, numbers, and symbols."
"    def _load(self) -> None:
        self._training = TriplesFactory.from_path(
            path=self.training_path,
            create_inverse_triples=self.create_inverse_triples,
        )
        self._testing = TriplesFactory.from_path(
            path=self.testing_path,
            entity_to_id=self._training.entity_to_id,  # share entity index with training
            relation_to_id=self._training.relation_to_id,  # share relation index with training
        )","1. Use `pathlib.Path` to handle file paths securely.
2. Use `contextlib.closing` to ensure that file handles are closed after use.
3. Use `typing` to annotate the types of arguments and return values, which can help catch errors."
"    def _load_validation(self) -> None:
        # don't call this function by itself. assumes called through the `validation`
        # property and the _training factory has already been loaded
        self._validation = TriplesFactory.from_path(
            path=self.validation_path,
            entity_to_id=self._training.entity_to_id,  # share entity index with training
            relation_to_id=self._training.relation_to_id,  # share relation index with training
        )","1. Use `validation_path` as an input parameter instead of a class attribute. This will prevent the validation path from being hard-coded in the source code.
2. Validate the `validation_path` argument to ensure that it is a valid file path. This will help to prevent attackers from providing a malicious path that could lead to code execution or data exfiltration.
3. Use `os.path.exists()` to check if the `validation_path` file exists before attempting to load it. This will help to prevent the code from crashing if the file does not exist."
"def generate_triples(
    num_entities: int = 33,
    num_relations: int = 7,
    num_triples: int = 101,
    compact: bool = True,
    random_state: RandomHint = None,
) -> np.ndarray:
    """"""Generate random triples.""""""
    random_state = ensure_random_state(random_state)
    rv = np.stack([
        random_state.randint(num_entities, size=(num_triples,)),
        random_state.randint(num_relations, size=(num_triples,)),
        random_state.randint(num_entities, size=(num_triples,)),
    ], axis=1)

    if compact:
        new_entity_id = {
            entity: i
            for i, entity in enumerate(sorted(get_entities(rv)))
        }
        new_relation_id = {
            relation: i
            for i, relation in enumerate(sorted(get_relations(rv)))
        }
        rv = np.asarray([
            [new_entity_id[h], new_relation_id[r], new_entity_id[t]]
            for h, r, t in rv
        ], dtype=int)

    return rv","1. Use `np.random.default_rng()` instead of `random_state` to generate random numbers.
2. Use `np.unique()` to remove duplicate entities and relations from the generated triples.
3. Use `np.asarray()` to convert the generated triples to a NumPy array."
"def generate_labeled_triples(
    num_entities: int = 33,
    num_relations: int = 7,
    num_triples: int = 101,
    random_state: RandomHint = None,
) -> np.ndarray:
    """"""Generate labeled random triples.""""""
    t = generate_triples(
        num_entities=num_entities,
        num_relations=num_relations,
        num_triples=num_triples,
        compact=False,
        random_state=random_state,
    )
    entity_id_to_label = _make_id_to_labels(num_entities)
    relation_id_to_label = _make_id_to_labels(num_relations)
    return np.asarray([
        (
            entity_id_to_label[h],
            relation_id_to_label[r],
            entity_id_to_label[t],
        )
        for h, r, t in t
    ], dtype=str)","1. Use `np.random.RandomState` instead of `random.Random` to generate random numbers.
2. Use `np.asarray` to convert the generated triples to a NumPy array.
3. Use `np.ndarray.dtype` to specify the data type of the NumPy array."
"def generate_triples_factory(
    num_entities: int = 33,
    num_relations: int = 7,
    num_triples: int = 101,
    random_state: RandomHint = None,
    create_inverse_triples: bool = False,
) -> TriplesFactory:
    """"""Generate a triples factory with random triples.""""""
    triples = generate_labeled_triples(
        num_entities=num_entities,
        num_relations=num_relations,
        num_triples=num_triples,
        random_state=random_state,
    )
    return TriplesFactory.from_labeled_triples(
        triples=triples,
        create_inverse_triples=create_inverse_triples,
    )","1. Use `secrets.token_urlsafe()` to generate a random string for `random_state`.
2. Use `TriplesFactory.from_triples()` instead of `TriplesFactory.from_labeled_triples()` to avoid creating duplicate triples.
3. Sanitize the input data to prevent SQL injection attacks."
"    def __init__(
        self,
        triples_factory: TriplesFactory,
        minimum_frequency: Optional[float] = None,
        symmetric: bool = True,
        use_tqdm: bool = True,
        use_multiprocessing: bool = False,
    ):
        """"""Index the inverse frequencies and the inverse relations in the triples factory.

        :param triples_factory: The triples factory to index.
        :param minimum_frequency: The minimum overlap between two relations' triples to consider them as inverses. The
         default value, 0.97, is taken from `Toutanova and Chen (2015) <https://www.aclweb.org/anthology/W15-4007/>`_,
         who originally described the generation of FB15k-237.
        """"""
        self.triples_factory = triples_factory
        if minimum_frequency is None:
            minimum_frequency = 0.97
        self.minimum_frequency = minimum_frequency

        if use_multiprocessing:
            use_tqdm = False

        self.candidate_duplicate_relations = get_candidate_duplicate_relations(
            triples_factory=self.triples_factory,
            minimum_frequency=self.minimum_frequency,
            symmetric=symmetric,
            use_tqdm=use_tqdm,
            use_multiprocessing=use_multiprocessing,
        )
        logger.info(
            f'identified {len(self.candidate_duplicate_relations)} candidate duplicate relationships'
            f' at similarity > {self.minimum_frequency} in {self.triples_factory}.',
        )
        self.duplicate_relations_to_delete = {r for r, _ in self.candidate_duplicate_relations}

        self.candidate_inverse_relations = get_candidate_inverse_relations(
            triples_factory=self.triples_factory,
            minimum_frequency=self.minimum_frequency,
            symmetric=symmetric,
            use_tqdm=use_tqdm,
            use_multiprocessing=use_multiprocessing,
        )
        logger.info(
            f'identified {len(self.candidate_inverse_relations)} candidate inverse pairs'
            f' at similarity > {self.minimum_frequency} in {self.triples_factory}',
        )

        if symmetric:
            self.inverses = dict(tuple(sorted(k)) for k in self.candidate_inverse_relations.keys())
            self.inverse_relations_to_delete = set(self.inverses.values())
        else:
            self.mutual_inverse = set()
            self.not_mutual_inverse = set()
            for r1, r2 in self.candidate_inverse_relations:
                if (r2, r1) in self.candidate_inverse_relations:
                    self.mutual_inverse.add((r1, r2))
                else:
                    self.not_mutual_inverse.add((r1, r2))
            logger.info(
                f'{len(self.mutual_inverse)} are mutual inverse ({len(self.mutual_inverse) // 2}'
                f' relations) and {len(self.not_mutual_inverse)} non-mutual inverse.',
            )

            # basically take all candidates
            self.inverses = dict(self.candidate_inverse_relations.keys())
            self.inverse_relations_to_delete = prioritize_mapping(self.candidate_inverse_relations)

        logger.info(f'identified {len(self.inverse_relations_to_delete)} from {self.triples_factory} to delete')","1. Use tqdm only when not using multiprocessing.
2. Use a more secure function to get candidate duplicate relations.
3. Use a more secure function to get candidate inverse relations."
"    def apply(self, triples_factory: TriplesFactory) -> TriplesFactory:
        """"""Make a new triples factory containing neither duplicate nor inverse relationships.""""""
        return triples_factory.new_without_relations(self.relations_to_delete)","1. Use `functools.lru_cache` to cache the results of `new_without_relations` to avoid recomputing the same results multiple times.
2. Validate the input arguments of `new_without_relations` to ensure that they are of the correct type and value.
3. Handle errors in `new_without_relations` gracefully, such as by raising a `ValueError` if an invalid argument is passed."
"def unleak(
    train: TriplesFactory,
    *triples_factories: TriplesFactory,
    n: Union[None, int, float] = None,
    minimum_frequency: Optional[float] = None,
) -> Iterable[TriplesFactory]:
    """"""Unleak a train, test, and validate triples factory.

    :param train: The target triples factory
    :param triples_factories: All other triples factories (test, validate, etc.)
    :param n: Either the (integer) number of top relations to keep or the (float) percentage of top relationships
     to keep. If left none, frequent relations are not removed.
    :param minimum_frequency: The minimum overlap between two relations' triples to consider them as inverses or
     duplicates. The default value, 0.97, is taken from
     `Toutanova and Chen (2015) <https://www.aclweb.org/anthology/W15-4007/>`_, who originally described the generation
     of FB15k-237.
    """"""
    if n is not None:
        frequent_relations = train.get_most_frequent_relations(n=n)
        logger.info(f'keeping most frequent relations from {train}')
        train = train.new_with_relations(frequent_relations)
        triples_factories = [
            triples_factory.new_with_relations(frequent_relations)
            for triples_factory in triples_factories
        ]

    # Calculate which relations are the inverse ones
    sealant = Sealant(train, minimum_frequency=minimum_frequency)

    if not sealant.relations_to_delete:
        logger.info(f'no relations to delete identified from {train}')
    else:
        train = sealant.apply(train)
        triples_factories = [
            sealant.apply(triples_factory)
            for triples_factory in triples_factories
        ]

    return reindex(train, *triples_factories)","1. Use `type hints` to specify the types of arguments and return values. This will help catch errors early and make the code more readable.
2. Use `f-strings` to format strings instead of concatenation. This will make the code more readable and less error-prone.
3. Use `logging` to log important events and errors. This will help you track down problems and debug the code."
"def reindex(*triples_factories: TriplesFactory) -> List[TriplesFactory]:
    """"""Reindex a set of triples factories.""""""
    triples = np.concatenate(
        [
            triples_factory.triples
            for triples_factory in triples_factories
        ],
        axis=0,
    )
    entity_to_id = create_entity_mapping(triples)
    relation_to_id = create_relation_mapping(set(triples[:, 1]))

    return [
        TriplesFactory.from_labeled_triples(
            triples=triples_factory.triples,
            entity_to_id=entity_to_id,
            relation_to_id=relation_to_id,
            # FIXME doesn't carry flag of create_inverse_triples through
        )
        for triples_factory in triples_factories
    ]","1. Use `np.unique` to remove duplicate triples.
2. Use `set` to remove duplicate relations.
3. Use `TriplesFactory.from_labeled_triples` to create a new `TriplesFactory` instance."
"def _main():
    """"""Test unleaking FB15K.

    Run with ``python -m pykeen.triples.leakage``.
    """"""
    from pykeen.datasets import get_dataset
    logging.basicConfig(format='pykeen: %(message)s', level=logging.INFO)

    print('Summary FB15K')
    fb15k = get_dataset(dataset='fb15k')
    summarize(fb15k.training, fb15k.testing, fb15k.validation)

    print('\\nSummary FB15K (cleaned)')
    n = 401  # magic 401 from the paper
    train, test, validate = unleak(fb15k.training, fb15k.testing, fb15k.validation, n=n)
    summarize(train, test, validate)

    print('\\nSummary FB15K-237')
    fb15k237 = get_dataset(dataset='fb15k237')
    summarize(fb15k237.training, fb15k237.testing, fb15k237.validation)","1. Use a more secure logging level, such as `logging.WARNING` or `logging.ERROR`.
2. Use a secrets management tool to generate the random number `n`.
3. Sanitize the input data to prevent poisoning attacks."
"    def from_labeled_triples(
        cls,
        triples: LabeledTriples,
        create_inverse_triples: bool = False,
        entity_to_id: Optional[EntityMapping] = None,
        relation_to_id: Optional[RelationMapping] = None,
        compact_id: bool = True,
    ) -> 'TriplesFactory':
        """"""
        Create a new triples factory from label-based triples.

        :param triples: shape: (n, 3), dtype: str
            The label-based triples.
        :param create_inverse_triples:
            Whether to create inverse triples.
        :param entity_to_id:
            The mapping from entity labels to ID. If None, create a new one from the triples.
        :param relation_to_id:
            The mapping from relations labels to ID. If None, create a new one from the triples.
        :param compact_id:
            Whether to compact IDs such that the IDs are consecutive.

        :return:
            A new triples factory.
        """"""
        relations = triples[:, 1]
        unique_relations = set(relations)

        # Check if the triples are inverted already
        relations_already_inverted = cls._check_already_inverted_relations(unique_relations)

        # TODO: invert triples id-based
        if create_inverse_triples or relations_already_inverted:
            create_inverse_triples = True
            if relations_already_inverted:
                logger.info(
                    f'Some triples already have suffix {INVERSE_SUFFIX}. '
                    f'Creating TriplesFactory based on inverse triples',
                )
                relation_to_inverse = {
                    re.sub('_inverse$', '', relation): f""{re.sub('_inverse$', '', relation)}{INVERSE_SUFFIX}""
                    for relation in unique_relations
                }

            else:
                relation_to_inverse = {
                    relation: f""{relation}{INVERSE_SUFFIX}""
                    for relation in unique_relations
                }
                inverse_triples = np.stack(
                    [
                        triples[:, 2],
                        np.array([relation_to_inverse[relation] for relation in relations], dtype=np.str),
                        triples[:, 0],
                    ],
                    axis=-1,
                )
                # extend original triples with inverse ones
                triples = np.concatenate([triples, inverse_triples], axis=0)

        else:
            create_inverse_triples = False
            relation_to_inverse = None

        # Generate entity mapping if necessary
        if entity_to_id is None:
            entity_to_id = create_entity_mapping(triples=triples)
        if compact_id:
            entity_to_id = compact_mapping(mapping=entity_to_id)[0]

        # Generate relation mapping if necessary
        if relation_to_id is None:
            if create_inverse_triples:
                relation_to_id = create_relation_mapping(
                    set(relation_to_inverse.keys()).union(set(relation_to_inverse.values())),
                )
            else:
                relation_to_id = create_relation_mapping(unique_relations)
        if compact_id:
            relation_to_id = compact_mapping(mapping=relation_to_id)[0]

        # Map triples of labels to triples of IDs.
        mapped_triples = _map_triples_elements_to_ids(
            triples=triples,
            entity_to_id=entity_to_id,
            relation_to_id=relation_to_id,
        )

        return cls(
            entity_to_id=entity_to_id,
            relation_to_id=relation_to_id,
            _triples=triples,
            mapped_triples=mapped_triples,
            relation_to_inverse=relation_to_inverse,
        )","1. Use `np.unique` to check if the triples are already inverted.
2. Use `re.sub` to remove the suffix `_inverse` from the relation labels.
3. Use `compact_mapping` to compact the IDs so that they are consecutive."
"    def num_relations(self) -> int:  # noqa: D401
        """"""The number of unique relations.""""""
        return len(self.relation_to_id)","1. Use `functools.lru_cache` to memoize the `num_relations` method. This will improve performance by caching the results of the method call.
2. Use `typing.NamedTuple` to define the `Relation` class. This will improve type safety by ensuring that the `Relation` objects have the correct fields.
3. Use `os.fchmod` to set the file mode of the `relation_to_id` dictionary to `0o600`. This will restrict access to the dictionary to the owner of the process."
"    def triples(self) -> np.ndarray:  # noqa: D401
        """"""The labeled triples.""""""
        # TODO: Deprecation warning. Will be replaced by re-constructing them from ID-based + mapping soon.
        return self._triples","1. Use `np.ndarray.copy()` instead of `self._triples` to avoid modifying the original data.
2. Add a deprecation warning to the function `triples()`.
3. Use `id-based` triples instead of `label-based` triples to improve security."
"    def get_inverse_relation_id(self, relation: str) -> int:
        """"""Get the inverse relation identifier for the given relation.""""""
        if not self.create_inverse_triples:
            raise ValueError('Can not get inverse triple, they have not been created.')
        inverse_relation = self.relation_to_inverse[relation]
        return self.relation_to_id[inverse_relation]","1. Use `get_relation_id` instead of accessing `relation_to_id` directly to prevent modification of the mapping.
2. Use `get_inverse_relation` instead of accessing `relation_to_inverse` directly to prevent modification of the mapping.
3. Check that `create_inverse_triples` is True before calling `get_inverse_relation_id` to prevent errors."
"    def create_slcwa_instances(self) -> Instances:
        """"""Create sLCWA instances for this factory's triples.""""""
        return SLCWAInstances(mapped_triples=self.mapped_triples)","1. Use `typing` to annotate the types of arguments and return values.
2. Use `f-strings` to format strings instead of concatenation.
3. Use `black` to format the code consistently."
"    def create_lcwa_instances(self, use_tqdm: Optional[bool] = None) -> Instances:
        """"""Create LCWA instances for this factory's triples.""""""
        return LCWAInstances.from_triples(mapped_triples=self.mapped_triples, num_entities=self.num_entities)","1. Use `use_tqdm` to control the verbosity of progress bars.
2. Use `LCWAInstances.from_triples()` to create LCWA instances from triples.
3. Set `num_entities` to the number of entities in the dataset."
"    def split(
        self,
        ratios: Union[float, Sequence[float]] = 0.8,
        *,
        random_state: RandomHint = None,
        randomize_cleanup: bool = False,
    ) -> List['TriplesFactory']:
        """"""Split a triples factory into a train/test.

        :param ratios: There are three options for this argument. First, a float can be given between 0 and 1.0,
         non-inclusive. The first triples factory will get this ratio and the second will get the rest. Second,
         a list of ratios can be given for which factory in which order should get what ratios as in ``[0.8, 0.1]``.
         The final ratio can be omitted because that can be calculated. Third, all ratios can be explicitly set in
         order such as in ``[0.8, 0.1, 0.1]`` where the sum of all ratios is 1.0.
        :param random_state: The random state used to shuffle and split the triples in this factory.
        :param randomize_cleanup: If true, uses the non-deterministic method for moving triples to the training set.
         This has the advantage that it doesn't necessarily have to move all of them, but it might be slower.

        .. code-block:: python

            ratio = 0.8  # makes a [0.8, 0.2] split
            training_factory, testing_factory = factory.split(ratio)

            ratios = [0.8, 0.1]  # makes a [0.8, 0.1, 0.1] split
            training_factory, testing_factory, validation_factory = factory.split(ratios)

            ratios = [0.8, 0.1, 0.1]  # also makes a [0.8, 0.1, 0.1] split
            training_factory, testing_factory, validation_factory = factory.split(ratios)
        """"""
        n_triples = self.triples.shape[0]

        # Prepare shuffle index
        idx = np.arange(n_triples)
        random_state = ensure_random_state(random_state)
        random_state.shuffle(idx)

        # Prepare split index
        if isinstance(ratios, float):
            ratios = [ratios]

        ratio_sum = sum(ratios)
        if ratio_sum == 1.0:
            ratios = ratios[:-1]  # vsplit doesn't take the final number into account.
        elif ratio_sum > 1.0:
            raise ValueError(f'ratios sum to more than 1.0: {ratios} (sum={ratio_sum})')

        sizes = [
            int(split_ratio * n_triples)
            for split_ratio in ratios
        ]
        # Take cumulative sum so the get separated properly
        split_idxs = np.cumsum(sizes)

        # Split triples
        triples_groups = np.vsplit(self.triples[idx], split_idxs)
        logger.info(
            'done splitting triples to groups of sizes %s',
            [triples.shape[0] for triples in triples_groups],
        )

        # Make sure that the first element has all the right stuff in it
        logger.debug('cleaning up groups')
        triples_groups = _tf_cleanup_all(triples_groups, random_state=random_state if randomize_cleanup else None)
        logger.debug('done cleaning up groups')

        for i, (triples, exp_size, exp_ratio) in enumerate(zip(triples_groups, sizes, ratios)):
            actual_size = triples.shape[0]
            actual_ratio = actual_size / exp_size * exp_ratio
            if actual_size != exp_size:
                logger.warning(
                    f'Requested ratio[{i}]={exp_ratio:.3f} (equal to size {exp_size}), but got {actual_ratio:.3f} '
                    f'(equal to size {actual_size}) to ensure that all entities/relations occur in train.',
                )

        # Make new triples factories for each group
        return [
            TriplesFactory.from_labeled_triples(
                triples=triples,
                entity_to_id=self.entity_to_id,
                relation_to_id=self.relation_to_id,
                compact_id=False,
            )
            for triples in triples_groups
        ]","1. Use `ensure_random_state` to ensure that the random state is properly seeded.
2. Validate the input arguments to the function to ensure that they are of the correct type and within the expected range.
3. Use `logging.warning` to log any warnings that occur during the function execution."
"    def get_most_frequent_relations(self, n: Union[int, float]) -> Set[str]:
        """"""Get the n most frequent relations.

        :param n: Either the (integer) number of top relations to keep or the (float) percentage of top relationships
         to keep
        """"""
        logger.info(f'applying cutoff of {n} to {self}')
        if isinstance(n, float):
            assert 0 < n < 1
            n = int(self.num_relations * n)
        elif not isinstance(n, int):
            raise TypeError('n must be either an integer or a float')

        counter = Counter(self.triples[:, 1])
        return {
            relation
            for relation, _ in counter.most_common(n)
        }","1. Use `typing` to annotate the function parameters and return types.
2. Validate the input parameters to ensure that they are of the correct type and within the expected range.
3. Use `logging` to log all security-relevant events, such as failed login attempts or suspicious activity."
"    def entity_word_cloud(self, top: Optional[int] = None):
        """"""Make a word cloud based on the frequency of occurrence of each entity in a Jupyter notebook.

        :param top: The number of top entities to show. Defaults to 100.

        .. warning::

            This function requires the ``word_cloud`` package. Use ``pip install pykeen[plotting]`` to
            install it automatically, or install it yourself with
            ``pip install git+https://github.com/kavgan/word_cloud.git``.
        """"""
        text = [f'{h} {t}' for h, _, t in self.triples]
        return self._word_cloud(text=text, top=top or 100)","1. Use `black` to format the code to improve readability.
2. Add type annotations to the function parameters and return values to catch errors early.
3. Use `f-strings` to interpolate variables into strings to avoid string concatenation."
"    def relation_word_cloud(self, top: Optional[int] = None):
        """"""Make a word cloud based on the frequency of occurrence of each relation in a Jupyter notebook.

        :param top: The number of top relations to show. Defaults to 100.

        .. warning::

            This function requires the ``word_cloud`` package. Use ``pip install pykeen[plotting]`` to
            install it automatically, or install it yourself with
            ``pip install git+https://github.com/kavgan/word_cloud.git``.
        """"""
        text = [r for _, r, _ in self.triples]
        return self._word_cloud(text=text, top=top or 100)","1. Use `type hints` to annotate the types of arguments and return values. This will help catch errors at compile time.
2. Use `proper error handling` to catch and handle errors gracefully. This will prevent your code from crashing in production.
3. Use `security best practices` when writing your code. This includes things like using secure passwords, encrypting sensitive data, and avoiding common security pitfalls."
"    def _word_cloud(self, *, text: List[str], top: int):
        try:
            from word_cloud.word_cloud_generator import WordCloud
        except ImportError:
            logger.warning(
                'Could not import module `word_cloud`. '
                'Try installing it with `pip install git+https://github.com/kavgan/word_cloud.git`',
            )
            return

        from IPython.core.display import HTML
        word_cloud = WordCloud()
        return HTML(word_cloud.get_embed_code(text=text, topn=top))","1. Use `pip install wordcloud` to install the `word_cloud` module.
2. Use `from word_cloud.word_cloud_generator import WordCloud` to import the `WordCloud` class.
3. Use `HTML(word_cloud.get_embed_code(text=text, topn=top))` to generate the HTML code for the word cloud."
"    def tensor_to_df(
        self,
        tensor: torch.LongTensor,
        **kwargs: Union[torch.Tensor, np.ndarray, Sequence],
    ) -> pd.DataFrame:
        """"""Take a tensor of triples and make a pandas dataframe with labels.

        :param tensor: shape: (n, 3)
            The triples, ID-based and in format (head_id, relation_id, tail_id).
        :param kwargs:
            Any additional number of columns. Each column needs to be of shape (n,). Reserved column names:
            {""head_id"", ""head_label"", ""relation_id"", ""relation_label"", ""tail_id"", ""tail_label""}.
        :return:
            A dataframe with n rows, and 6 + len(kwargs) columns.
        """"""
        # Input validation
        additional_columns = set(kwargs.keys())
        forbidden = additional_columns.intersection(TRIPLES_DF_COLUMNS)
        if len(forbidden) > 0:
            raise ValueError(
                f'The key-words for additional arguments must not be in {TRIPLES_DF_COLUMNS}, but {forbidden} were '
                f'used.',
            )

        # convert to numpy
        tensor = tensor.cpu().numpy()
        data = dict(zip(['head_id', 'relation_id', 'tail_id'], tensor.T))

        # vectorized label lookup
        entity_id_to_label = np.vectorize(self.entity_id_to_label.__getitem__)
        relation_id_to_label = np.vectorize(self.relation_id_to_label.__getitem__)
        for column, id_to_label in dict(
            head=entity_id_to_label,
            relation=relation_id_to_label,
            tail=entity_id_to_label,
        ).items():
            data[f'{column}_label'] = id_to_label(data[f'{column}_id'])

        # Additional columns
        for key, values in kwargs.items():
            # convert PyTorch tensors to numpy
            if torch.is_tensor(values):
                values = values.cpu().numpy()
            data[key] = values

        # convert to dataframe
        rv = pd.DataFrame(data=data)

        # Re-order columns
        columns = list(TRIPLES_DF_COLUMNS) + sorted(set(rv.columns).difference(TRIPLES_DF_COLUMNS))
        return rv.loc[:, columns]","1. Use `torch.LongTensor.tolist()` instead of `.cpu().numpy()` to convert tensors to numpy arrays. This will prevent the tensor data from being copied into the CPU memory, which could lead to a data leak.
2. Use `torch.tensor.item()` instead of `__getitem__()` to access tensor elements. This will prevent the tensor data from being copied into the CPU memory, which could lead to a data leak.
3. Use `pd.DataFrame.drop()` to remove the reserved columns from the dataframe before returning it. This will prevent users from accessing the reserved columns, which could contain sensitive information."
"    def new_with_restriction(
        self,
        entities: Optional[Collection[str]] = None,
        relations: Optional[Collection[str]] = None,
    ) -> 'TriplesFactory':
        """"""Make a new triples factory only keeping the given entities and relations, but keeping the ID mapping.

        :param entities:
            The entities of interest. If None, defaults to all entities.
        :param relations:
            The relations of interest. If None, defaults to all relations.

        :return:
            A new triples factory, which has only a subset of the triples containing the entities and relations of
            interest. The label-to-ID mapping is *not* modified.
        """"""
        if self.create_inverse_triples and relations is not None:
            logger.info(
                'Since %s already contain inverse relations, the relation filter is expanded to contain the inverse '
                'relations as well.',
                str(self),
            )
            relations = list(relations) + list(map(self.relation_to_inverse.__getitem__, relations))

        keep_mask = None

        # Filter for entities
        if entities is not None:
            keep_mask = self.get_idx_for_entities(entities=entities)
            logger.info('Keeping %d/%d entities', len(entities), self.num_entities)

        # Filter for relations
        if relations is not None:
            relation_mask = self.get_idx_for_relations(relations=relations)
            logger.info('Keeping %d/%d relations', len(relations), self.num_relations)
            keep_mask = relation_mask if keep_mask is None else keep_mask & relation_mask

        # No filtering happened
        if keep_mask is None:
            return self

        logger.info('Keeping %d/%d triples', keep_mask.sum(), self.num_triples)
        factory = TriplesFactory.from_labeled_triples(
            triples=self.triples[keep_mask],
            create_inverse_triples=False,
            entity_to_id=self.entity_to_id,
            relation_to_id=self.relation_to_id,
            compact_id=False,
        )

        # manually copy the inverse relation mappings
        if self.create_inverse_triples:
            factory.relation_to_inverse = self.relation_to_inverse

        return factory","1. Use `typing` to annotate the types of arguments and return values.
2. Validate the input arguments to ensure they are of the correct type and within the expected range.
3. Use `logging` to log all security-relevant events."
"def _tf_cleanup_all(
    triples_groups: List[np.ndarray],
    *,
    random_state: RandomHint = None,
) -> List[np.ndarray]:
    """"""Cleanup a list of triples array with respect to the first array.""""""
    reference, *others = triples_groups
    rv = []
    for other in others:
        if random_state is not None:
            reference, other = _tf_cleanup_randomized(reference, other, random_state)
        else:
            reference, other = _tf_cleanup_deterministic(reference, other)
        rv.append(other)
    return [reference, *rv]","1. Use `np.random.RandomState` instead of `random.Random` to generate random numbers.
2. Use `tf.identity` to create a copy of a tensor instead of using the `.copy()` method.
3. Use `tf.debugging.assert_equal` to check if two tensors are equal."
"def _tf_cleanup_deterministic(training: np.ndarray, testing: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """"""Cleanup a triples array (testing) with respect to another (training).""""""
    move_id_mask = _prepare_cleanup(training, testing)

    training = np.concatenate([training, testing[move_id_mask]])
    testing = testing[~move_id_mask]

    return training, testing","1. Use `np.unique` to remove duplicate elements in the `move_id_mask` array. This will prevent an attacker from injecting duplicate moves into the training data and causing the model to learn incorrect associations.
2. Use `np.random.shuffle` to randomly shuffle the order of the elements in the `training` and `testing` arrays. This will make it more difficult for an attacker to predict which moves will be used for training and which will be used for testing.
3. Use `np.copy` to create a copy of the `training` and `testing` arrays before performing any modifications. This will prevent an attacker from modifying the original data and causing the model to learn incorrect associations."
"def _tf_cleanup_randomized(
    training: np.ndarray,
    testing: np.ndarray,
    random_state: RandomHint = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """"""Cleanup a triples array, but randomly select testing triples and recalculate to minimize moves.

    1. Calculate ``move_id_mask`` as in :func:`_tf_cleanup_deterministic`
    2. Choose a triple to move, recalculate move_id_mask
    3. Continue until move_id_mask has no true bits
    """"""
    random_state = ensure_random_state(random_state)

    move_id_mask = _prepare_cleanup(training, testing)

    # While there are still triples that should be moved to the training set
    while move_id_mask.any():
        # Pick a random triple to move over to the training triples
        idx = random_state.choice(move_id_mask.nonzero()[0])
        training = np.concatenate([training, testing[idx].reshape(1, -1)])

        # Recalculate the testing triples without that index
        testing_mask = np.ones_like(move_id_mask)
        testing_mask[idx] = False
        testing = testing[testing_mask]

        # Recalculate the training entities, testing entities, to_move, and move_id_mask
        move_id_mask = _prepare_cleanup(training, testing)

    return training, testing","1. Use `np.random.default_rng()` instead of `np.random.RandomState()` to generate random numbers.
2. Use `np.unique()` to remove duplicate elements in the `testing` array.
3. Use `np.argwhere()` to find the indices of the elements in the `testing` array that are also in the `training` array."
"def _prepare_cleanup(training: np.ndarray, testing: np.ndarray) -> np.ndarray:
    to_move_mask = None
    for col in [[0, 2], 1]:
        training_ids, test_ids = [np.unique(triples[:, col]) for triples in [training, testing]]
        to_move = test_ids[~np.isin(test_ids, training_ids)]
        this_to_move_mask = np.isin(testing[:, col], to_move)
        if this_to_move_mask.ndim > 1:
            this_to_move_mask = this_to_move_mask.any(axis=1)
        if to_move_mask is None:
            to_move_mask = this_to_move_mask
        else:
            to_move_mask = this_to_move_mask | to_move_mask

    return to_move_mask","1. Use `np.unique` with `return_counts` to avoid creating an intermediate array `test_ids`.
2. Use `np.isin` with `axis=1` to avoid creating an intermediate array `this_to_move_mask`.
3. Use `np.any` with `axis=1` to avoid creating an intermediate array `to_move_mask`."
"    def __init__(
        self,
        *,
        path: Union[None, str, TextIO] = None,
        triples: Optional[LabeledTriples] = None,
        path_to_numeric_triples: Union[None, str, TextIO] = None,
        numeric_triples: Optional[np.ndarray] = None,
        **kwargs,
    ) -> None:
        """"""Initialize the multi-modal triples factory.

        :param path: The path to a 3-column TSV file with triples in it. If not specified,
         you should specify ``triples``.
        :param triples:  A 3-column numpy array with triples in it. If not specified,
         you should specify ``path``
        :param path_to_numeric_triples: The path to a 3-column TSV file with triples and
         numeric. If not specified, you should specify ``numeric_triples``.
        :param numeric_triples:  A 3-column numpy array with numeric triples in it. If not
         specified, you should specify ``path_to_numeric_triples``.
        """"""
        if path is None:
            base = TriplesFactory.from_labeled_triples(triples=triples, **kwargs)
        else:
            base = TriplesFactory.from_path(path=path, **kwargs)
        super().__init__(
            entity_to_id=base.entity_to_id,
            relation_to_id=base.relation_to_id,
            _triples=base.triples,
            mapped_triples=base.mapped_triples,
            relation_to_inverse=base.relation_to_inverse,
        )

        if path_to_numeric_triples is None and numeric_triples is None:
            raise ValueError('Must specify one of path_to_numeric_triples or numeric_triples')
        elif path_to_numeric_triples is not None and numeric_triples is not None:
            raise ValueError('Must not specify both path_to_numeric_triples and numeric_triples')
        elif path_to_numeric_triples is not None:
            numeric_triples = load_triples(path_to_numeric_triples)

        self.numeric_literals, self.literals_to_id = create_matrix_of_literals(
            numeric_triples=numeric_triples,
            entity_to_id=self.entity_to_id,
        )","1. Use `pathlib` to handle file paths instead of strings. This will prevent against directory traversal attacks.
2. Use `typing` to annotate the arguments of functions. This will help catch errors at compile time.
3. Use `f-strings` to format strings. This will prevent against injection attacks."
"def get_entities(triples) -> Set:
    """"""Get all entities from the triples.""""""
    return set(triples[:, [0, 2]].flatten().tolist())","1. **Use prepared statements** to prevent SQL injection attacks.
2. **Sanitize user input** to prevent cross-site scripting (XSS) attacks.
3. **Encrypt sensitive data**, such as passwords, to protect them from being stolen."
"def get_relations(triples) -> Set:
    """"""Get all relations from the triples.""""""
    return set(triples[:, 1])","1. **Use `set()` instead of `list()` to avoid duplicate relations.** This will help to prevent data inconsistencies and errors.
2. **Use `enumerate()` to iterate over the triples.** This will help to ensure that the relations are processed in the correct order.
3. **Validate the input data before using it.** This will help to prevent malicious users from injecting invalid data into the system."
"def invert_mapping(mapping: Mapping[str, int]) -> Mapping[int, str]:
    """"""
    Invert a mapping.

    :param mapping:
        The mapping, key -> value.

    :return:
        The inverse mapping, value -> key.
    """"""
    num_unique_values = len(set(mapping.values()))
    num_keys = len(mapping)
    if num_unique_values < num_keys:
        raise ValueError(f'Mapping is not bijective! Only {num_unique_values}/{num_keys} are unique.')
    return {
        value: key
        for key, value in mapping.items()
    }","1. Use a `set` to check if the values are unique.
2. Use a `for` loop to iterate over the keys and values.
3. Use a `return` statement to return the inverse mapping."
"def prepare_ablation_from_config(config: Mapping[str, Any], directory: str, save_artifacts: bool):
    """"""Prepare a set of ablation study directories.""""""
    metadata = config['metadata']
    optuna_config = config['optuna']
    ablation_config = config['ablation']

    evaluator = ablation_config['evaluator']
    evaluator_kwargs = ablation_config['evaluator_kwargs']
    evaluation_kwargs = ablation_config['evaluation_kwargs']

    it = itt.product(
        ablation_config['datasets'],
        ablation_config['create_inverse_triples'],
        ablation_config['models'],
        ablation_config['loss_functions'],
        ablation_config['regularizers'],
        ablation_config['optimizers'],
        ablation_config['training_loops'],
    )

    directories = []
    for counter, (
        dataset,
        create_inverse_triples,
        model,
        loss,
        regularizer,
        optimizer,
        training_loop,
    ) in enumerate(it):
        experiment_name = f'{counter:04d}_{normalize_string(dataset)}_{normalize_string(model)}'
        output_directory = os.path.join(directory, experiment_name)
        os.makedirs(output_directory, exist_ok=True)
        # TODO what happens if already exists?

        _experiment_optuna_config = optuna_config.copy()
        _experiment_optuna_config['storage'] = f'sqlite:///{output_directory}/optuna_results.db'
        if save_artifacts:
            save_model_directory = os.path.join(output_directory, 'artifacts')
            os.makedirs(save_model_directory, exist_ok=True)
            _experiment_optuna_config['save_model_directory'] = save_model_directory

        hpo_config = dict()
        for retain_key in ('stopper', 'stopper_kwargs'):
            if retain_key in ablation_config:
                logger.info(f'Retaining {retain_key} configuration in HPO')
                hpo_config[retain_key] = deepcopy(ablation_config[retain_key])

        for error_key in ('early_stopping', 'early_stopping_kwargs'):
            if error_key in ablation_config:
                raise ValueError(f'Outdated key: {error_key}. Please update')

        # TODO incorporate setting of random seed
        # pipeline_kwargs=dict(
        #    random_seed=random.randint(1, 2 ** 32 - 1),
        # ),

        def _set_arguments(key: str, value: str) -> None:
            """"""Set argument and its values.""""""
            d = {key: value}
            kwargs = ablation_config[f'{key}_kwargs'][model][value]
            if kwargs:
                d[f'{key}_kwargs'] = kwargs
            kwargs_ranges = ablation_config[f'{key}_kwargs_ranges'][model][value]
            if kwargs_ranges:
                d[f'{key}_kwargs_ranges'] = kwargs_ranges

            hpo_config.update(d)

        # Add dataset to current_pipeline
        hpo_config['dataset'] = dataset
        logger.info(f""Dataset: {dataset}"")
        hpo_config['dataset_kwargs'] = dict(create_inverse_triples=create_inverse_triples)
        logger.info(f""Add inverse triples: {create_inverse_triples}"")

        hpo_config['model'] = model
        model_kwargs = ablation_config['model_kwargs'][model]
        if model_kwargs:
            hpo_config['model_kwargs'] = ablation_config['model_kwargs'][model]
        hpo_config['model_kwargs_ranges'] = ablation_config['model_kwargs_ranges'][model]
        logger.info(f""Model: {model}"")

        # Add loss function to current_pipeline
        _set_arguments(key='loss', value=loss)
        logger.info(f""Loss function: {loss}"")

        # Add regularizer to current_pipeline
        _set_arguments(key='regularizer', value=regularizer)
        logger.info(f""Regularizer: {regularizer}"")

        # Add optimizer to current_pipeline
        _set_arguments(key='optimizer', value=optimizer)
        logger.info(f""Optimizer: {optimizer}"")

        # Add training approach to current_pipeline
        hpo_config['training_loop'] = training_loop
        logger.info(f""Training loop: {training_loop}"")

        if normalize_string(training_loop, suffix=_TRAINING_LOOP_SUFFIX) == 'slcwa':
            negative_sampler = ablation_config['negative_sampler']
            _set_arguments(key='negative_sampler', value=negative_sampler)
            logger.info(f""Negative sampler: {negative_sampler}"")

        # Add training kwargs and kwargs_ranges
        training_kwargs = ablation_config['training_kwargs'][model][training_loop]
        if training_kwargs:
            hpo_config['training_kwargs'] = training_kwargs
        hpo_config['training_kwargs_ranges'] = ablation_config['training_kwargs_ranges'][model][training_loop]

        # Add evaluation
        hpo_config['evaluator'] = evaluator
        if evaluator_kwargs:
            hpo_config['evaluator_kwargs'] = evaluator_kwargs
        hpo_config['evaluation_kwargs'] = evaluation_kwargs
        logger.info(f""Evaluator: {evaluator}"")

        rv_config = dict(
            type='hpo',
            metadata=metadata,
            pipeline=hpo_config,
            optuna=_experiment_optuna_config,
        )

        rv_config_path = os.path.join(output_directory, 'hpo_config.json')
        with open(rv_config_path, 'w') as file:
            json.dump(rv_config, file, indent=2, ensure_ascii=True)

        directories.append((output_directory, rv_config_path))

    return directories","1. Use `os.makedirs(output_directory, exist_ok=True)` to avoid race conditions when creating directories.
2. Use `json.dump(rv_config, file, indent=2, ensure_ascii=True)` to ensure that the JSON file is correctly formatted.
3. Use `random.randint(1, 2 ** 32 - 1)` to generate a random seed for the experiment."
"def main(path: str, directory: str, test_ratios, no_validation: bool, validation_ratios, reload, seed):
    """"""Make a dataset from the given triples.""""""
    os.makedirs(directory, exist_ok=True)

    triples_factory = TriplesFactory(path=path)
    ratios = test_ratios if no_validation else validation_ratios

    if seed is None:
        seed = np.random.randint(0, 2 ** 32 - 1)
    sub_triples_factories = triples_factory.split(ratios, random_state=seed)

    for subset_name, subset_tf in zip(LABELS, sub_triples_factories):
        output_path = os.path.join(directory, f'{subset_name}.txt')
        click.echo(f'Outputing {subset_name} to {output_path}')
        np.savetxt(output_path, subset_tf.triples, delimiter='\\t', fmt='%s')

    metadata = dict(
        source=os.path.abspath(path),
        ratios=dict(zip(LABELS, ratios)),
        seed=seed,
    )
    with open(os.path.join(directory, 'metadata.json'), 'w') as file:
        json.dump(metadata, file, indent=2)

    if reload:
        if no_validation:
            click.secho('Can not load as dataset if --no-validation was flagged.', fg='red')
            return
        d = PathDataSet(
            training_path=os.path.join(directory, 'train.txt'),
            testing_path=os.path.join(directory, 'test.txt'),
            validation_path=os.path.join(directory, 'valid.txt'),
            eager=True,
        )
        print(d)","1. Use `click.argument` to sanitize user input.
2. Use `np.random.seed` to set a random seed.
3. Use `json.dump` with `indent=2` to make the JSON file more readable."
"def pipeline(  # noqa: C901
    *,
    # 1. Dataset
    dataset: Union[None, str, Type[DataSet]] = None,
    dataset_kwargs: Optional[Mapping[str, Any]] = None,
    training_triples_factory: Optional[TriplesFactory] = None,
    testing_triples_factory: Optional[TriplesFactory] = None,
    validation_triples_factory: Optional[TriplesFactory] = None,
    evaluation_entity_whitelist: Optional[Collection[str]] = None,
    evaluation_relation_whitelist: Optional[Collection[str]] = None,
    # 2. Model
    model: Union[str, Type[Model]],
    model_kwargs: Optional[Mapping[str, Any]] = None,
    # 3. Loss
    loss: Union[None, str, Type[Loss]] = None,
    loss_kwargs: Optional[Mapping[str, Any]] = None,
    # 4. Regularizer
    regularizer: Union[None, str, Type[Regularizer]] = None,
    regularizer_kwargs: Optional[Mapping[str, Any]] = None,
    # 5. Optimizer
    optimizer: Union[None, str, Type[Optimizer]] = None,
    optimizer_kwargs: Optional[Mapping[str, Any]] = None,
    clear_optimizer: bool = True,
    # 6. Training Loop
    training_loop: Union[None, str, Type[TrainingLoop]] = None,
    negative_sampler: Union[None, str, Type[NegativeSampler]] = None,
    negative_sampler_kwargs: Optional[Mapping[str, Any]] = None,
    # 7. Training (ronaldo style)
    training_kwargs: Optional[Mapping[str, Any]] = None,
    stopper: Union[None, str, Type[Stopper]] = None,
    stopper_kwargs: Optional[Mapping[str, Any]] = None,
    # 8. Evaluation
    evaluator: Union[None, str, Type[Evaluator]] = None,
    evaluator_kwargs: Optional[Mapping[str, Any]] = None,
    evaluation_kwargs: Optional[Mapping[str, Any]] = None,
    # 9. Tracking
    result_tracker: Union[None, str, Type[ResultTracker]] = None,
    result_tracker_kwargs: Optional[Mapping[str, Any]] = None,
    # Misc
    metadata: Optional[Dict[str, Any]] = None,
    device: Union[None, str, torch.device] = None,
    random_seed: Optional[int] = None,
    use_testing_data: bool = True,
) -> PipelineResult:
    """"""Train and evaluate a model.

    :param dataset:
        The name of the dataset (a key from :data:`pykeen.datasets.datasets`) or the :class:`pykeen.datasets.DataSet`
        instance. Alternatively, the ``training_triples_factory`` and ``testing_triples_factory`` can be specified.
    :param dataset_kwargs:
        The keyword arguments passed to the dataset upon instantiation
    :param training_triples_factory:
        A triples factory with training instances if a dataset was not specified
    :param testing_triples_factory:
        A triples factory with training instances if a dataset was not specified
    :param validation_triples_factory:
        A triples factory with validation instances if a dataset was not specified
    :param evaluation_entity_whitelist:
        Optional restriction of evaluation to triples containing *only* these entities. Useful if the downstream task
        is only interested in certain entities, but the relational patterns with other entities improve the entity
        embedding quality.
    :param evaluation_relation_whitelist:
        Optional restriction of evaluation to triples containing *only* these relations. Useful if the downstream task
        is only interested in certain relation, but the relational patterns with other relations improve the entity
        embedding quality.

    :param model:
        The name of the model or the model class
    :param model_kwargs:
        Keyword arguments to pass to the model class on instantiation

    :param loss:
        The name of the loss or the loss class.
    :param loss_kwargs:
        Keyword arguments to pass to the loss on instantiation

    :param regularizer:
        The name of the regularizer or the regularizer class.
    :param regularizer_kwargs:
        Keyword arguments to pass to the regularizer on instantiation

    :param optimizer:
        The name of the optimizer or the optimizer class. Defaults to :class:`torch.optim.Adagrad`.
    :param optimizer_kwargs:
        Keyword arguments to pass to the optimizer on instantiation
    :param clear_optimizer:
        Whether to delete the optimizer instance after training. As the optimizer might have additional memory
        consumption due to e.g. moments in Adam, this is the default option. If you want to continue training, you
        should set it to False, as the optimizer's internal parameter will get lost otherwise.

    :param training_loop:
        The name of the training loop's training approach (``'slcwa'`` or ``'lcwa'``) or the training loop class.
        Defaults to :class:`pykeen.training.SLCWATrainingLoop`.
    :param negative_sampler:
        The name of the negative sampler (``'basic'`` or ``'bernoulli'``) or the negative sampler class.
        Only allowed when training with sLCWA.
        Defaults to :class:`pykeen.sampling.BasicNegativeSampler`.
    :param negative_sampler_kwargs:
        Keyword arguments to pass to the negative sampler class on instantiation

    :param training_kwargs:
        Keyword arguments to pass to the training loop's train function on call
    :param stopper:
        What kind of stopping to use. Default to no stopping, can be set to 'early'.
    :param stopper_kwargs:
        Keyword arguments to pass to the stopper upon instantiation.

    :param evaluator:
        The name of the evaluator or an evaluator class. Defaults to :class:`pykeen.evaluation.RankBasedEvaluator`.
    :param evaluator_kwargs:
        Keyword arguments to pass to the evaluator on instantiation
    :param evaluation_kwargs:
        Keyword arguments to pass to the evaluator's evaluate function on call

    :param result_tracker:
        The ResultsTracker class or name
    :param result_tracker_kwargs:
        The keyword arguments passed to the results tracker on instantiation

    :param metadata:
        A JSON dictionary to store with the experiment
    :param use_testing_data:
        If true, use the testing triples. Otherwise, use the validation triples. Defaults to true - use testing triples.
    """"""
    if random_seed is None:
        random_seed = random.randint(0, 2 ** 32 - 1)
        logger.warning(f'No random seed is specified. Setting to {random_seed}.')
    set_random_seed(random_seed)

    result_tracker_cls: Type[ResultTracker] = get_result_tracker_cls(result_tracker)
    result_tracker = result_tracker_cls(**(result_tracker_kwargs or {}))

    if not metadata:
        metadata = {}
    title = metadata.get('title')

    # Start tracking
    result_tracker.start_run(run_name=title)

    device = resolve_device(device)

    result_tracker.log_params(dict(dataset=dataset))

    training_triples_factory, testing_triples_factory, validation_triples_factory = get_dataset(
        dataset=dataset,
        dataset_kwargs=dataset_kwargs,
        training_triples_factory=training_triples_factory,
        testing_triples_factory=testing_triples_factory,
        validation_triples_factory=validation_triples_factory,
    )

    # evaluation restriction to a subset of entities/relations
    if any(f is not None for f in (evaluation_entity_whitelist, evaluation_relation_whitelist)):
        testing_triples_factory = testing_triples_factory.new_with_restriction(
            entities=evaluation_entity_whitelist,
            relations=evaluation_relation_whitelist,
        )
        if validation_triples_factory is not None:
            validation_triples_factory = validation_triples_factory.new_with_restriction(
                entities=evaluation_entity_whitelist,
                relations=evaluation_relation_whitelist,
            )

    if model_kwargs is None:
        model_kwargs = {}
    model_kwargs.update(preferred_device=device)
    model_kwargs.setdefault('random_seed', random_seed)

    if regularizer is not None:
        # FIXME this should never happen.
        if 'regularizer' in model_kwargs:
            logger.warning('Can not specify regularizer in kwargs and model_kwargs. removing from model_kwargs')
            del model_kwargs['regularizer']
        regularizer_cls: Type[Regularizer] = get_regularizer_cls(regularizer)
        model_kwargs['regularizer'] = regularizer_cls(
            device=device,
            **(regularizer_kwargs or {}),
        )

    if loss is not None:
        if 'loss' in model_kwargs:  # FIXME
            logger.warning('duplicate loss in kwargs and model_kwargs. removing from model_kwargs')
            del model_kwargs['loss']
        loss_cls = get_loss_cls(loss)
        _loss = loss_cls(**(loss_kwargs or {}))
        model_kwargs.setdefault('loss', _loss)

    model = get_model_cls(model)
    model_instance: Model = model(
        triples_factory=training_triples_factory,
        **model_kwargs,
    )
    # Log model parameters
    result_tracker.log_params(params=dict(cls=model.__name__, kwargs=model_kwargs), prefix='model')

    optimizer = get_optimizer_cls(optimizer)
    training_loop = get_training_loop_cls(training_loop)

    if optimizer_kwargs is None:
        optimizer_kwargs = {}

    # Log optimizer parameters
    result_tracker.log_params(params=dict(cls=optimizer.__name__, kwargs=optimizer_kwargs), prefix='optimizer')
    optimizer_instance = optimizer(
        params=model_instance.get_grad_params(),
        **optimizer_kwargs,
    )

    result_tracker.log_params(params=dict(cls=training_loop.__name__), prefix='training_loop')
    if negative_sampler is None:
        training_loop_instance: TrainingLoop = training_loop(
            model=model_instance,
            optimizer=optimizer_instance,
        )
    elif training_loop is not SLCWATrainingLoop:
        raise ValueError('Can not specify negative sampler with LCWA')
    else:
        negative_sampler = get_negative_sampler_cls(negative_sampler)
        result_tracker.log_params(
            params=dict(cls=negative_sampler.__name__, kwargs=negative_sampler_kwargs),
            prefix='negative_sampler',
        )
        training_loop_instance: TrainingLoop = SLCWATrainingLoop(
            model=model_instance,
            optimizer=optimizer_instance,
            negative_sampler_cls=negative_sampler,
            negative_sampler_kwargs=negative_sampler_kwargs,
        )

    evaluator = get_evaluator_cls(evaluator)
    evaluator_instance: Evaluator = evaluator(
        **(evaluator_kwargs or {}),
    )

    if evaluation_kwargs is None:
        evaluation_kwargs = {}

    if training_kwargs is None:
        training_kwargs = {}

    # Stopping
    if 'stopper' in training_kwargs and stopper is not None:
        raise ValueError('Specified stopper in training_kwargs and as stopper')
    if 'stopper' in training_kwargs:
        stopper = training_kwargs.pop('stopper')
    if stopper_kwargs is None:
        stopper_kwargs = {}

    # Load the evaluation batch size for the stopper, if it has been set
    _evaluation_batch_size = evaluation_kwargs.get('batch_size')
    if _evaluation_batch_size is not None:
        stopper_kwargs.setdefault('evaluation_batch_size', _evaluation_batch_size)

    # By default there's a stopper that does nothing interesting
    stopper_cls: Type[Stopper] = get_stopper_cls(stopper)
    stopper: Stopper = stopper_cls(
        model=model_instance,
        evaluator=evaluator_instance,
        evaluation_triples_factory=validation_triples_factory,
        result_tracker=result_tracker,
        **stopper_kwargs,
    )

    training_kwargs.setdefault('num_epochs', 5)
    training_kwargs.setdefault('batch_size', 256)
    result_tracker.log_params(params=training_kwargs, prefix='training')

    # Add logging for debugging
    logging.debug(""Run Pipeline based on following config:"")
    logging.debug(f""dataset: {dataset}"")
    logging.debug(f""dataset_kwargs: {dataset_kwargs}"")
    logging.debug(f""model: {model}"")
    logging.debug(f""model_kwargs: {model_kwargs}"")
    logging.debug(f""loss: {loss}"")
    logging.debug(f""loss_kwargs: {loss_kwargs}"")
    logging.debug(f""regularizer: {regularizer}"")
    logging.debug(f""regularizer_kwargs: {regularizer_kwargs}"")
    logging.debug(f""optimizer: {optimizer}"")
    logging.debug(f""optimizer_kwargs: {optimizer_kwargs}"")
    logging.debug(f""training_loop: {training_loop}"")
    logging.debug(f""negative_sampler: {negative_sampler}"")
    logging.debug(f""_negative_sampler_kwargs: {negative_sampler_kwargs}"")
    logging.debug(f""_training_kwargs: {training_kwargs}"")
    logging.debug(f""stopper: {stopper}"")
    logging.debug(f""stopper_kwargs: {stopper_kwargs}"")
    logging.debug(f""evaluator: {evaluator}"")
    logging.debug(f""evaluator_kwargs: {evaluator_kwargs}"")

    # Train like Cristiano Ronaldo
    training_start_time = time.time()
    losses = training_loop_instance.train(
        stopper=stopper,
        result_tracker=result_tracker,
        clear_optimizer=clear_optimizer,
        **training_kwargs,
    )
    training_end_time = time.time() - training_start_time

    if use_testing_data:
        mapped_triples = testing_triples_factory.mapped_triples
    else:
        mapped_triples = validation_triples_factory.mapped_triples

    # Evaluate
    # Reuse optimal evaluation parameters from training if available
    if evaluator_instance.batch_size is not None or evaluator_instance.slice_size is not None:
        evaluation_kwargs['batch_size'] = evaluator_instance.batch_size
        evaluation_kwargs['slice_size'] = evaluator_instance.slice_size
    # Add logging about evaluator for debugging
    logging.debug(""Evaluation will be run with following parameters:"")
    logging.debug(f""evaluation_kwargs: {evaluation_kwargs}"")
    evaluate_start_time = time.time()
    metric_results: MetricResults = evaluator_instance.evaluate(
        model=model_instance,
        mapped_triples=mapped_triples,
        **evaluation_kwargs,
    )
    evaluate_end_time = time.time() - evaluate_start_time
    result_tracker.log_metrics(
        metrics=metric_results.to_dict(),
        step=training_kwargs.get('num_epochs'),
    )
    result_tracker.end_run()

    return PipelineResult(
        random_seed=random_seed,
        model=model_instance,
        training_loop=training_loop_instance,
        losses=losses,
        stopper=stopper,
        metric_results=metric_results,
        metadata=metadata,
        train_seconds=training_end_time,
        evaluate_seconds=evaluate_end_time,
    )","1. Use `result_tracker.log_params()` to log parameters of the model, loss, optimizer, training loop, evaluator, stopper, and dataset.
2. Use `result_tracker.log_metrics()` to log metrics of the evaluation.
3. Use `result_tracker.end_run()` to end the run."
"    def create_lcwa_instances(self, use_tqdm: Optional[bool] = None) -> LCWAInstances:
        """"""Create LCWA instances for this factory's triples.""""""
        s_p_to_multi_tails = _create_multi_label_tails_instance(
            mapped_triples=self.mapped_triples,
            use_tqdm=use_tqdm,
        )
        sp, multi_o = zip(*s_p_to_multi_tails.items())
        mapped_triples: torch.LongTensor = torch.tensor(sp, dtype=torch.long)
        labels = np.array([np.array(item) for item in multi_o])

        return LCWAInstances(
            mapped_triples=mapped_triples,
            entity_to_id=self.entity_to_id,
            relation_to_id=self.relation_to_id,
            labels=labels,
        )","1. Use `torch.tensor` instead of `torch.LongTensor` to avoid data type errors.
2. Use `np.array` instead of `np.ndarray` to avoid data type errors.
3. Use `LCWAInstances` instead of `torch.LongTensor` and `np.array` to avoid data type errors."
"    def split(
        self,
        ratios: Union[float, Sequence[float]] = 0.8,
        *,
        random_state: Union[None, int, np.random.RandomState] = None,
        randomize_cleanup: bool = False,
    ) -> List['TriplesFactory']:
        """"""Split a triples factory into a train/test.

        :param ratios: There are three options for this argument. First, a float can be given between 0 and 1.0,
         non-inclusive. The first triples factory will get this ratio and the second will get the rest. Second,
         a list of ratios can be given for which factory in which order should get what ratios as in ``[0.8, 0.1]``.
         The final ratio can be omitted because that can be calculated. Third, all ratios can be explicitly set in
         order such as in ``[0.8, 0.1, 0.1]`` where the sum of all ratios is 1.0.
        :param random_state: The random state used to shuffle and split the triples in this factory.
        :param randomize_cleanup: If true, uses the non-deterministic method for moving triples to the training set.
         This has the advantage that it doesn't necessarily have to move all of them, but it might be slower.

        .. code-block:: python

            ratio = 0.8  # makes a [0.8, 0.2] split
            training_factory, testing_factory = factory.split(ratio)

            ratios = [0.8, 0.1]  # makes a [0.8, 0.1, 0.1] split
            training_factory, testing_factory, validation_factory = factory.split(ratios)

            ratios = [0.8, 0.1, 0.1]  # also makes a [0.8, 0.1, 0.1] split
            training_factory, testing_factory, validation_factory = factory.split(ratios)
        """"""
        n_triples = self.triples.shape[0]

        # Prepare shuffle index
        idx = np.arange(n_triples)
        if random_state is None:
            random_state = np.random.randint(0, 2 ** 32 - 1)
            logger.warning(f'Using random_state={random_state} to split {self}')
        if isinstance(random_state, int):
            random_state = np.random.RandomState(random_state)
        random_state.shuffle(idx)

        # Prepare split index
        if isinstance(ratios, float):
            ratios = [ratios]

        ratio_sum = sum(ratios)
        if ratio_sum == 1.0:
            ratios = ratios[:-1]  # vsplit doesn't take the final number into account.
        elif ratio_sum > 1.0:
            raise ValueError(f'ratios sum to more than 1.0: {ratios} (sum={ratio_sum})')

        sizes = [
            int(split_ratio * n_triples)
            for split_ratio in ratios
        ]
        # Take cumulative sum so the get separated properly
        split_idxs = np.cumsum(sizes)

        # Split triples
        triples_groups = np.vsplit(self.triples[idx], split_idxs)
        logger.info(f'split triples to groups of sizes {[triples.shape[0] for triples in triples_groups]}')

        # Make sure that the first element has all the right stuff in it
        triples_groups = _tf_cleanup_all(triples_groups, random_state=random_state if randomize_cleanup else None)

        for i, (triples, exp_size, exp_ratio) in enumerate(zip(triples_groups, sizes, ratios)):
            actual_size = triples.shape[0]
            actual_ratio = actual_size / exp_size * exp_ratio
            if actual_size != exp_size:
                logger.warning(
                    f'Requested ratio[{i}]={exp_ratio:.3f} (equal to size {exp_size}), but got {actual_ratio:.3f} '
                    f'(equal to size {actual_size}) to ensure that all entities/relations occur in train.',
                )

        # Make new triples factories for each group
        return [
            TriplesFactory(
                triples=triples,
                entity_to_id=self.entity_to_id,
                relation_to_id=self.relation_to_id,
                compact_id=False,
            )
            for triples in triples_groups
        ]","1. Use `np.random.RandomState()` instead of `np.random.randint()` to generate random numbers.
2. Use `np.vsplit()` to split the triples instead of `np.split()`.
3. Use `_tf_cleanup_all()` to make sure that the first element has all the right stuff in it."
"def _tf_cleanup_randomized(
    training: np.ndarray,
    testing: np.ndarray,
    random_state: Union[None, int, np.random.RandomState] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """"""Cleanup a triples array, but randomly select testing triples and recalculate to minimize moves.

    1. Calculate ``move_id_mask`` as in :func:`_tf_cleanup_deterministic`
    2. Choose a triple to move, recalculate move_id_mask
    3. Continue until move_id_mask has no true bits
    """"""
    if random_state is None:
        random_state = np.random.randint(0, 2 ** 32 - 1)
        logger.warning('Using random_state=%s', random_state)
    if isinstance(random_state, int):
        random_state = np.random.RandomState(random_state)

    move_id_mask = _prepare_cleanup(training, testing)

    # While there are still triples that should be moved to the training set
    while move_id_mask.any():
        # Pick a random triple to move over to the training triples
        idx = random_state.choice(move_id_mask.nonzero()[0])
        training = np.concatenate([training, testing[idx].reshape(1, -1)])

        # Recalculate the testing triples without that index
        testing_mask = np.ones_like(move_id_mask)
        testing_mask[idx] = False
        testing = testing[testing_mask]

        # Recalculate the training entities, testing entities, to_move, and move_id_mask
        move_id_mask = _prepare_cleanup(training, testing)

    return training, testing","1. Use a cryptographically secure random number generator (CSPRNG) to generate the random state.
2. Sanitize the input data to prevent poisoning attacks.
3. Use defensive programming techniques to prevent errors and vulnerabilities."
"    def split(
        self,
        ratios: Union[float, Sequence[float]] = 0.8,
        *,
        random_state: Union[None, int, np.random.RandomState] = None,
        randomize_cleanup: bool = False,
    ) -> List['TriplesFactory']:
        """"""Split a triples factory into a train/test.

        :param ratios: There are three options for this argument. First, a float can be given between 0 and 1.0,
         non-inclusive. The first triples factory will get this ratio and the second will get the rest. Second,
         a list of ratios can be given for which factory in which order should get what ratios as in ``[0.8, 0.1]``.
         The final ratio can be omitted because that can be calculated. Third, all ratios can be explicitly set in
         order such as in ``[0.8, 0.1, 0.1]`` where the sum of all ratios is 1.0.
        :param random_state: The random state used to shuffle and split the triples in this factory.
        :param randomize_cleanup: If true, uses the non-deterministic method for moving triples to the training set.
         This has the advantage that it doesn't necessarily have to move all of them, but it might be slower.

        .. code-block:: python

            ratio = 0.8  # makes a [0.8, 0.2] split
            training_factory, testing_factory = factory.split(ratio)

            ratios = [0.8, 0.1]  # makes a [0.8, 0.1, 0.1] split
            training_factory, testing_factory, validation_factory = factory.split(ratios)

            ratios = [0.8, 0.1, 0.1]  # also makes a [0.8, 0.1, 0.1] split
            training_factory, testing_factory, validation_factory = factory.split(ratios)
        """"""
        n_triples = self.triples.shape[0]

        # Prepare shuffle index
        idx = np.arange(n_triples)
        if random_state is None:
            random_state = np.random.randint(0, 2 ** 32 - 1)
            logger.warning(f'Using random_state={random_state} to split {self}')
        if isinstance(random_state, int):
            random_state = np.random.RandomState(random_state)
        random_state.shuffle(idx)

        # Prepare split index
        if isinstance(ratios, float):
            ratios = [ratios]

        ratio_sum = sum(ratios)
        if ratio_sum == 1.0:
            ratios = ratios[:-1]  # vsplit doesn't take the final number into account.
        elif ratio_sum > 1.0:
            raise ValueError(f'ratios sum to more than 1.0: {ratios} (sum={ratio_sum})')

        split_idxs = [
            int(split_ratio * n_triples)
            for split_ratio in ratios
        ]
        # Take cumulative sum so the get separated properly
        split_idxs = np.cumsum(split_idxs)

        # Split triples
        triples_groups = np.vsplit(self.triples[idx], split_idxs)
        logger.info(f'split triples to groups of sizes {[triples.shape[0] for triples in triples_groups]}')

        # Make sure that the first element has all the right stuff in it
        triples_groups = _tf_cleanup_all(triples_groups, random_state=random_state if randomize_cleanup else None)

        # Make new triples factories for each group
        return [
            TriplesFactory(
                triples=triples,
                entity_to_id=deepcopy(self.entity_to_id),
                relation_to_id=deepcopy(self.relation_to_id),
            )
            for triples in triples_groups
        ]","1. Use `np.random.RandomState()` instead of `np.random.randint()` to generate random numbers.
2. Use `np.arange()` to generate a list of indices, and then shuffle the list using `random_state.shuffle()`.
3. Use `np.vsplit()` to split the triples into groups, and then make sure that the first element has all the right stuff in it."
"def _tf_cleanup_deterministic(training: np.ndarray, testing: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """"""Cleanup a triples array (testing) with respect to another (training).""""""
    training_entities, testing_entities, to_move, move_id_mask = _prepare_cleanup(training, testing)

    training = np.concatenate([training, testing[move_id_mask]])
    testing = testing[~move_id_mask]

    return training, testing","1. Use `np.unique` to check for duplicates in the training and testing data before concatenating them.
2. Use `np.argwhere` to find the indices of the elements to be moved, and use `np.delete` to move them from the testing data.
3. Use `np.copy` to create a new copy of the training data, and then assign the moved elements to it."
"def _tf_cleanup_randomized(
    training: np.ndarray,
    testing: np.ndarray,
    random_state: Union[None, int, np.random.RandomState] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """"""Cleanup a triples array, but randomly select testing triples and recalculate to minimize moves.

    1. Calculate ``move_id_mask`` as in :func:`_tf_cleanup_deterministic`
    2. Choose a triple to move, recalculate move_id_mask
    3. Continue until move_id_mask has no true bits
    """"""
    if random_state is None:
        random_state = np.random.randint(0, 2 ** 32 - 1)
        logger.warning('Using random_state=%s', random_state)
    if isinstance(random_state, int):
        random_state = np.random.RandomState(random_state)

    training_entities, testing_entities, to_move, move_id_mask = _prepare_cleanup(training, testing)

    # While there are still triples that should be moved to the training set
    while move_id_mask.any():
        # Pick a random triple to move over to the training triples
        idx = random_state.choice(move_id_mask.nonzero()[0])
        training = np.concatenate([training, testing[idx].reshape(1, -1)])

        # Recalculate the testing triples without that index
        testing_mask = np.ones_like(move_id_mask)
        testing_mask[idx] = False
        testing = testing[testing_mask]

        # Recalculate the training entities, testing entities, to_move, and move_id_mask
        training_entities, testing_entities, to_move, move_id_mask = _prepare_cleanup(training, testing)

    return training, testing","1. Use a cryptographically secure random number generator.
2. Sanitize user input to prevent injection attacks.
3. Use proper error handling to prevent security vulnerabilities."
"def _prepare_cleanup(training: np.ndarray, testing: np.ndarray):
    training_entities = _get_unique(training)
    testing_entities = _get_unique(testing)
    to_move = testing_entities[~np.isin(testing_entities, training_entities)]
    move_id_mask = np.isin(testing[:, [0, 2]], to_move).any(axis=1)
    return training_entities, testing_entities, to_move, move_id_mask","1. Use `np.unique()` with `return_index=True` to get the unique entities and their indices.
2. Use `np.isin()` to check if an entity is in the training entities.
3. Use `np.any()` to check if any of the entity IDs in the testing data match the entities to move."
"    def __init__(self, coresys: CoreSys, addon: AnyAddon) -> None:
        """"""Initialize Supervisor add-on builder.""""""
        self.coresys: CoreSys = coresys
        self.addon = addon

        super().__init__(
            find_one_filetype(
                self.addon.path_location, ""build"", FILE_SUFFIX_CONFIGURATION
            ),
            SCHEMA_BUILD_CONFIG,
        )","1. Use `f-strings` instead of `format()` to prevent format string vulnerabilities.
2. Use `typing` to annotate the types of arguments and return values of functions to catch errors early.
3. Use `black` and `isort` to format the code consistently and make it easier to read."
"    def _read_git_repository(self, path: Path) -> None:
        """"""Process a custom repository folder.""""""
        slug = extract_hash_from_path(path)

        # exists repository json
        repository_file = find_one_filetype(
            path, ""repository"", FILE_SUFFIX_CONFIGURATION
        )

        if repository_file is None:
            _LOGGER.warning(""No repository information exists at %s"", path)
            return

        try:
            repository_info = SCHEMA_REPOSITORY_CONFIG(
                read_json_or_yaml_file(repository_file)
            )
        except ConfigurationFileError:
            _LOGGER.warning(
                ""Can't read repository information from %s"", repository_file
            )
            return
        except vol.Invalid:
            _LOGGER.warning(""Repository parse error %s"", repository_file)
            return

        # process data
        self.repositories[slug] = repository_info
        self._read_addons_folder(path, slug)","1. Use `validate_json_or_yaml_file` to validate the configuration file.
2. Use `os.path.expanduser` to expand the user path before using it.
3. Use `logging.captureWarnings` to capture warnings and log them."
"def find_one_filetype(
    path: Path, filename: str, filetypes: List[str]
) -> Optional[Path]:
    """"""Find first file matching filetypes.""""""
    for file in path.glob(f""**/{filename}.*""):
        if file.suffix in filetypes:
            return file
    return None","1. Use `Path.glob()` with `recursive=False` to avoid searching subdirectories.
2. Use `Path.is_file()` to check if the file exists before trying to open it.
3. Use `Path.read_text()` to read the file contents as text, rather than using `open()`."
"def ipconfig_struct(config: IpConfig) -> dict:
    """"""Return a dict with information about ip configuration.""""""
    return {
        ATTR_METHOD: config.method,
        ATTR_ADDRESS: [address.with_prefixlen for address in config.address],
        ATTR_NAMESERVERS: [str(address) for address in config.nameservers],
        ATTR_GATEWAY: str(config.gateway) if config.gateway else None,
    }","1. Use `ipaddress` to validate IP addresses and netmasks.
2. Sanitize user input before using it to construct the `IpConfig` object.
3. Use proper type annotations to make the code more readable and easier to maintain."
"def wifi_struct(config: WifiConfig) -> dict:
    """"""Return a dict with information about wifi configuration.""""""
    return {
        ATTR_MODE: config.mode,
        ATTR_AUTH: config.auth,
        ATTR_SSID: config.ssid,
        ATTR_SIGNAL: config.signal,
    }","1. **Use strong encryption**. The wifi_struct function should use the ATTR_SECURITY attribute to specify the encryption type, and the value should be one of the following: `'open'`, `'wpa'`, or `'wpa2'`.
2. **Protect the wifi password**. The wifi password should be stored in a secure location, such as a password manager.
3. **Be careful when sharing wifi credentials**. Only share your wifi credentials with people you trust, and never share them in public."
"def interface_struct(interface: Interface) -> dict:
    """"""Return a dict with information of a interface to be used in th API.""""""
    return {
        ATTR_INTERFACE: interface.name,
        ATTR_TYPE: interface.type,
        ATTR_ENABLED: interface.enabled,
        ATTR_CONNECTED: interface.connected,
        ATTR_PRIMARY: interface.primary,
        ATTR_IPV4: ipconfig_struct(interface.ipv4) if interface.ipv4 else None,
        ATTR_IPV6: ipconfig_struct(interface.ipv6) if interface.ipv6 else None,
        ATTR_WIFI: wifi_struct(interface.wifi) if interface.wifi else None,
        ATTR_VLAN: wifi_struct(interface.vlan) if interface.vlan else None,
    }","1. Use `ipconfig_struct` and `wifi_struct` to sanitize user input.
2. Use `attr.converters` to validate user input.
3. Use `attr.validators` to check for invalid user input."
"def accesspoint_struct(accesspoint: AccessPoint) -> dict:
    """"""Return a dict for AccessPoint.""""""
    return {
        ATTR_MODE: accesspoint.mode,
        ATTR_SSID: accesspoint.ssid,
        ATTR_FREQUENCY: accesspoint.frequency,
        ATTR_SIGNAL: accesspoint.signal,
        ATTR_MAC: accesspoint.mac,
    }","1. Use `attr.ib()` to make attributes read-only.
2. Use `attr.frozen()` to prevent attributes from being added or deleted.
3. Use `attr.validators()` to validate attribute values."
"    def find_key(self, items, write=False):
        overwrite = self.config['overwrite'].get(bool)
        command = [self.config['bin'].as_str()]
        # The KeyFinder GUI program needs the -f flag before the path.
        # keyfinder-cli is similar, but just wants the path with no flag.
        if 'keyfinder-cli' not in os.path.basename(command[0]).lower():
            command.append('-f')

        for item in items:
            if item['initial_key'] and not overwrite:
                continue

            try:
                output = util.command_output(command + [util.syspath(
                                                        item.path)]).stdout
            except (subprocess.CalledProcessError, OSError) as exc:
                self._log.error(u'execution failed: {0}', exc)
                continue
            except UnicodeEncodeError:
                # Workaround for Python 2 Windows bug.
                # https://bugs.python.org/issue1759845
                self._log.error(u'execution failed for Unicode path: {0!r}',
                                item.path)
                continue

            key_raw = output.rsplit(None, 1)[-1]
            try:
                key = util.text_string(key_raw)
            except UnicodeDecodeError:
                self._log.error(u'output is invalid UTF-8')
                continue

            item['initial_key'] = key
            self._log.info(u'added computed initial key {0} for {1}',
                           key, util.displayable_path(item.path))

            if write:
                item.try_write()
            item.store()","1. Use `subprocess.check_output` instead of `subprocess.call` to get the output of the command. This will prevent the command from failing silently if it exits with a non-zero exit code.
2. Use `os.path.normpath` to normalize the path before passing it to the command. This will prevent the command from failing if the path contains spaces or other special characters.
3. Use `six.ensure_text` to convert the output of the command to a text string. This will prevent the command from failing if the output is not in UTF-8 encoding."
"    def commands(self):
        cmd = ui.Subcommand('lyrics', help='fetch song lyrics')
        cmd.parser.add_option(
            u'-p', u'--print', dest='printlyr',
            action='store_true', default=False,
            help=u'print lyrics to console',
        )
        cmd.parser.add_option(
            u'-r', u'--write-rest', dest='writerest',
            action='store', default=None, metavar='dir',
            help=u'write lyrics to given directory as ReST files',
        )
        cmd.parser.add_option(
            u'-f', u'--force', dest='force_refetch',
            action='store_true', default=False,
            help=u'always re-download lyrics',
        )
        cmd.parser.add_option(
            u'-l', u'--local', dest='local_only',
            action='store_true', default=False,
            help=u'do not fetch missing lyrics',
        )

        def func(lib, opts, args):
            # The ""write to files"" option corresponds to the
            # import_write config value.
            write = ui.should_write()
            if opts.writerest:
                self.writerest_indexes(opts.writerest)
            for item in lib.items(ui.decargs(args)):
                if not opts.local_only and not self.config['local']:
                    self.fetch_item_lyrics(
                        lib, item, write,
                        opts.force_refetch or self.config['force'],
                    )
                if item.lyrics:
                    if opts.printlyr:
                        ui.print_(item.lyrics)
                    if opts.writerest:
                        self.writerest(opts.writerest, item)
            if opts.writerest:
                # flush last artist
                self.writerest(opts.writerest, None)
                ui.print_(u'ReST files generated. to build, use one of:')
                ui.print_(u'  sphinx-build -b html %s _build/html'
                          % opts.writerest)
                ui.print_(u'  sphinx-build -b epub %s _build/epub'
                          % opts.writerest)
                ui.print_((u'  sphinx-build -b latex %s _build/latex '
                           u'&& make -C _build/latex all-pdf')
                          % opts.writerest)
        cmd.func = func
        return [cmd]","1. Use `requests.get()` with `verify=False` to disable SSL certificate verification.
2. Use `requests.post()` to send data to the server.
3. Use `json.dumps()` to convert the data to JSON format."
"        def func(lib, opts, args):
            # The ""write to files"" option corresponds to the
            # import_write config value.
            write = ui.should_write()
            if opts.writerest:
                self.writerest_indexes(opts.writerest)
            for item in lib.items(ui.decargs(args)):
                if not opts.local_only and not self.config['local']:
                    self.fetch_item_lyrics(
                        lib, item, write,
                        opts.force_refetch or self.config['force'],
                    )
                if item.lyrics:
                    if opts.printlyr:
                        ui.print_(item.lyrics)
                    if opts.writerest:
                        self.writerest(opts.writerest, item)
            if opts.writerest:
                # flush last artist
                self.writerest(opts.writerest, None)
                ui.print_(u'ReST files generated. to build, use one of:')
                ui.print_(u'  sphinx-build -b html %s _build/html'
                          % opts.writerest)
                ui.print_(u'  sphinx-build -b epub %s _build/epub'
                          % opts.writerest)
                ui.print_((u'  sphinx-build -b latex %s _build/latex '
                           u'&& make -C _build/latex all-pdf')
                          % opts.writerest)","1. Use `verify_user` to check if the user is authorized to access the data.
2. Use `sanitize_input` to sanitize user input to prevent injection attacks.
3. Use `encrypt_data` to encrypt sensitive data before storing it in a database."
"    def writerest(self, directory, item):
        """"""Write the item to an ReST file

        This will keep state (in the `rest` variable) in order to avoid
        writing continuously to the same files.
        """"""

        if item is None or slug(self.artist) != slug(item.albumartist):
            if self.rest is not None:
                path = os.path.join(directory, 'artists',
                                    slug(self.artist) + u'.rst')
                with open(path, 'wb') as output:
                    output.write(self.rest.encode('utf-8'))
                self.rest = None
                if item is None:
                    return
            self.artist = item.albumartist.strip()
            self.rest = u""%s\\n%s\\n\\n.. contents::\\n   :local:\\n\\n"" \\
                        % (self.artist,
                           u'=' * len(self.artist))
        if self.album != item.album:
            tmpalbum = self.album = item.album.strip()
            if self.album == '':
                tmpalbum = u'Unknown album'
            self.rest += u""%s\\n%s\\n\\n"" % (tmpalbum, u'-' * len(tmpalbum))
        title_str = u"":index:`%s`"" % item.title.strip()
        block = u'| ' + item.lyrics.replace(u'\\n', u'\\n| ')
        self.rest += u""%s\\n%s\\n\\n%s\\n\\n"" % (title_str,
                                            u'~' * len(title_str),
                                            block)","1. Use `with open()` to open files instead of `open()`.
2. Use `encode()` to convert strings to bytes before writing them to files.
3. Use `strip()` to remove whitespace from strings before comparing them."
"    def album_for_id(self, album_id):
        """"""Fetches an album by its Discogs ID and returns an AlbumInfo object
        or None if the album is not found.
        """"""
        if not self.discogs_client:
            return

        self._log.debug(u'Searching for release {0}', album_id)
        # Discogs-IDs are simple integers. We only look for those at the end
        # of an input string as to avoid confusion with other metadata plugins.
        # An optional bracket can follow the integer, as this is how discogs
        # displays the release ID on its webpage.
        match = re.search(r'(^|\\[*r|discogs\\.com/.+/release/)(\\d+)($|\\])',
                          album_id)
        if not match:
            return None
        result = Release(self.discogs_client, {'id': int(match.group(2))})
        # Try to obtain title to verify that we indeed have a valid Release
        try:
            getattr(result, 'title')
        except DiscogsAPIError as e:
            if e.status_code != 404:
                self._log.debug(u'API Error: {0} (query: {1})', e, result._uri)
                if e.status_code == 401:
                    self.reset_auth()
                    return self.album_for_id(album_id)
            return None
        except CONNECTION_ERRORS:
            self._log.debug(u'Connection error in album lookup', exc_info=True)
            return None
        return self.get_album_info(result)","1. Use `requests` library instead of `urllib2` to avoid insecure connections.
2. Use `requests.get()` with `params` instead of `requests.urlopen()` to avoid leaking sensitive information in the URL.
3. Use `requests.Response.json()` to parse the response body instead of `eval()` to avoid potential code injection attacks."
"    def get_master_year(self, master_id):
        """"""Fetches a master release given its Discogs ID and returns its year
        or None if the master release is not found.
        """"""
        self._log.debug(u'Searching for master release {0}', master_id)
        result = Master(self.discogs_client, {'id': master_id})

        self.request_start()
        try:
            year = result.fetch('year')
            self.request_finished()
            return year
        except DiscogsAPIError as e:
            if e.status_code != 404:
                self._log.debug(u'API Error: {0} (query: {1})', e, result._uri)
                if e.status_code == 401:
                    self.reset_auth()
                    return self.get_master_year(master_id)
            return None
        except CONNECTION_ERRORS:
            self._log.debug(u'Connection error in master release lookup',
                            exc_info=True)
            return None","1. Use `requests` library instead of `urllib` to avoid insecure methods like `urllib.urlopen`.
2. Use `requests.get` with `params` instead of `requests.post` to avoid sending sensitive data in the request body.
3. Use `requests.auth` to authenticate the requests and avoid unauthorized access."
"    def open_audio_file(self, item):
        """"""Open the file to read the PCM stream from the using
        ``item.path``.

        :return: the audiofile instance
        :rtype: :class:`audiotools.AudioFile`
        :raises :exc:`ReplayGainError`: if the file is not found or the
        file format is not supported
        """"""
        try:
            audiofile = self._mod_audiotools.open(item.path)
        except IOError:
            raise ReplayGainError(
                u""File {} was not found"".format(item.path)
            )
        except self._mod_audiotools.UnsupportedFile:
            raise ReplayGainError(
                u""Unsupported file type {}"".format(item.format)
            )

        return audiofile","1. Use `pathlib.Path` instead of `str` to prevent `os.path.join()` from
                    being used to construct paths outside the intended directory.
2. Use `contextlib.closing()` to ensure that the audio file is closed
                    after it is used.
3. Use `typing.TYPE_CHECKING` to annotate the arguments of `open_audio_file()`
                    to ensure that they are of the correct type."
"    def authenticate(self):
        if self.m.is_authenticated():
            return
        # Checks for OAuth2 credentials,
        # if they don't exist - performs authorization
        oauth_file = self.config['oauth_file'].as_str()
        if os.path.isfile(oauth_file):
            uploader_id = self.config['uploader_id']
            uploader_name = self.config['uploader_name']
            self.m.login(oauth_credentials=oauth_file,
                         uploader_id=uploader_id.as_str().upper() or None,
                         uploader_name=uploader_name.as_str() or None)
        else:
            self.m.perform_oauth(oauth_file)","1. Use `os.path.abspath()` to get the absolute path of the OAuth2 credentials file. This will prevent directory traversal attacks.
2. Use `os.path.expanduser()` to expand the home directory path in the OAuth2 credentials file path. This will prevent relative path attacks.
3. Use `json.load()` to load the OAuth2 credentials file contents into a Python dictionary. This will prevent JSON injection attacks."
"    def parse_tool_output(self, text, path_list, is_album):
        """"""Given the  output from bs1770gain, parse the text and
        return a list of dictionaries
        containing information about each analyzed file.
        """"""
        per_file_gain = {}
        album_gain = {}  # mutable variable so it can be set from handlers
        parser = xml.parsers.expat.ParserCreate(encoding='utf-8')
        state = {'file': None, 'gain': None, 'peak': None}

        def start_element_handler(name, attrs):
            if name == u'track':
                state['file'] = bytestring_path(attrs[u'file'])
                if state['file'] in per_file_gain:
                    raise ReplayGainError(
                        u'duplicate filename in bs1770gain output')
            elif name == u'integrated':
                state['gain'] = float(attrs[u'lu'])
            elif name == u'sample-peak':
                state['peak'] = float(attrs[u'factor'])

        def end_element_handler(name):
            if name == u'track':
                if state['gain'] is None or state['peak'] is None:
                    raise ReplayGainError(u'could not parse gain or peak from '
                                          'the output of bs1770gain')
                per_file_gain[state['file']] = Gain(state['gain'],
                                                    state['peak'])
                state['gain'] = state['peak'] = None
            elif name == u'summary':
                if state['gain'] is None or state['peak'] is None:
                    raise ReplayGainError(u'could not parse gain or peak from '
                                          'the output of bs1770gain')
                album_gain[""album""] = Gain(state['gain'], state['peak'])
                state['gain'] = state['peak'] = None
        parser.StartElementHandler = start_element_handler
        parser.EndElementHandler = end_element_handler
        parser.Parse(text, True)

        if len(per_file_gain) != len(path_list):
            raise ReplayGainError(
                u'the number of results returned by bs1770gain does not match '
                'the number of files passed to it')

        # bs1770gain does not return the analysis results in the order that
        # files are passed on the command line, because it is sorting the files
        # internally. We must recover the order from the filenames themselves.
        try:
            out = [per_file_gain[os.path.basename(p)] for p in path_list]
        except KeyError:
            raise ReplayGainError(
                u'unrecognized filename in bs1770gain output '
                '(bs1770gain can only deal with utf-8 file names)')
        if is_album:
            out.append(album_gain[""album""])
        return out","1. Use `xml.etree.ElementTree` instead of `xml.parsers.expat` to parse XML.
2. Validate the XML input before parsing it.
3. Use `bytestring_path` to ensure that all filenames are UTF-8 encoded."
"    def __init__(self):
        super(LyricsPlugin, self).__init__()
        self.import_stages = [self.imported]
        self.config.add({
            'auto': True,
            'bing_client_secret': None,
            'bing_lang_from': [],
            'bing_lang_to': None,
            'google_API_key': None,
            'google_engine_ID': u'009217259823014548361:lndtuqkycfu',
            'genius_api_key':
                ""Ryq93pUGm8bM6eUWwD_M3NOFFDAtp2yEE7W""
                ""76V-uFL5jks5dNvcGCdarqFjDhP9c"",
            'fallback': None,
            'force': False,
            'local': False,
            'sources': self.SOURCES,
        })
        self.config['bing_client_secret'].redact = True
        self.config['google_API_key'].redact = True
        self.config['google_engine_ID'].redact = True
        self.config['genius_api_key'].redact = True

        # State information for the ReST writer.
        # First, the current artist we're writing.
        self.artist = u'Unknown artist'
        # The current album: False means no album yet.
        self.album = False
        # The current rest file content. None means the file is not
        # open yet.
        self.rest = None

        available_sources = list(self.SOURCES)
        sources = plugins.sanitize_choices(
            self.config['sources'].as_str_seq(), available_sources)

        if 'google' in sources:
            if not self.config['google_API_key'].get():
                # We log a *debug* message here because the default
                # configuration includes `google`. This way, the source
                # is silent by default but can be enabled just by
                # setting an API key.
                self._log.debug(u'Disabling google source: '
                                u'no API key configured.')
                sources.remove('google')
            elif not HAS_BEAUTIFUL_SOUP:
                self._log.warning(u'To use the google lyrics source, you must '
                                  u'install the beautifulsoup4 module. See '
                                  u'the documentation for further details.')
                sources.remove('google')

        if 'genius' in sources and not HAS_BEAUTIFUL_SOUP:
            self._log.debug(
                u'The Genius backend requires BeautifulSoup, which is not '
                u'installed, so the source is disabled.'
            )
            sources.remove('google')

        self.config['bing_lang_from'] = [
            x.lower() for x in self.config['bing_lang_from'].as_str_seq()]
        self.bing_auth_token = None

        if not HAS_LANGDETECT and self.config['bing_client_secret'].get():
            self._log.warning(u'To use bing translations, you need to '
                              u'install the langdetect module. See the '
                              u'documentation for further details.')

        self.backends = [self.SOURCE_BACKENDS[source](self.config, self._log)
                         for source in sources]","1. Use `get()` instead of `config['key'].get()` to avoid KeyError.
2. Use `plugins.sanitize_choices()` to sanitize user input.
3. Use `logging.warning()` instead of `logging.debug()` to log warnings."
"    def lyrics_from_song_api_path(self, song_api_path):
      song_url = self.base_url + song_api_path
      response = requests.get(song_url, headers=self.headers)
      json = response.json()
      path = json[""response""][""song""][""path""]
      #gotta go regular html scraping... come on Genius
      page_url = ""https://genius.com"" + path
      page = requests.get(page_url)
      html = BeautifulSoup(page.text, ""html.parser"")
      #remove script tags that they put in the middle of the lyrics
      [h.extract() for h in html('script')]
      #at least Genius is nice and has a tag called 'lyrics'!
      lyrics = html.find(""div"", class_=""lyrics"").get_text() #updated css where the lyrics are based in HTML
      return lyrics","1. Use `requests.get()` with `verify=False` to avoid certificate validation errors.
2. Use `requests.post()` with `data=json.dumps(data)` to send data as JSON.
3. Use `requests.cookies()` to manage cookies."
"def item_file(item_id):
    item = g.lib.get_item(item_id)
    item_path = util.syspath(item.path) if os.name == 'nt' else util.py3_path(item.path)
    response = flask.send_file(
        item_path,
        as_attachment=True,
        attachment_filename=os.path.basename(util.py3_path(item.path)),
    )
    response.headers['Content-Length'] = os.path.getsize(item_path)
    return response","1. Use `flask.send_file()` with `as_attachment=True` to ensure that the file is downloaded as an attachment, rather than being displayed in the browser.
2. Set the `attachment_filename` header to the desired filename for the download.
3. Set the `Content-Length` header to the actual size of the file, to prevent a client from requesting more data than is actually available."
"def item_file(item_id):
    item = g.lib.get_item(item_id)
    response = flask.send_file(
        util.py3_path(item.path),
        as_attachment=True,
        attachment_filename=os.path.basename(util.py3_path(item.path)),
    )
    response.headers['Content-Length'] = os.path.getsize(item.path)
    return response","1. Use `flask.send_from_directory` instead of `flask.send_file` to prevent directory traversal attacks.
2. Set the `Content-Security-Policy` header to prevent cross-site scripting attacks.
3. Use `os.path.basename` to generate the attachment filename instead of using the raw path, to prevent leaking sensitive information."
"def _rep(obj, expand=False):
    """"""Get a flat -- i.e., JSON-ish -- representation of a beets Item or
    Album object. For Albums, `expand` dictates whether tracks are
    included.
    """"""
    out = dict(obj)

    if isinstance(obj, beets.library.Item):
        if app.config.get('INCLUDE_PATHS', False):
            out['path'] = util.displayable_path(out['path'])
        else:
            del out['path']

        # Filter all bytes attributes and convert them to strings
        for key in filter(lambda key: isinstance(out[key], bytes), out):
                out[key] = base64.b64encode(out[key]).decode('ascii')

        # Get the size (in bytes) of the backing file. This is useful
        # for the Tomahawk resolver API.
        try:
            out['size'] = os.path.getsize(util.syspath(obj.path))
        except OSError:
            out['size'] = 0

        return out

    elif isinstance(obj, beets.library.Album):
        del out['artpath']
        if expand:
            out['items'] = [_rep(item) for item in obj.items()]
        return out","1. **Use proper escaping** to prevent XSS attacks.
2. **Sanitize user input** to prevent injection attacks.
3. **Use strong passwords** for all user accounts."
"def _rep(obj, expand=False):
    """"""Get a flat -- i.e., JSON-ish -- representation of a beets Item or
    Album object. For Albums, `expand` dictates whether tracks are
    included.
    """"""
    out = dict(obj)

    if isinstance(obj, beets.library.Item):
        if app.config.get('INCLUDE_PATHS', False):
            out['path'] = util.displayable_path(out['path'])
        else:
            del out['path']

        # Get the size (in bytes) of the backing file. This is useful
        # for the Tomahawk resolver API.
        try:
            out['size'] = os.path.getsize(util.syspath(obj.path))
        except OSError:
            out['size'] = 0

        return out

    elif isinstance(obj, beets.library.Album):
        del out['artpath']
        if expand:
            out['items'] = [_rep(item) for item in obj.items()]
        return out","1. Use `functools.lru_cache` to cache the results of `os.path.getsize` to prevent repeated calls.
2. Sanitize the input of `util.displayable_path` to prevent malicious users from injecting harmful characters.
3. Use `beets.library.Item.items()` instead of iterating over `obj.items()` to prevent a user from accessing items that they do not have permission to view."
"    def get_albums(self, query):
        """"""Returns a list of AlbumInfo objects for a discogs search query.
        """"""
        # Strip non-word characters from query. Things like ""!"" and ""-"" can
        # cause a query to return no results, even if they match the artist or
        # album title. Use `re.UNICODE` flag to avoid stripping non-english
        # word characters.
        # FIXME: Encode as ASCII to work around a bug:
        # https://github.com/beetbox/beets/issues/1051
        # When the library is fixed, we should encode as UTF-8.
        query = re.sub(r'(?u)\\W+', ' ', query).encode('ascii', ""replace"")
        # Strip medium information from query, Things like ""CD1"" and ""disk 1""
        # can also negate an otherwise positive result.
        query = re.sub(br'(?i)\\b(CD|disc)\\s*\\d+', b'', query)
        try:
            releases = self.discogs_client.search(query,
                                                  type='release').page(1)
        except CONNECTION_ERRORS:
            self._log.debug(u""Communication error while searching for {0!r}"",
                            query, exc_info=True)
            return []
        return [self.get_album_info(release) for release in releases[:5]]","1. Use `re.compile()` to compile the regular expression once, instead of calling `re.sub()` multiple times. This will improve performance.
2. Use `urllib.parse.quote()` to encode the query string before sending it to Discogs. This will prevent malicious users from injecting code into the query string.
3. Use `requests.get()` with the `verify=False` parameter to disable SSL certificate verification. This is necessary because Discogs uses a self-signed certificate."
"    def get_album_info(self, result):
        """"""Returns an AlbumInfo object for a discogs Release object.
        """"""
        artist, artist_id = self.get_artist([a.data for a in result.artists])
        album = re.sub(r' +', ' ', result.title)
        album_id = result.data['id']
        # Use `.data` to access the tracklist directly instead of the
        # convenient `.tracklist` property, which will strip out useful artist
        # information and leave us with skeleton `Artist` objects that will
        # each make an API call just to get the same data back.
        tracks = self.get_tracks(result.data['tracklist'])
        albumtype = ', '.join(
            result.data['formats'][0].get('descriptions', [])) or None
        va = result.data['artists'][0]['name'].lower() == 'various'
        if va:
            artist = config['va_name'].as_str()
        year = result.data['year']
        label = result.data['labels'][0]['name']
        mediums = len(set(t.medium for t in tracks))
        catalogno = result.data['labels'][0]['catno']
        if catalogno == 'none':
            catalogno = None
        country = result.data.get('country')
        media = result.data['formats'][0]['name']
        # Explicitly set the `media` for the tracks, since it is expected by
        # `autotag.apply_metadata`, and set `medium_total`.
        for track in tracks:
            track.media = media
            track.medium_total = mediums
        data_url = result.data['uri']
        return AlbumInfo(album, album_id, artist, artist_id, tracks, asin=None,
                         albumtype=albumtype, va=va, year=year, month=None,
                         day=None, label=label, mediums=mediums,
                         artist_sort=None, releasegroup_id=None,
                         catalognum=catalogno, script=None, language=None,
                         country=country, albumstatus=None, media=media,
                         albumdisambig=None, artist_credit=None,
                         original_year=None, original_month=None,
                         original_day=None, data_source='Discogs',
                         data_url=data_url)","1. Use `.data` to access the tracklist directly instead of the
    convenient `.tracklist` property, which will strip out useful artist
    information and leave us with skeleton `Artist` objects that will
    each make an API call just to get the same data back.
2. Explicitly set the `media` for the tracks, since it is expected by
    `autotag.apply_metadata`, and set `medium_total`.
3. Use `config['va_name'].as_str()` to get the value of the `va_name` config variable as a string, rather than as a `ConfigValue` object."
"    def match(self, item):
        timestamp = float(item[self.field])
        date = datetime.utcfromtimestamp(timestamp)
        return self.interval.contains(date)","1. **Use `datetime.datetime.fromtimestamp()` instead of `datetime.utcfromtimestamp()`**. This will ensure that the timestamp is interpreted in the correct timezone.
2. **Check that the timestamp is valid before using it to create a `datetime` object**. This will prevent attacks that attempt to pass invalid timestamps.
3. **Use `datetime.timedelta()` to create the interval instead of hard-coding it**. This will make it easier to change the interval in the future."
"def _is_hidden_osx(path):
    """"""Return whether or not a file is hidden on OS X.

    This uses os.lstat to work out if a file has the ""hidden"" flag.
    """"""
    file_stat = os.lstat(path)

    if hasattr(file_stat, 'st_flags') and hasattr(stat, 'UF_HIDDEN'):
        return bool(file_stat.st_flags & stat.UF_HIDDEN)
    else:
        return False","1. Use `os.path.isfile` to check if the path is a file.
2. Use `os.access` to check if the user has permission to read the file.
3. Use `os.stat` to get the file's permissions and check if the file is hidden."
"def _is_hidden_win(path):
    """"""Return whether or not a file is hidden on Windows.

    This uses GetFileAttributes to work out if a file has the ""hidden"" flag
    (FILE_ATTRIBUTE_HIDDEN).
    """"""
    # FILE_ATTRIBUTE_HIDDEN = 2 (0x2) from GetFileAttributes documentation.
    hidden_mask = 2

    # Retrieve the attributes for the file.
    attrs = ctypes.windll.kernel32.GetFileAttributesW(path)

    # Ensure we have valid attribues and compare them against the mask.
    return attrs >= 0 and attrs & hidden_mask","1. Use `os.access()` instead of `ctypes.windll.kernel32.GetFileAttributesW()` to check if a file is hidden.
2. Use `os.umask()` to set the file mode mask for newly created files.
3. Use `os.chmod()` to change the permissions of a file."
"def _is_hidden_dot(path):
    """"""Return whether or not a file starts with a dot.

    Files starting with a dot are seen as ""hidden"" files on Unix-based OSes.
    """"""
    return os.path.basename(path).startswith('.')","1. **Use `pathlib.Path` instead of `os.path`.** `pathlib.Path` is a more modern and secure way to work with paths in Python. It provides a number of features that `os.path` does not, such as support for symbolic links and case-insensitive filesystems.
2. **Use `os.access` to check if a file is readable or writable.** `os.access` takes a path and a mode as arguments, and returns True if the file exists and the user has the specified permissions. This is more secure than using `os.path.isfile` or `os.path.isdir`, which do not check permissions.
3. **Use `os.umask` to set the default permissions for newly created files.** `os.umask` takes a mode as an argument, and returns the previous umask value. This can be used to ensure that newly created files are not world-writable by default."
"def is_hidden(path):
    """"""Return whether or not a file is hidden.

    This method works differently depending on the platform it is called on.

    On OS X, it uses both the result of `is_hidden_osx` and `is_hidden_dot` to
    work out if a file is hidden.

    On Windows, it uses the result of `is_hidden_win` to work out if a file is
    hidden.

    On any other operating systems (i.e. Linux), it uses `is_hidden_dot` to
    work out if a file is hidden.
    """"""
    # Convert the path to unicode if it is not already.
    if not isinstance(path, six.text_type):
        path = path.decode('utf-8')

    # Run platform specific functions depending on the platform
    if sys.platform == 'darwin':
        return _is_hidden_osx(path) or _is_hidden_dot(path)
    elif sys.platform == 'win32':
        return _is_hidden_win(path)
    else:
        return _is_hidden_dot(path)","1. Use `pathlib.Path` instead of `os.path` to avoid `os.path.isfile()` race condition.
2. Use `pathlib.Path.is_hidden()` instead of custom implementation to avoid security issues.
3. Use `pathlib.Path.expanduser()` to expand user-specific paths to avoid security issues."
"    def sort(self, objs):
        # TODO: Conversion and null-detection here. In Python 3,
        # comparisons with None fail. We should also support flexible
        # attributes with different types without falling over.

        def key(item):
            field_val = getattr(item, self.field)
            if self.case_insensitive and isinstance(field_val, unicode):
                field_val = field_val.lower()
            return field_val

        return sorted(objs, key=key, reverse=not self.ascending)","1. Use `functools.lru_cache` to cache the results of `key()` to avoid repeated calls.
2. Use `type()` to check the type of `field_val` and raise a `TypeError` if it is not a string.
3. Use `six.text_type()` to convert `field_val` to a `str` in Python 2."
"        def key(item):
            field_val = getattr(item, self.field)
            if self.case_insensitive and isinstance(field_val, unicode):
                field_val = field_val.lower()
            return field_val","1. Use `functools.lru_cache` to memoize the `key()` function to improve performance.
2. Use `typing.TYPE_CHECKING` to check the type of `item` and `field_val` to prevent errors.
3. Use `six.ensure_str` to convert `field_val` to a string to prevent UnicodeDecodeError."
"    def is_page_candidate(self, url_link, url_title, title, artist):
        """"""Return True if the URL title makes it a good candidate to be a
        page that contains lyrics of title by artist.
        """"""
        title = self.slugify(title.lower())
        artist = self.slugify(artist.lower())
        sitename = re.search(u""//([^/]+)/.*"",
                             self.slugify(url_link.lower())).group(1)
        url_title = self.slugify(url_title.lower())
        # Check if URL title contains song title (exact match)
        if url_title.find(title) != -1:
            return True
        # or try extracting song title from URL title and check if
        # they are close enough
        tokens = [by + '_' + artist for by in self.BY_TRANS] + \\
                 [artist, sitename, sitename.replace('www.', '')] + \\
            self.LYRICS_TRANS
        song_title = re.sub(u'(%s)' % u'|'.join(tokens), u'', url_title)
        song_title = song_title.strip('_|')
        typo_ratio = .9
        ratio = difflib.SequenceMatcher(None, song_title, title).ratio()
        return ratio >= typo_ratio","1. Use `urllib.parse.unquote` to unquote the URL before processing it. This will prevent malicious code from being injected into the URL.
2. Use `re.compile` to create a regular expression for the song title. This will prevent false positives from being returned.
3. Use `difflib.SequenceMatcher.ratio()` to compare the song title to the URL title. This will ensure that the two titles are similar enough to be considered a match."
"    def __init__(self):
        super(DuplicatesPlugin, self).__init__()

        self.config.add({
            'format': '',
            'count': False,
            'album': False,
            'full': False,
            'strict': False,
            'path': False,
            'keys': ['mb_trackid', 'mb_albumid'],
            'checksum': None,
            'copy': False,
            'move': False,
            'delete': False,
            'tag': False,
        })

        self._command = Subcommand('duplicates',
                                   help=__doc__,
                                   aliases=['dup'])
        self._command.parser.add_option('-c', '--count', dest='count',
                                        action='store_true',
                                        help='show duplicate counts')

        self._command.parser.add_option('-C', '--checksum', dest='checksum',
                                        action='store', metavar='PROG',
                                        help='report duplicates based on'
                                        ' arbitrary command')

        self._command.parser.add_option('-d', '--delete', dest='delete',
                                        action='store_true',
                                        help='delete items from library and '
                                        'disk')

        self._command.parser.add_option('-F', '--full', dest='full',
                                        action='store_true',
                                        help='show all versions of duplicate'
                                        ' tracks or albums')

        self._command.parser.add_option('-s', '--strict', dest='strict',
                                        action='store_true',
                                        help='report duplicates only if all'
                                        ' attributes are set')

        self._command.parser.add_option('-k', '--keys', dest='keys',
                                        action='callback', metavar='KEY1 KEY2',
                                        callback=vararg_callback,
                                        help='report duplicates based on keys')

        self._command.parser.add_option('-m', '--move', dest='move',
                                        action='store', metavar='DEST',
                                        help='move items to dest')

        self._command.parser.add_option('-o', '--copy', dest='copy',
                                        action='store', metavar='DEST',
                                        help='copy items to dest')

        self._command.parser.add_option('-t', '--tag', dest='tag',
                                        action='store',
                                        help='tag matched items with \\'k=v\\''
                                        ' attribute')
        self._command.parser.add_all_common_options()","1. Use `argparse.ArgumentParser.add_argument()` instead of `add_option()` to avoid passing options as strings.
2. Use `argparse.FileType()` to validate file paths.
3. Use `argparse.ArgumentParser.parse_args()` to parse the arguments instead of `optparse.OptionParser.parse_args()`."
"    def commands(self):

        def _dup(lib, opts, args):
            self.config.set_args(opts)
            fmt = self.config['format'].get()
            album = self.config['album'].get(bool)
            full = self.config['full'].get(bool)
            strict = self.config['strict'].get(bool)
            keys = self.config['keys'].get()
            checksum = self.config['checksum'].get()
            copy = self.config['copy'].get()
            move = self.config['move'].get()
            delete = self.config['delete'].get(bool)
            tag = self.config['tag'].get()

            if album:
                keys = ['mb_albumid']
                items = lib.albums(decargs(args))
            else:
                items = lib.items(decargs(args))

            if self.config['path']:
                fmt = '$path'

            # Default format string for count mode.
            if self.config['count'] and not fmt:
                if album:
                    fmt = '$albumartist - $album'
                else:
                    fmt = '$albumartist - $album - $title'
                fmt += ': {0}'

            if checksum:
                if not isinstance(checksum, basestring):
                    raise UserError(
                        'duplicates: ""checksum"" option must be a command'
                    )
                for i in items:
                    k, _ = self._checksum(i, checksum)
                keys = [k]

            for obj_id, obj_count, objs in self._duplicates(items,
                                                            keys=keys,
                                                            full=full,
                                                            strict=strict):
                if obj_id:  # Skip empty IDs.
                    for o in objs:
                        self._process_item(o, lib,
                                           copy=copy,
                                           move=move,
                                           delete=delete,
                                           tag=tag,
                                           fmt=fmt.format(obj_count))

        self._command.func = _dup
        return [self._command]","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `inspect.getfullargspec` to get the full argument spec of the original function.
3. Use `inspect.ismethod` to check if the original function is a method."
"        def _dup(lib, opts, args):
            self.config.set_args(opts)
            fmt = self.config['format'].get()
            album = self.config['album'].get(bool)
            full = self.config['full'].get(bool)
            strict = self.config['strict'].get(bool)
            keys = self.config['keys'].get()
            checksum = self.config['checksum'].get()
            copy = self.config['copy'].get()
            move = self.config['move'].get()
            delete = self.config['delete'].get(bool)
            tag = self.config['tag'].get()

            if album:
                keys = ['mb_albumid']
                items = lib.albums(decargs(args))
            else:
                items = lib.items(decargs(args))

            if self.config['path']:
                fmt = '$path'

            # Default format string for count mode.
            if self.config['count'] and not fmt:
                if album:
                    fmt = '$albumartist - $album'
                else:
                    fmt = '$albumartist - $album - $title'
                fmt += ': {0}'

            if checksum:
                if not isinstance(checksum, basestring):
                    raise UserError(
                        'duplicates: ""checksum"" option must be a command'
                    )
                for i in items:
                    k, _ = self._checksum(i, checksum)
                keys = [k]

            for obj_id, obj_count, objs in self._duplicates(items,
                                                            keys=keys,
                                                            full=full,
                                                            strict=strict):
                if obj_id:  # Skip empty IDs.
                    for o in objs:
                        self._process_item(o, lib,
                                           copy=copy,
                                           move=move,
                                           delete=delete,
                                           tag=tag,
                                           fmt=fmt.format(obj_count))","1. Use `functools.lru_cache` to cache the results of expensive functions.
2. Use `validation.validate_args` to validate the arguments passed to the function.
3. Use `logging.exception` to log exceptions and their stack traces."
"    def write(self, path=None, tags=None):
        """"""Write the item's metadata to a media file.

        All fields in `_media_fields` are written to disk according to
        the values on this object.

        `path` is the path of the mediafile to wirte the data to. It
        defaults to the item's path.

        `tags` is a dictionary of additional metadata the should be
        written to the file.

        Can raise either a `ReadError` or a `WriteError`.
        """"""
        if path is None:
            path = self.path
        else:
            path = normpath(path)

        item_tags = dict(self)
        if tags is not None:
            item_tags.update(tags)
        plugins.send('write', item=self, path=path, tags=item_tags)

        try:
            mediafile = MediaFile(syspath(path),
                                  id3v23=beets.config['id3v23'].get(bool))
        except (OSError, IOError, UnreadableFileError) as exc:
            raise ReadError(self.path, exc)

        mediafile.update(item_tags)
        try:
            mediafile.save()
        except (OSError, IOError, MutagenError) as exc:
            raise WriteError(self.path, exc)

        # The file has a new mtime.
        if path == self.path:
            self.mtime = self.current_mtime()
        plugins.send('after_write', item=self, path=path)","1. Use `pathlib.Path` instead of `os.path` to handle file paths.
2. Use `contextlib.closing` to ensure that the file is closed after it is used.
3. Use `typing` to annotate the function parameters and return values."
"    def authenticate(self, c_key, c_secret):
        # Get the link for the OAuth page.
        auth_client = Client(USER_AGENT, c_key, c_secret)
        _, _, url = auth_client.get_authorize_url()
        beets.ui.print_(""To authenticate with Discogs, visit:"")
        beets.ui.print_(url)

        # Ask for the code and validate it.
        code = beets.ui.input_(""Enter the code:"")
        try:
            token, secret = auth_client.get_access_token(code)
        except DiscogsAPIError:
            raise beets.ui.UserError('Discogs authorization failed')
        except CONNECTION_ERRORS as e:
            self._log.debug(u'connection error: {0}', e)
            raise beets.ui.UserError('communication with Discogs failed')

        # Save the token for later use.
        self._log.debug('Discogs token {0}, secret {1}', token, secret)
        with open(self._tokenfile(), 'w') as f:
            json.dump({'token': token, 'secret': secret}, f)

        return token, secret","1. Use HTTPS instead of HTTP to protect the transmission of credentials.
2. Use a secure secret instead of a plaintext password.
3. Validate the code before using it to get the access token."
"    def _getters(cls):
        getters = plugins.item_field_getters()
        getters['singleton'] = lambda i: i.album_id is None
        # Filesize is given in bytes
        getters['filesize'] = lambda i: os.path.getsize(syspath(i.path))
        return getters","1. Use `os.path.realpath` instead of `syspath` to get the absolute path of a file, to avoid directory traversal attacks.
2. Use `os.stat` instead of `os.path.getsize` to get the file size, to avoid integer overflow attacks.
3. Sanitize user input before using it in any security-sensitive operations, to avoid code injection attacks."
"    def _getters(cls):
        getters = plugins.item_field_getters()
        getters['singleton'] = lambda i: i.album_id is None
        # Filesize is given in bytes
        getters['filesize'] = lambda i: i.try_filesize()
        return getters","1. Use `functools.lru_cache` to cache the results of expensive functions.
2. Use `typing` to annotate the function arguments and return values.
3. Use `unittest` to test the function."
"    def try_filesize(self):
        try:
            return os.path.getsize(syspath(self.path))
        except (OSError, Exception) as exc:
            log.warning(u'could not get filesize: {0}', exc)
            return 0","1. Use `pathlib.Path` instead of `os.path` to avoid path traversal vulnerabilities.
2. Handle `FileNotFoundError` instead of `OSError` to avoid catching other errors that could hide a security vulnerability.
3. Log the exception with a more descriptive message, including the full path to the file that was being accessed."
"def _process_item(item, lib, copy=False, move=False, delete=False,
                  tag=False, format=''):
    """"""Process Item `item` in `lib`.
    """"""
    if copy:
        item.move(basedir=copy, copy=True)
        item.store()
    if move:
        item.move(basedir=move, copy=False)
        item.store()
    if delete:
        item.remove(delete=True)
    if tag:
        try:
            k, v = tag.split('=')
        except:
            raise UserError('%s: can\\'t parse k=v tag: %s' % (PLUGIN, tag))
        setattr(k, v)
        item.store()
    print_(format(item, format))","1. Use `item.copy()` instead of `item.move()` to avoid accidentally deleting the original item.
2. Use `item.delete()` instead of `item.remove()` to avoid accidentally leaving behind orphaned items.
3. Use `item.store()` to persist changes to the item."
"    def commands(self):

        def _dup(lib, opts, args):
            self.config.set_args(opts)
            fmt = self.config['format'].get()
            album = self.config['album'].get(bool)
            full = self.config['full'].get(bool)
            keys = self.config['keys'].get()
            checksum = self.config['checksum'].get()
            copy = self.config['copy'].get()
            move = self.config['move'].get()
            delete = self.config['delete'].get(bool)
            tag = self.config['tag'].get()

            if album:
                keys = ['mb_albumid']
                items = lib.albums(decargs(args))
            else:
                items = lib.items(decargs(args))

            if self.config['path']:
                fmt = '$path'

            # Default format string for count mode.
            if self.config['count'] and not fmt:
                if album:
                    fmt = '$albumartist - $album'
                else:
                    fmt = '$albumartist - $album - $title'
                fmt += ': {0}'

            if checksum:
                if not isinstance(checksum, basestring):
                    raise UserError(
                        'duplicates: ""checksum"" option must be a command'
                    )
                for i in items:
                    k, _ = self._checksum(i, checksum, self._log)
                keys = [k]

            for obj_id, obj_count, objs in _duplicates(items,
                                                       keys=keys,
                                                       full=full,
                                                       log=self._log):
                if obj_id:  # Skip empty IDs.
                    for o in objs:
                        _process_item(o, lib,
                                      copy=copy,
                                      move=move,
                                      delete=delete,
                                      tag=tag,
                                      format=fmt.format(obj_count))

        self._command.func = _dup
        return [self._command]","1. Use `functools.lru_cache` to cache the results of the `_checksum` function.
2. Use `functools.partial` to create a new function that takes the `lib` argument as a keyword argument.
3. Use `inspect.getfullargspec` to get the list of arguments for the `_process_item` function."
"        def _dup(lib, opts, args):
            self.config.set_args(opts)
            fmt = self.config['format'].get()
            album = self.config['album'].get(bool)
            full = self.config['full'].get(bool)
            keys = self.config['keys'].get()
            checksum = self.config['checksum'].get()
            copy = self.config['copy'].get()
            move = self.config['move'].get()
            delete = self.config['delete'].get(bool)
            tag = self.config['tag'].get()

            if album:
                keys = ['mb_albumid']
                items = lib.albums(decargs(args))
            else:
                items = lib.items(decargs(args))

            if self.config['path']:
                fmt = '$path'

            # Default format string for count mode.
            if self.config['count'] and not fmt:
                if album:
                    fmt = '$albumartist - $album'
                else:
                    fmt = '$albumartist - $album - $title'
                fmt += ': {0}'

            if checksum:
                if not isinstance(checksum, basestring):
                    raise UserError(
                        'duplicates: ""checksum"" option must be a command'
                    )
                for i in items:
                    k, _ = self._checksum(i, checksum, self._log)
                keys = [k]

            for obj_id, obj_count, objs in _duplicates(items,
                                                       keys=keys,
                                                       full=full,
                                                       log=self._log):
                if obj_id:  # Skip empty IDs.
                    for o in objs:
                        _process_item(o, lib,
                                      copy=copy,
                                      move=move,
                                      delete=delete,
                                      tag=tag,
                                      format=fmt.format(obj_count))","1. Use `functools.lru_cache` to cache the results of the `_checksum` function.
2. Use `os.path.expanduser` to expand the user-provided paths in the `args` parameter.
3. Use `subprocess.check_output` to execute the external command specified by the `checksum` option."
"    def tmpl_time(s, format):
        """"""Format a time value using `strftime`.
        """"""
        cur_fmt = beets.config['time_format'].get(unicode)
        return time.strftime(format, time.strptime(s, cur_fmt))","1. Use `strptime` with `strftime` to avoid format string vulnerabilities.
2. Use `beets.config.get(type)` to get the config value as the correct type.
3. Use `time.strptime()` to parse the time string, instead of using a regular expression."
"def get_format(format=None):
    """"""Return the command tempate and the extension from the config.
    """"""
    if not format:
        format = config['convert']['format'].get(unicode).lower()
    format = ALIASES.get(format, format)

    try:
        format_info = config['convert']['formats'][format].get(dict)
        command = format_info['command']
        extension = format_info['extension']
    except KeyError:
        raise ui.UserError(
            u'convert: format {0} needs ""command"" and ""extension"" fields'
            .format(format)
        )
    except ConfigTypeError:
        command = config['convert']['formats'][format].get(bytes)
        extension = format

    # Convenience and backwards-compatibility shortcuts.
    keys = config['convert'].keys()
    if 'command' in keys:
        command = config['convert']['command'].get(unicode)
    elif 'opts' in keys:
        # Undocumented option for backwards compatibility with < 1.3.1.
        command = u'ffmpeg -i $source -y {0} $dest'.format(
            config['convert']['opts'].get(unicode)
        )
    if 'extension' in keys:
        extension = config['convert']['extension'].get(unicode)

    return (command.encode('utf8'), extension.encode('utf8'))","1. Use `config.get(key, default)` instead of `config[key]` to
    prevent KeyError.
2. Use `config['convert']['formats'][format].get(type)` instead of
    `config['convert']['formats'][format]` to catch ConfigTypeError.
3. Use `command.encode('utf8')` and `extension.encode('utf8')` to
    return bytestrings instead of unicode strings."
"def should_transcode(item, format):
    """"""Determine whether the item should be transcoded as part of
    conversion (i.e., its bitrate is high or it has the wrong format).
    """"""
    if config['convert']['never_convert_lossy_files'] and \\
            not (item.format.lower() in LOSSLESS_FORMATS):
        return False
    maxbr = config['convert']['max_bitrate'].get(int)
    return format.lower() != item.format.lower() or \\
        item.bitrate >= 1000 * maxbr","1. Use `item.bitrate` instead of `item.bitrate >= 1000 * maxbr` to avoid integer overflow.
2. Use `config['convert']['never_convert_lossy_files'] and \\
            not (item.format.lower() in LOSSLESS_FORMATS)` instead of `not config['convert']['never_convert_lossy_files'] or \\
            item.format.lower() in LOSSLESS_FORMATS` to avoid `NoneType` comparison.
3. Use `config['convert']['max_bitrate'].get(int)` instead of `config['convert']['max_bitrate']` to avoid `KeyError`."
"    def convert_item(self, dest_dir, keep_new, path_formats, format,
                     pretend=False):
        command, ext = get_format(format)
        item, original, converted = None, None, None
        while True:
            item = yield (item, original, converted)
            dest = item.destination(basedir=dest_dir,
                                    path_formats=path_formats)

            # When keeping the new file in the library, we first move the
            # current (pristine) file to the destination. We'll then copy it
            # back to its old path or transcode it to a new path.
            if keep_new:
                original = dest
                converted = item.path
                if should_transcode(item, format):
                    converted = replace_ext(converted, ext)
            else:
                original = item.path
                if should_transcode(item, format):
                    dest = replace_ext(dest, ext)
                converted = dest

            # Ensure that only one thread tries to create directories at a
            # time. (The existence check is not atomic with the directory
            # creation inside this function.)
            if not pretend:
                with _fs_lock:
                    util.mkdirall(dest)

            if os.path.exists(util.syspath(dest)):
                self._log.info(u'Skipping {0} (target file exists)',
                               util.displayable_path(item.path))
                continue

            if keep_new:
                if pretend:
                    self._log.info(u'mv {0} {1}',
                                   util.displayable_path(item.path),
                                   util.displayable_path(original))
                else:
                    self._log.info(u'Moving to {0}',
                                   util.displayable_path(original))
                    util.move(item.path, original)

            if should_transcode(item, format):
                try:
                    self.encode(command, original, converted, pretend)
                except subprocess.CalledProcessError:
                    continue
            else:
                if pretend:
                    self._log.info(u'cp {0} {1}',
                                   util.displayable_path(original),
                                   util.displayable_path(converted))
                else:
                    # No transcoding necessary.
                    self._log.info(u'Copying {0}',
                                   util.displayable_path(item.path))
                    util.copy(original, converted)

            if pretend:
                continue

            # Write tags from the database to the converted file.
            item.try_write(path=converted)

            if keep_new:
                # If we're keeping the transcoded file, read it again (after
                # writing) to get new bitrate, duration, etc.
                item.path = converted
                item.read()
                item.store()  # Store new path and audio data.

            if self.config['embed']:
                album = item.get_album()
                if album and album.artpath:
                    EmbedCoverArtPlugin().embed_item(item, album.artpath,
                                                     itempath=converted)

            if keep_new:
                plugins.send('after_convert', item=item,
                             dest=dest, keepnew=True)
            else:
                plugins.send('after_convert', item=item,
                             dest=converted, keepnew=False)","1. Use `os.path.exists()` to check if the file exists before trying to move or copy it.
2. Use `subprocess.check_call()` instead of `subprocess.call()` to catch errors.
3. Use `util.displayable_path()` to format paths in a way that is safe to log."
"    def convert_on_import(self, lib, item):
        """"""Transcode a file automatically after it is imported into the
        library.
        """"""
        format = self.config['format'].get(unicode).lower()
        if should_transcode(item, format):
            command, ext = get_format()
            fd, dest = tempfile.mkstemp('.' + ext)
            os.close(fd)
            _temp_files.append(dest)  # Delete the transcode later.
            try:
                self.encode(command, item.path, dest)
            except subprocess.CalledProcessError:
                return
            item.path = dest
            item.write()
            item.read()  # Load new audio information data.
            item.store()","1. Use `subprocess.check_output()` instead of `subprocess.call()` to avoid
                                                                   subprocess.CalledProcessError.
2. Use `tempfile.mkdtemp()` instead of `tempfile.mkstemp()` to create a
                                                    temporary directory instead of a temporary file.
3. Delete the temporary files using `shutil.rmtree()` instead of manually
                                                    adding them to a list."
"    def authenticate(self, c_key, c_secret):
        # Get the link for the OAuth page.
        auth_client = Client(USER_AGENT, c_key, c_secret)
        _, _, url = auth_client.get_authorize_url()
        beets.ui.print_(""To authenticate with Discogs, visit:"")
        beets.ui.print_(url)

        # Ask for the code and validate it.
        code = beets.ui.input_(""Enter the code:"")
        try:
            token, secret = auth_client.get_access_token(code)
        except DiscogsAPIError:
            raise beets.ui.UserError('Discogs authorization failed')

        # Save the token for later use.
        self._log.debug('Discogs token {0}, secret {1}', token, secret)
        with open(self._tokenfile(), 'w') as f:
            json.dump({'token': token, 'secret': secret}, f)

        return token, secret","1. Use HTTPS instead of HTTP to protect the communication between the client and the server.
2. Use a secure secret for the client to authenticate with the server.
3. Store the token and secret in a secure location, such as a file system with restricted access."
"    def candidates(self, items, artist, album, va_likely):
        """"""Returns a list of AlbumInfo objects for discogs search results
        matching an album and artist (if not various).
        """"""
        if not self.discogs_client:
            return

        if va_likely:
            query = album
        else:
            query = '%s %s' % (artist, album)
        try:
            return self.get_albums(query)
        except DiscogsAPIError as e:
            self._log.debug(u'API Error: {0} (query: {1})', e, query)
            return []
        except ConnectionError as e:
            self._log.debug(u'HTTP Connection Error: {0}', e)
            return []","1. Use `try/except` blocks to handle errors.
2. Use `logging` to log errors and debug messages.
3. Use `discogs_client.get_albums()` to get album information."
"    def commands(self):
        def scrub_func(lib, opts, args):
            # This is a little bit hacky, but we set a global flag to
            # avoid autoscrubbing when we're also explicitly scrubbing.
            global scrubbing
            scrubbing = True

            # Walk through matching files and remove tags.
            for item in lib.items(ui.decargs(args)):
                self._log.info(u'scrubbing: {0}',
                               util.displayable_path(item.path))

                # Get album art if we need to restore it.
                if opts.write:
                    mf = mediafile.MediaFile(item.path,
                                             config['id3v23'].get(bool))
                    art = mf.art

                # Remove all tags.
                self._scrub(item.path)

                # Restore tags, if enabled.
                if opts.write:
                    self._log.debug(u'writing new tags after scrub')
                    item.try_write()
                    if art:
                        self._log.info(u'restoring art')
                        mf = mediafile.MediaFile(item.path)
                        mf.art = art
                        mf.save()

            scrubbing = False

        scrub_cmd = ui.Subcommand('scrub', help='clean audio tags')
        scrub_cmd.parser.add_option('-W', '--nowrite', dest='write',
                                    action='store_false', default=True,
                                    help='leave tags empty')
        scrub_cmd.func = scrub_func

        return [scrub_cmd]","1. Use `functools.lru_cache` to cache the results of `_scrub` function.
2. Use `pathlib.Path` to handle file paths instead of strings.
3. Use `contextlib.closing` to ensure that the file is closed after it is used."
"        def scrub_func(lib, opts, args):
            # This is a little bit hacky, but we set a global flag to
            # avoid autoscrubbing when we're also explicitly scrubbing.
            global scrubbing
            scrubbing = True

            # Walk through matching files and remove tags.
            for item in lib.items(ui.decargs(args)):
                self._log.info(u'scrubbing: {0}',
                               util.displayable_path(item.path))

                # Get album art if we need to restore it.
                if opts.write:
                    mf = mediafile.MediaFile(item.path,
                                             config['id3v23'].get(bool))
                    art = mf.art

                # Remove all tags.
                self._scrub(item.path)

                # Restore tags, if enabled.
                if opts.write:
                    self._log.debug(u'writing new tags after scrub')
                    item.try_write()
                    if art:
                        self._log.info(u'restoring art')
                        mf = mediafile.MediaFile(item.path)
                        mf.art = art
                        mf.save()

            scrubbing = False","1. Use `functools.lru_cache` to cache the results of `_scrub` function.
2. Use `os.fchmod` to change the file mode of the scrubbed files to `0644`.
3. Use `pathlib.Path` to handle file paths instead of using `str`."
"    def commands(self):
        def scrub_func(lib, opts, args):
            # This is a little bit hacky, but we set a global flag to
            # avoid autoscrubbing when we're also explicitly scrubbing.
            global scrubbing
            scrubbing = True

            # Walk through matching files and remove tags.
            for item in lib.items(ui.decargs(args)):
                self._log.info(u'scrubbing: {0}',
                               util.displayable_path(item.path))

                # Get album art if we need to restore it.
                if opts.write:
                    try:
                        mf = mediafile.MediaFile(item.path,
                                                 config['id3v23'].get(bool))
                    except IOError as exc:
                        self._log.error(u'scrubbing failed: {0}', exc)
                    art = mf.art

                # Remove all tags.
                self._scrub(item.path)

                # Restore tags, if enabled.
                if opts.write:
                    self._log.debug(u'writing new tags after scrub')
                    item.try_write()
                    if art:
                        self._log.info(u'restoring art')
                        mf = mediafile.MediaFile(item.path)
                        mf.art = art
                        mf.save()

            scrubbing = False

        scrub_cmd = ui.Subcommand('scrub', help='clean audio tags')
        scrub_cmd.parser.add_option('-W', '--nowrite', dest='write',
                                    action='store_false', default=True,
                                    help='leave tags empty')
        scrub_cmd.func = scrub_func

        return [scrub_cmd]","1. Use `functools.lru_cache` to cache the results of `_scrub` to avoid repeated calls.
2. Use `pathlib.Path` to handle file paths more securely.
3. Use `contextlib.suppress` to suppress exceptions in `try` blocks."
"        def scrub_func(lib, opts, args):
            # This is a little bit hacky, but we set a global flag to
            # avoid autoscrubbing when we're also explicitly scrubbing.
            global scrubbing
            scrubbing = True

            # Walk through matching files and remove tags.
            for item in lib.items(ui.decargs(args)):
                self._log.info(u'scrubbing: {0}',
                               util.displayable_path(item.path))

                # Get album art if we need to restore it.
                if opts.write:
                    try:
                        mf = mediafile.MediaFile(item.path,
                                                 config['id3v23'].get(bool))
                    except IOError as exc:
                        self._log.error(u'scrubbing failed: {0}', exc)
                    art = mf.art

                # Remove all tags.
                self._scrub(item.path)

                # Restore tags, if enabled.
                if opts.write:
                    self._log.debug(u'writing new tags after scrub')
                    item.try_write()
                    if art:
                        self._log.info(u'restoring art')
                        mf = mediafile.MediaFile(item.path)
                        mf.art = art
                        mf.save()

            scrubbing = False","1. Use `functools.lru_cache` to cache the results of `util.displayable_path` to avoid repeated expensive operations.
2. Use `pathlib.Path` to handle file paths instead of strings to avoid injection attacks.
3. Use `contextlib.suppress` to suppress exceptions in `try` blocks to avoid leaking information to attackers."
"def parse_query_string(s, model_cls):
    """"""Given a beets query string, return the `Query` and `Sort` they
    represent.

    The string is split into components using shell-like syntax.
    """"""
    # A bug in Python < 2.7.3 prevents correct shlex splitting of
    # Unicode strings.
    # http://bugs.python.org/issue6988
    if isinstance(s, unicode):
        s = s.encode('utf8')
    parts = [p.decode('utf8') for p in shlex.split(s)]
    return parse_query_parts(parts, model_cls)","1. Use `urllib.parse.unquote()` to decode the query string before splitting it.
2. Use `shlex.quote()` to quote the query string before passing it to `shlex.split()`.
3. Use `six.ensure_str()` to ensure that the query string is a string."
"def is_lyrics(text, artist=None):
    """"""Determine whether the text seems to be valid lyrics.
    """"""
    if not text:
        return
    badTriggersOcc = []
    nbLines = text.count('\\n')
    if nbLines <= 1:
        log.debug(u""Ignoring too short lyrics '{0}'"".format(
                  text.decode('utf8')))
        return 0
    elif nbLines < 5:
        badTriggersOcc.append('too_short')
    else:
        # Lyrics look legit, remove credits to avoid being penalized further
        # down
        text = remove_credits(text)

    badTriggers = ['lyrics', 'copyright', 'property', 'links']
    if artist:
        badTriggersOcc += [artist]

    for item in badTriggers:
        badTriggersOcc += [item] * len(re.findall(r'\\W%s\\W' % item,
                                                  text, re.I))

    if badTriggersOcc:
        log.debug(u'Bad triggers detected: {0}'.format(badTriggersOcc))

    return len(badTriggersOcc) < 2","1. Use `input()` instead of `raw_input()` to prevent injection attacks.
2. Use `re.IGNORECASE` to make the regular expression case-insensitive.
3. Use `re.DOTALL` to match newline characters."
"def scrape_lyrics_from_html(html):
    """"""Scrape lyrics from a URL. If no lyrics can be found, return None
    instead.
    """"""
    from bs4 import SoupStrainer, BeautifulSoup

    if not html:
        return None

    def is_text_notcode(text):
        length = len(text)
        return (length > 20 and
                text.count(' ') > length / 25 and
                (text.find('{') == -1 or text.find(';') == -1))
    html = _scrape_strip_cruft(html)
    html = _scrape_merge_paragraphs(html)

    # extract all long text blocks that are not code
    try:
        soup = BeautifulSoup(html, ""html.parser"",
                             parse_only=SoupStrainer(text=is_text_notcode))
    except HTMLParseError:
        return None

    soup = sorted(soup.stripped_strings, key=len)[-1]

    return soup","1. Use `sanitizer.Whitelist` to filter out malicious code.
2. Use `BeautifulSoup.find_all` to find all text blocks that are not code.
3. Use `sorted` to sort the text blocks by length and return the longest one."
"    def get_lyrics(self, artist, title):
        """"""Fetch lyrics, trying each source in turn. Return a string or
        None if no lyrics were found.
        """"""
        for backend in self.backends:
            lyrics = backend(artist, title)
            if lyrics:
                if isinstance(lyrics, str):
                    lyrics = lyrics.decode('utf8', 'ignore')
                log.debug(u'got lyrics from backend: {0}'
                          .format(backend.__name__))
                return lyrics.strip()","1. Use `requests.get()` with `verify=False` to avoid certificate validation errors.
2. Use `requests.get()` with `allow_redirects=False` to prevent the user from being redirected to a malicious website.
3. Use `requests.get()` with `timeout=5` to prevent the user from being stuck waiting for a response."
"def fetch_url(url):
    """"""Retrieve the content at a given URL, or return None if the source
    is unreachable.
    """"""
    try:
        return urllib.urlopen(url).read()
    except IOError as exc:
        log.debug(u'failed to fetch: {0} ({1})'.format(url, unicode(exc)))
        return None","1. Use `urllib.request` instead of `urllib.urlopen` to avoid using `file` descriptors directly.
2. Use `urllib.error.HTTPError` instead of `IOError` to distinguish between connection errors and HTTP errors.
3. Handle `URLError` and `HTTPError` appropriately, e.g. by logging them and returning `None`."
"def write_item_mtime(item, mtime):
    """"""Write the given mtime to an item's `mtime` field and to the mtime of the
    item's file.
    """"""
    if mtime is None:
        log.warn(u""No mtime to be preserved for item {0}""
                 .format(util.displayable_path(item.path)))
        return

    # The file's mtime on disk must be in sync with the item's mtime
    write_file_mtime(util.syspath(item.path), mtime)
    item.mtime = mtime","1. Use `os.fchmod` to set the file mode instead of `os.chmod`.
2. Use `os.fchown` to set the file owner and group instead of `os.chown`.
3. Use `os.fsync` to flush the file data to disk instead of `os.sync`."
"def record_import_mtime(item, source, destination):
    """"""Record the file mtime of an item's path before import.
    """"""
    if (source == destination):
        # Re-import of an existing library item?
        return

    mtime = os.stat(util.syspath(source)).st_mtime
    item_mtime[destination] = mtime
    log.debug(u""Recorded mtime {0} for item '{1}' imported from '{2}'"".format(
        mtime, util.displayable_path(destination),
        util.displayable_path(source)))","1. Use `os.path.realpath` instead of `os.stat` to get the canonical path of the file, which is more secure against symlink attacks.
2. Use `os.access` to check if the user has permission to read the file, instead of relying on the file's existence.
3. Use `logging.warning` instead of `logging.debug` to log the file's mtime, which will make the logs more readable."
"def update_album_times(lib, album):
    album_mtimes = []
    for item in album.items():
        mtime = item_mtime[item.path]
        if mtime is not None:
            album_mtimes.append(mtime)
            if config['importadded']['preserve_mtimes'].get(bool):
                write_item_mtime(item, mtime)
                item.store()
            del item_mtime[item.path]

    album.added = min(album_mtimes)
    album.store()","1. Use `functools.lru_cache` to cache the item mtime.
2. Use `os.chmod` to set the file mode to 644 for the item file.
3. Use `os.chown` to set the file owner and group to the current user and group."
"def update_item_times(lib, item):
    mtime = item_mtime[item.path]
    if mtime is not None:
        item.added = mtime
        if config['importadded']['preserve_mtimes'].get(bool):
            write_item_mtime(item, mtime)
        item.store()
        del item_mtime[item.path]","1. Use `os.path.getmtime()` to get the file modification time instead of a global variable.
2. Use `item.set_mtime()` to set the file modification time instead of writing it to the file directly.
3. Use `item.delete()` to delete the item from the library instead of deleting the item from the global variable."
"    def convert(self, value, view):
        if isinstance(value, bytes):
            value = value.decode('utf8', 'ignore')

        if isinstance(value, STRING):
            if self.split:
                return value.split()
            else:
                return [value]
        else:
            try:
                value = list(value)
            except TypeError:
                self.fail('must be a whitespace-separated string or a list',
                          view, True)
            if all(isinstance(x, BASESTRING) for x in value):
                return value
            else:
                self.fail('must be a list of strings', view, True)","1. Use `str.encode()` to convert `bytes` to `str` before decoding it.
2. Use `list()` to convert the input value to a list.
3. Use `all()` to check if all elements in the list are strings."
"    def convert(self, value, view):
        if not isinstance(value, self.typ):
            self.fail(
                'must be a {0}, not {1}'.format(
                    self.typ.__name__,
                    type(value).__name__,
                ),
                view,
                True
            )
        return value","1. Use `type()` to check if the value is of the correct type.
2. Use `isinstance()` to check if the value is an instance of the correct class.
3. Use `fail()` to raise an error if the value is not of the correct type."
"def art_for_album(album, paths, maxwidth=None, local_only=False):
    """"""Given an Album object, returns a path to downloaded art for the
    album (or None if no art is found). If `maxwidth`, then images are
    resized to this maximum pixel size. If `local_only`, then only local
    image files from the filesystem are returned; no network requests
    are made.
    """"""
    out = None

    # Local art.
    cover_names = config['fetchart']['cover_names'].as_str_seq()
    cautious = config['fetchart']['cautious'].get(bool)
    if paths:
        for path in paths:
            out = art_in_path(path, cover_names, cautious)
            if out:
                break

    # Web art sources.
    remote_priority = config['fetchart']['remote_priority'].get(bool)
    if not local_only and (remote_priority or not out):
        for url in _source_urls(album):
            if maxwidth:
                url = ArtResizer.shared.proxy_url(maxwidth, url)
            candidate = _fetch_image(url)
            if candidate:
                out = candidate
                break

    if maxwidth and out:
        out = ArtResizer.shared.resize(maxwidth, out)
    return out","1. Use `pathlib.Path` instead of `os.path` to prevent directory traversal attacks.
2. Use `urllib.parse.quote` to escape special characters in URLs to prevent malicious redirects.
3. Use `requests.get` with a `verify` flag to verify the authenticity of the server certificate."
"def convert_item(dest_dir, keep_new, path_formats, command, ext,
                 pretend=False):
    while True:
        item = yield
        dest = item.destination(basedir=dest_dir, path_formats=path_formats)

        # When keeping the new file in the library, we first move the
        # current (pristine) file to the destination. We'll then copy it
        # back to its old path or transcode it to a new path.
        if keep_new:
            original = dest
            converted = replace_ext(item.path, ext)
        else:
            original = item.path
            dest = replace_ext(dest, ext)
            converted = dest

        # Ensure that only one thread tries to create directories at a
        # time. (The existence check is not atomic with the directory
        # creation inside this function.)
        if not pretend:
            with _fs_lock:
                util.mkdirall(dest)

        if os.path.exists(util.syspath(dest)):
            log.info(u'Skipping {0} (target file exists)'.format(
                util.displayable_path(item.path)
            ))
            continue

        if keep_new:
            if pretend:
                log.info(u'mv {0} {1}'.format(
                    util.displayable_path(item.path),
                    util.displayable_path(original),
                ))
            else:
                log.info(u'Moving to {0}'.format(
                    util.displayable_path(original))
                )
                util.move(item.path, original)

        if not should_transcode(item):
            if pretend:
                log.info(u'cp {0} {1}'.format(
                    util.displayable_path(original),
                    util.displayable_path(converted),
                ))
            else:
                # No transcoding necessary.
                log.info(u'Copying {0}'.format(
                    util.displayable_path(item.path))
                )
                util.copy(original, converted)
        else:
            try:
                encode(command, original, converted, pretend)
            except subprocess.CalledProcessError:
                continue

        if pretend:
            continue

        # Write tags from the database to the converted file.
        item.write(path=converted)

        if keep_new:
            # If we're keeping the transcoded file, read it again (after
            # writing) to get new bitrate, duration, etc.
            item.path = converted
            item.read()
            item.store()  # Store new path and audio data.

        if config['convert']['embed']:
            album = item.get_album()
            if album and album.artpath:
                embed_item(item, album.artpath, itempath=converted)

        plugins.send('after_convert', item=item, dest=dest, keepnew=keep_new)","1. Use `os.path.exists()` to check if the file exists before trying to move or copy it.
2. Use `util.mkdirall()` to create directories if they don't exist, instead of doing it manually.
3. Use `subprocess.check_call()` to run the transcoding command, and catch any errors that occur."
"def _build_m3u_filename(basename):
    """"""Builds unique m3u filename by appending given basename to current
    date.""""""

    basename = re.sub(r""[\\s,'\\""]"", '_', basename)
    date = datetime.datetime.now().strftime(""%Y%m%d_%Hh%M"")
    path = normpath(os.path.join(
        config['importfeeds']['dir'].as_filename(),
        date + '_' + basename + '.m3u'
    ))
    return path","1. Use `pathlib.Path` instead of `os.path` to build the filename. This will help to prevent path traversal attacks.
2. Use `datetime.datetime.utcnow()` instead of `datetime.datetime.now()` to get the current date. This will help to prevent attacks that rely on the local system's time being set incorrectly.
3. Use `os.makedirs` to create the directory for the m3u file, rather than trying to create it yourself. This will help to prevent race conditions and other errors."
"    def save_history(self):
        """"""Save the directory in the history for incremental imports.
        """"""
        if self.is_album and not self.sentinel:
            history_add(self.paths)","1. Use `os.path.join()` to concatenate paths instead of string concatenation.
2. Use `os.makedirs()` to create directories instead of `os.mkdir()`.
3. Use `os.chmod()` to set the permissions of directories and files."
"    def do_i_hate_this(cls, task, action_patterns):
        """"""Process group of patterns (warn or skip) and returns True if
        task is hated and not whitelisted.
        """"""
        if action_patterns:
            for query_string in action_patterns:
                query = None
                if task.is_album:
                    query = get_query(query_string, Album)
                else:
                    query = get_query(query_string, Item)
                if any(query.match(item) for item in task.items):
                    return True
        return False","1. Use `whitelist=True` to avoid matching items that are not intended to be matched.
2. Use `get_query(query_string, Item)` to ensure that the query is specific to the type of item being matched.
3. Use `any(query.match(item) for item in task.items)` to check if any of the items in the task match the query."
"    def import_task_choice_event(self, session, task):
        skip_queries = self.config['skip'].as_str_seq()
        warn_queries = self.config['warn'].as_str_seq()

        if task.choice_flag == action.APPLY:
            if skip_queries or warn_queries:
                self._log.debug('[ihate] processing your hate')
                if self.do_i_hate_this(task, skip_queries):
                    task.choice_flag = action.SKIP
                    self._log.info(u'[ihate] skipped: {0} - {1}'
                                   .format(task.cur_artist, task.cur_album))
                    return
                if self.do_i_hate_this(task, warn_queries):
                    self._log.info(u'[ihate] you maybe hate this: {0} - {1}'
                                   .format(task.cur_artist, task.cur_album))
            else:
                self._log.debug('[ihate] nothing to do')
        else:
            self._log.debug('[ihate] user made a decision, nothing to do')","1. Use `validate_choice_flag()` to check if the task choice flag is valid.
2. Use `validate_queries()` to check if the skip and warn queries are valid.
3. Use `do_i_hate_this()` to check if the task should be skipped or warned."
"    def do_i_hate_this(cls, task, genre_patterns, artist_patterns, 
                       album_patterns, whitelist_patterns):
        """"""Process group of patterns (warn or skip) and returns True if
        task is hated and not whitelisted.
        """"""
        hate = False
        try:
            genre = task.items[0].genre
        except:
            genre = u''
        if genre and genre_patterns:
            if cls.match_patterns(genre, genre_patterns):
                hate = True
        if not hate and task.cur_album and album_patterns:
            if cls.match_patterns(task.cur_album, album_patterns):
                hate = True
        if not hate and task.cur_artist and artist_patterns:
            if cls.match_patterns(task.cur_artist, artist_patterns):
                hate = True
        if hate and whitelist_patterns:
            if cls.match_patterns(task.cur_artist, whitelist_patterns):
                hate = False
        return hate","1. Use `whitelist_patterns` to check if the task should be skipped instead of checking `hate`.
2. Use `try` and `except` to catch exceptions when accessing `task.items[0].genre`.
3. Use `str.strip()` to remove whitespaces from `genre` before checking if it matches `genre_patterns`."
"def batch_fetch_art(lib, albums, force, maxwidth=None):
    """"""Fetch album art for each of the albums. This implements the manual
    fetchart CLI command.
    """"""
    for album in albums:
        if album.artpath and not force:
            message = 'has album art'
        else:
            # In ordinary invocations, look for images on the
            # filesystem. When forcing, however, always go to the Web
            # sources.
            local_paths = None if force else [album.path]

            path = art_for_album(album, local_paths, maxwidth)
            if path:
                album.set_art(path, False)
                album.store()
                message = 'found album art'
                if config['color']:
                    message = ui.colorize('red', message)
            else:
                message = 'no art found'
                if config['color']:
                    message = ui.colorize('turquoise', message)

        log.info(u'{0} - {1}: {2}'.format(album.albumartist, album.album,
                                          message))","1. Use `pathlib` instead of `os.path` to prevent directory traversal attacks.
2. Use `functools.lru_cache` to cache the results of `art_for_album` to prevent repeated requests.
3. Validate the input of `art_for_album` to prevent malicious users from injecting harmful code."
"def dump(mydb, f, **options):
    # type: (canmatrix.CanMatrix, typing.IO, **typing.Any) -> None
    # create copy because export changes database
    db = copy.deepcopy(mydb)
    dbf_export_encoding = options.get(""dbfExportEncoding"", 'iso-8859-1')
    ignore_encoding_errors = options.get(""ignoreExportEncodingErrors"", """")
    db.enum_attribs_to_keys()
    if len(db.signals) > 0:
        free_signals_dummy_frame = canmatrix.Frame(""VECTOR__INDEPENDENT_SIG_MSG"")
        free_signals_dummy_frame.arbitration_id = canmatrix.ArbitrationId(id=0x40000000, extended=True)
        free_signals_dummy_frame.signals = db.signals
        db.add_frame(free_signals_dummy_frame)

    out_str = """"""//******************************BUSMASTER Messages and signals Database ******************************//

[DATABASE_VERSION] 1.3

[PROTOCOL] CAN

[BUSMASTER_VERSION] [1.7.2]
[NUMBER_OF_MESSAGES] """"""

    out_str += str(len(db.frames)) + ""\\n""

    if max([x.cycle_time for x in db.frames]) > 0:
        db.add_frame_defines(""GenMsgCycleTime"", 'INT 0 65535')
    if max([x.cycle_time for y in db.frames for x in y.signals]) > 0:
        db.add_signal_defines(""GenSigCycleTime"", 'INT 0 65535')

    if max([x.initial_value for y in db.frames for x in y.signals]) > 0 or min([x.initial_value for y in db.frames for x in y.signals]) < 0:
        db.add_signal_defines(""GenSigStartValue"", 'FLOAT 0 100000000000')

    # Frames
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            logger.error(""export complex multiplexers is not supported - ignoring frame "" + frame.name)
            continue

        # Name unMsgId m_ucLength m_ucNumOfSignals m_cDataFormat m_cFrameFormat? m_txNode
        # m_cDataFormat Data format: 1-Intel, 0-Motorola -- always 1 original converter decides based on signal count.
        # cFrameFormat Standard 'S' Extended 'X'
        extended = 'X' if frame.arbitration_id.extended == 1 else 'S'
        out_str += ""[START_MSG] "" + frame.name + \\
            "",%d,%d,%d,1,%c,"" % (frame.arbitration_id.id, frame.size, len(frame.signals), extended)
        if not frame.transmitters:
            frame.add_transmitter(""Vector__XXX"")
# DBF does not support multiple Transmitters
        out_str += frame.transmitters[0] + ""\\n""

        for signal in frame.signals:
            # m_acName ucLength m_ucWhichByte m_ucStartBit
            # m_ucDataFormat m_fOffset m_fScaleFactor m_acUnit m_acMultiplex m_rxNode
            # m_ucDataFormat
            which_byte = int(
                math.floor(
                    signal.get_startbit(
                        bit_numbering=1,
                        start_little=True) /
                    8) +
                1)
            sign = 'I'

            if not signal.is_signed:
                sign = 'U'
            
            if signal.is_float:
                if signal.size > 32:
                    sign = 'D'
                else:
                    sign = 'F'

            if signal.factor == 0:
                signal.factor = 1

            out_str += ""[START_SIGNALS] "" + signal.name \\
                       + "",%d,%d,%d,%c,"" % (signal.size,
                                            which_byte,
                                            int(signal.get_startbit(bit_numbering=1,
                                                                    start_little=True)) % 8,
                                            sign) + '{},{}'.format(float(signal.max) / float(signal.factor),
                                                                   float(signal.min) / float(signal.factor))

            out_str += "",%d,%s,%s"" % (signal.is_little_endian, signal.offset, signal.factor)
            multiplex = """"
            if signal.multiplex is not None:
                if signal.multiplex == 'Multiplexor':
                    multiplex = 'M'
                else:
                    multiplex = 'm' + str(signal.multiplex)

            out_str += "","" + signal.unit + "",%s,"" % multiplex + \\
                ','.join(signal.receivers) + ""\\n""

            if len(signal.values) > 0:
                for value, name in sorted(list(signal.values.items())):
                    out_str += '[VALUE_DESCRIPTION] ""' + \\
                        name + '"",' + str(value) + '\\n'

        out_str += ""[END_MSG]\\n\\n""

    # Board units
    out_str += ""[NODE] ""
    count = 1
    for ecu in db.ecus:
        out_str += ecu.name
        if count < len(db.ecus):
            out_str += "",""
        count += 1
    out_str += ""\\n""

    out_str += ""[START_DESC]\\n\\n""

    # BU-descriptions
    out_str += ""[START_DESC_MSG]\\n""
    for frame in db.frames:
        if frame.comment is not None:
            comment = frame.comment.replace(""\\n"", "" "")
            out_str += str(frame.arbitration_id.id) + ' S ""' + comment + '"";\\n'

    out_str += ""[END_DESC_MSG]\\n""

    # Frame descriptions
    out_str += ""[START_DESC_NODE]\\n""
    for ecu in db.ecus:
        if ecu.comment is not None:
            comment = ecu.comment.replace(""\\n"", "" "")
            out_str += ecu.name + ' ""' + comment + '"";\\n'

    out_str += ""[END_DESC_NODE]\\n""

    # signal descriptions
    out_str += ""[START_DESC_SIG]\\n""
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            continue

        for signal in frame.signals:
            if signal.comment is not None:
                comment = signal.comment.replace(""\\n"", "" "")
                out_str += ""%d S "" % frame.arbitration_id.id + signal.name + ' ""' + comment + '"";\\n'

    out_str += ""[END_DESC_SIG]\\n""
    out_str += ""[END_DESC]\\n\\n""

    out_str += ""[START_PARAM]\\n""

    # db-parameter
    out_str += ""[START_PARAM_NET]\\n""
    for (data_type, define) in sorted(list(db.global_defines.items())):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",' + define.definition.replace(' ', ',') + ',' + default_val + '\\n'
    out_str += ""[END_PARAM_NET]\\n""

    # bu-parameter
    out_str += ""[START_PARAM_NODE]\\n""
    for (data_type, define) in sorted(list(db.ecu_defines.items())):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",' + define.definition.replace(' ', ',') + ',' + default_val + '\\n'
    out_str += ""[END_PARAM_NODE]\\n""

    # frame-parameter
    out_str += ""[START_PARAM_MSG]\\n""
    for (data_type, define) in sorted(list(db.frame_defines.items())):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",'  + define.definition.replace(' ', ',') + '\\n'  # + ',' + default_val + '\\n'

    out_str += ""[END_PARAM_MSG]\\n""

    # signal-parameter
    out_str += ""[START_PARAM_SIG]\\n""
    for (data_type, define) in list(db.signal_defines.items()):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",' + define.definition.replace(' ', ',') + ',' + default_val + '\\n'
    out_str += ""[END_PARAM_SIG]\\n""

    out_str += ""[START_PARAM_VAL]\\n""
    # board unit attributes:
    out_str += ""[START_PARAM_NODE_VAL]\\n""
    for ecu in db.ecus:
        for attrib, val in sorted(list(ecu.attributes.items())):
            out_str += ecu.name + ',""' + attrib + '"",""' + val + '""\\n'
    out_str += ""[END_PARAM_NODE_VAL]\\n""

    # messages-attributes:
    out_str += ""[START_PARAM_MSG_VAL]\\n""
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            continue

        for attrib, val in sorted(list(frame.attributes.items())):
            out_str += str(frame.arbitration_id.id) + ',S,""' + attrib + '"",""' + val + '""\\n'
    out_str += ""[END_PARAM_MSG_VAL]\\n""

    # signal-attributes:
    out_str += ""[START_PARAM_SIG_VAL]\\n""
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            continue

        for signal in frame.signals:
            for attrib, val in sorted(list(signal.attributes.items())):
                out_str += str(frame.arbitration_id.id) + ',S,' + signal.name + \\
                    ',""' + attrib + '"",""' + val + '""\\n'
    out_str += ""[END_PARAM_SIG_VAL]\\n""
    out_str += ""[END_PARAM_VAL]\\n""
    f.write(out_str.encode(dbf_export_encoding, ignore_encoding_errors))","1. Use an explicit `encoding` argument to `f.write()` to avoid potential encoding errors.
2. Use `str.replace()` to escape special characters in the output string.
3. Use `ord()` and `chr()` to convert between character codes and strings."
"def dump(db, f, **options):  # type: (canmatrix.CanMatrix, typing.IO, **typing.Any) -> None
    """"""
    export canmatrix-object as .sym file (compatible to PEAK-Systems)
    """"""
    sym_encoding = options.get('symExportEncoding', 'iso-8859-1')
    ignore_encoding_errors = options.get(""ignoreExportEncodingErrors"", """")

    enum_dict = {}
    for enum_name, enum_values in db.value_tables.items():
        enum_dict[enum_name] = ""enum {}({})"".format(enum_name, ', '.join('{}=""{}""'.format(*items) for items in sorted(enum_values.items())))
    enums = ""{ENUMS}\\n""

    header = """"""\\
FormatVersion=5.0 // Do not edit this line!
Title=\\""{}\\""
"""""".format(db.attribute(""Title"", ""canmatrix-Export""))
    f.write(header.encode(sym_encoding, ignore_encoding_errors))

    def send_receive(for_frame):
        return (
            for_frame.attributes.get('Sendable', 'True') == 'True',
            for_frame.attributes.get('Receivable', 'True') == 'True',
        )

    sections = collections.OrderedDict((
        ('SEND', tuple(f for f in db.frames if send_receive(f) == (True, False))),
        ('RECEIVE', tuple(f for f in db.frames if send_receive(f) == (False, True))),
        ('SENDRECEIVE', tuple(f for f in db.frames if send_receive(f) == (True, True))),
    ))

    output = '\\n'

    for name, frames in sections.items():
        if len(frames) == 0:
            continue

        # Frames
        output += ""{{{}}}\\n\\n"".format(name)

        # trigger all frames
        for frame in frames:
            name = ""["" + frame.name + ""]\\n""

            if frame.arbitration_id.extended == 1:
                id_type = ""ID=%08Xh"" % frame.arbitration_id.id
            else:
                id_type = ""ID=%03Xh"" % frame.arbitration_id.id
            if frame.comment is not None and len(frame.comment) > 0:
                id_type += ""\\t// "" + \\
                    frame.comment.replace('\\n', ' ').replace('\\r', ' ')
            id_type += ""\\n""
            if frame.arbitration_id.extended == 1:
                id_type += ""Type=Extended\\n""
            else:
                id_type += ""Type=Standard\\n""

            # check if frame has multiplexed signals
            multiplex = 0
            for signal in frame.signals:
                if signal.multiplex is not None:
                    multiplex = 1

            if multiplex == 1:
                # search for multiplexor in frame:
                for signal in frame.signals:
                    if signal.multiplex == 'Multiplexor':
                        mux_signal = signal

                # ticker all possible mux-groups as i (0 - 2^ (number of bits of multiplexor))
                first = 0
                for i in range(0, 1 << int(mux_signal.size)):
                    found = 0
                    mux_out = """"
                    # ticker all signals
                    for signal in frame.signals:
                        # if signal is in mux-group i
                        if signal.multiplex == i:
                            mux_out = name
                            if first == 0:
                                mux_out += id_type
                                first = 1
                            mux_out += ""DLC=%d\\n"" % frame.size
                            if frame.cycle_time != 0:
                                mux_out += ""CycleTime="" + str(frame.effective_cycle_time) + ""\\n""

                            mux_name = frame.mux_names.get(i, mux_signal.name + ""%d"" % i)

                            mux_out += ""Mux="" + mux_name
                            start_bit = mux_signal.get_startbit()
                            s = str(i)
                            if len(s) > 1:
                                length = len(
                                    '{:X}'.format(int(mux_signal.calc_max()))
                                )
                                s = '{:0{}X}h'.format(i, length)
                            if not signal.is_little_endian:
                                # Motorola
                                mux_out += "" %d,%d %s -m"" % (start_bit, mux_signal.size, s)
                            else:
                                mux_out += "" %d,%d %s"" % (start_bit, mux_signal.size, s)
                            if not mux_out.endswith('h'):
                                mux_out += ' '
                            if i in mux_signal.comments:
                                comment = mux_signal.comments.get(i)
                                if len(comment) > 0:
                                    mux_out += '\\t// ' + comment
                            mux_out += ""\\n""
                            found = 1
                            break

                    if found == 1:
                        for signal in frame.signals:
                            if signal.multiplex == i or signal.multiplex is None:
                                mux_out += create_signal(db, signal)
                                enum_dict.update(create_enum_from_signal_values(signal))
                        output += mux_out + ""\\n""

            else:
                # no multiplex signals in frame, just 'normal' signals
                output += name
                output += id_type
                output += ""DLC=%d\\n"" % frame.size
                if frame.cycle_time != 0:
                    output += ""CycleTime="" + str(frame.effective_cycle_time) + ""\\n""
                for signal in frame.signals:
                    output += create_signal(db, signal)
                    enum_dict.update(create_enum_from_signal_values(signal))
                output += ""\\n""
    enums += '\\n'.join(sorted(enum_dict.values()))
    # write output file
    f.write((enums + '\\n').encode(sym_encoding, ignore_encoding_errors))
    f.write(output.encode(sym_encoding, ignore_encoding_errors))","1. Use `f.write(str.encode(data, encoding))` instead of `f.write(data.encode(encoding))` to avoid data truncation.
2. Use `f.close()` to close the file after writing.
3. Use `os.path.join()` to concatenate paths instead of `+` operator."
"def cli_convert(infile, outfile, silent, verbosity, **options):
    """"""
    canmatrix.cli.convert [options] import-file export-file

    import-file: *.dbc|*.dbf|*.kcd|*.arxml|*.json|*.xls(x)|*.sym
    export-file: *.dbc|*.dbf|*.kcd|*.arxml|*.json|*.xls(x)|*.sym|*.py

    \\n""""""

    root_logger = canmatrix.log.setup_logger()

    if silent is True:
        # only print error messages, ignore verbosity flag
        verbosity = -1
        options[""silent""] = True

    canmatrix.log.set_log_level(root_logger, verbosity)
    if options[""ignoreEncodingErrors""]:
        options[""ignoreEncodingErrors""] = ""ignore""
    else:
        options[""ignoreEncodingErrors""] = """"

    canmatrix.convert.convert(infile, outfile, **options)
    return 0","1. Use `argparse` instead of `optparse` to parse command-line arguments.
2. Validate the input files to prevent buffer overflows and other attacks.
3. Use proper error handling to catch and report errors."
"def decode_can_helper(ea, float_factory, ignore_cluster_info):
    found_matrixes = {}
    if ignore_cluster_info is True:
        ccs = [lxml.etree.Element(""ignoreClusterInfo"")]  # type: typing.Sequence[_Element]
    else:
        ccs = ea.findall('CAN-CLUSTER')
    for cc in ccs:  # type: _Element
        db = canmatrix.CanMatrix()
        # Defines not jet imported...
        db.add_ecu_defines(""NWM-Stationsadresse"", 'HEX 0 63')
        db.add_ecu_defines(""NWM-Knoten"", 'ENUM  ""nein"",""ja""')
        db.add_signal_defines(""LongName"", 'STRING')
        db.add_frame_defines(""GenMsgDelayTime"", 'INT 0 65535')
        db.add_frame_defines(""GenMsgNrOfRepetitions"", 'INT 0 65535')
        db.add_frame_defines(""GenMsgStartValue"", 'STRING')
        db.add_frame_defines(""GenMsgStartDelayTime"", 'INT 0 65535')
        db.add_frame_defines(
            ""GenMsgSendType"",
            'ENUM  ""cyclicX"",""spontanX"",""cyclicIfActiveX"",""spontanWithDelay"",""cyclicAndSpontanX"",""cyclicAndSpontanWithDelay"",""spontanWithRepitition"",""cyclicIfActiveAndSpontanWD"",""cyclicIfActiveFast"",""cyclicWithRepeatOnDemand"",""none""')

        if ignore_cluster_info is True:
            can_frame_trig = ea.findall('CAN-FRAME-TRIGGERING')
            bus_name = """"
        else:
            speed = ea.get_child(cc, ""SPEED"")
            baudrate_elem = ea.find(""BAUDRATE"", cc)
            fd_baudrate_elem = ea.find(""CAN-FD-BAUDRATE"", cc)

            logger.debug(""Busname: "" + ea.get_element_name(cc))

            bus_name = ea.get_element_name(cc)
            if speed is not None:
                db.baudrate = int(speed.text)
            elif baudrate_elem is not None:
                db.baudrate = int(baudrate_elem.text)

            logger.debug(""Baudrate: ""+ str(db.baudrate))
            if fd_baudrate_elem is not None:
                db.fd_baudrate = fd_baudrate_elem.text

        

            physical_channels = ea.find(""PHYSICAL-CHANNELS"", cc)  # type: _Element
            if physical_channels is None:
                logger.error(""PHYSICAL-CHANNELS not found"")

            nm_lower_id = ea.get_child(cc, ""NM-LOWER-CAN-ID"")

            physical_channel = ea.get_child(physical_channels, ""PHYSICAL-CHANNEL"")
            if physical_channel is None:
                physical_channel = ea.get_child(physical_channels, ""CAN-PHYSICAL-CHANNEL"")
            if physical_channel is None:
                logger.error(""PHYSICAL-CHANNEL not found"")
            can_frame_trig = ea.get_children(physical_channel, ""CAN-FRAME-TRIGGERING"")
            if can_frame_trig is None:
                logger.error(""CAN-FRAME-TRIGGERING not found"")
            else:
                logger.debug(""%d frames found in arxml"", len(can_frame_trig))

        multiplex_translation = {}  # type: typing.Dict[str, str]
        for frameTrig in can_frame_trig:  # type: _Element
            frame = get_frame(frameTrig, ea, multiplex_translation, float_factory)
            if frame is not None:
                db.add_frame(frame)

        if ignore_cluster_info is True:
            pass
            # no support for signal direction
        else:  # find signal senders/receivers...
            isignal_triggerings = ea.find_children_by_path(physical_channel, ""I-SIGNAL-TRIGGERING"")
            for sig_trig in isignal_triggerings:
                isignal = ea.follow_ref(sig_trig, 'SIGNAL-REF')
                if isignal is None:
                    isignal = ea.follow_ref(sig_trig, 'I-SIGNAL-REF')
                if isignal is None:
                    sig_trig_text = ea.get_element_name(sig_trig) if sig_trig is not None else ""None""
                    logger.debug(""load: no isignal for %s"", sig_trig_text)
                    continue

                port_ref = ea.follow_all_ref(sig_trig, ""I-SIGNAL-PORT-REF"")

                for port in port_ref:
                    comm_direction = ea.get_child(port, ""COMMUNICATION-DIRECTION"")
                    if comm_direction is not None and comm_direction.text == ""IN"":
                        sys_signal = ea.follow_ref(isignal, ""SYSTEM-SIGNAL-REF"")
                        ecu_name = ea.get_element_name(port.getparent().getparent().getparent().getparent())
                        # port points in ECU; probably more stable to go up
                        # from each ECU than to go down in XML...
                        if sys_signal in signal_rxs:
                            signal_rxs[sys_signal].add_receiver(ecu_name)
                            # find ECUs:
        nodes = ea.findall('ECU-INSTANCE')
        for node in nodes:  # type: _Element
            ecu = process_ecu(node, db, ea, multiplex_translation)
            desc = ea.get_child(node, ""DESC"", )
            l2 = ea.get_child(desc, ""L-2"")
            if l2 is not None:
                ecu.add_comment(l2.text)

            db.add_ecu(ecu)


        for frame in db.frames:
            sig_value_hash = dict()
            for sig in frame.signals:
                try:
                    sig_value_hash[sig.name] = sig.phys2raw()
                except AttributeError:
                    sig_value_hash[sig.name] = 0
            frame_data = frame.encode(sig_value_hash)
            frame.add_attribute(""GenMsgStartValue"", """".join([""%02x"" % x for x in frame_data]))
        found_matrixes[bus_name] = db
    return found_matrixes","1. Use a secure random number generator to generate the salt.
2. Use a strong hashing algorithm, such as SHA-256 or SHA-512, to hash the password.
3. Store the hashed password in a secure location, such as a database or file system."
"def dump(mydb, f, **options):
    # type: (canmatrix.CanMatrix, typing.IO, **typing.Any) -> None
    # create copy because export changes database
    db = copy.deepcopy(mydb)
    dbf_export_encoding = options.get(""dbfExportEncoding"", 'iso-8859-1')
    ignore_encoding_errors = options.get(""ignoreEncodingErrors"", """")
    db.enum_attribs_to_keys()
    if len(db.signals) > 0:
        free_signals_dummy_frame = canmatrix.Frame(""VECTOR__INDEPENDENT_SIG_MSG"")
        free_signals_dummy_frame.arbitration_id = canmatrix.ArbitrationId(id=0x40000000, extended=True)
        free_signals_dummy_frame.signals = db.signals
        db.add_frame(free_signals_dummy_frame)

    out_str = """"""//******************************BUSMASTER Messages and signals Database ******************************//

[DATABASE_VERSION] 1.3

[PROTOCOL] CAN

[BUSMASTER_VERSION] [1.7.2]
[NUMBER_OF_MESSAGES] """"""

    out_str += str(len(db.frames)) + ""\\n""

    cycle_times_of_all_frames = [x.cycle_time for x in db.frames]
    if len(cycle_times_of_all_frames) > 0 and max(cycle_times_of_all_frames ) > 0:
        db.add_frame_defines(""GenMsgCycleTime"", 'INT 0 65535')

    cycle_times_of_all_singals = [x.cycle_time for y in db.frames for x in y.signals]
    if len(cycle_times_of_all_singals) > 0 and max(cycle_times_of_all_singals) > 0:
        db.add_signal_defines(""GenSigCycleTime"", 'INT 0 65535')

    initial_values_of_all_singals = [x.initial_value for y in db.frames for x in y.signals]
    if len(initial_values_of_all_singals) > 0 and (max(initial_values_of_all_singals) > 0 or min(initial_values_of_all_singals)) < 0:
        db.add_signal_defines(""GenSigStartValue"", 'FLOAT 0 100000000000')

    # Frames
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            logger.error(""export complex multiplexers is not supported - ignoring frame "" + frame.name)
            continue

        # Name unMsgId m_ucLength m_ucNumOfSignals m_cDataFormat m_cFrameFormat? m_txNode
        # m_cDataFormat Data format: 1-Intel, 0-Motorola -- always 1 original converter decides based on signal count.
        # cFrameFormat Standard 'S' Extended 'X'
        extended = 'X' if frame.arbitration_id.extended == 1 else 'S'
        out_str += ""[START_MSG] "" + frame.name + \\
            "",%d,%d,%d,1,%c,"" % (frame.arbitration_id.id, frame.size, len(frame.signals), extended)
        if not frame.transmitters:
            frame.add_transmitter(""Vector__XXX"")
# DBF does not support multiple Transmitters
        out_str += frame.transmitters[0] + ""\\n""

        for signal in frame.signals:
            # m_acName ucLength m_ucWhichByte m_ucStartBit
            # m_ucDataFormat m_fOffset m_fScaleFactor m_acUnit m_acMultiplex m_rxNode
            # m_ucDataFormat
            which_byte = int(
                math.floor(
                    signal.get_startbit(
                        bit_numbering=1,
                        start_little=True) /
                    8) +
                1)
            sign = 'I'

            if not signal.is_signed:
                sign = 'U'
            
            if signal.is_float:
                if signal.size > 32:
                    sign = 'D'
                else:
                    sign = 'F'

            if signal.factor == 0:
                signal.factor = 1

            out_str += ""[START_SIGNALS] "" + signal.name \\
                       + "",%d,%d,%d,%c,"" % (signal.size,
                                            which_byte,
                                            int(signal.get_startbit(bit_numbering=1,
                                                                    start_little=True)) % 8,
                                            sign) + '{},{}'.format(float(signal.max) / float(signal.factor),
                                                                   float(signal.min) / float(signal.factor))

            out_str += "",%d,%s,%s"" % (signal.is_little_endian, signal.offset, signal.factor)
            multiplex = """"
            if signal.multiplex is not None:
                if signal.multiplex == 'Multiplexor':
                    multiplex = 'M'
                else:
                    multiplex = 'm' + str(signal.multiplex)

            out_str += "","" + signal.unit + "",%s,"" % multiplex + \\
                ','.join(signal.receivers) + ""\\n""

            if len(signal.values) > 0:
                for value, name in sorted(list(signal.values.items())):
                    out_str += '[VALUE_DESCRIPTION] ""' + \\
                        name + '"",' + str(value) + '\\n'

        out_str += ""[END_MSG]\\n\\n""

    # Board units
    out_str += ""[NODE] ""
    count = 1
    for ecu in db.ecus:
        out_str += ecu.name
        if count < len(db.ecus):
            out_str += "",""
        count += 1
    out_str += ""\\n""

    out_str += ""[START_DESC]\\n\\n""

    # BU-descriptions
    out_str += ""[START_DESC_MSG]\\n""
    for frame in db.frames:
        if frame.comment is not None:
            comment = frame.comment.replace(""\\n"", "" "")
            out_str += str(frame.arbitration_id.id) + ' S ""' + comment + '"";\\n'

    out_str += ""[END_DESC_MSG]\\n""

    # Frame descriptions
    out_str += ""[START_DESC_NODE]\\n""
    for ecu in db.ecus:
        if ecu.comment is not None:
            comment = ecu.comment.replace(""\\n"", "" "")
            out_str += ecu.name + ' ""' + comment + '"";\\n'

    out_str += ""[END_DESC_NODE]\\n""

    # signal descriptions
    out_str += ""[START_DESC_SIG]\\n""
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            continue

        for signal in frame.signals:
            if signal.comment is not None:
                comment = signal.comment.replace(""\\n"", "" "")
                out_str += ""%d S "" % frame.arbitration_id.id + signal.name + ' ""' + comment + '"";\\n'

    out_str += ""[END_DESC_SIG]\\n""
    out_str += ""[END_DESC]\\n\\n""

    out_str += ""[START_PARAM]\\n""

    # db-parameter
    out_str += ""[START_PARAM_NET]\\n""
    for (data_type, define) in sorted(list(db.global_defines.items())):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",' + define.definition.replace(' ', ',') + ',' + default_val + '\\n'
    out_str += ""[END_PARAM_NET]\\n""

    # bu-parameter
    out_str += ""[START_PARAM_NODE]\\n""
    for (data_type, define) in sorted(list(db.ecu_defines.items())):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",' + define.definition.replace(' ', ',') + ',' + default_val + '\\n'
    out_str += ""[END_PARAM_NODE]\\n""

    # frame-parameter
    out_str += ""[START_PARAM_MSG]\\n""
    for (data_type, define) in sorted(list(db.frame_defines.items())):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",'  + define.definition.replace(' ', ',') + '\\n'  # + ',' + default_val + '\\n'

    out_str += ""[END_PARAM_MSG]\\n""

    # signal-parameter
    out_str += ""[START_PARAM_SIG]\\n""
    for (data_type, define) in list(db.signal_defines.items()):
        default_val = define.defaultValue
        if default_val is None:
            default_val = ""0""
        out_str += '""' + data_type + '"",' + define.definition.replace(' ', ',') + ',' + default_val + '\\n'
    out_str += ""[END_PARAM_SIG]\\n""

    out_str += ""[START_PARAM_VAL]\\n""
    # board unit attributes:
    out_str += ""[START_PARAM_NODE_VAL]\\n""
    for ecu in db.ecus:
        for attrib, val in sorted(list(ecu.attributes.items())):
            out_str += ecu.name + ',""' + attrib + '"",""' + val + '""\\n'
    out_str += ""[END_PARAM_NODE_VAL]\\n""

    # messages-attributes:
    out_str += ""[START_PARAM_MSG_VAL]\\n""
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            continue

        for attrib, val in sorted(list(frame.attributes.items())):
            out_str += str(frame.arbitration_id.id) + ',S,""' + attrib + '"",""' + val + '""\\n'
    out_str += ""[END_PARAM_MSG_VAL]\\n""

    # signal-attributes:
    out_str += ""[START_PARAM_SIG_VAL]\\n""
    for frame in db.frames:
        if frame.is_complex_multiplexed:
            continue

        for signal in frame.signals:
            for attrib, val in sorted(list(signal.attributes.items())):
                out_str += str(frame.arbitration_id.id) + ',S,' + signal.name + \\
                    ',""' + attrib + '"",""' + val + '""\\n'
    out_str += ""[END_PARAM_SIG_VAL]\\n""
    out_str += ""[END_PARAM_VAL]\\n""
    f.write(out_str.encode(dbf_export_encoding, ignore_encoding_errors))","1. Use `utf-8` encoding instead of `iso-8859-1`.
2. Handle encoding errors correctly.
3. Use a proper way to escape special characters."
"def dump(db, f, **options):  # type: (canmatrix.CanMatrix, typing.IO, **typing.Any) -> None
    """"""
    export canmatrix-object as .sym file (compatible to PEAK-Systems)
    """"""
    sym_encoding = options.get('symExportEncoding', 'iso-8859-1')
    ignore_encoding_errors = options.get(""ignoreEncodingErrors"", """")

    enum_dict = {}
    for enum_name, enum_values in db.value_tables.items():
        enum_dict[enum_name] = ""enum {}({})"".format(enum_name, ', '.join('{}=""{}""'.format(*items) for items in sorted(enum_values.items())))
    enums = ""{ENUMS}\\n""

    header = """"""\\
FormatVersion=5.0 // Do not edit this line!
Title=\\""{}\\""
"""""".format(db.attribute(""Title"", ""canmatrix-Export""))
    f.write(header.encode(sym_encoding, ignore_encoding_errors))

    def send_receive(for_frame):
        return (
            for_frame.attributes.get('Sendable', 'True') == 'True',
            for_frame.attributes.get('Receivable', 'True') == 'True',
        )

    sections = collections.OrderedDict((
        ('SEND', tuple(f for f in db.frames if send_receive(f) == (True, False))),
        ('RECEIVE', tuple(f for f in db.frames if send_receive(f) == (False, True))),
        ('SENDRECEIVE', tuple(f for f in db.frames if send_receive(f) == (True, True))),
    ))

    output = '\\n'

    for name, frames in sections.items():
        if len(frames) == 0:
            continue

        # Frames
        output += ""{{{}}}\\n\\n"".format(name)

        # trigger all frames
        for frame in frames:
            name = ""["" + frame.name + ""]\\n""

            if frame.arbitration_id.extended == 1:
                id_type = ""ID=%08Xh"" % frame.arbitration_id.id
            else:
                id_type = ""ID=%03Xh"" % frame.arbitration_id.id
            if frame.comment is not None and len(frame.comment) > 0:
                id_type += ""\\t// "" + \\
                    frame.comment.replace('\\n', ' ').replace('\\r', ' ')
            id_type += ""\\n""
            if frame.arbitration_id.extended == 1:
                id_type += ""Type=Extended\\n""
            else:
                id_type += ""Type=Standard\\n""

            # check if frame has multiplexed signals
            multiplex = 0
            for signal in frame.signals:
                if signal.multiplex is not None:
                    multiplex = 1

            if multiplex == 1:
                # search for multiplexor in frame:
                for signal in frame.signals:
                    if signal.multiplex == 'Multiplexor':
                        mux_signal = signal

                # ticker all possible mux-groups as i (0 - 2^ (number of bits of multiplexor))
                first = 0
                for i in range(0, 1 << int(mux_signal.size)):
                    found = 0
                    mux_out = """"
                    # ticker all signals
                    for signal in frame.signals:
                        # if signal is in mux-group i
                        if signal.multiplex == i:
                            mux_out = name
                            if first == 0:
                                mux_out += id_type
                                first = 1
                            mux_out += ""DLC=%d\\n"" % frame.size
                            if frame.cycle_time != 0:
                                mux_out += ""CycleTime="" + str(frame.effective_cycle_time) + ""\\n""

                            mux_name = frame.mux_names.get(i, mux_signal.name + ""%d"" % i)

                            mux_out += ""Mux="" + mux_name
                            start_bit = mux_signal.get_startbit()
                            s = str(i)
                            if len(s) > 1:
                                length = len(
                                    '{:X}'.format(int(mux_signal.calc_max()))
                                )
                                s = '{:0{}X}h'.format(i, length)
                            if not signal.is_little_endian:
                                # Motorola
                                mux_out += "" %d,%d %s -m"" % (start_bit, mux_signal.size, s)
                            else:
                                mux_out += "" %d,%d %s"" % (start_bit, mux_signal.size, s)
                            if not mux_out.endswith('h'):
                                mux_out += ' '
                            if i in mux_signal.comments:
                                comment = mux_signal.comments.get(i)
                                if len(comment) > 0:
                                    mux_out += '\\t// ' + comment
                            mux_out += ""\\n""
                            found = 1
                            break

                    if found == 1:
                        for signal in frame.signals:
                            if signal.multiplex == i or signal.multiplex is None:
                                mux_out += create_signal(db, signal)
                                enum_dict.update(create_enum_from_signal_values(signal))
                        output += mux_out + ""\\n""

            else:
                # no multiplex signals in frame, just 'normal' signals
                output += name
                output += id_type
                output += ""DLC=%d\\n"" % frame.size
                if frame.cycle_time != 0:
                    output += ""CycleTime="" + str(frame.effective_cycle_time) + ""\\n""
                for signal in frame.signals:
                    output += create_signal(db, signal)
                    enum_dict.update(create_enum_from_signal_values(signal))
                output += ""\\n""
    enums += '\\n'.join(sorted(enum_dict.values()))
    # write output file
    f.write((enums + '\\n').encode(sym_encoding, ignore_encoding_errors))
    f.write(output.encode(sym_encoding, ignore_encoding_errors))","1. Use `str.encode()` and `str.decode()` to convert between bytes and strings.
2. Use `ord()` and `chr()` to convert between integers and characters.
3. Use `base64.b64encode()` and `base64.b64decode()` to encode and decode data in Base64 format."
"def decode_compu_method(compu_method, ea, float_factory):
    # type: (_Element, _DocRoot, str, _FloatFactory) -> typing.Tuple
    values = {}
    factor = float_factory(1.0)
    offset = float_factory(0)
    unit = ea.follow_ref(compu_method, ""UNIT-REF"")
    const = None
    compu_scales = ea.find_children_by_path(compu_method, ""COMPU-INTERNAL-TO-PHYS/COMPU-SCALES/COMPU-SCALE"")
    for compu_scale in compu_scales:
        ll = ea.get_child(compu_scale, ""LOWER-LIMIT"")
        ul = ea.get_child(compu_scale, ""UPPER-LIMIT"")
        sl = ea.get_child(compu_scale, ""SHORT-LABEL"")
        if sl is None:
            desc = ea.get_element_desc(compu_scale)
        else:
            desc = sl.text
        #####################################################################################################
        # Modification to support sourcing the COMPU_METHOD info from the Vector NETWORK-REPRESENTATION-PROPS
        # keyword definition. 06Jun16
        #####################################################################################################
     
        if ll is not None and desc is not None and canmatrix.utils.decode_number(ul.text) == canmatrix.utils.decode_number(ll.text):
            #####################################################################################################
            #####################################################################################################
            values[ll.text] = desc

        scale_desc = ea.get_element_desc(compu_scale)
        rational = ea.get_child(compu_scale, ""COMPU-RATIONAL-COEFFS"")
        if rational is not None:
            numerator_parent = ea.get_child(rational, ""COMPU-NUMERATOR"")
            numerator = ea.get_children(numerator_parent, ""V"")
            denominator_parent = ea.get_child(rational, ""COMPU-DENOMINATOR"")
            denominator = ea.get_children(denominator_parent, ""V"")
            try:
                factor = float_factory(numerator[1].text) / float_factory(denominator[0].text)
                offset = float_factory(numerator[0].text) / float_factory(denominator[0].text)
            except decimal.DivisionByZero:
                if numerator[0].text != denominator[0].text or numerator[1].text != denominator[1].text:
                    logger.warning(""ARXML signal scaling: polynom is not supported and it is replaced by factor=1 and offset =0."")
                factor = float_factory(1)
                offset = float_factory(0)
        else:
            const = ea.get_child(compu_scale, ""COMPU-CONST"")
            # add value
            if const is None:
                logger.warning(""Unknown Compu-Method: at sourceline %d "", compu_method.sourceline)
    return values, factor, offset, unit, const","1. Use a secure random number generator to generate the values.
2. Sanitize all user input before using it in the code.
3. Use proper error handling to prevent security vulnerabilities."
"def get_signals(signal_array, frame, ea, multiplex_id, float_factory, bit_offset=0):
    # type: (typing.Sequence[_Element], canmatrix.Frame, _DocRoot, str, _MultiplexId, typing.Callable, int) -> None
    """"""Add signals from xml to the Frame.""""""
    global signal_rxs
    group_id = 1
    if signal_array is None:  # Empty signalarray - nothing to do
        return
    for signal in signal_array:
        compu_method = None
        motorola = ea.get_child(signal, ""PACKING-BYTE-ORDER"")
        start_bit = ea.get_child(signal, ""START-POSITION"")

        isignal = ea.follow_ref(signal, ""SIGNAL-REF"")
        if isignal is None:
            isignal = ea.follow_ref(signal, ""I-SIGNAL-REF"")

        if isignal is None:
            isignal = ea.follow_ref(signal, ""I-SIGNAL-GROUP-REF"")
            if isignal is not None:
                logger.debug(""get_signals: found I-SIGNAL-GROUP "")

                isignal_array = ea.find_children_by_path(isignal, ""I-SIGNAL"")
                system_signal_array = [ea.follow_ref(isignal, ""SYSTEM-SIGNAL-REF"") for isignal in isignal_array]
                system_signal_group = ea.follow_ref(isignal, ""SYSTEM-SIGNAL-GROUP-REF"")
                get_sys_signals(system_signal_group, system_signal_array, frame, group_id, ea)
                group_id = group_id + 1
                continue
        if isignal is None:
            logger.debug(
                'Frame %s, no isignal for %s found',
                frame.name, ea.get_child(signal, ""SHORT-NAME"").text)

        base_type = ea.follow_ref(isignal, ""BASE-TYPE-REF"")
        try:
            type_encoding = ea.get_child(base_type, ""BASE-TYPE-ENCODING"").text
        except AttributeError:
            type_encoding = ""None""
        signal_name = None  # type: typing.Optional[str]
        signal_name_elem = ea.get_child(isignal, ""LONG-NAME"")
        if signal_name_elem is not None:
            signal_name_elem = ea.get_child(signal_name_elem, ""L-4"")
            if signal_name_elem is not None:
                signal_name = signal_name_elem.text

        system_signal = ea.follow_ref(isignal, ""SYSTEM-SIGNAL-REF"")

        if system_signal is None:
            logger.debug('Frame %s, signal %s has no system-signal', frame.name, isignal.tag)

        if system_signal is not None and ""SYSTEM-SIGNAL-GROUP"" in system_signal.tag:
            system_signals = ea.find_children_by_path(system_signal, ""SYSTEM-SIGNAL-REFS/SYSTEM-SIGNAL"")
            get_sys_signals(system_signal, system_signals, frame, group_id, ns)
            group_id = group_id + 1
            continue

        length = ea.get_child(isignal, ""LENGTH"")
        if length is None:
            length = ea.get_child(system_signal, ""LENGTH"")

        name = ea.get_child(system_signal, ""SHORT-NAME"")
        unit_element = ea.get_child(isignal, ""UNIT"")
        display_name = ea.get_child(unit_element, ""DISPLAY-NAME"")
        if display_name is not None:
            signal_unit = display_name.text
        else:
            signal_unit = """"

        signal_min = None  # type: canmatrix.types.OptionalPhysicalValue
        signal_max = None  # type: canmatrix.types.OptionalPhysicalValue
        receiver = []  # type: typing.List[str]

        signal_description = ea.get_element_desc(system_signal)

        datatype = ea.follow_ref(system_signal, ""DATA-TYPE-REF"")
        if datatype is None:  # AR4?
            data_constr = None
            compu_method = None
            base_type = None
            for test_signal in [isignal, system_signal]:
                if data_constr is None:
                    data_constr = ea.follow_ref(test_signal, ""DATA-CONSTR-REF"")
                if compu_method is None:
                    compu_method = ea.follow_ref(test_signal, ""COMPU-METHOD-REF"")
                if base_type is None:
                    base_type = ea.follow_ref(test_signal, ""BASE-TYPE-REF"")

            lower = ea.get_child(data_constr, ""LOWER-LIMIT"")
            upper = ea.get_child(data_constr, ""UPPER-LIMIT"")
            encoding = None  # TODO - find encoding in AR4
        else:
            lower = ea.get_child(datatype, ""LOWER-LIMIT"")
            upper = ea.get_child(datatype, ""UPPER-LIMIT"")
            type_encoding = ea.get_child(datatype, ""ENCODING"")

        if lower is not None and upper is not None:
            signal_min = float_factory(lower.text)
            signal_max = float_factory(upper.text)

        datdefprops = ea.get_child(datatype, ""SW-DATA-DEF-PROPS"")

        if compu_method is None:
            compu_method = ea.follow_ref(datdefprops, ""COMPU-METHOD-REF"")
        if compu_method is None:  # AR4
            compu_method = ea.get_child(isignal, ""COMPU-METHOD"")
            base_type = ea.follow_ref(isignal, ""BASE-TYPE-REF"")
            encoding = ea.get_child(base_type, ""BASE-TYPE-ENCODING"")
            if encoding is not None and encoding.text == ""IEEE754"":
                is_float = True
        if compu_method is None:
            logger.debug('No Compmethod found!! - try alternate scheme 1.')
            networkrep = ea.get_child(isignal, ""NETWORK-REPRESENTATION-PROPS"")
            data_def_props_var = ea.get_child(networkrep, ""SW-DATA-DEF-PROPS-VARIANTS"")
            data_def_props_cond = ea.get_child(data_def_props_var, ""SW-DATA-DEF-PROPS-CONDITIONAL"")
            if data_def_props_cond is not None:
                try:
                    compu_method = ea.get_child(data_def_props_cond, ""COMPU-METHOD"")
                except:
                    logger.debug('No valid compu method found for this - check ARXML file!!')
                    compu_method = None
        #####################################################################################################
        # no found compu-method fuzzy search in systemsignal:
        #####################################################################################################
        if compu_method is None:
            logger.debug('No Compmethod found!! - fuzzy search in syssignal.')
            compu_method = ea.get_child(system_signal, ""COMPU-METHOD"")

        # decode compuMethod:
        (values, factor, offset, unit_elem, const) = decode_compu_method(compu_method, ea, float_factory)

        if signal_min is not None:
            signal_min *= factor
            signal_min += offset
        if signal_max is not None:
            signal_max *= factor
            signal_max += offset

        if base_type is None:
            base_type = ea.get_child(datdefprops, ""BASE-TYPE"")

        (is_signed, is_float) = eval_type_of_signal(type_encoding, base_type, ea)

        if unit_elem is not None:
            longname = ea.get_child(unit_elem, ""LONG-NAME"")
        #####################################################################################################
        # Modification to support obtaining the Signals Unit by DISPLAY-NAME. 07June16
        #####################################################################################################
            display_name = None
            try:
                display_name = ea.get_child(unit_elem, ""DISPLAY-NAME"")
            except:
                logger.debug('No Unit Display name found!! - using long name')
            if display_name is not None:
                signal_unit = display_name.text
            else:
                l4 = ea.get_child(longname, ""L-4"")
                if l4 is not None:
                    signal_unit = l4.text

        init_list = ea.find_children_by_path(system_signal, ""INIT-VALUE/VALUE"")

        if not init_list:
            init_list = ea.find_children_by_path(isignal, ""INIT-VALUE/NUMERICAL-VALUE-SPECIFICATION/VALUE"")  # #AR4.2
        if init_list:
            initvalue = init_list[0]
        else:
            initvalue = None

        is_little_endian = False
        if motorola is not None:
            if motorola.text == 'MOST-SIGNIFICANT-BYTE-LAST':
                is_little_endian = True
        else:
            logger.debug('no name byte order for signal' + name.text)

        if name is None:
            logger.debug('no name for signal given')
            name = ea.get_child(isignal, ""SHORT-NAME"")
        if start_bit is None:
            logger.debug('no startBit for signal given')
        if length is None:
            logger.debug('no length for signal given')

        if start_bit is not None:
            new_signal = canmatrix.Signal(
                name.text,
                start_bit=int(start_bit.text) + bit_offset,
                size=int(length.text) if length is not None else 0,
                is_little_endian=is_little_endian,
                is_signed=is_signed,
                factor=factor,
                offset=offset,
                unit=signal_unit,
                receivers=receiver,
                multiplex=multiplex_id,
                comment=signal_description,
                is_float=is_float)

            if signal_min is not None:
                new_signal.min = signal_min
            if signal_max is not None:
                new_signal.max = signal_max

            if not new_signal.is_little_endian:
                # startbit of motorola coded signals are MSB in arxml
                new_signal.set_startbit(int(start_bit.text) + bit_offset, bitNumbering=1)

            # save signal, to determin receiver-ECUs for this signal later
            signal_rxs[system_signal] = new_signal

            if base_type is not None:
                temp = ea.get_child(base_type, ""SHORT-NAME"")
                if temp is not None and ""boolean"" == temp.text:
                    new_signal.add_values(1, ""TRUE"")
                    new_signal.add_values(0, ""FALSE"")

            if initvalue is not None and initvalue.text is not None:
                initvalue.text = canmatrix.utils.guess_value(initvalue.text)
                new_signal.initial_value = float_factory(initvalue.text)

            for key, value in list(values.items()):
                new_signal.add_values(canmatrix.utils.decode_number(key), value)
            if signal_name is not None:
                new_signal.add_attribute(""LongName"", signal_name)
            frame.add_signal(new_signal)","1. Use `TYPE_CHECKING` to check the types of arguments passed to functions.
2. Use `@functools.lru_cache` to cache the results of expensive computations.
3. Use `@wraps` to preserve the metadata of wrapped functions."
"def decode_can_helper(ea, float_factory, ignore_cluster_info):
    found_matrixes = {}
    if ignore_cluster_info is True:
        ccs = [lxml.etree.Element(""ignoreClusterInfo"")]  # type: typing.Sequence[_Element]
    else:
        ccs = ea.findall('CAN-CLUSTER')
    for cc in ccs:  # type: _Element
        db = canmatrix.CanMatrix()
        # Defines not jet imported...
        db.add_ecu_defines(""NWM-Stationsadresse"", 'HEX 0 63')
        db.add_ecu_defines(""NWM-Knoten"", 'ENUM  ""nein"",""ja""')
        db.add_signal_defines(""LongName"", 'STRING')
        db.add_frame_defines(""GenMsgDelayTime"", 'INT 0 65535')
        db.add_frame_defines(""GenMsgNrOfRepetitions"", 'INT 0 65535')
        db.add_frame_defines(""GenMsgStartValue"", 'STRING')
        db.add_frame_defines(""GenMsgStartDelayTime"", 'INT 0 65535')
        db.add_frame_defines(
            ""GenMsgSendType"",
            'ENUM  ""cyclicX"",""spontanX"",""cyclicIfActiveX"",""spontanWithDelay"",""cyclicAndSpontanX"",""cyclicAndSpontanWithDelay"",""spontanWithRepitition"",""cyclicIfActiveAndSpontanWD"",""cyclicIfActiveFast"",""cyclicWithRepeatOnDemand"",""none""')

        if ignore_cluster_info is True:
            can_frame_trig = ea.findall('CAN-FRAME-TRIGGERING')
            bus_name = """"
        else:
            speed = ea.get_child(cc, ""SPEED"")
            baudrate_elem = ea.find(""BAUDRATE"", cc)
            fd_baudrate_elem = ea.find(""CAN-FD-BAUDRATE"", cc)

            logger.debug(""Busname: "" + get_element_name(cc, ns))

            bus_name = ea.get_element_name(cc)
            if speed is not None:
                db.baudrate = int(speed.text)
            elif baudrate_elem is not None:
                db.baudrate = int(baudrate_elem.text)

            logger.debug(""Baudrate: ""+ str(db.baudrate))
            if fd_baudrate_elem is not None:
                db.fd_baudrate = fd_baudrate_elem.text

        

            physical_channels = ea.find(""PHYSICAL-CHANNELS"", cc)  # type: _Element
            if physical_channels is None:
                logger.error(""PHYSICAL-CHANNELS not found"")

            nm_lower_id = ea.get_child(cc, ""NM-LOWER-CAN-ID"")

            physical_channel = ea.get_child(physical_channels, ""PHYSICAL-CHANNEL"")
            if physical_channel is None:
                physical_channel = ea.get_child(physical_channels, ""CAN-PHYSICAL-CHANNEL"")
            if physical_channel is None:
                logger.error(""PHYSICAL-CHANNEL not found"")
            can_frame_trig = ea.get_children(physical_channel, ""CAN-FRAME-TRIGGERING"")
            if can_frame_trig is None:
                logger.error(""CAN-FRAME-TRIGGERING not found"")
            else:
                logger.debug(""%d frames found in arxml"", len(can_frame_trig))

        multiplex_translation = {}  # type: typing.Dict[str, str]
        for frameTrig in can_frame_trig:  # type: _Element
            frame = get_frame(frameTrig, ea, multiplex_translation, float_factory)
            if frame is not None:
                db.add_frame(frame)

        if ignore_cluster_info is True:
            pass
            # no support for signal direction
        else:  # find signal senders/receivers...
            isignal_triggerings = ea.find_children_by_path(physical_channel, ""I-SIGNAL-TRIGGERING"")
            for sig_trig in isignal_triggerings:
                isignal = ea.follow_ref(sig_trig, 'SIGNAL-REF')
                if isignal is None:
                    isignal = ea.follow_ref(sig_trig, 'I-SIGNAL-REF')
                if isignal is None:
                    sig_trig_text = ea.get_element_name(sig_trig) if sig_trig is not None else ""None""
                    logger.debug(""load: no isignal for %s"", sig_trig_text)
                    continue

                port_ref = ea.follow_all_ref(sig_trig, ""I-SIGNAL-PORT-REF"")

                for port in port_ref:
                    comm_direction = ea.get_child(port, ""COMMUNICATION-DIRECTION"")
                    if comm_direction is not None and comm_direction.text == ""IN"":
                        sys_signal = ea.follow_ref(isignal, ""SYSTEM-SIGNAL-REF"")
                        ecu_name = ea.get_element_name(port.getparent().getparent().getparent().getparent())
                        # port points in ECU; probably more stable to go up
                        # from each ECU than to go down in XML...
                        if sys_signal in signal_rxs:
                            signal_rxs[sys_signal].add_receiver(ecu_name)
                            # find ECUs:
        nodes = ea.findall('ECU-INSTANCE')
        for node in nodes:  # type: _Element
            ecu = process_ecu(node, db, ea, multiplex_translation)
            desc = ea.get_child(node, ""DESC"", )
            l2 = ea.get_child(desc, ""L-2"")
            if l2 is not None:
                ecu.add_comment(l2.text)

            db.add_ecu(ecu)

        if 0:
        #for frame in db.frames:
            sig_value_hash = dict()
            for sig in frame.signals:
                try:
                    sig_value_hash[sig.name] = sig.phys2raw()
                except AttributeError:
                    sig_value_hash[sig.name] = 0
            frame_data = frame.encode(sig_value_hash)
            frame.add_attribute(""GenMsgStartValue"", """".join([""%02x"" % x for x in frame_data]))
        found_matrixes[bus_name] = db
    return found_matrixes","1. Use a secure random number generator to generate the salt.
2. Use a strong hashing algorithm, such as bcrypt or SHA-256, to hash the password.
3. Store the hashed password in a secure location, such as a database or file system."
"def decode_number(value):  # type(string) -> (int)
    """"""
    Decode string to integer and guess correct base
    :param value: string input value
    :return: integer
    """"""

    value = value.strip()
    base = 10
    if len(value) > 1 and value[1] == 'b':  # bin coded
        base = 2
        value = value[2:]
    if len(value) > 1 and value[1] == 'x':  # hex coded
        base = 16
        value = value[2:]

    return int(value, base)","1. Use `str.isdigit()` to check if the input is a valid number.
2. Use `ord()` to convert the input to an integer.
3. Use `basestring.decode()` to decode the input to a string."
"def load(filename, **options):
    from sys import modules

    # use xlrd excel reader if available, because its more robust
    try:
        import canmatrix.xls
        return canmatrix.xls.load(filename, **options)
    except:
        logger.error(""xlsx: using legacy xlsx-reader - please get xlrd working for better results!"")
        pass

    # else use this hack to read xlsx
    if 'xlsMotorolaBitFormat' in options:
        motorolaBitFormat = options[""xlsMotorolaBitFormat""]
    else:
        motorolaBitFormat = ""msbreverse""

    sheet = readXlsx(filename, sheet=1, header=True)
    db = CanMatrix()
    letterIndex = []
    for a in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
        letterIndex.append(a)
    for a in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
        for b in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
            letterIndex.append(""%s%s"" % (a, b))

    # Defines not imported...
    db.addFrameDefines(""GenMsgCycleTime"", 'INT 0 65535')
    db.addFrameDefines(""GenMsgDelayTime"", 'INT 0 65535')
    db.addFrameDefines(""GenMsgCycleTimeActive"", 'INT 0 65535')
    db.addFrameDefines(""GenMsgNrOfRepetitions"", 'INT 0 65535')
    launchTypes = []

    db.addSignalDefines(""GenSigSNA"", 'STRING')

    if 'Byteorder' in list(sheet[0].values()):
        for key in sheet[0]:
            if sheet[0][key].strip() == 'Byteorder':
                _BUstart = letterIndex.index(key) + 1
                break
    else:
        for key in sheet[0]:
            if sheet[0][key].strip() == 'Signal Not Available':
                _BUstart = letterIndex.index(key) + 1

    for key in sheet[0]:
        if sheet[0][key].strip() == 'Value':
            _BUend = letterIndex.index(key)

    # BoardUnits:
    for x in range(_BUstart, _BUend):
        db.boardUnits.add(BoardUnit(sheet[0][letterIndex[x]]))

    # initialize:
    frameId = None
    signalName = """"
    newBo = None

    for row in sheet[1]:
        # ignore empty row
        if not 'ID' in row:
            continue
        # new frame detected
        if row['ID'] != frameId:
            sender = []
            # new Frame
            frameId = row['ID']
            frameName = row['Frame Name']
            cycleTime = getIfPossible(row, ""Cycle Time [ms]"")
            launchType = getIfPossible(row, 'Launch Type')
            dlc = 8
            launchParam = getIfPossible(row, 'Launch Parameter')
            if type(launchParam).__name__ != ""float"":
                launchParam = 0.0
            launchParam = str(int(launchParam))

            if frameId.endswith(""xh""):
                newBo = Frame(frameName, id=int(frameId[:-2], 16), dlc=dlc, extended=True)
            else:
                newBo = Frame(frameName, id=int(frameId[:-1], 16), dlc=dlc)

            db.frames.addFrame(newBo)

            # eval launchtype
            if launchType is not None:
                newBo.addAttribute(""GenMsgSendType"", launchType)
                if launchType not in launchTypes:
                    launchTypes.append(launchType)

#                       #eval cycletime
            if type(cycleTime).__name__ != ""float"":
                cycleTime = 0.0
            newBo.addAttribute(""GenMsgCycleTime"", str(int(cycleTime)))

        # new signal detected
        if 'Signal Name' in row and row['Signal Name'] != signalName:
            # new Signal
            receiver = []
            startbyte = int(row[""Signal Byte No.""])
            startbit = int(row['Signal Bit No.'])
            signalName = row['Signal Name']
            signalComment = getIfPossible(row, 'Signal Function')
            signalLength = int(row['Signal Length [Bit]'])
            signalDefault = getIfPossible(row, 'Signal Default')
            signalSNA = getIfPossible(row, 'Signal Not Available')
            multiplex = None
            if signalComment is not None and signalComment.startswith(
                    'Mode Signal:'):
                multiplex = 'Multiplexor'
                signalComment = signalComment[12:]
            elif signalComment is not None and signalComment.startswith('Mode '):
                mux, signalComment = signalComment[4:].split(':', 1)
                multiplex = int(mux.strip())

            signalByteorder = getIfPossible(row, 'Byteorder')
            if signalByteorder is not None:
                if 'i' in signalByteorder:
                    is_little_endian = True
                else:
                    is_little_endian = False
            else:
                is_little_endian = True  # Default Intel

            is_signed = False

            if signalName != ""-"":
                for x in range(_BUstart, _BUend):
                    buName = sheet[0][letterIndex[x]].strip()
                    buSenderReceiver = getIfPossible(row, buName)
                    if buSenderReceiver is not None:
                        if 's' in buSenderReceiver:
                            newBo.addTransmitter(buName)
                        if 'r' in buSenderReceiver:
                            receiver.append(buName)
#                if signalLength > 8:
#                    newSig = Signal(signalName, (startbyte-1)*8+startbit, signalLength, is_little_endian, is_signed, 1, 0, 0, 1, """", receiver, multiplex)
                newSig = Signal(signalName,
                                startBit=(startbyte - 1) * 8 + startbit,
                                signalSize=signalLength,
                                is_little_endian=is_little_endian,
                                is_signed=is_signed,
                                receiver=receiver,
                                multiplex=multiplex)

#                else:
#                    newSig = Signal(signalName, (startbyte-1)*8+startbit, signalLength, is_little_endian, is_signed, 1, 0, 0, 1, """", receiver, multiplex)
                if is_little_endian == False:
                    # motorola
                    if motorolaBitFormat == ""msb"":
                        newSig.setStartbit(
                            (startbyte - 1) * 8 + startbit, bitNumbering=1)
                    elif motorolaBitFormat == ""msbreverse"":
                        newSig.setStartbit((startbyte - 1) * 8 + startbit)
                    else:  # motorolaBitFormat == ""lsb""
                        newSig.setStartbit(
                            (startbyte - 1) * 8 + startbit,
                            bitNumbering=1,
                            startLittle=True)

                newBo.addSignal(newSig)
                newSig.addComment(signalComment)
                function = getIfPossible(row, 'Function / Increment Unit')
        value = getIfPossible(row, 'Value')
        valueName = getIfPossible(row, 'Name / Phys. Range')

        if valueName == 0 or valueName is None:
            valueName = ""0""
        elif valueName == 1:
            valueName = ""1""
        test = valueName
        #.encode('utf-8')

        factor = 0
        unit = """"

        factor = getIfPossible(row, 'Function / Increment Unit')
        if type(factor).__name__ == ""unicode"" or type(
                factor).__name__ == ""str"":
            factor = factor.strip()
            if "" "" in factor and factor[0].isdigit():
                (factor, unit) = factor.strip().split("" "", 1)
                factor = factor.strip()
                unit = unit.strip()
                newSig.unit = unit
                newSig.factor = float(factor)
            else:
                unit = factor.strip()
                newSig.unit = unit
                newSig.factor = 1

        if "".."" in test:
            (mini, maxi) = test.strip().split("".."", 2)
            unit = """"
            try:
                newSig.offset = float(mini)
                newSig.min = float(mini)
                newSig.max = float(maxi)
            except:
                newSig.offset = 0
                newSig.min = None
                newSig.max = None

        elif valueName.__len__() > 0:
            if value is not None and value.strip().__len__() > 0:
                value = int(float(value))
                newSig.addValues(value, valueName)
            maxi = pow(2, signalLength) - 1
            newSig.max = float(maxi)
        else:
            newSig.offset = 0
            newSig.min = None
            newSig.max = None

    # dlc-estimation / dlc is not in xls, thus calculate a minimum-dlc:
    for frame in db.frames:
        frame.updateReceiver()
        frame.calcDLC()

    launchTypeEnum = ""ENUM""
    for launchType in launchTypes:
        if len(launchType) > 0:
            launchTypeEnum += ' ""' + launchType + '"",'
    db.addFrameDefines(""GenMsgSendType"", launchTypeEnum[:-1])

    db.setFdType()
    return db","1. Use `xlrd` to read xlsx files instead of the hacky implementation.
2. Use `getIfPossible` to handle missing values instead of `try`/`except`.
3. Use `str.encode()` to convert unicode strings to bytes before using them in `xml.etree.ElementTree`."
"def load(filename, **options):
    from sys import modules

    # use xlrd excel reader if available, because its more robust
#    try:
#        import canmatrix.xls
#        return canmatrix.xls.load(filename, **options)
#    except:
#        logger.error(""xlsx: using legacy xlsx-reader - please get xlrd working for better results!"")
#        pass

    # else use this hack to read xlsx
    if 'xlsMotorolaBitFormat' in options:
        motorolaBitFormat = options[""xlsMotorolaBitFormat""]
    else:
        motorolaBitFormat = ""msbreverse""

    sheet = readXlsx(filename, sheet=1, header=True)
    db = CanMatrix()
    letterIndex = []
    for a in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
        letterIndex.append(a)
    for a in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
        for b in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
            letterIndex.append(""%s%s"" % (a, b))

    # Defines not imported...
    db.addFrameDefines(""GenMsgCycleTime"", 'INT 0 65535')
    db.addFrameDefines(""GenMsgDelayTime"", 'INT 0 65535')
    db.addFrameDefines(""GenMsgCycleTimeActive"", 'INT 0 65535')
    db.addFrameDefines(""GenMsgNrOfRepetitions"", 'INT 0 65535')
    launchTypes = []

    db.addSignalDefines(""GenSigSNA"", 'STRING')

    if 'Byteorder' in list(sheet[0].values()):
        for key in sheet[0]:
            if sheet[0][key].strip() == 'Byteorder':
                _BUstart = letterIndex.index(key) + 1
                break
    else:
        for key in sheet[0]:
            if sheet[0][key].strip() == 'Signal Not Available':
                _BUstart = letterIndex.index(key) + 1

    for key in sheet[0]:
        if sheet[0][key].strip() == 'Value':
            _BUend = letterIndex.index(key)

    # BoardUnits:
    for x in range(_BUstart, _BUend):
        db.addEcu(BoardUnit(sheet[0][letterIndex[x]]))

    # initialize:
    frameId = None
    signalName = """"
    newBo = None

    for row in sheet[1]:
        # ignore empty row
        if not 'ID' in row:
            continue
        # new frame detected
        if row['ID'] != frameId:
            sender = []
            # new Frame
            frameId = row['ID']
            frameName = row['Frame Name']
            cycleTime = getIfPossible(row, ""Cycle Time [ms]"")
            launchType = getIfPossible(row, 'Launch Type')
            dlc = 8
            launchParam = getIfPossible(row, 'Launch Parameter')
            if type(launchParam).__name__ != ""float"":
                launchParam = 0.0
            launchParam = str(int(launchParam))

            if frameId.endswith(""xh""):
                newBo = Frame(frameName, id=int(frameId[:-2], 16), size=dlc, extended=True)
            else:
                newBo = Frame(frameName, id=int(frameId[:-1], 16), size=dlc)

            db.addFrame(newBo)

            # eval launchtype
            if launchType is not None:
                newBo.addAttribute(""GenMsgSendType"", launchType)
                if launchType not in launchTypes:
                    launchTypes.append(launchType)

#                       #eval cycletime
            if type(cycleTime).__name__ != ""float"":
                cycleTime = 0.0
            newBo.addAttribute(""GenMsgCycleTime"", str(int(cycleTime)))

        # new signal detected
        if 'Signal Name' in row and row['Signal Name'] != signalName:
            # new Signal
            receiver = []
            startbyte = int(row[""Signal Byte No.""])
            startbit = int(row['Signal Bit No.'])
            signalName = row['Signal Name']
            signalComment = getIfPossible(row, 'Signal Function')
            signalLength = int(row['Signal Length [Bit]'])
            signalDefault = getIfPossible(row, 'Signal Default')
            signalSNA = getIfPossible(row, 'Signal Not Available')
            multiplex = None
            if signalComment is not None and signalComment.startswith(
                    'Mode Signal:'):
                multiplex = 'Multiplexor'
                signalComment = signalComment[12:]
            elif signalComment is not None and signalComment.startswith('Mode '):
                mux, signalComment = signalComment[4:].split(':', 1)
                multiplex = int(mux.strip())

            signalByteorder = getIfPossible(row, 'Byteorder')
            if signalByteorder is not None:
                if 'i' in signalByteorder:
                    is_little_endian = True
                else:
                    is_little_endian = False
            else:
                is_little_endian = True  # Default Intel

            is_signed = False

            if signalName != ""-"":
                for x in range(_BUstart, _BUend):
                    buName = sheet[0][letterIndex[x]].strip()
                    buSenderReceiver = getIfPossible(row, buName)
                    if buSenderReceiver is not None:
                        if 's' in buSenderReceiver:
                            newBo.addTransmitter(buName)
                        if 'r' in buSenderReceiver:
                            receiver.append(buName)
#                if signalLength > 8:
#                    newSig = Signal(signalName, (startbyte-1)*8+startbit, signalLength, is_little_endian, is_signed, 1, 0, 0, 1, """", receiver, multiplex)
                newSig = Signal(signalName,
                                startBit=(startbyte - 1) * 8 + startbit,
                                size=signalLength,
                                is_little_endian=is_little_endian,
                                is_signed=is_signed,
                                receiver=receiver,
                                multiplex=multiplex)

#                else:
#                    newSig = Signal(signalName, (startbyte-1)*8+startbit, signalLength, is_little_endian, is_signed, 1, 0, 0, 1, """", receiver, multiplex)
                if is_little_endian == False:
                    # motorola
                    if motorolaBitFormat == ""msb"":
                        newSig.setStartbit(
                            (startbyte - 1) * 8 + startbit, bitNumbering=1)
                    elif motorolaBitFormat == ""msbreverse"":
                        newSig.setStartbit((startbyte - 1) * 8 + startbit)
                    else:  # motorolaBitFormat == ""lsb""
                        newSig.setStartbit(
                            (startbyte - 1) * 8 + startbit,
                            bitNumbering=1,
                            startLittle=True)

                newBo.addSignal(newSig)
                newSig.addComment(signalComment)
                function = getIfPossible(row, 'Function / Increment Unit')
        value = getIfPossible(row, 'Value')
        valueName = getIfPossible(row, 'Name / Phys. Range')

        if valueName == 0 or valueName is None:
            valueName = ""0""
        elif valueName == 1:
            valueName = ""1""
        test = valueName
        #.encode('utf-8')

        factor = 0
        unit = """"

        factor = getIfPossible(row, 'Function / Increment Unit')
        if type(factor).__name__ == ""unicode"" or type(
                factor).__name__ == ""str"":
            factor = factor.strip()
            if "" "" in factor and factor[0].isdigit():
                (factor, unit) = factor.strip().split("" "", 1)
                factor = factor.strip()
                unit = unit.strip()
                newSig.unit = unit
                newSig.factor = float(factor)
            else:
                unit = factor.strip()
                newSig.unit = unit
                newSig.factor = 1

        if "".."" in test:
            (mini, maxi) = test.strip().split("".."", 2)
            unit = """"
            try:
                newSig.offset = float(mini)
                newSig.min = float(mini)
                newSig.max = float(maxi)
            except:
                newSig.offset = 0
                newSig.min = None
                newSig.max = None

        elif valueName.__len__() > 0:
            if value is not None and value.strip().__len__() > 0:
                value = int(float(value))
                newSig.addValues(value, valueName)
            maxi = pow(2, signalLength) - 1
            newSig.max = float(maxi)
        else:
            newSig.offset = 0
            newSig.min = None
            newSig.max = None

    # dlc-estimation / dlc is not in xls, thus calculate a minimum-dlc:
    for frame in db.frames:
        frame.updateReceiver()
        frame.calcDLC()

    launchTypeEnum = ""ENUM""
    for launchType in launchTypes:
        if len(launchType) > 0:
            launchTypeEnum += ' ""' + launchType + '"",'
    db.addFrameDefines(""GenMsgSendType"", launchTypeEnum[:-1])

    db.setFdType()
    return db","1. Use `type()` to check the data type of `row['ID']` before casting it to an integer.
2. Use `str.strip()` to remove whitespace from the start and end of `row['ID']` before casting it to an integer.
3. Use `int()` to cast `row['ID']` to an integer."
"def dump(dbs, f, **options):
    if 'arVersion' in options:
        arVersion = options[""arVersion""]
    else:
        arVersion = ""3.2.3""

    for name in dbs:
        db = dbs[name]
        for frame in db.frames:
            for signal in frame.signals:
                for rec in signal.receiver:
                    if rec.strip() not in frame.receiver:
                        frame.receiver.append(rec.strip())

    if arVersion[0] == ""3"":
        xsi = 'http://www.w3.org/2001/XMLSchema-instance'
        root = etree.Element(
            'AUTOSAR',
            nsmap={
                None: 'http://autosar.org/' + arVersion,
                'xsi': xsi})
        root.attrib['{{{pre}}}schemaLocation'.format(
            pre=xsi)] = 'http://autosar.org/' + arVersion + ' AUTOSAR_' + arVersion.replace('.', '') + '.xsd'
        toplevelPackages = createSubElement(root, 'TOP-LEVEL-PACKAGES')
    else:
        xsi = 'http://www.w3.org/2001/XMLSchema-instance'
        root = etree.Element(
            'AUTOSAR',
            nsmap={
                None: ""http://autosar.org/schema/r4.0"",
                'xsi': xsi})
        root.attrib['{{{pre}}}schemaLocation'.format(
            pre=xsi)] = 'http://autosar.org/schema/r4.0 AUTOSAR_' + arVersion.replace('.', '-') + '.xsd'
        toplevelPackages = createSubElement(root, 'AR-PACKAGES')

    #
    # AR-PACKAGE Cluster
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'Cluster')
    elements = createSubElement(arPackage, 'ELEMENTS')

    for name in dbs:
        db = dbs[name]
#        if len(name) == 0:
#            (path, ext) = os.path.splitext(filename)
#            busName = path
#        else:
        if len(name) > 0:
            busName = name
        else:
            busName = ""CAN""

        cancluster = createSubElement(elements, 'CAN-CLUSTER')
        createSubElement(cancluster, 'SHORT-NAME', busName)
        if arVersion[0] == ""3"":
         #       createSubElement(cancluster, 'SPEED', '50000')
            physicalChannels = createSubElement(
                cancluster, 'PHYSICAL-CHANNELS')
            physicalChannel = createSubElement(
                physicalChannels, 'PHYSICAL-CHANNEL')
            createSubElement(physicalChannel, 'SHORT-NAME', 'CAN')
            frameTriggering = createSubElement(
                physicalChannel, 'FRAME-TRIGGERINGSS')
        else:
            canClusterVaraints = createSubElement(
                cancluster, 'CAN-CLUSTER-VARIANTS')
            canClusterConditional = createSubElement(
                canClusterVaraints, 'CAN-CLUSTER-CONDITIONAL')
            physicalChannels = createSubElement(
                canClusterConditional, 'PHYSICAL-CHANNELS')
            physicalChannel = createSubElement(
                physicalChannels, 'CAN-PHYSICAL-CHANNEL')
            createSubElement(physicalChannel, 'SHORT-NAME', 'CAN')
            frameTriggering = createSubElement(
                physicalChannel, 'FRAME-TRIGGERINGS')
        for frame in db.frames:
            canFrameTriggering = createSubElement(
                frameTriggering, 'CAN-FRAME-TRIGGERING')
            createSubElement(canFrameTriggering, 'SHORT-NAME', frame.name)
            framePortRefs = createSubElement(
                canFrameTriggering, 'FRAME-PORT-REFS')
            for transmitter in frame.transmitter:
                framePortRef = createSubElement(
                    framePortRefs, 'FRAME-PORT-REF')
                framePortRef.set('DEST', 'FRAME-PORT')
                framePortRef.text = ""/ECU/"" + transmitter + \\
                    ""/CN_"" + transmitter + ""/"" + frame.name
            for rec in frame.receiver:
                framePortRef = createSubElement(
                    framePortRefs, 'FRAME-PORT-REF')
                framePortRef.set('DEST', 'FRAME-PORT')
                framePortRef.text = ""/ECU/"" + rec + ""/CN_"" + rec + ""/"" + frame.name
            frameRef = createSubElement(canFrameTriggering, 'FRAME-REF')
            if arVersion[0] == ""3"":
                frameRef.set('DEST', 'FRAME')
                frameRef.text = ""/Frame/FRAME_"" + frame.name
                pduTriggeringRefs = createSubElement(
                    canFrameTriggering, 'I-PDU-TRIGGERING-REFS')
                pduTriggeringRef = createSubElement(
                    pduTriggeringRefs, 'I-PDU-TRIGGERING-REF')
                pduTriggeringRef.set('DEST', 'I-PDU-TRIGGERING')
            else:
                frameRef.set('DEST', 'CAN-FRAME')
                frameRef.text = ""/CanFrame/FRAME_"" + frame.name
                pduTriggering = createSubElement(
                    canFrameTriggering, 'PDU-TRIGGERINGS')
                pduTriggeringRefConditional = createSubElement(
                    pduTriggering, 'PDU-TRIGGERING-REF-CONDITIONAL')
                pduTriggeringRef = createSubElement(
                    pduTriggeringRefConditional, 'PDU-TRIGGERING-REF')
                pduTriggeringRef.set('DEST', 'PDU-TRIGGERING')

            if frame.extended == 0:
                createSubElement(
                    canFrameTriggering,
                    'CAN-ADDRESSING-MODE',
                    'STANDARD')
            else:
                createSubElement(
                    canFrameTriggering,
                    'CAN-ADDRESSING-MODE',
                    'EXTENDED')
            createSubElement(canFrameTriggering, 'IDENTIFIER', str(frame.id))

            pduTriggeringRef.text = ""/Cluster/CAN/IPDUTRIGG_"" + frame.name

        if arVersion[0] == ""3"":
            ipduTriggerings = createSubElement(
                physicalChannel, 'I-PDU-TRIGGERINGS')
            for frame in db.frames:
                ipduTriggering = createSubElement(
                    ipduTriggerings, 'I-PDU-TRIGGERING')
                createSubElement(
                    ipduTriggering,
                    'SHORT-NAME',
                    ""IPDUTRIGG_"" +
                    frame.name)
                ipduRef = createSubElement(ipduTriggering, 'I-PDU-REF')
                ipduRef.set('DEST', 'SIGNAL-I-PDU')
                ipduRef.text = ""/PDU/PDU_"" + frame.name
            isignalTriggerings = createSubElement(
                physicalChannel, 'I-SIGNAL-TRIGGERINGS')
            for frame in db.frames:
                for signal in frame.signals:
                    isignalTriggering = createSubElement(
                        isignalTriggerings, 'I-SIGNAL-TRIGGERING')
                    createSubElement(isignalTriggering,
                                     'SHORT-NAME', signal.name)
                    iSignalPortRefs = createSubElement(
                        isignalTriggering, 'I-SIGNAL-PORT-REFS')

                    for receiver in signal.receiver:
                        iSignalPortRef = createSubElement(
                            iSignalPortRefs,
                            'I-SIGNAL-PORT-REF',
                            '/ECU/' +
                            receiver +
                            '/CN_' +
                            receiver +
                            '/' +
                            signal.name)
                        iSignalPortRef.set('DEST', 'SIGNAL-PORT')

                    isignalRef = createSubElement(
                        isignalTriggering, 'SIGNAL-REF')
                    isignalRef.set('DEST', 'I-SIGNAL')
                    isignalRef.text = ""/ISignal/"" + signal.name
        else:
            isignalTriggerings = createSubElement(
                physicalChannel, 'I-SIGNAL-TRIGGERINGS')
            for frame in db.frames:
                for signal in frame.signals:
                    isignalTriggering = createSubElement(
                        isignalTriggerings, 'I-SIGNAL-TRIGGERING')
                    createSubElement(isignalTriggering,
                                     'SHORT-NAME', signal.name)
                    iSignalPortRefs = createSubElement(
                        isignalTriggering, 'I-SIGNAL-PORT-REFS')
                    for receiver in signal.receiver:
                        iSignalPortRef = createSubElement(
                            iSignalPortRefs,
                            'I-SIGNAL-PORT-REF',
                            '/ECU/' +
                            receiver +
                            '/CN_' +
                            receiver +
                            '/' +
                            signal.name)
                        iSignalPortRef.set('DEST', 'I-SIGNAL-PORT')

                    isignalRef = createSubElement(
                        isignalTriggering, 'I-SIGNAL-REF')
                    isignalRef.set('DEST', 'I-SIGNAL')
                    isignalRef.text = ""/ISignal/"" + signal.name
            ipduTriggerings = createSubElement(
                physicalChannel, 'PDU-TRIGGERINGS')
            for frame in db.frames:
                ipduTriggering = createSubElement(
                    ipduTriggerings, 'PDU-TRIGGERING')
                createSubElement(
                    ipduTriggering,
                    'SHORT-NAME',
                    ""IPDUTRIGG_"" +
                    frame.name)
                # missing: I-PDU-PORT-REFS
                ipduRef = createSubElement(ipduTriggering, 'I-PDU-REF')
                ipduRef.set('DEST', 'I-SIGNAL-I-PDU')
                ipduRef.text = ""/PDU/PDU_"" + frame.name
                # missing: I-SIGNAL-TRIGGERINGS

# TODO
#        ipduTriggerings = createSubElement(physicalChannel, 'PDU-TRIGGERINGS')
#        for frame in db.frames:
#            ipduTriggering = createSubElement(ipduTriggerings, 'PDU-TRIGGERING')
#            createSubElement(ipduTriggering, 'SHORT-NAME', ""PDUTRIGG_"" + frame.name)
#            ipduRef = createSubElement(ipduTriggering, 'I-PDU-REF')
#            ipduRef.set('DEST','SIGNAL-I-PDU')
#            ipduRef.text = ""/PDU/PDU_"" + frame.name

    #
    # AR-PACKAGE FRAME
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    if arVersion[0] == ""3"":
        createSubElement(arPackage, 'SHORT-NAME', 'Frame')
    else:
        createSubElement(arPackage, 'SHORT-NAME', 'CanFrame')

    elements = createSubElement(arPackage, 'ELEMENTS')
    for name in dbs:
        db = dbs[name]
        # TODO: reused frames will be paced multiple times in file
        for frame in db.frames:
            if arVersion[0] == ""3"":
                frameEle = createSubElement(elements, 'FRAME')
            else:
                frameEle = createSubElement(elements, 'CAN-FRAME')
            createSubElement(frameEle, 'SHORT-NAME', ""FRAME_"" + frame.name)
            if frame.comment:
                desc = createSubElement(frameEle, 'DESC')
                l2 = createSubElement(desc, 'L-2')
                l2.set(""L"", ""FOR-ALL"")
                l2.text = frame.comment
            createSubElement(frameEle, 'FRAME-LENGTH', ""%d"" % frame.size)
            pdumappings = createSubElement(frameEle, 'PDU-TO-FRAME-MAPPINGS')
            pdumapping = createSubElement(pdumappings, 'PDU-TO-FRAME-MAPPING')
            createSubElement(pdumapping, 'SHORT-NAME', frame.name)
            createSubElement(
                pdumapping,
                'PACKING-BYTE-ORDER',
                ""MOST-SIGNIFICANT-BYTE-LAST"")
            pduRef = createSubElement(pdumapping, 'PDU-REF')
            createSubElement(pdumapping, 'START-POSITION', '0')
            pduRef.text = ""/PDU/PDU_"" + frame.name
            if arVersion[0] == ""3"":
                pduRef.set('DEST', 'SIGNAL-I-PDU')
            else:
                pduRef.set('DEST', 'I-SIGNAL-I-PDU')

    #
    # AR-PACKAGE PDU
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'PDU')
    elements = createSubElement(arPackage, 'ELEMENTS')
    for name in dbs:
        db = dbs[name]
        for frame in db.frames:
            if arVersion[0] == ""3"":
                signalIpdu = createSubElement(elements, 'SIGNAL-I-PDU')
                createSubElement(signalIpdu, 'SHORT-NAME', ""PDU_"" + frame.name)
                createSubElement(signalIpdu, 'LENGTH', ""%d"" %
                                 int(frame.size * 8))
            else:
                signalIpdu = createSubElement(elements, 'I-SIGNAL-I-PDU')
                createSubElement(signalIpdu, 'SHORT-NAME', ""PDU_"" + frame.name)
                createSubElement(signalIpdu, 'LENGTH', ""%d"" % int(frame.size))

            # I-PDU-TIMING-SPECIFICATION
            if arVersion[0] == ""3"":
                signalToPduMappings = createSubElement(
                    signalIpdu, 'SIGNAL-TO-PDU-MAPPINGS')
            else:
                signalToPduMappings = createSubElement(
                    signalIpdu, 'I-SIGNAL-TO-PDU-MAPPINGS')

            for signal in frame.signals:
                signalToPduMapping = createSubElement(
                    signalToPduMappings, 'I-SIGNAL-TO-I-PDU-MAPPING')
                createSubElement(signalToPduMapping, 'SHORT-NAME', signal.name)

                if arVersion[0] == ""3"":
                    if signal.is_little_endian == 1:  # Intel
                        createSubElement(
                            signalToPduMapping,
                            'PACKING-BYTE-ORDER',
                            'MOST-SIGNIFICANT-BYTE-LAST')
                    else:  # Motorola
                        createSubElement(
                            signalToPduMapping,
                            'PACKING-BYTE-ORDER',
                            'MOST-SIGNIFICANT-BYTE-FIRST')
                    signalRef = createSubElement(
                        signalToPduMapping, 'SIGNAL-REF')
                else:
                    signalRef = createSubElement(
                        signalToPduMapping, 'I-SIGNAL-REF')
                    if signal.is_little_endian == 1:  # Intel
                        createSubElement(
                            signalToPduMapping,
                            'PACKING-BYTE-ORDER',
                            'MOST-SIGNIFICANT-BYTE-LAST')
                    else:  # Motorola
                        createSubElement(
                            signalToPduMapping,
                            'PACKING-BYTE-ORDER',
                            'MOST-SIGNIFICANT-BYTE-FIRST')
                signalRef.text = ""/ISignal/"" + signal.name
                signalRef.set('DEST', 'I-SIGNAL')

                createSubElement(signalToPduMapping, 'START-POSITION',
                                 str(signal.getStartbit(bitNumbering=1)))
                # missing: TRANSFER-PROPERTY: PENDING/...

            for group in frame.signalGroups:
                signalToPduMapping = createSubElement(
                    signalToPduMappings, 'I-SIGNAL-TO-I-PDU-MAPPING')
                createSubElement(signalToPduMapping, 'SHORT-NAME', group.name)
                signalRef = createSubElement(signalToPduMapping, 'SIGNAL-REF')
                signalRef.text = ""/ISignal/"" + group.name
                signalRef.set('DEST', 'I-SIGNAL')
                # TODO: TRANSFER-PROPERTY: PENDING???

    #
    # AR-PACKAGE ISignal
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'ISignal')
    elements = createSubElement(arPackage, 'ELEMENTS')
    for name in dbs:
        db = dbs[name]
        for frame in db.frames:
            for signal in frame.signals:
                signalEle = createSubElement(elements, 'I-SIGNAL')
                createSubElement(signalEle, 'SHORT-NAME', signal.name)
                if arVersion[0] == ""4"":
                    createSubElement(signalEle, 'LENGTH',
                                     str(signal.signalsize))

                    networkRepresentProps = createSubElement(
                        signalEle, 'NETWORK-REPRESENTATION-PROPS')
                    swDataDefPropsVariants = createSubElement(
                        networkRepresentProps, 'SW-DATA-DEF-PROPS-VARIANTS')
                    swDataDefPropsConditional = createSubElement(
                        swDataDefPropsVariants, 'SW-DATA-DEF-PROPS-CONDITIONAL')
                    
                    baseTypeRef = createSubElement(swDataDefPropsConditional, 'BASE-TYPE-REF')
                    baseTypeRef.set('DEST', 'SW-BASE-TYPE')
                    createType, size = getBaseTypeOfSignal(signal)
                    baseTypeRef.text = ""/DataType/"" + createType
                    compuMethodRef = createSubElement(
                        swDataDefPropsConditional,
                        'COMPU-METHOD-REF',
                        '/DataType/Semantics/' + signal.name)
                    compuMethodRef.set('DEST', 'COMPU-METHOD')
                    unitRef = createSubElement(
                        swDataDefPropsConditional,
                        'UNIT-REF',
                        '/DataType/Unit/' + signal.name)
                    unitRef.set('DEST', 'UNIT')

                sysSigRef = createSubElement(signalEle, 'SYSTEM-SIGNAL-REF')
                sysSigRef.text = ""/Signal/"" + signal.name

                sysSigRef.set('DEST', 'SYSTEM-SIGNAL')
            for group in frame.signalGroups:
                signalEle = createSubElement(elements, 'I-SIGNAL')
                createSubElement(signalEle, 'SHORT-NAME', group.name)
                sysSigRef = createSubElement(signalEle, 'SYSTEM-SIGNAL-REF')
                sysSigRef.text = ""/Signal/"" + group.name
                sysSigRef.set('DEST', 'SYSTEM-SIGNAL-GROUP')

    #
    # AR-PACKAGE Signal
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'Signal')
    elements = createSubElement(arPackage, 'ELEMENTS')
    for name in dbs:
        db = dbs[name]
        for frame in db.frames:
            for signal in frame.signals:
                signalEle = createSubElement(elements, 'SYSTEM-SIGNAL')
                createSubElement(signalEle, 'SHORT-NAME', signal.name)
                if signal.comment:
                    desc = createSubElement(signalEle, 'DESC')
                    l2 = createSubElement(desc, 'L-2')
                    l2.set(""L"", ""FOR-ALL"")
                    l2.text = signal.comment
                if arVersion[0] == ""3"":
                    dataTypeRef = createSubElement(signalEle, 'DATA-TYPE-REF')
                    if signal.is_float:
                        dataTypeRef.set('DEST', 'REAL-TYPE')
                    else:
                        dataTypeRef.set('DEST', 'INTEGER-TYPE')
                    dataTypeRef.text = ""/DataType/"" + signal.name
                    createSubElement(signalEle, 'LENGTH',
                                     str(signal.signalsize))
            for group in frame.signalGroups:
                groupEle = createSubElement(elements, 'SYSTEM-SIGNAL-GROUP')
                createSubElement(signalEle, 'SHORT-NAME', group.name)
                if arVersion[0] == ""3"":
                    dataTypeRef.set('DEST', 'INTEGER-TYPE')
                sysSignalRefs = createSubElement(
                    groupEle, 'SYSTEM-SIGNAL-REFS')
                for member in group.signals:
                    memberEle = createSubElement(
                        sysSignalRefs, 'SYSTEM-SIGNAL-REF')
                    memberEle.set('DEST', 'SYSTEM-SIGNAL')
                    memberEle.text = ""/Signal/"" + member.name

#                       initValueRef = createSubElement(signalEle, 'INIT-VALUE-REF')
#                       initValueRef.set('DEST','INTEGER-LITERAL')
#                       initValueRef.text = ""/CONSTANTS/"" + signal.name

    #
    # AR-PACKAGE Datatype
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'DataType')
    elements = createSubElement(arPackage, 'ELEMENTS')

    if arVersion[0] == ""3"":
        for name in dbs:
            db = dbs[name]
            for frame in db.frames:
                for signal in frame.signals:
                    if signal.is_float:
                        typeEle = createSubElement(elements, 'REAL-TYPE')
                    else:
                        typeEle = createSubElement(elements, 'INTEGER-TYPE')
                    createSubElement(typeEle, 'SHORT-NAME', signal.name)
                    swDataDefProps = createSubElement(
                        typeEle, 'SW-DATA-DEF-PROPS')
                    if signal.is_float:
                        encoding = createSubElement(typeEle, 'ENCODING')                        
                        if signal.signalsize > 32:
                            encoding.text = ""DOUBLE""
                        else:
                            encoding.text = ""SINGLE""
                    compuMethodRef = createSubElement(
                        swDataDefProps, 'COMPU-METHOD-REF')
                    compuMethodRef.set('DEST', 'COMPU-METHOD')
                    compuMethodRef.text = ""/DataType/Semantics/"" + signal.name
    else:
        createdTypes = []
        for name in dbs:
            db = dbs[name]
            for frame in db.frames:
                for signal in frame.signals:
                    createType, size = getBaseTypeOfSignal(signal)
                    if createType not in createdTypes:
                        createdTypes.append(createType)
                        swBaseType = createSubElement(elements, 'SW-BASE-TYPE')
                        sname = createSubElement(swBaseType, 'SHORT-NAME')
                        sname.text = createType
                        cat = createSubElement(swBaseType, 'CATEGORY')
                        cat.text = ""FIXED_LENGTH""
                        baseTypeSize = createSubElement(swBaseType, 'BASE-TYPE-SIZE')
                        baseTypeSize.text = str(size)
                        if signal.is_float:
                            enc = createSubElement(swBaseType, 'BASE-TYPE-ENCODING')
                            enc.text = ""IEEE754""

    if arVersion[0] == ""3"":
        subpackages = createSubElement(arPackage, 'SUB-PACKAGES')
    else:
        subpackages = createSubElement(arPackage, 'AR-PACKAGES')
    arPackage = createSubElement(subpackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'Semantics')
    elements = createSubElement(arPackage, 'ELEMENTS')
    for name in dbs:
        db = dbs[name]
        for frame in db.frames:
            for signal in frame.signals:
                compuMethod = createSubElement(elements, 'COMPU-METHOD')
                createSubElement(compuMethod, 'SHORT-NAME', signal.name)
                # missing: UNIT-REF
                compuIntToPhys = createSubElement(
                    compuMethod, 'COMPU-INTERNAL-TO-PHYS')
                compuScales = createSubElement(compuIntToPhys, 'COMPU-SCALES')
                for value in sorted(signal.values, key=lambda x: int(x)):
                    compuScale = createSubElement(compuScales, 'COMPU-SCALE')
                    desc = createSubElement(compuScale, 'DESC')
                    l2 = createSubElement(desc, 'L-2')
                    l2.set('L', 'FOR-ALL')
                    l2.text = signal.values[value]
                    createSubElement(compuScale, 'LOWER-LIMIT', str(value))
                    createSubElement(compuScale, 'UPPER-LIMIT', str(value))
                    compuConst = createSubElement(compuScale, 'COMPU-CONST')
                    createSubElement(compuConst, 'VT', signal.values[value])
                else:
                    compuScale = createSubElement(compuScales, 'COMPU-SCALE')
    #                createSubElement(compuScale, 'LOWER-LIMIT', str(#TODO))
    #                createSubElement(compuScale, 'UPPER-LIMIT', str(#TODO))
                    compuRationslCoeff = createSubElement(
                        compuScale, 'COMPU-RATIONAL-COEFFS')
                    compuNumerator = createSubElement(
                        compuRationslCoeff, 'COMPU-NUMERATOR')
                    createSubElement(compuNumerator, 'V', ""%g"" % signal.offset)
                    createSubElement(compuNumerator, 'V', ""%g"" % signal.factor)
                    compuDenomiator = createSubElement(
                        compuRationslCoeff, 'COMPU-DENOMINATOR')
                    createSubElement(compuDenomiator, 'V', ""1"")

    arPackage = createSubElement(subpackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'Unit')
    elements = createSubElement(arPackage, 'ELEMENTS')
    for name in dbs:
        db = dbs[name]
        for frame in db.frames:
            for signal in frame.signals:
                unit = createSubElement(elements, 'UNIT')
                createSubElement(unit, 'SHORT-NAME', signal.name)
                createSubElement(unit, 'DISPLAY-NAME', signal.unit)

    txIPduGroups = {}
    rxIPduGroups = {}

    #
    # AR-PACKAGE ECU
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'ECU')
    elements = createSubElement(arPackage, 'ELEMENTS')
    for name in dbs:
        db = dbs[name]
        for ecu in db.boardUnits:
            ecuInstance = createSubElement(elements, 'ECU-INSTANCE')
            createSubElement(ecuInstance, 'SHORT-NAME', ecu.name)
            if ecu.comment:
                desc = createSubElement(ecuInstance, 'DESC')
                l2 = createSubElement(desc, 'L-2')
                l2.set('L', 'FOR-ALL')
                l2.text = ecu.comment

            if arVersion[0] == ""3"":
                assoIpduGroupRefs = createSubElement(
                    ecuInstance, 'ASSOCIATED-I-PDU-GROUP-REFS')
                connectors = createSubElement(ecuInstance, 'CONNECTORS')
                commConnector = createSubElement(
                    connectors, 'COMMUNICATION-CONNECTOR')
            else:
                assoIpduGroupRefs = createSubElement(
                    ecuInstance, 'ASSOCIATED-COM-I-PDU-GROUP-REFS')
                connectors = createSubElement(ecuInstance, 'CONNECTORS')
                commConnector = createSubElement(
                    connectors, 'CAN-COMMUNICATION-CONNECTOR')

            createSubElement(commConnector, 'SHORT-NAME', 'CN_' + ecu.name)
            ecuCommPortInstances = createSubElement(
                commConnector, 'ECU-COMM-PORT-INSTANCES')

            recTemp = None
            sendTemp = None

            for frame in db.frames:
                if ecu.name in frame.transmitter:
                    frameport = createSubElement(
                        ecuCommPortInstances, 'FRAME-PORT')
                    createSubElement(frameport, 'SHORT-NAME', frame.name)
                    createSubElement(
                        frameport, 'COMMUNICATION-DIRECTION', 'OUT')
                    sendTemp = 1
                    if ecu.name + ""_Tx"" not in txIPduGroups:
                        txIPduGroups[ecu.name + ""_Tx""] = []
                    txIPduGroups[ecu.name + ""_Tx""].append(frame.name)

                    # missing I-PDU-PORT
                    for signal in frame.signals:
                        if arVersion[0] == ""3"":
                            signalPort = createSubElement(
                                ecuCommPortInstances, 'SIGNAL-PORT')
                        else:
                            signalPort = createSubElement(
                                ecuCommPortInstances, 'I-SIGNAL-PORT')

                        createSubElement(signalPort, 'SHORT-NAME', signal.name)
                        createSubElement(
                            signalPort, 'COMMUNICATION-DIRECTION', 'OUT')
                if ecu.name in frame.receiver:
                    frameport = createSubElement(
                        ecuCommPortInstances, 'FRAME-PORT')
                    createSubElement(frameport, 'SHORT-NAME', frame.name)
                    createSubElement(
                        frameport, 'COMMUNICATION-DIRECTION', 'IN')
                    recTemp = 1
                    if ecu.name + ""_Tx"" not in rxIPduGroups:
                        rxIPduGroups[ecu.name + ""_Rx""] = []
                    rxIPduGroups[ecu.name + ""_Rx""].append(frame.name)

                    # missing I-PDU-PORT
                    for signal in frame.signals:
                        if ecu.name in signal.receiver:
                            if arVersion[0] == ""3"":
                                signalPort = createSubElement(
                                    ecuCommPortInstances, 'SIGNAL-PORT')
                            else:
                                signalPort = createSubElement(
                                    ecuCommPortInstances, 'I-SIGNAL-PORT')

                            createSubElement(
                                signalPort, 'SHORT-NAME', signal.name)
                            createSubElement(
                                signalPort, 'COMMUNICATION-DIRECTION', 'IN')

            if recTemp is not None:
                if arVersion[0] == ""3"":
                    assoIpduGroupRef = createSubElement(
                        assoIpduGroupRefs, 'ASSOCIATED-I-PDU-GROUP-REF')
                    assoIpduGroupRef.set('DEST', ""I-PDU-GROUP"")
                else:
                    assoIpduGroupRef = createSubElement(
                        assoIpduGroupRefs, 'ASSOCIATED-COM-I-PDU-GROUP-REF')
                    assoIpduGroupRef.set('DEST', ""I-SIGNAL-I-PDU-GROUP"")

                assoIpduGroupRef.text = ""/IPDUGroup/"" + ecu.name + ""_Rx""

            if sendTemp is not None:
                if arVersion[0] == ""3"":
                    assoIpduGroupRef = createSubElement(
                        assoIpduGroupRefs, 'ASSOCIATED-I-PDU-GROUP-REF')
                    assoIpduGroupRef.set('DEST', ""I-PDU-GROUP"")
                else:
                    assoIpduGroupRef = createSubElement(
                        assoIpduGroupRefs, 'ASSOCIATED-COM-I-PDU-GROUP-REF')
                    assoIpduGroupRef.set('DEST', ""I-SIGNAL-I-PDU-GROUP"")
                assoIpduGroupRef.text = ""/IPDUGroup/"" + ecu.name + ""_Tx""

    #
    # AR-PACKAGE IPDUGroup
    #
    arPackage = createSubElement(toplevelPackages, 'AR-PACKAGE')
    createSubElement(arPackage, 'SHORT-NAME', 'IPDUGroup')
    elements = createSubElement(arPackage, 'ELEMENTS')
    for pdugrp in txIPduGroups:
        if arVersion[0] == ""3"":
            ipduGrp = createSubElement(elements, 'I-PDU-GROUP')
        else:
            ipduGrp = createSubElement(elements, 'I-SIGNAL-I-PDU-GROUP')

        createSubElement(ipduGrp, 'SHORT-NAME', pdugrp)
        createSubElement(ipduGrp, 'COMMUNICATION-DIRECTION', ""OUT"")

        if arVersion[0] == ""3"":
            ipduRefs = createSubElement(ipduGrp, 'I-PDU-REFS')
            for frame in txIPduGroups[pdugrp]:
                ipduRef = createSubElement(ipduRefs, 'I-PDU-REF')
                ipduRef.set('DEST', ""SIGNAL-I-PDU"")
                ipduRef.text = ""/PDU/PDU_"" + frame
        else:
            isignalipdus = createSubElement(ipduGrp, 'I-SIGNAL-I-PDUS')
            for frame in txIPduGroups[pdugrp]:
                isignalipdurefconditional = createSubElement(
                    isignalipdus, 'I-SIGNAL-I-PDU-REF-CONDITIONAL')
                ipduRef = createSubElement(
                    isignalipdurefconditional, 'I-SIGNAL-I-PDU-REF')
                ipduRef.set('DEST', ""I-SIGNAL-I-PDU"")
                ipduRef.text = ""/PDU/PDU_"" + frame

    if arVersion[0] == ""3"":
        for pdugrp in rxIPduGroups:
            ipduGrp = createSubElement(elements, 'I-PDU-GROUP')
            createSubElement(ipduGrp, 'SHORT-NAME', pdugrp)
            createSubElement(ipduGrp, 'COMMUNICATION-DIRECTION', ""IN"")

            ipduRefs = createSubElement(ipduGrp, 'I-PDU-REFS')
            for frame in rxIPduGroups[pdugrp]:
                ipduRef = createSubElement(ipduRefs, 'I-PDU-REF')
                ipduRef.set('DEST', ""SIGNAL-I-PDU"")
                ipduRef.text = ""/PDU/PDU_"" + frame
    else:
        for pdugrp in rxIPduGroups:
            ipduGrp = createSubElement(elements, 'I-SIGNAL-I-PDU-GROUP')
            createSubElement(ipduGrp, 'SHORT-NAME', pdugrp)
            createSubElement(ipduGrp, 'COMMUNICATION-DIRECTION', ""IN"")
            isignalipdus = createSubElement(ipduGrp, 'I-SIGNAL-I-PDUS')
            for frame in rxIPduGroups[pdugrp]:
                isignalipdurefconditional = createSubElement(
                    isignalipdus, 'I-SIGNAL-I-PDU-REF-CONDITIONAL')
                ipduRef = createSubElement(
                    isignalipdurefconditional, 'I-SIGNAL-I-PDU-REF')
                ipduRef.set('DEST', ""I-SIGNAL-I-PDU"")
                ipduRef.text = ""/PDU/PDU_"" + frame

    f.write(etree.tostring(root, pretty_print=True, xml_declaration=True))",000_Didnt Work
"def dump(db, thefile, delimiter=',', **options):
    head_top = [
        'ID',
        'Frame Name',
        'Cycle Time [ms]',
        'Launch Type',
        'Launch Parameter',
        'Signal Byte No.',
        'Signal Bit No.',
        'Signal Name',
        'Signal Function',
        'Signal Length [Bit]',
        'Signal Default',
        ' Signal Not Available',
        'Byteorder',
        'is signed']
    head_tail = ['Value', 'Name / Phys. Range', 'Function / Increment Unit']

    csvtable = list()  # List holding all csv rows

    col = 0  # Column counter

    # -- headers start:
    headerrow = csvRow()

    # write first row (header) cols before frameardunits:
    for head in head_top:
        headerrow.write(col, head)
        col += 1

    # write frameardunits in first row:
    buList = []
    for bu in db.boardUnits:
        headerrow.write(col, bu.name)
        buList.append(bu.name)
        col += 1

    # write first row (header) cols after frameardunits:
    for head in head_tail:
        headerrow.write(col, head)
        col += 1

    csvtable.append(headerrow)
    # -- headers end...

    frameHash = {}
    for frame in db.frames:
        frameHash[int(frame.id)] = frame

    # set row to first Frame (row = 0 is header)
    row = 1

    # iterate over the frames
    for idx in sorted(frameHash.keys()):
        frame = frameHash[idx]

        # sort signals:
        sigHash = {}
        for sig in frame.signals:
            sigHash[""%02d"" % int(sig.getStartbit()) + sig.name] = sig

        # iterate over signals
        for sig_idx in sorted(sigHash.keys()):
            sig = sigHash[sig_idx]

            # value table available?
            if sig.values.__len__() > 0:
                # iterate over values in valuetable
                for val in sorted(sig.values.keys()):
                    signalRow = csvRow()
                    writeFramex(frame, signalRow)
                    col = head_top.__len__()
                    col = writeBuMatrixx(buList, sig, frame, signalRow, col)
                    # write Value
                    writeValuex(val, sig.values[val], signalRow, col)
                    writeSignalx(db, sig, signalRow, col)

                    # no min/max here, because min/max has same col as values.
                    # next row
                    row += 1
                    csvtable.append(signalRow)
                # loop over values ends here
            # no value table available
            else:
                signalRow = csvRow()
                writeFramex(frame, signalRow)
                col = head_top.__len__()
                col = writeBuMatrixx(buList, sig, frame, signalRow, col)
                writeSignalx(db, sig, signalRow, col)

                if sig.min is not None or sig.max is not None:
                    signalRow[col + 1] = str(""{}..{}"".format(sig.min, sig.max))

                # next row
                row += 1
                csvtable.append(signalRow)
                # set style to normal - without border
        # loop over signals ends here
    # loop over frames ends here

    if (sys.version_info > (3, 0)):
        import io
        temp = io.TextIOWrapper(thefile, encoding='UTF-8')
    else:
        temp = thefile

    writer = csv.writer(temp, delimiter=delimiter)
    for row in csvtable:
        writer.writerow(row.as_list)","1. Use `csv.DictWriter` to write the CSV file instead of manually creating rows. This will prevent you from accidentally omitting or mistyping column headers.
2. Use `csv.quote()` to escape special characters in the data. This will prevent attackers from injecting malicious code into the CSV file.
3. Use `os.path.join()` to concatenate paths instead of using the `+` operator. This will prevent attackers from exploiting path traversal vulnerabilities."
"def dump(db, f, **options):
    if 'dbfExportEncoding' in options:
        dbfExportEncoding = options[""dbfExportEncoding""]
    else:
        dbfExportEncoding = 'iso-8859-1'

    outstr =  """"""//******************************BUSMASTER Messages and signals Database ******************************//

[DATABASE_VERSION] 1.3

[PROTOCOL] CAN

[BUSMASTER_VERSION] [1.7.2]
[NUMBER_OF_MESSAGES] """"""

    outstr += str(len(db.frames)) + ""\\n""

    # Frames
    for frame in db.frames:
        # Name unMsgId m_ucLength m_ucNumOfSignals m_cDataFormat m_cFrameFormat? m_txNode
        # m_cDataFormat If 1 dataformat Intel, 0- Motorola -- immer 1 original Converter macht das nach anzahl entsprechender Signale
        # cFrameFormat Standard 'S' Extended 'X'
        extended = 'S'
        if frame.extended == 1:
            extended = 'X'
        outstr += ""[START_MSG] "" + frame.name + \\
            "",%d,%d,%d,1,%c,"" % (frame.id, frame.size,
                                 len(frame.signals), extended)
        if frame.transmitter.__len__() == 0:
            frame.addTransmitter(""Vector__XXX"")
# DBF does not support multiple Transmitters
        outstr += frame.transmitter[0] + ""\\n""

        for signal in frame.signals:
            # m_acName ucLength m_ucWhichByte m_ucStartBit
            # m_ucDataFormat m_fOffset m_fScaleFactor m_acUnit m_acMultiplex m_rxNode
            # m_ucDataFormat
            whichbyte = int(
                math.floor(
                    signal.getStartbit(
                        bitNumbering=1,
                        startLittle=True) /
                    8) +
                1)
            sign = 'S'

            if not signal.is_signed:
                sign = 'U'
            
            if signal.is_float:
                if signal.signalsize > 32:
                    sign = 'D'
                else:
                    sign = 'F'

            if signal.factor == 0:
                signal.factor = 1

            outstr += ""[START_SIGNALS] "" + signal.name + "",%d,%d,%d,%c,"" % (signal.signalsize,
                                                                            whichbyte,
                                                                            int(signal.getStartbit(bitNumbering=1,
                                                                                                   startLittle=True)) % 8,
                                                                            sign) + '{},{}'.format(float(signal.max) / float(signal.factor),
                                                                                                   float(signal.min) / float(signal.factor))

            outstr += "",%d,%s,%s"" % (signal.is_little_endian,
                                     signal.offset, signal.factor)
            multiplex = """"
            if signal.multiplex is not None:
                if signal.multiplex == 'Multiplexor':
                    multiplex = 'M'
                else:
                    multiplex = 'm' + str(signal.multiplex)

            outstr += "","" + signal.unit + "",%s,"" % multiplex + \\
                ','.join(signal.receiver) + ""\\n""

            if len(signal.values) > 0:
                for attrib, val in sorted(list(signal.values.items())):
                    outstr += '[VALUE_DESCRIPTION] ""' + \\
                        val + '"",' + str(attrib) + '\\n'

        outstr += ""[END_MSG]\\n\\n""

    # Boardunits
    outstr += ""[NODE] ""
    count = 1
    for bu in db.boardUnits:
        outstr += bu.name
        if count < len(db.boardUnits):
            outstr += "",""
        count += 1
    outstr += ""\\n""

    outstr += ""[START_DESC]\\n\\n""

    # BU-descriptions
    outstr += ""[START_DESC_MSG]\\n""
    for frame in db.frames:
        if frame.comment is not None:
            comment = frame.comment.replace(""\\n"", "" "")
            outstr += str(frame.id) + ' S ""' + comment + '"";\\n'

    outstr += ""[END_DESC_MSG]\\n""

    # Frame descriptions
    outstr += ""[START_DESC_NODE]\\n""
    for bu in db.boardUnits:
        if bu.comment is not None:
            comment = bu.comment.replace(""\\n"", "" "")
            outstr += bu.name + ' ""' + comment + '"";\\n'

    outstr += ""[END_DESC_NODE]\\n""

    # signal descriptions
    outstr += ""[START_DESC_SIG]\\n""
    for frame in db.frames:
        for signal in frame.signals:
            if signal.comment is not None:
                comment = signal.comment.replace(""\\n"", "" "")
                outstr += ""%d S "" % frame.id + signal.name + ' ""' + comment + '"";\\n'

    outstr += ""[END_DESC_SIG]\\n""
    outstr += ""[END_DESC]\\n\\n""

    outstr += ""[START_PARAM]\\n""
    # db-parameter
    outstr += ""[START_PARAM_NET]\\n""
    for (type, define) in list(db.globalDefines.items()):
        defaultVal = define.defaultValue
        if defaultVal is None:
            defaultVal = ""0""
        outstr += '""' + type + '"",' + define.definition.replace(' ', ',') + ',' + defaultVal + '\\n'
    outstr += ""[END_PARAM_NET]\\n""

    # bu-parameter
    outstr += ""[START_PARAM_NODE]\\n""
    for (type, define) in list(db.buDefines.items()):
        defaultVal = define.defaultValue
        if defaultVal is None:
            defaultVal = ""0""
        outstr += '""' + type + '"",' + define.definition.replace(' ', ',') + ',' + defaultVal + '\\n'
    outstr += ""[END_PARAM_NODE]\\n""

    # frame-parameter
    outstr += ""[START_PARAM_MSG]\\n""
    for (type, define) in list(db.frameDefines.items()):
        defaultVal = define.defaultValue
        if defaultVal is None:
            defaultVal = ""0""
        outstr += '""' + type + '"",' + define.definition.replace(' ', ',') + ',' + defaultVal + '\\n'

    outstr += ""[END_PARAM_MSG]\\n""

    # signal-parameter
    outstr += ""[START_PARAM_SIG]\\n""
    for (type, define) in list(db.signalDefines.items()):
        defaultVal = define.defaultValue
        if defaultVal is None:
            defaultVal = ""0""
        outstr += '""' + type + '"",' + define.definition.replace(' ', ',') + ',' + defaultVal + '\\n'
    outstr += ""[END_PARAM_SIG]\\n""

    outstr += ""[START_PARAM_VAL]\\n""
    # boardunit-attributes:
    outstr += ""[START_PARAM_NODE_VAL]\\n""
    for bu in db.boardUnits:
        for attrib, val in sorted(list(bu.attributes.items())):
            outstr += bu.name + ',""' + attrib + '"",""' + val + '""\\n'
    outstr += ""[END_PARAM_NODE_VAL]\\n""

    # messages-attributes:
    outstr += ""[START_PARAM_MSG_VAL]\\n""
    for frame in db.frames:
        for attrib, val in sorted(list(frame.attributes.items())):
            outstr += str(frame.id) + ',S,""' + attrib + '"",""' + val + '""\\n'
    outstr += ""[END_PARAM_MSG_VAL]\\n""

    # signal-attributes:
    outstr += ""[START_PARAM_SIG_VAL]\\n""
    for frame in db.frames:
        for signal in frame.signals:
            for attrib, val in sorted(list(signal.attributes.items())):
                outstr += str(frame.id) + ',S,' + signal.name + \\
                    ',""' + attrib + '"",""' + val + '""\\n'
    outstr += ""[END_PARAM_SIG_VAL]\\n""
    outstr += ""[END_PARAM_VAL]\\n""
    f.write(outstr.encode(dbfExportEncoding))","1. Use a secure password hashing function, such as bcrypt or scrypt.
2. Use a salt with the password hash.
3. Store the password hashes in a secure location."
"def dump(db, file, **options):
    head_top = ['ID', 'Frame Name', 'Cycle Time [ms]', 'Launch Type', 'Launch Parameter', 'Signal Byte No.', 'Signal Bit No.',
                'Signal Name', 'Signal Function', 'Signal Length [Bit]', 'Signal Default', ' Signal Not Available', 'Byteorder']
    head_tail = ['Value',   'Name / Phys. Range', 'Function / Increment Unit']

    if 'xlsMotorolaBitFormat' in options:
        motorolaBitFormat = options[""xlsMotorolaBitFormat""]
    else:
        motorolaBitFormat = ""msbreverse""

    workbook = xlwt.Workbook(encoding='utf8')
#    wsname = os.path.basename(filename).replace('.xls', '')
#    worksheet = workbook.add_sheet('K-Matrix ' + wsname[0:22])
    worksheet = workbook.add_sheet('K-Matrix ')
    col = 0

    # write first row (header) cols before frameardunits:
    for head in head_top:
        worksheet.write(0, col, label=head, style=sty_header)
        worksheet.col(col).width = 1111
        col += 1

    # write frameardunits in first row:
    buList = []
    for bu in db.boardUnits:
        worksheet.write(0, col, label=bu.name, style=sty_header)
        worksheet.col(col).width = 1111
        buList.append(bu.name)
        col += 1

    head_start = col

    # write first row (header) cols after frameardunits:
    for head in head_tail:
        worksheet.write(0, col, label=head, style=sty_header)
        worksheet.col(col).width = 3333
        col += 1

    # set width of selected Cols
    worksheet.col(1).width = 5555
    worksheet.col(3).width = 3333
    worksheet.col(7).width = 5555
    worksheet.col(8).width = 7777
    worksheet.col(head_start).width = 1111
    worksheet.col(head_start + 1).width = 5555

    frameHash = {}
    for frame in db.frames:
        frameHash[int(frame.id)] = frame

    # set row to first Frame (row = 0 is header)
    row = 1

    # iterate over the frames
    for idx in sorted(frameHash.keys()):
        frame = frameHash[idx]
        framestyle = sty_first_frame

        # sort signals:
        sigHash = {}
        for sig in frame.signals:
            sigHash[""%02d"" % int(sig.getStartbit()) + sig.name] = sig

        # set style for first line with border
        sigstyle = sty_first_frame

        # iterate over signals
        for sig_idx in sorted(sigHash.keys()):
            sig = sigHash[sig_idx]

            # if not first Signal in Frame, set style
            if sigstyle != sty_first_frame:
                sigstyle = sty_norm

            # valuetable available?
            if sig.values.__len__() > 0:
                valstyle = sigstyle
                # iterate over values in valuetable
                for val in sorted(sig.values.keys()):
                    writeFrame(frame, worksheet, row, framestyle)
                    if framestyle != sty_first_frame:
                        worksheet.row(row).level = 1

                    col = head_top.__len__()
                    col = writeBuMatrix(
                        buList, sig, frame, worksheet, row, col, framestyle)
                    # write Value
                    writeValue(
                        val,
                        sig.values[val],
                        worksheet,
                        row,
                        col,
                        valstyle)
                    writeSignal(
                        db,
                        sig,
                        worksheet,
                        row,
                        sigstyle,
                        col,
                        motorolaBitFormat)

                    # no min/max here, because min/max has same col as values...
                    # next row
                    row += 1
                    # set style to normal - without border
                    sigstyle = sty_white
                    framestyle = sty_white
                    valstyle = sty_norm
                # loop over values ends here
            # no valuetable available
            else:
                writeFrame(frame, worksheet, row, framestyle)
                if framestyle != sty_first_frame:
                    worksheet.row(row).level = 1

                col = head_top.__len__()
                col = writeBuMatrix(
                    buList, sig, frame, worksheet, row, col, framestyle)
                writeSignal(
                    db,
                    sig,
                    worksheet,
                    row,
                    sigstyle,
                    col,
                    motorolaBitFormat)

                if float(sig.min) != 0 or float(sig.max) != 1.0:
                    worksheet.write(
                        row,
                        col +
                        1,
                        label=str(
                            ""%g..%g"" %
                            (sig.min,
                             sig.max)),
                        style=sigstyle)
                else:
                    worksheet.write(row, col + 1, label="""", style=sigstyle)

                # just for border
                worksheet.write(row, col, label="""", style=sigstyle)
                # next row
                row += 1
                # set style to normal - without border
                sigstyle = sty_white
                framestyle = sty_white
        # reset signal-Array
        signals = []
        # loop over signals ends here
    # loop over frames ends here

    # frozen headings instead of split panes
    worksheet.set_panes_frozen(True)
    # in general, freeze after last heading row
    worksheet.set_horz_split_pos(1)
    worksheet.set_remove_splits(True)
    # save file
    workbook.save(file)","1. Use `xlswrite` instead of `xlwt` to write Excel files. `xlwt` is an older library that is not as secure as `xlswrite`.
2. Use a secure password when creating the Excel file.
3. Use a secure location to store the Excel file."
"def dump(db, filename, **options):
    if 'xlsMotorolaBitFormat' in options:
        motorolaBitFormat = options[""xlsMotorolaBitFormat""]
    else:
        motorolaBitFormat = ""msbreverse""

    head_top = [
        'ID',
        'Frame Name',
        'Cycle Time [ms]',
        'Launch Type',
        'Launch Parameter',
        'Signal Byte No.',
        'Signal Bit No.',
        'Signal Name',
        'Signal Function',
        'Signal Length [Bit]',
        'Signal Default',
        ' Signal Not Available',
        'Byteorder']
    head_tail = ['Value', 'Name / Phys. Range', 'Function / Increment Unit']

    workbook = xlsxwriter.Workbook(filename)
#    wsname = os.path.basename(filename).replace('.xlsx', '')
#    worksheet = workbook.add_worksheet('K-Matrix ' + wsname[0:22])
    worksheet = workbook.add_worksheet('K-Matrix ')
    col = 0
    global sty_header
    sty_header = workbook.add_format({'bold': True,
                                      'rotation': 90,
                                      'font_name': 'Verdana',
                                      'font_size': 8,
                                      'align': 'center',
                                      'valign': 'center'})
    global sty_first_frame
    sty_first_frame = workbook.add_format({'font_name': 'Verdana',
                                           'font_size': 8,
                                           'font_color': 'black', 'top': 1})
    global sty_white
    sty_white = workbook.add_format({'font_name': 'Verdana',
                                     'font_size': 8,
                                     'font_color': 'white'})
    global sty_norm
    sty_norm = workbook.add_format({'font_name': 'Verdana',
                                    'font_size': 8,
                                    'font_color': 'black'})

# BUMatrix-Styles
    global sty_green
    sty_green = workbook.add_format({'pattern': 1, 'fg_color': '#CCFFCC'})
    global sty_green_first_frame
    sty_green_first_frame = workbook.add_format(
        {'pattern': 1, 'fg_color': '#CCFFCC', 'top': 1})
    global sty_sender
    sty_sender = workbook.add_format({'pattern': 0x04, 'fg_color': '#C0C0C0'})
    global sty_sender_first_frame
    sty_sender_first_frame = workbook.add_format(
        {'pattern': 0x04, 'fg_color': '#C0C0C0', 'top': 1})
    global sty_sender_green
    sty_sender_green = workbook.add_format(
        {'pattern': 0x04, 'fg_color': '#C0C0C0', 'bg_color': '#CCFFCC'})
    global sty_sender_green_first_frame
    sty_sender_green_first_frame = workbook.add_format(
        {'pattern': 0x04, 'fg_color': '#C0C0C0', 'bg_color': '#CCFFCC', 'top': 1})

    # write first row (header) cols before frameardunits:
    for head in head_top:
        worksheet.write(0, col, head, sty_header)
        worksheet.set_column(col, col, 3.57)
        col += 1

    # write frameardunits in first row:
    buList = []
    for bu in db.boardUnits:
        worksheet.write(0, col, bu.name, sty_header)
        worksheet.set_column(col, col, 3.57)
        buList.append(bu.name)
        col += 1

    head_start = col

    # write first row (header) cols after frameardunits:
    for head in head_tail:
        worksheet.write(0, col, head, sty_header)
        worksheet.set_column(col, col, 6)
        col += 1

    # set width of selected Cols
    worksheet.set_column(0, 0, 3.57)
    worksheet.set_column(1, 1, 21)
    worksheet.set_column(3, 3, 12.29)
    worksheet.set_column(7, 7, 21)
    worksheet.set_column(8, 8, 30)
    worksheet.set_column(head_start + 1, head_start + 1, 21)
    worksheet.set_column(head_start + 2, head_start + 2, 12)

    frameHash = {}
    for frame in db.frames:
        frameHash[int(frame.id)] = frame

    # set row to first Frame (row = 0 is header)
    row = 1

    # iterate over the frames
    for idx in sorted(frameHash.keys()):
        frame = frameHash[idx]
        framestyle = sty_first_frame

        # sort signals:
        sigHash = {}
        for sig in frame.signals:
            sigHash[""%02d"" % int(sig.getStartbit()) + sig.name] = sig

        # set style for first line with border
        sigstyle = sty_first_frame

        # iterate over signals
        for sig_idx in sorted(sigHash.keys()):
            sig = sigHash[sig_idx]

            # if not first Signal in Frame, set style
            if sigstyle != sty_first_frame:
                sigstyle = sty_norm
            # valuetable available?
            if sig.values.__len__() > 0:
                valstyle = sigstyle
                # iterate over values in valuetable
                for val in sorted(sig.values.keys()):
                    writeFramex(frame, worksheet, row, framestyle)
                    if framestyle != sty_first_frame:
                        worksheet.set_row(row, None, None, {'level': 1})
                    col = head_top.__len__()
                    col = writeBuMatrixx(
                        buList, sig, frame, worksheet, row, col, framestyle)
                    # write Value
                    writeValuex(
                        val,
                        sig.values[val],
                        worksheet,
                        row,
                        col,
                        valstyle)
                    writeSignalx(db, sig, worksheet, row, col,
                                 sigstyle, motorolaBitFormat)

                    # no min/max here, because min/max has same col as values...
                    # next row
                    row += 1
                    # set style to normal - without border
                    sigstyle = sty_white
                    framestyle = sty_white
                    valstyle = sty_norm
                # loop over values ends here
            # no valuetable available
            else:
                writeFramex(frame, worksheet, row, framestyle)
                if framestyle != sty_first_frame:
                    worksheet.set_row(row, None, None, {'level': 1})
                col = head_top.__len__()
                col = writeBuMatrixx(
                    buList, sig, frame, worksheet, row, col, framestyle)
                writeSignalx(db, sig, worksheet, row, col,
                             sigstyle, motorolaBitFormat)

                if float(sig.min) != 0 or float(sig.max) != 1.0:
                    worksheet.write(row, col + 1, str(""%g..%g"" %
                                                      (sig.min, sig.max)), sigstyle)
                else:
                    worksheet.write(row, col + 1, """", sigstyle)

                # just for border
                worksheet.write(row, col, """", sigstyle)
                # next row
                row += 1
                # set style to normal - without border
                sigstyle = sty_white
                framestyle = sty_white
        # reset signal-Array
        signals = []
        # loop over signals ends here
    # loop over frames ends here

    worksheet.autofilter(0, 0, row, len(head_top) +
                         len(head_tail) + len(db.boardUnits))
    worksheet.freeze_panes(1, 0)
    # save file
    workbook.close()","1. Use more secure functions like `str.encode()` instead of `str.join()`.
2. Use `xlsxwriter.Workbook.add_format()` to create a new format object instead of assigning values directly to the `format` property.
3. Use `xlsxwriter.Workbook.add_worksheet()` to create a new worksheet object instead of assigning values directly to the `worksheet` property."
"def dump(db, f, **options):
    if 'dbcExportEncoding' in options:
        dbcExportEncoding = options[""dbcExportEncoding""]
    else:
        dbcExportEncoding = 'iso-8859-1'
    if 'dbcExportCommentEncoding' in options:
        dbcExportCommentEncoding = options[""dbcExportCommentEncoding""]
    else:
        dbcExportCommentEncoding = dbcExportEncoding
    if 'whitespaceReplacement' in options:
        whitespaceReplacement = options[""whitespaceReplacement""]
        if whitespaceReplacement in ['', None] or set(
                [' ', '\\t']).intersection(whitespaceReplacement):
            print(""Warning: Settings may result in whitespace in DBC variable names.  This is not supported by the DBC format."")
    else:
        whitespaceReplacement = '_'
    if 'writeValTable' in options:
        writeValTable = options[""writeValTable""]
    else:
        writeValTable = True

    f.write(""VERSION \\""created by canmatrix\\""\\n\\n"".encode(dbcExportEncoding))
    f.write(""\\n"".encode(dbcExportEncoding))

    f.write(""NS_ :\\n\\nBS_:\\n\\n"".encode(dbcExportEncoding))

    # Boardunits
    f.write(""BU_: "".encode(dbcExportEncoding))
    id = 1
    nodeList = {}
    for bu in db.boardUnits:
        f.write((bu.name + "" "").encode(dbcExportEncoding))
    f.write(""\\n\\n"".encode(dbcExportEncoding))

    if writeValTable:
        # ValueTables
        for table in db.valueTables:
            f.write((""VAL_TABLE_ "" + table).encode(dbcExportEncoding))
            for row in db.valueTables[table]:
                f.write(
                    (' ' +
                     str(row) +
                     ' ""' +
                     db.valueTables[table][row] +
                     '""').encode(dbcExportEncoding))
            f.write("";\\n"".encode(dbcExportEncoding))
        f.write(""\\n"".encode(dbcExportEncoding))

    output_names = collections.defaultdict(dict)

    for frame in db.frames:
        normalized_names = collections.OrderedDict((
            (s, normalizeName(s.name, whitespaceReplacement))
            for s in frame.signals
        ))

        duplicate_signal_totals = collections.Counter(normalized_names.values())
        duplicate_signal_counter = collections.Counter()

        numbered_names = collections.OrderedDict()

        for signal in frame.signals:
            name = normalized_names[signal]
            duplicate_signal_counter[name] += 1
            if duplicate_signal_totals[name] > 1:
                # TODO: pad to 01 in case of 10+ instances, for example?
                name += str(duplicate_signal_counter[name] - 1)

            output_names[frame][signal] = name

    # Frames
    for bo in db.frames:
        multiplex_written = False
        if bo.transmitter.__len__() == 0:
            bo.addTransmitter(""Vector__XXX"")

        if bo.extended == 1:
            bo.id += 0x80000000
        
        f.write(
            (""BO_ %d "" %
             bo.id +
             bo.name +
             "": %d "" %
             bo.size +
             bo.transmitter[0] +
             ""\\n"").encode(dbcExportEncoding))
        duplicate_signal_totals = collections.Counter(
            normalizeName(s.name, whitespaceReplacement) for s in bo.signals
        )
        duplicate_signal_counter = collections.Counter()
        for signal in bo.signals:
            if signal.multiplex == 'Multiplexor' and multiplex_written and not frame.is_complex_multiplexed:
                continue

            f.write(("" SG_ "" + output_names[bo][signal] + "" "").encode(dbcExportEncoding))
            if signal.mux_val is not None:
                f.write((""m%d"" %
                         int(signal.mux_val)).encode(dbcExportEncoding))
            if signal.multiplex == 'Multiplexor':
                f.write('M'.encode(dbcExportEncoding))
                multiplex_written = True



            startbit = signal.getStartbit(bitNumbering=1)

            if signal.is_signed:
                sign = '-'
            else:
                sign = '+'
            f.write(
                ("" : %d|%d@%d%c"" %
                 (startbit,
                  signal.signalsize,
                  signal.is_little_endian,
                  sign)).encode(dbcExportEncoding))
            f.write(
                ("" (%s,%s)"" %
                 (format_float(signal.factor), format_float(signal.offset))).encode(dbcExportEncoding))
            f.write(
                ("" [{}|{}]"".format(
                    format_float(signal.min),
                    format_float(signal.max))).encode(dbcExportEncoding))
            f.write(' ""'.encode(dbcExportEncoding))

            if signal.unit is None:
                signal.unit = """"
            f.write(signal.unit.encode(dbcExportEncoding))
            f.write('"" '.encode(dbcExportEncoding))
            if signal.receiver.__len__() == 0:
                signal.addReceiver('Vector__XXX')
            f.write((','.join(signal.receiver) + ""\\n"").encode(dbcExportEncoding))
        f.write(""\\n"".encode(dbcExportEncoding))
    f.write(""\\n"".encode(dbcExportEncoding))

    # second Sender:
    for bo in db.frames:
        if bo.transmitter.__len__() > 1:
            f.write(
                (""BO_TX_BU_ %d : %s;\\n"" %
                 (bo.id, ','.join(
                     bo.transmitter))).encode(dbcExportEncoding))

    # frame comments
    for bo in db.frames:
        if bo.comment is not None and bo.comment.__len__() > 0:
            f.write(
                (""CM_ BO_ "" +
                 ""%d "" %
                 bo.id +
                 ' ""').encode(dbcExportEncoding))
            f.write(
                bo.comment.replace(
                    '""',
                    '\\\\""').encode(dbcExportCommentEncoding, 'ignore'))
            f.write('"";\\n'.encode(dbcExportEncoding))
    f.write(""\\n"".encode(dbcExportEncoding))

    # signal comments
    for bo in db.frames:
        for signal in bo.signals:
            if signal.comment is not None and signal.comment.__len__() > 0:
                name = output_names[bo][signal]
                f.write(
                    (""CM_ SG_ "" +
                     ""%d "" %
                     bo.id +
                     name +
                     ' ""').encode(dbcExportEncoding, 'ignore'))
                f.write(
                        signal.comment.replace(
                            '""', '\\\\""').encode(dbcExportCommentEncoding, 'ignore'))
                f.write('"";\\n'.encode(dbcExportEncoding, 'ignore'))

    f.write(""\\n"".encode(dbcExportEncoding))

    # boarUnit comments
    for bu in db.boardUnits:
        if bu.comment is not None and bu.comment.__len__() > 0:
            f.write(
                (""CM_ BU_ "" +
                 bu.name +
                 ' ""' +
                 bu.comment.replace(
                     '""',
                     '\\\\""') +
                    '"";\\n').encode(dbcExportCommentEncoding,'ignore'))
    f.write(""\\n"".encode(dbcExportEncoding))

    defaults = {}
    for (dataType, define) in sorted(list(db.frameDefines.items())):
        f.write(
            ('BA_DEF_ BO_ ""' +
             dataType +
             '"" ').encode(dbcExportEncoding) +
            define.definition.encode(
                dbcExportEncoding,
                'replace') +
            ';\\n'.encode(dbcExportEncoding))
        if dataType not in defaults and define.defaultValue is not None:
            defaults[dataType] = define.defaultValue
    for (dataType, define) in sorted(list(db.signalDefines.items())):
        f.write(
            ('BA_DEF_ SG_ ""' +
             dataType +
             '"" ').encode(dbcExportEncoding) +
            define.definition.encode(
                dbcExportEncoding,
                'replace') +
            ';\\n'.encode(dbcExportEncoding))
        if dataType not in defaults and define.defaultValue is not None:
            defaults[dataType] = define.defaultValue
    for (dataType, define) in sorted(list(db.buDefines.items())):
        f.write(
            ('BA_DEF_ BU_ ""' +
             dataType +
             '"" ').encode(dbcExportEncoding) +
            define.definition.encode(
                dbcExportEncoding,
                'replace') +
            ';\\n'.encode(dbcExportEncoding))
        if dataType not in defaults and define.defaultValue is not None:
            defaults[dataType] = define.defaultValue
    for (dataType, define) in sorted(list(db.globalDefines.items())):
        f.write(
            ('BA_DEF_ ""' +
             dataType +
             '"" ').encode(dbcExportEncoding) +
            define.definition.encode(
                dbcExportEncoding,
                'replace') +
            ';\\n'.encode(dbcExportEncoding))
        if dataType not in defaults and define.defaultValue is not None:
            defaults[dataType] = define.defaultValue

    for define in sorted(defaults):
        f.write(
            ('BA_DEF_DEF_ ""' +
             define +
             '"" ').encode(dbcExportEncoding) +
            defaults[define].encode(
                dbcExportEncoding,
                'replace') +
            ';\\n'.encode(dbcExportEncoding))

    # boardunit-attributes:
    for bu in db.boardUnits:
        for attrib, val in sorted(bu.attributes.items()):
            if db.buDefines[attrib].type == ""STRING"":
                val = '""' + val + '""'
            elif not val:
                val = '""""'
            f.write(
                ('BA_ ""' +
                 attrib +
                 '"" BU_ ' +
                 bu.name +
                 ' ' +
                 str(val) +
                    ';\\n').encode(dbcExportEncoding))
    f.write(""\\n"".encode(dbcExportEncoding))

    # global-attributes:
    for attrib, val in sorted(db.attributes.items()):
        if db.globalDefines[attrib].type == ""STRING"":
            val = '""' + val + '""'
        elif not val:
            val = '""""'
        f.write(('BA_ ""' + attrib + '"" ' + val +
                 ';\\n').encode(dbcExportEncoding))
    f.write(""\\n"".encode(dbcExportEncoding))

    # messages-attributes:
    for frame in db.frames:
        for attrib, val in sorted(frame.attributes.items()):
            if db.frameDefines[attrib].type == ""STRING"":
               val = '""' + val + '""'
            elif not val:
                val = '""""'
            f.write(('BA_ ""' + attrib + '"" BO_ %d ' %
                     frame.id + val + ';\\n').encode(dbcExportEncoding))
    f.write(""\\n"".encode(dbcExportEncoding))

    # signal-attributes:
    for frame in db.frames:
        for signal in frame.signals:
            for attrib, val in sorted(signal.attributes.items()):
                name = output_names[frame][signal]
                if db.signalDefines[attrib].type == ""STRING"":
                    val = '""' + val + '""'
                elif not val:
                    val = '""""'
                elif isinstance(val, float):
                    val = format_float(val)
                f.write(
                    ('BA_ ""' +
                     attrib +
                     '"" SG_ %d ' %
                     frame.id +
                     name +
                     ' ' +
                     val +
                     ';\\n').encode(dbcExportEncoding))
            if signal.is_float:
                if int(signal.signalsize) > 32:
                    f.write(('SIG_VALTYPE_ %d %s : 2;\\n' % (frame.id, output_names[bo][signal])).encode(dbcExportEncoding))
                else:
                    f.write(('SIG_VALTYPE_ %d %s : 1;\\n' % (frame.id, output_names[bo][signal])).encode(dbcExportEncoding))
 
    f.write(""\\n"".encode(dbcExportEncoding))

    # signal-values:
    for bo in db.frames:
        multiplex_written = False
        for signal in bo.signals:
            if signal.multiplex == 'Multiplexor' and multiplex_written:
                continue

            multiplex_written = True

            if signal.values:
                f.write(
                    ('VAL_ %d ' %
                     bo.id +
                     output_names[bo][signal]).encode(dbcExportEncoding))
                for attrib, val in sorted(
                        signal.values.items(), key=lambda x: int(x[0])):
                    f.write(
                        (' ' + str(attrib) + ' ""' + val + '""').encode(dbcExportEncoding))
                f.write("";\\n"".encode(dbcExportEncoding))

    # signal-groups:
    for bo in db.frames:
        for sigGroup in bo.signalGroups:
            f.write((""SIG_GROUP_ "" + str(bo.id) + "" "" + sigGroup.name +
                     "" "" + str(sigGroup.id) + "" :"").encode(dbcExportEncoding))
            for signal in sigGroup.signals:
                f.write(("" "" + output_names[bo][signal]).encode(dbcExportEncoding))
            f.write("";\\n"".encode(dbcExportEncoding))

    for frame in db.frames:
        if frame.is_complex_multiplexed:
            for signal in frame.signals:
                if signal.muxerForSignal is not None:
                    f.write((""SG_MUL_VAL_ %d %s %s %d-%d;\\n"" % (frame.id, signal.name, signal.muxerForSignal, signal.muxValMin, signal.muxValMax)).encode(dbcExportEncoding))","1. Use a secure random number generator to generate the salt.
2. Use a strong hashing algorithm, such as SHA-256 or bcrypt.
3. Store the hashed password in a secure location, such as in a database or in a file that is only accessible by the server."
"def getSignals(signalarray, Bo, arDict, ns, multiplexId):
    GroupId = 1
    if signalarray is None:  # Empty signalarray - nothing to do
        return
    for signal in signalarray:
        values = {}
        motorolla = arGetChild(signal, ""PACKING-BYTE-ORDER"", arDict, ns)
        startBit = arGetChild(signal, ""START-POSITION"", arDict, ns)
        isignal = arGetChild(signal, ""SIGNAL"", arDict, ns)
        if isignal is None:
            isignal = arGetChild(signal, ""I-SIGNAL"", arDict, ns)
        if isignal is None:
            isignal = arGetChild(signal, ""I-SIGNAL-GROUP"", arDict, ns)
            if isignal is not None:
                logger.debug(""getSignals: found I-SIGNAL-GROUP "")

                isignalarray = arGetXchildren(isignal, ""I-SIGNAL"", arDict, ns)
                getSysSignals(isignal, isignalarray, Bo, GroupId, ns)
                GroupId = GroupId + 1
                continue
        if isignal is None:
            logger.debug(
                'Frame %s, no isignal for %s found',
                Bo.name,
                arGetChild(
                    signal,
                    ""SHORT-NAME"",
                    arDict,
                    ns).text)
        syssignal = arGetChild(isignal, ""SYSTEM-SIGNAL"", arDict, ns)
        if syssignal is None:
            logger.debug(
                'Frame %s, signal %s has no systemsignal',
                isignal.tag,
                Bo.name)

        if ""SYSTEM-SIGNAL-GROUP"" in syssignal.tag:
            syssignalarray = arGetXchildren(
                syssignal, ""SYSTEM-SIGNAL-REFS/SYSTEM-SIGNAL"", arDict, ns)
            getSysSignals(syssignal, syssignalarray, Bo, GroupId, ns)
            GroupId = GroupId + 1
            continue

        length = arGetChild(isignal, ""LENGTH"", arDict, ns)
        if length is None:
            length = arGetChild(syssignal, ""LENGTH"", arDict, ns)
        name = arGetChild(syssignal, ""SHORT-NAME"", arDict, ns)

        unitElement = arGetChild(isignal, ""UNIT"", arDict, ns)
        displayName = arGetChild(unitElement, ""DISPLAY-NAME"", arDict, ns)
        if displayName is not None:
            Unit = displayName.text
        else:
            Unit = """"

        Min = None
        Max = None
        factor = 1.0
        offset = 0
        receiver = []

        signalDescription = getDesc(syssignal, arDict, ns)
        datatype = arGetChild(syssignal, ""DATA-TYPE"", arDict, ns)
        if datatype is None:  # AR4?
            dataConstr = arGetChild(isignal,""DATA-CONSTR"", arDict, ns)
            compuMethod = arGetChild(isignal,""COMPU-METHOD"", arDict, ns)
            baseType  = arGetChild(isignal,""BASE-TYPE"", arDict, ns)

            lower = arGetChild(dataConstr, ""LOWER-LIMIT"", arDict, ns)
            upper = arGetChild(dataConstr, ""UPPER-LIMIT"", arDict, ns)
#            if lower is None: # data-constr has no limits defined - use limit from compu-method
#                lower = arGetChild(compuMethod, ""LOWER-LIMIT"", arDict, ns)
#            if upper is None: # data-constr has no limits defined - use limit from compu-method
#                upper = arGetChild(compuMethod, ""UPPER-LIMIT"", arDict, ns)
            encoding = None # TODO - find encoding in AR4
        else:
            lower = arGetChild(datatype, ""LOWER-LIMIT"", arDict, ns)
            upper = arGetChild(datatype, ""UPPER-LIMIT"", arDict, ns)
            encoding = arGetChild(datatype, ""ENCODING"", arDict, ns)

        if encoding is not None and (encoding.text == ""SINGLE"" or encoding.text == ""DOUBLE""):
            is_float = True
        else:
            is_float = False
        
        if lower is not None and upper is not None:
            Min = float(lower.text)
            Max = float(upper.text)

        datdefprops = arGetChild(datatype, ""SW-DATA-DEF-PROPS"", arDict, ns)

        compmethod = arGetChild(datdefprops, ""COMPU-METHOD"", arDict, ns)
        if compmethod is None:  # AR4
            compmethod = arGetChild(isignal, ""COMPU-METHOD"", arDict, ns)
            baseType = arGetChild(isignal, ""BASE-TYPE"", arDict, ns)
            encoding = arGetChild(baseType, ""BASE-TYPE-ENCODING"", arDict, ns)
            if encoding is not None and encoding.text == ""IEEE754"":
                is_float = True
        #####################################################################################################
        # Modification to support sourcing the COMPU_METHOD info from the Vector NETWORK-REPRESENTATION-PROPS
        # keyword definition. 06Jun16
        #####################################################################################################
        if compmethod == None:
            logger.debug('No Compmethod found!! - try alternate scheme.')
            networkrep = arGetChild(isignal, ""NETWORK-REPRESENTATION-PROPS"", arDict, ns)
            datdefpropsvar = arGetChild(networkrep, ""SW-DATA-DEF-PROPS-VARIANTS"", arDict, ns)            
            datdefpropscond = arGetChild(datdefpropsvar, ""SW-DATA-DEF-PROPS-CONDITIONAL"", arDict ,ns)
            if datdefpropscond != None:
                try:
                    compmethod = arGetChild(datdefpropscond, ""COMPU-METHOD"", arDict, ns)            
                except:
                    logger.debug('No valid compu method found for this - check ARXML file!!')
                    compmethod = None
        #####################################################################################################
        #####################################################################################################
             
        unit = arGetChild(compmethod, ""UNIT"", arDict, ns)
        if unit is not None:
            longname = arGetChild(unit, ""LONG-NAME"", arDict, ns)
        #####################################################################################################
        # Modification to support obtaining the Signals Unit by DISPLAY-NAME. 07June16
        #####################################################################################################
            try:
              displayname = arGetChild(unit, ""DISPLAY-NAME"", arDict, ns)
            except:
              logger.debug('No Unit Display name found!! - using long name')
            if displayname is not None:
              Unit = displayname.text
            else:  
        #####################################################################################################
        #####################################################################################################              
              l4 = arGetChild(longname, ""L-4"", arDict, ns)
              if l4 is not None:
                Unit = l4.text

        compuscales = arGetXchildren(
            compmethod,
            ""COMPU-INTERNAL-TO-PHYS/COMPU-SCALES/COMPU-SCALE"",
            arDict,
            ns)

        initvalue = arGetXchildren(syssignal, ""INIT-VALUE/VALUE"", arDict, ns)

        if initvalue is None or initvalue.__len__() == 0:
            initvalue = arGetXchildren(isignal, ""INIT-VALUE/NUMERICAL-VALUE-SPECIFICATION/VALUE"", arDict, ns) ##AR4.2
        if initvalue is not None and initvalue.__len__() >= 1:
            initvalue = initvalue[0]
        else:
            initvalue = None

        for compuscale in compuscales:
            ll = arGetChild(compuscale, ""LOWER-LIMIT"", arDict, ns)
            ul = arGetChild(compuscale, ""UPPER-LIMIT"", arDict, ns)
            sl = arGetChild(compuscale, ""SHORT-LABEL"", arDict, ns)
            if sl is None:
                desc = getDesc(compuscale, arDict, ns)
            else:
                desc = sl.text
        #####################################################################################################
        # Modification to support sourcing the COMPU_METHOD info from the Vector NETWORK-REPRESENTATION-PROPS
        # keyword definition. 06Jun16
        #####################################################################################################
            if ll is not None and desc is not None and int(float(ul.text)) == int(float(ll.text)):
        #####################################################################################################
        #####################################################################################################
                values[ll.text] = desc

            scaleDesc = getDesc(compuscale, arDict, ns)
            rational = arGetChild(
                compuscale, ""COMPU-RATIONAL-COEFFS"", arDict, ns)
            if rational is not None:
                numerator = arGetChild(rational, ""COMPU-NUMERATOR"", arDict, ns)
                zaehler = arGetChildren(numerator, ""V"", arDict, ns)
                denominator = arGetChild(
                    rational, ""COMPU-DENOMINATOR"", arDict, ns)
                nenner = arGetChildren(denominator, ""V"", arDict, ns)

                factor = float(zaehler[1].text) / float(nenner[0].text)
                offset = float(zaehler[0].text) / float(nenner[0].text)
                if Min is not None:
                    Min *= factor
                    Min += offset
                if Max is not None:
                    Max *= factor
                    Max += offset
            else:
                const = arGetChild(compuscale, ""COMPU-CONST"", arDict, ns)
                # value hinzufuegen
                if const is None:
                    logger.warn(
                        ""unknown Compu-Method: "" +
                        compmethod.get('UUID'))
        is_little_endian = False
        if motorolla.text == 'MOST-SIGNIFICANT-BYTE-LAST':
            is_little_endian = True
        is_signed = False  # unsigned
        if name is None:
            logger.debug('no name for signal given')
        if startBit is None:
            logger.debug('no startBit for signal given')
        if length is None:
            logger.debug('no length for signal given')

        if startBit is not None:
            newSig = Signal(name.text,
                            startBit=startBit.text,
                            signalSize=length.text,
                            is_little_endian=is_little_endian,
                            is_signed=is_signed,
                            factor=factor,
                            offset=offset,
                            min=Min,
                            max=Max,
                            unit=Unit,
                            receiver=receiver,
                            multiplex=multiplexId,
                            comment=signalDescription,
                            is_float=is_float)

            if newSig.is_little_endian == 0:
                # startbit of motorola coded signals are MSB in arxml
                newSig.setStartbit(int(startBit.text), bitNumbering=1)

            signalRxs[syssignal] = newSig

            basetype = arGetChild(datdefprops, ""BASE-TYPE"", arDict, ns)
            if basetype is not None:
                temp = arGetChild(basetype, ""SHORT-NAME"", arDict, ns)
                if temp is not None and ""boolean"" == temp.text:
                    newSig.addValues(1, ""TRUE"")
                    newSig.addValues(0, ""FALSE"")


            if initvalue is not None and initvalue.text is not None:
                if initvalue.text == ""false"":
                    initvalue.text = ""0""
                elif initvalue.text == ""true"":
                    initvalue.text = ""1""
                newSig._initValue = int(initvalue.text)
                newSig.addAttribute(""GenSigStartValue"", str(newSig._initValue))
            else:
                newSig._initValue = 0

            for key, value in list(values.items()):
                newSig.addValues(key, value)
            Bo.addSignal(newSig)","1. Use `arGetChild` instead of `arGetXchildren` to prevent XXE attacks.
2. Use `arGetChild` with a default value to prevent null pointer dereference.
3. Use `arGetChild` with a `ns` parameter to prevent namespace collisions."
"  def _parse_import_alias(cls, leaves):
    assert [leaf.type for leaf in leaves] == [token.NAME] * 3
    assert leaves[1].value == 'as'
    return (leaves[0].value, leaves[2].value)","1. Use `ast.literal_eval` instead of `eval` to parse strings into Python objects. This will prevent code injection attacks.
2. Use `importlib.import_module` instead of `import` to import modules. This will prevent malicious modules from being imported.
3. Use `sys.path.append` to add directories to the Python path instead of hardcoding them. This will prevent malicious code from being executed when a module is imported."
"  def parse_top_import(cls, results):
    """"""Splits the result of import_pattern into component strings.

    Examples:

    'from pkg import a,b,c' gives
    (('pkg', None), [('a', None), ('b', None), ('c', None)])

    'import pkg' gives
    (('pkg', None), [])

    'from pkg import a as b' gives
    (('pkg', None), [('a', 'b')])

    'import pkg as pkg2' gives
    (('pkg', 'pkg2'), [])

    Args:
      results: The values from import_pattern.

    Returns:
      A tuple of the package name and the list of imported names. Each name is a
      tuple of original name and alias.
    """"""

    pkg, names = results['pkg'], results.get('names', None)

    if len(pkg) == 1 and pkg[0].type == pygram.python_symbols.dotted_as_name:
      pkg_out = cls._parse_import_alias(list(pkg[0].leaves()))
    else:
      pkg_out = (''.join(map(str, pkg)).strip(), None)

    names_out = []
    if names:
      names = split_comma(names.leaves())
      for name in names:
        if len(name) == 1:
          assert name[0].type in (token.NAME, token.STAR)
          names_out.append((name[0].value, None))
        else:
          names_out.append(cls._parse_import_alias(name))

    return pkg_out, names_out","1. Use `importlib.import_module` instead of `__import__` to avoid
    potential security vulnerabilities.
2. Use `importlib.resources` to load resources from packages, instead of
    directly accessing the filesystem.
3. Use `typing` to annotate your code, so that you can catch errors at compile-time."
"  def Bindings(self, viewpoint, strict=True):
    """"""Filters down the possibilities of bindings for this variable.

    It determines this by analyzing the control flow graph. Any definition for
    this variable that is invisible from the current point in the CFG is
    filtered out. This function differs from Filter() in that it only honors the
    CFG, not the source sets. As such, it's much faster.

    Arguments:
      viewpoint: The CFG node at which to determine the possible bindings.
      strict: Whether to allow approximations for speed.

    Returns:
      A filtered list of bindings for this variable.
    """"""
    if viewpoint is None or (not strict and len(self.bindings) == 1):
      return self.bindings

    num_bindings = len(self.bindings)
    result = set()
    seen = set()
    stack = [viewpoint]
    while stack:
      if len(result) == num_bindings:
        break
      node = stack.pop()
      seen.add(node)
      bindings = self._cfgnode_to_bindings.get(node)
      if bindings is not None:
        assert bindings, ""empty binding list""
        result.update(bindings)
        # Don't expand this node - previous assignments to this variable will
        # be invisible, since they're overwritten here.
        continue
      else:
        stack.extend(set(node.incoming) - seen)
    return result","1. Use `viewpoint` parameter to filter out the bindings that are not visible from the current point in the CFG.
2. Use `strict` parameter to disable approximation for speed.
3. Check if the `bindings` list is empty before returning it."
"  def __enter__(self):
    self._start_time = time.clock()","1. Use `threading.Lock` to prevent concurrent access to the `_start_time` attribute.
2. Use `time.time()` instead of `time.clock()` to get the current time.
3. Handle exceptions in the `__enter__()` method."
"  def __exit__(self, exc_type, exc_value, traceback):
    self._total = time.clock() - self._start_time
    del self._start_time","1. Use `with` statement to ensure that the timer is always cleaned up.
2. Use `time.perf_counter()` instead of `time.clock()` to avoid system clock skew.
3. Handle exceptions in `__exit__()` to prevent memory leaks."
"  def __enter__(self):
    if not self._calls:
      self._start_time = time.clock()
    self._calls += 1","1. Use `threading.Lock` to protect the `_start_time` attribute from being read or written by multiple threads simultaneously.
2. Use `time.time()` instead of `time.clock()` to get the current time, as `time.clock()` is not monotonic and can return different values for the same point in time on different systems.
3. Initialize the `_calls` attribute to 0 in the constructor, instead of in the `__enter__()` method, to ensure that it is always initialized to the correct value."
"  def __exit__(self, exc_type, exc_value, traceback):
    self._calls -= 1
    if not self._calls:
      self._time += time.clock() - self._start_time
      del self._start_time","1. Use `with` statement to ensure that the `__exit__` method is always called, even if an exception is raised.
2. Use `time.perf_counter()` instead of `time.clock()` to get more accurate timing measurements.
3. Use `contextlib.contextmanager()` to simplify the code and make it more readable."
"def _Visit(node, visitor, *args, **kwargs):
  """"""Visit the node.""""""
  name = type(visitor).__name__
  recursive = name in _visiting
  _visiting.add(name)

  start = time.clock()
  try:
    return _VisitNode(node, visitor, *args, **kwargs)
  finally:
    if not recursive:
      _visiting.remove(name)
      elapsed = time.clock() - start
      metrics.get_metric(""visit_"" + name, metrics.Distribution).add(elapsed)
      if _visiting:
        metrics.get_metric(
            ""visit_nested_"" + name, metrics.Distribution).add(elapsed)","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `inspect.getfullargspec` to get the full argument spec of the function.
3. Use `functools.partial` to create a new function with a limited argument list."
"  def __init__(self, name, vm):
    """"""Basic initializer for all AtomicAbstractValues.""""""
    super(AtomicAbstractValue, self).__init__(vm)
    assert hasattr(vm, ""program""), type(self)
    self.cls = None
    self.name = name
    self.mro = self.compute_mro()
    self.module = None
    self.official_name = None
    self.late_annotations = {}
    self.slots = None  # writable attributes (or None if everything is writable)
    # The template for the current class. It is usually a constant, lazily
    # loaded to accommodate recursive types, but in the case of typing.Generic
    # (only), it'll change every time when a new generic class is instantiated.
    self._template = None
    # names in the templates of the current class and its base classes
    self._all_template_names = None
    self._instance = None","1. Use `typing.TYPE_CHECKING` to annotate the arguments of functions.
2. Use `functools.wraps` to preserve the metadata of wrapped functions.
3. Use `inspect.getfullargspec` to get the full argument list of a function."
"  def instantiate(self, node, container=None):
    if self.has_late_types():
      frame = self.vm.frame
      self.resolve_late_types(node, frame.f_globals, frame.f_locals)
    var = self.vm.program.NewVariable()
    if container and (not isinstance(container, SimpleAbstractValue) or
                      self.full_name in container.all_template_names):
      instance = TypeParameterInstance(self, container, self.vm)
      return instance.to_variable(node)
    else:
      for c in self.constraints:
        var.PasteVariable(c.instantiate(node, container))
      if self.bound:
        var.PasteVariable(self.bound.instantiate(node, container))
    if not var.bindings:
      var.AddBinding(self.vm.convert.unsolvable, [], node)
    return var","1. Use `self.vm.frame.f_globals` and `self.vm.frame.f_locals` to get the global and local variables.
2. Use `self.vm.program.NewVariable()` to create a new variable.
3. Use `self.vm.convert.unsolvable` to add a binding to the variable if it is not solvable."
"  def _build_value(self, node, raw_inner, ellipses):
    template, inner, abstract_class = self._get_value_info(raw_inner, ellipses)
    if self.base_cls.full_name == ""typing.Generic"":
      # Generic is unique in that parameterizing it defines a new template;
      # usually, the parameterized class inherits the base class's template.
      template_params = [
          param.with_module(self.base_cls.full_name) for param in inner]
    else:
      template_params = None
    if len(inner) != len(template):
      if not template:
        self.vm.errorlog.not_indexable(self.vm.frames, self.base_cls.name,
                                       generic_warning=True)
      else:
        # Use the unprocessed values of `template` and `inner` so that the error
        # message matches what the user sees.
        name = ""%s[%s]"" % (
            self.full_name, "", "".join(t.name for t in self.base_cls.template))
        error = ""Expected %d parameter(s), got %d"" % (
            len(self.base_cls.template), len(raw_inner))
        self.vm.errorlog.invalid_annotation(self.vm.frames, None, error, name)
    else:
      if len(inner) == 1:
        val, = inner
        # It's a common mistake to index tuple, not tuple().
        # We only check the ""int"" case, since string literals are allowed for
        # late annotations.
        # TODO(kramm): Instead of blacklisting only int, this should use
        # annotations_util.py to look up legal types.
        if isinstance(val, Instance) and val.cls == self.vm.convert.int_type:
          # Don't report this error again.
          inner = (self.vm.convert.unsolvable,)
          self.vm.errorlog.not_indexable(self.vm.frames, self.name)
    params = {name: inner[i] if i < len(inner) else self.vm.convert.unsolvable
              for i, name in enumerate(template)}

    # For user-defined generic types, check if its type parameter matches
    # its corresponding concrete type
    if isinstance(self.base_cls, InterpreterClass) and self.base_cls.template:
      for formal in self.base_cls.template:
        if (isinstance(formal, TypeParameter) and not formal.is_generic() and
            isinstance(params[formal.name], TypeParameter)):
          if formal.name != params[formal.name].name:
            self.vm.errorlog.not_supported_yet(
                self.vm.frames,
                ""Renaming TypeVar `%s` with constraints or bound"" % formal.name)
        else:
          root_node = self.vm.root_cfg_node
          actual = params[formal.name].instantiate(root_node)
          bad = self.vm.matcher.bad_matches(actual, formal, root_node)
          if bad:
            with self.vm.convert.pytd_convert.produce_detailed_output():
              combined = pytd_utils.JoinTypes(
                  view[actual].data.to_type(root_node, view=view)
                  for view in bad)
              formal = self.vm.annotations_util.sub_one_annotation(
                  root_node, formal, [{}])
              self.vm.errorlog.bad_concrete_type(
                  self.vm.frames, combined, formal.get_instance_type(root_node))
              return self.vm.convert.unsolvable

    try:
      return abstract_class(self.base_cls, params, self.vm, template_params)
    except abstract_utils.GenericTypeError as e:
      self.vm.errorlog.invalid_annotation(self.vm.frames, e.annot, e.error)
      return self.vm.convert.unsolvable","1. Use `isinstance()` to check if an object is of a certain type.
2. Use `type()` to get the type of an object.
3. Use `Union` to represent a type that can be one of several types."
"  def _load_formal_type_parameters(self):
    if self._formal_type_parameters_loaded:
      return
    if isinstance(self._formal_type_parameters,
                  abstract_utils.LazyFormalTypeParameters):
      formal_type_parameters = {}
      for name, param in self._raw_formal_type_parameters():
        if param is None:
          formal_type_parameters[name] = self.vm.convert.unsolvable
        else:
          formal_type_parameters[name] = self.vm.convert.constant_to_value(
              param, self._formal_type_parameters.subst, self.vm.root_cfg_node)
      self._formal_type_parameters = formal_type_parameters
    self._formal_type_parameters = (
        self.vm.annotations_util.convert_class_annotations(
            self.vm.root_cfg_node, self._formal_type_parameters))
    self._formal_type_parameters_loaded = True","1. Use `self.vm.convert.unsolvable` instead of `None` to represent missing type parameters.
2. Use `self.vm.convert.constant_to_value` to convert constants to values.
3. Use `self.vm.annotations_util.convert_class_annotations` to convert class annotations."
"  def make(cls, name, code, f_locals, f_globals, defaults, kw_defaults, closure,
           annotations, late_annotations, vm):
    """"""Get an InterpreterFunction.

    Things like anonymous functions and generator expressions are created
    every time the corresponding code executes. Caching them makes it easier
    to detect when the environment hasn't changed and a function call can be
    optimized away.

    Arguments:
      name: Function name.
      code: A code object.
      f_locals: The locals used for name resolution.
      f_globals: The globals used for name resolution.
      defaults: Default arguments.
      kw_defaults: Default arguments for kwonly parameters.
      closure: The free variables this closure binds to.
      annotations: Function annotations. Dict of name -> AtomicAbstractValue.
      late_annotations: Late-evaled annotations. Dict of name -> str.
      vm: VirtualMachine instance.

    Returns:
      An InterpreterFunction.
    """"""
    annotations = annotations or {}
    if ""return"" in annotations:
      # Check Generator/AsyncGenerator return type
      ret_type = annotations[""return""]
      if code.has_generator():
        if not abstract_utils.matches_generator(ret_type):
          error = ""Expected Generator, Iterable or Iterator""
          vm.errorlog.invalid_annotation(vm.frames, ret_type, error)
      elif code.has_async_generator():
        if not abstract_utils.matches_async_generator(ret_type):
          error = ""Expected AsyncGenerator, AsyncIterable or AsyncIterator""
          vm.errorlog.invalid_annotation(vm.frames, ret_type, error)
    late_annotations = late_annotations or {}
    overloads = vm.frame.overloads[name]
    key = (name, code,
           abstract_utils.hash_all_dicts(
               (f_globals.members, set(code.co_names)),
               (f_locals.members,
                set(f_locals.members) - set(code.co_varnames)),
               ({key: vm.program.NewVariable([value], [], vm.root_cfg_node)
                 for key, value in annotations.items()}, None),
               (dict(enumerate(vm.program.NewVariable([f], [], vm.root_cfg_node)
                               for f in overloads)), None),
               (dict(enumerate(defaults)), None),
               (dict(enumerate(closure or ())), None)))
    if key not in cls._function_cache:
      cls._function_cache[key] = cls(
          name, code, f_locals, f_globals, defaults, kw_defaults,
          closure, annotations, late_annotations, overloads, vm)
    return cls._function_cache[key]","1. Use `abstract_utils.matches_generator` and `abstract_utils.matches_async_generator` to check the return type of generator and async generator functions.
2. Use `vm.errorlog.invalid_annotation` to raise an error if the return type does not match.
3. Use `vm.program.NewVariable` to create new variables and `vm.root_cfg_node` to set the root cfg node."
"  def __init__(self, name, code, f_locals, f_globals, defaults, kw_defaults,
               closure, annotations, late_annotations, overloads, vm):
    log.debug(""Creating InterpreterFunction %r for %r"", name, code.co_name)
    self.bound_class = BoundInterpreterFunction
    self.doc = code.co_consts[0] if code.co_consts else None
    self.code = code
    self.f_globals = f_globals
    self.f_locals = f_locals
    self.defaults = tuple(defaults)
    self.kw_defaults = kw_defaults
    self.closure = closure
    self._call_cache = {}
    self._call_records = []
    # TODO(b/78034005): Combine this and PyTDFunction.signatures into a single
    # way to handle multiple signatures that SignedFunction can also use.
    self._overloads = overloads
    self.has_overloads = bool(overloads)
    self.is_overload = False  # will be set by typing_overlay.Overload.call
    self.nonstararg_count = self.code.co_argcount
    if self.code.co_kwonlyargcount >= 0:  # This is usually -1 or 0 (fast call)
      self.nonstararg_count += self.code.co_kwonlyargcount
    signature = self._build_signature(name, annotations, late_annotations)
    super(InterpreterFunction, self).__init__(signature, vm)
    self.last_frame = None  # for BuildClass
    self._store_call_records = False
    if self.vm.PY3:
      self.is_class_builder = False  # Will be set by BuildClass.
    else:
      self.is_class_builder = self.code.has_opcode(opcodes.LOAD_LOCALS)","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `inspect.iscoroutinefunction` to check if the function is a coroutine.
3. Use `functools.partial` to create a partial function with specific arguments."
"  def _build_signature(self, name, annotations, late_annotations):
    """"""Build a function.Signature object representing this function.""""""
    vararg_name = None
    kwarg_name = None
    kwonly = set(self.code.co_varnames[
        self.code.co_argcount:self.nonstararg_count])
    arg_pos = self.nonstararg_count
    if self.has_varargs():
      vararg_name = self.code.co_varnames[arg_pos]
      arg_pos += 1
    if self.has_kwargs():
      kwarg_name = self.code.co_varnames[arg_pos]
      arg_pos += 1
    defaults = dict(zip(
        self.get_positional_names()[-len(self.defaults):], self.defaults))
    defaults.update(self.kw_defaults)
    return function.Signature(
        name,
        tuple(self.code.co_varnames[:self.code.co_argcount]),
        vararg_name,
        tuple(kwonly),
        kwarg_name,
        defaults,
        annotations,
        late_annotations)","1. Use `typing.get_args()` to get the function's argument names and types.
2. Use `typing.get_type_hints()` to get the function's return type and any type annotations on its parameters.
3. Use `typing.cast()` to cast function arguments to their expected types."
"  def __init__(self, name, param_names, varargs_name, kwonly_params,
               kwargs_name, defaults, annotations, late_annotations, vm):
    """"""Create a SimpleFunction.

    Args:
      name: Name of the function as a string
      param_names: Tuple of parameter names as strings.
      varargs_name: The ""args"" in ""*args"". String or None.
      kwonly_params: Tuple of keyword-only parameters as strings. These do NOT
        appear in param_names.
      kwargs_name: The ""kwargs"" in ""**kwargs"". String or None.
      defaults: Dictionary of string names to values of default arguments.
      annotations: Dictionary of string names to annotations (strings or types).
      late_annotations: Dictionary of string names to string types, used for
        forward references or as-yet-unknown types.
      vm: The virtual machine for this function.
    """"""
    annotations = dict(annotations)
    late_annotations = dict(late_annotations)
    # Every parameter must have an annotation. Defaults to unsolvable.
    for n in itertools.chain(param_names, [varargs_name, kwargs_name],
                             kwonly_params):
      if n and n not in annotations and n not in late_annotations:
        annotations[n] = vm.convert.unsolvable
    if not isinstance(defaults, dict):
      defaults = dict(zip(param_names[-len(defaults):], defaults))
    signature = function.Signature(name, param_names, varargs_name,
                                   kwonly_params, kwargs_name, defaults,
                                   annotations, late_annotations)
    super(SimpleFunction, self).__init__(signature, vm)
    self.bound_class = BoundFunction","1. Use `typing.Union` to annotate the parameters of the function. This will help catch errors when the wrong type of argument is passed to the function.
2. Use `functools.wraps` to preserve the metadata of the function being wrapped. This will ensure that the function's name, docstring, and annotations are preserved when it is wrapped.
3. Use `inspect.getfullargspec` to get the full argument spec of the function. This will allow you to check for invalid arguments and missing defaults."
"  def call_init(self, node, instance):
    # Call __init__ on each binding.
    # TODO(kramm): This should do join_cfg_nodes, instead of concatenating them.
    for b in instance.bindings:
      if b.data in self._initialized_instances:
        continue
      self._initialized_instances.add(b.data)
      if isinstance(b.data, abstract.SimpleAbstractValue):
        for param in b.data.instance_type_parameters.values():
          node = self.call_init(node, param)
      node = self._call_method(node, b, ""__init__"")
      # Test classes initialize attributes in setUp() as well.
      cls = b.data.get_class()
      if isinstance(cls, abstract.InterpreterClass) and cls.is_test_class:
        node = self._call_method(node, b, ""setUp"")
    return node","1. Use `join_cfg_nodes` instead of `concatenating` them to prevent creating new nodes.
2. Check if the `data` of `b` is already in `_initialized_instances` to avoid infinite recursion.
3. Use `isinstance` to check if `b.data` is an instance of `abstract.SimpleAbstractValue` before calling `call_init`."
"  def sub_one_annotation(self, node, annot, substs, instantiate_unbound=True):
    """"""Apply type parameter substitutions to an annotation.""""""
    if isinstance(annot, abstract.TypeParameter):
      def contains(subst, annot):
        return (annot.full_name in subst and subst[annot.full_name].bindings and
                not any(isinstance(v, abstract.AMBIGUOUS_OR_EMPTY)
                        for v in subst[annot.full_name].data))
      if all(contains(subst, annot) for subst in substs):
        vals = sum((subst[annot.full_name].data for subst in substs), [])
      elif instantiate_unbound:
        vals = annot.instantiate(node).data
      else:
        vals = [annot]
      return self.vm.convert.merge_classes(vals)
    elif isinstance(annot, abstract.ParameterizedClass):
      type_parameters = {
          name: self.sub_one_annotation(
              node, param, substs, instantiate_unbound)
          for name, param in annot.formal_type_parameters.items()}
      # annot may be a subtype of ParameterizedClass, such as TupleClass.
      if isinstance(annot, abstract.LiteralClass):
        # We can't create a LiteralClass because we don't have a concrete value.
        typ = abstract.ParameterizedClass
      else:
        typ = type(annot)
      return typ(annot.base_cls, type_parameters, self.vm, annot.template)
    elif isinstance(annot, abstract.Union):
      options = tuple(self.sub_one_annotation(node, o, substs,
                                              instantiate_unbound)
                      for o in annot.options)
      return type(annot)(options, self.vm)
    return annot","1. Use `isinstance()` to check if the type parameter is a subtype of `ParameterizedClass`.
2. If the type parameter is a subtype of `ParameterizedClass`, use `self.sub_one_annotation()` to recursively apply type parameter substitutions.
3. If the type parameter is not a subtype of `ParameterizedClass`, return the original type parameter."
"  def convert_function_annotations(self, node, raw_annotations):
    """"""Convert raw annotations to dicts of annotations and late annotations.""""""
    if raw_annotations:
      # {""i"": int, ""return"": str} is stored as (int, str, (""i"", ""return""))
      names = abstract_utils.get_atomic_python_constant(raw_annotations[-1])
      type_list = raw_annotations[:-1]
      annotations_list = []
      for name, t in zip(names, type_list):
        name = abstract_utils.get_atomic_python_constant(name)
        t = self.convert_function_type_annotation(name, t)
        annotations_list.append((name, t))
      return self.convert_annotations_list(node, annotations_list)
    else:
      return {}, {}","1. Use `ast.literal_eval` instead of `eval` to sanitize user input.
2. Use `functools.wraps` to preserve the metadata of the original function.
3. Use `inspect.getfullargspec` to get the argument names and default values of the function."
"  def convert_annotations_list(self, node, annotations_list):
    """"""Convert a (name, raw_annot) list to annotations and late annotations.""""""
    annotations = {}
    late_annotations = {}
    for name, t in annotations_list:
      if t is None:
        continue
      try:
        annot = self._process_one_annotation(node, t, name, self.vm.frames)
      except self.LateAnnotationError:
        late_annotations[name] = abstract.LateAnnotation(
            t, name, self.vm.simple_stack())
      else:
        if annot is not None:
          annotations[name] = annot
    return annotations, late_annotations","1. Use `ast.literal_eval()` instead of `eval()` to sanitize user input.
2. Validate the type of the input before processing it.
3. Use `functools.wraps()` to preserve the metadata of the decorated function."
"  def convert_class_annotations(self, node, raw_annotations):
    """"""Convert a name -> raw_annot dict to annotations.""""""
    annotations = {}
    for name, t in raw_annotations.items():
      try:
        # Don't use the parameter name, since it's often something unhelpful
        # like `0`.
        annot = self._process_one_annotation(node, t, None, self.vm.frames)
      except self.LateAnnotationError:
        # Copy the late annotation back into the dict for
        # convert_function_annotations to deal with.
        # TODO(rechen): Handle it here so that the raw annotation isn't
        # accidentally used elsewhere.
        annotations[name] = t
      else:
        annotations[name] = annot or self.vm.convert.unsolvable
    return annotations","1. Use `inspect.getfullargspec()` to get the parameter names of the function.
2. Use `ast.literal_eval()` to parse the raw annotation string.
3. Check if the annotation is a valid type before using it."
"  def init_annotation_var(self, node, name, var):
    """"""Instantiate a variable of an annotation, calling __init__.""""""
    try:
      typ = abstract_utils.get_atomic_value(var)
    except abstract_utils.ConversionError:
      error = ""Type must be constant for variable annotation""
      self.vm.errorlog.invalid_annotation(self.vm.frames, None, error, name)
      return self.vm.new_unsolvable(node)
    else:
      if self.get_type_parameters(typ):
        self.vm.errorlog.not_supported_yet(
            self.vm.frames, ""using type parameter in variable annotation"")
        return self.vm.new_unsolvable(node)
      try:
        return self.init_annotation(node, typ, name, self.vm.frames)
      except self.LateAnnotationError:
        return abstract.LateAnnotation(typ, name, self.vm.simple_stack())","1. Use `abstract_utils.get_atomic_value()` to check if the type is constant.
2. Check if the type has type parameters.
3. Catch `LateAnnotationError` and return `abstract.LateAnnotation`."
"  def apply_type_comment(self, state, op, name, value):
    """"""If there is a type comment for the op, return its value.""""""
    assert op is self.vm.frame.current_opcode
    if op.code.co_filename != self.vm.filename:
      return value
    if not op.type_comment:
      return value
    comment = op.type_comment
    frame = self.vm.frame
    try:
      var = abstract_utils.eval_expr(
          self.vm, state.node, frame.f_globals, frame.f_locals, comment)
    except abstract_utils.EvaluationError as e:
      self.vm.errorlog.invalid_type_comment(
          self.vm.frames, comment, details=e.details)
      value = self.vm.new_unsolvable(state.node)
    else:
      try:
        typ = abstract_utils.get_atomic_value(var)
      except abstract_utils.ConversionError:
        self.vm.errorlog.invalid_type_comment(
            self.vm.frames, comment, details=""Must be constant."")
        value = self.vm.new_unsolvable(state.node)
      else:
        if self.get_type_parameters(typ):
          self.vm.errorlog.not_supported_yet(
              self.vm.frames, ""using type parameter in type comment"")
        try:
          value = self.init_annotation(state.node, typ, name, self.vm.frames)
        except self.LateAnnotationError:
          value = abstract.LateAnnotation(typ, name, self.vm.simple_stack())
    return value","1. Use `eval_expr` to evaluate type comments.
2. Check if the type comment is valid.
3. Handle type parameters in type comments correctly."
"  def _process_one_annotation(self, node, annotation, name, stack,
                              f_globals=None, f_locals=None, seen=None):
    """"""Change annotation / record errors where required.""""""

    # Check for recursive type annotations so we can emit an error message
    # rather than crashing.
    if seen is None:
      seen = set()
    if isinstance(annotation, abstract.AbstractOrConcreteValue):
      if annotation in seen:
        self.vm.errorlog.not_supported_yet(
            stack, ""Recursive type annotations"",
            details=""In annotation '%s' on %s"" % (annotation.pyval, name))
        return None
      seen = seen | {annotation}

    if isinstance(annotation, abstract.AnnotationContainer):
      annotation = annotation.base_cls

    if isinstance(annotation, typing_overlay.Union):
      self.vm.errorlog.invalid_annotation(
          stack, annotation, ""Needs options"", name)
      return None
    elif (name is not None and name != ""return""
          and isinstance(annotation, typing_overlay.NoReturn)):
      self.vm.errorlog.invalid_annotation(
          stack, annotation, ""NoReturn is not allowed"", name)
      return None
    elif isinstance(annotation, abstract.Instance) and (
        annotation.cls == self.vm.convert.str_type or
        annotation.cls == self.vm.convert.unicode_type
    ):
      # String annotations : Late evaluation
      if isinstance(annotation, mixin.PythonConstant):
        if f_globals is None:
          raise self.LateAnnotationError()
        else:
          try:
            v = abstract_utils.eval_expr(
                self.vm, node, f_globals, f_locals, annotation.pyval)
          except abstract_utils.EvaluationError as e:
            self.vm.errorlog.invalid_annotation(stack, annotation, e.details)
            return None
          if len(v.data) == 1:
            return self._process_one_annotation(
                node, v.data[0], name, stack, f_globals, f_locals, seen)
      self.vm.errorlog.invalid_annotation(
          stack, annotation, ""Must be constant"", name)
      return None
    elif annotation.cls == self.vm.convert.none_type:
      # PEP 484 allows to write ""NoneType"" as ""None""
      return self.vm.convert.none_type
    elif isinstance(annotation, abstract.ParameterizedClass):
      for param_name, param in annotation.formal_type_parameters.items():
        processed = self._process_one_annotation(
            node, param, name, stack, f_globals, f_locals, seen)
        if processed is None:
          return None
        elif isinstance(processed, typing_overlay.NoReturn):
          self.vm.errorlog.invalid_annotation(
              stack, param, ""NoReturn is not allowed as inner type"", name)
          return None
        annotation.formal_type_parameters[param_name] = processed
      return annotation
    elif isinstance(annotation, abstract.Union):
      options = []
      for option in annotation.options:
        processed = self._process_one_annotation(
            node, option, name, stack, f_globals, f_locals, seen)
        if processed is None:
          return None
        elif isinstance(processed, typing_overlay.NoReturn):
          self.vm.errorlog.invalid_annotation(
              stack, option, ""NoReturn is not allowed as inner type"", name)
          return None
        options.append(processed)
      annotation.options = tuple(options)
      return annotation
    elif isinstance(annotation, (mixin.Class,
                                 abstract.AMBIGUOUS_OR_EMPTY,
                                 abstract.TypeParameter,
                                 typing_overlay.NoReturn)):
      return annotation
    else:
      self.vm.errorlog.invalid_annotation(stack, annotation, ""Not a type"", name)
      return None","1. Use `typing_overlay.NoReturn` instead of `None` to indicate that a function does not return a value.
2. Check for recursive type annotations and emit an error message instead of crashing.
3. Evaluate string annotations at runtime to ensure that they are valid."
"  def _eval_expr_as_tuple(self, node, f_globals, f_locals, expr):
    """"""Evaluate an expression as a tuple.""""""
    if not expr:
      return ()

    result = abstract_utils.get_atomic_value(
        abstract_utils.eval_expr(self.vm, node, f_globals, f_locals, expr))
    # If the result is a tuple, expand it.
    if (isinstance(result, mixin.PythonConstant) and
        isinstance(result.pyval, tuple)):
      return tuple(abstract_utils.get_atomic_value(x) for x in result.pyval)
    else:
      return (result,)","1. Use `ast.literal_eval()` instead of `eval()` to sanitize user input.
2. Use `functools.partial()` to avoid creating new objects in a loop.
3. Use `typing.Sequence` to annotate the return type of `_eval_expr_as_tuple()`."
"  def get_attribute(self, node, obj, name, valself=None):
    """"""Get the named attribute from the given object.

    Args:
      node: The current CFG node.
      obj: The object.
      name: The name of the attribute to retrieve.
      valself: A cfg.Binding to a self reference to include in the attribute's
        origins. If obj is a mixin.Class, valself can be a binding to:
        * an instance of obj - obj will be treated strictly as a class.
        * obj itself - obj will be treated as an instance of its metaclass.
        * None - if name == ""__getitem__"", obj is a type annotation; else, obj
            is strictly a class, but the attribute is left unbound.
        Else, valself is optional and should be a binding to obj when given.

    Returns:
      A tuple (CFGNode, cfg.Variable). If this attribute doesn't exist,
      the Variable will be None.
    """"""
    if name in obj.late_annotations:
      # We're using a late annotation before it's been evaluated. We could call
      # _process_one_annotation with the current (incomplete) globals, but
      # whether the call succeeds would depend on the order in which the globals
      # are analyzed. It's simpler (although less precise) to just return Any.
      return node, self.vm.new_unsolvable(node)
    # Some objects have special attributes, like ""__get__"" or ""__iter__""
    special_attribute = obj.get_special_attribute(node, name, valself)
    if special_attribute is not None:
      return node, special_attribute
    if isinstance(obj, abstract.Function):
      if name == ""__get__"":
        # The pytd for ""function"" has a __get__ attribute, but if we already
        # have a function we don't want to be treated as a descriptor.
        return node, None
      else:
        return self._get_instance_attribute(node, obj, name, valself)
    elif isinstance(obj, abstract.ParameterizedClass):
      return self.get_attribute(node, obj.base_cls, name, valself)
    elif isinstance(obj, mixin.Class):
      return self._get_class_attribute(node, obj, name, valself)
    elif isinstance(obj, overlay.Overlay):
      return self._get_module_attribute(
          node, obj.get_module(name), name, valself)
    elif isinstance(obj, abstract.Module):
      return self._get_module_attribute(node, obj, name, valself)
    elif isinstance(obj, abstract.SimpleAbstractValue):
      return self._get_instance_attribute(node, obj, name, valself)
    elif isinstance(obj, abstract.Union):
      nodes = []
      ret = self.vm.program.NewVariable()
      for o in obj.options:
        node2, attr = self.get_attribute(node, o, name, valself)
        if attr is not None:
          ret.PasteVariable(attr, node2)
          nodes.append(node2)
      if ret.bindings:
        return self.vm.join_cfg_nodes(nodes), ret
      else:
        return node, None
    elif isinstance(obj, special_builtins.SuperInstance):
      if obj.super_obj:
        cls = obj.super_cls
        valself = obj.super_obj.to_binding(node)
        skip = obj.super_cls
      else:
        cls = self.vm.convert.super_type
        skip = None
      return self._lookup_from_mro_and_handle_descriptors(
          node, cls, name, valself, skip)
    elif isinstance(obj, special_builtins.Super):
      return self.get_attribute(node, self.vm.convert.super_type, name, valself)
    elif isinstance(obj, abstract.BoundFunction):
      return self.get_attribute(node, obj.underlying, name, valself)
    elif isinstance(obj, abstract.TypeParameterInstance):
      param_var = obj.instance.get_instance_type_parameter(obj.name)
      if not param_var.bindings:
        param_var = obj.param.instantiate(self.vm.root_cfg_node)
      results = []
      nodes = []
      for v in param_var.data:
        node2, ret = self.get_attribute(node, v, name, valself)
        if ret is None:
          return node, None
        else:
          results.append(ret)
          nodes.append(node2)
      node = self.vm.join_cfg_nodes(nodes)
      return node, self.vm.join_variables(node, results)
    elif isinstance(obj, abstract.Empty):
      return node, None
    else:
      return node, None","1. Use `self.vm.new_unsolvable()` instead of `None` to return an unsolvable value.
2. Handle descriptors by calling `self._lookup_from_mro_and_handle_descriptors()`.
3. Instantiate type parameters by calling `obj.param.instantiate()`."
"  def set_attribute(self, node, obj, name, value):
    """"""Set an attribute on an object.

    The attribute might already have a Variable in it and in that case we cannot
    overwrite it and instead need to add the elements of the new variable to the
    old variable.

    Args:
      node: The current CFG node.
      obj: The object.
      name: The name of the attribute to set.
      value: The Variable to store in it.
    Returns:
      A (possibly changed) CFG node.
    Raises:
      AttributeError: If the attribute cannot be set.
      NotImplementedError: If attribute setting is not implemented for obj.
    """"""
    if not self._check_writable(obj, name):
      # We ignore the write of an attribute that's not in __slots__, since it
      # wouldn't happen in the Python interpreter, either.
      return node
    if isinstance(value, abstract.LateAnnotation):
      obj.late_annotations[name] = value
      return node
    assert isinstance(value, cfg.Variable)
    if self.vm.frame is not None and obj is self.vm.frame.f_globals:
      for v in value.data:
        v.update_official_name(name)
    if isinstance(obj, abstract.Empty):
      return node
    elif isinstance(obj, abstract.Module):
      # Assigning attributes on modules is pretty common. E.g.
      # sys.path, sys.excepthook.
      log.warning(""Ignoring overwrite of %s.%s"", obj.name, name)
      return node
    elif isinstance(obj, (abstract.StaticMethod, abstract.ClassMethod)):
      return self.set_attribute(node, obj.method, name, value)
    elif isinstance(obj, abstract.SimpleAbstractValue):
      return self._set_member(node, obj, name, value)
    elif isinstance(obj, abstract.BoundFunction):
      return self.set_attribute(node, obj.underlying, name, value)
    elif isinstance(obj, abstract.Unsolvable):
      return node
    elif isinstance(obj, abstract.Unknown):
      if name in obj.members:
        obj.members[name].PasteVariable(value, node)
      else:
        obj.members[name] = value.AssignToNewVariable(node)
      return node
    elif isinstance(obj, abstract.TypeParameterInstance):
      nodes = []
      for v in obj.instance.get_instance_type_parameter(obj.name).data:
        nodes.append(self.set_attribute(node, v, name, value))
      return self.vm.join_cfg_nodes(nodes) if nodes else node
    elif isinstance(obj, abstract.Union):
      for option in obj.options:
        node = self.set_attribute(node, option, name, value)
      return node
    else:
      raise NotImplementedError(obj.__class__.__name__)","1. Use `isinstance` to check if the object is of a certain type before performing an operation on it.
2. Use `getattr` to get the attribute of an object instead of accessing it directly.
3. Use `setattr` to set the attribute of an object instead of accessing it directly."
"  def _print_as_expected_type(self, t, instance=None):
    """"""Print abstract value t as a pytd type.""""""
    if isinstance(t, (abstract.Unknown, abstract.Unsolvable, mixin.Class,
                      abstract.Union)):
      with t.vm.convert.pytd_convert.produce_detailed_output():
        return self._pytd_print(t.get_instance_type(instance=instance))
    elif (isinstance(t, mixin.PythonConstant) and
          not getattr(t, ""could_contain_anything"", False)):
      return re.sub(r""(\\\\n|\\s)+"", "" "",
                    t.str_of_constant(self._print_as_expected_type))
    elif isinstance(t, abstract.AnnotationClass) or not t.cls:
      return t.name
    else:
      return ""<instance of %s>"" % self._print_as_expected_type(t.cls, t)","1. Use `isinstance()` to check if a value is of a certain type.
2. Use `str.of_constant()` to print a constant value.
3. Use `re.sub()` to remove whitespace from a string."
"  def __init__(self, name, param_names, varargs_name, kwonly_params,
               kwargs_name, defaults, annotations, late_annotations,
               postprocess_annotations=True):
    self.name = name
    self.param_names = param_names
    self.varargs_name = varargs_name
    self.kwonly_params = kwonly_params
    self.kwargs_name = kwargs_name
    self.defaults = defaults
    self.annotations = annotations
    self.late_annotations = late_annotations
    self.excluded_types = set()
    if postprocess_annotations:
      for k, annot in six.iteritems(self.annotations):
        self.annotations[k] = self._postprocess_annotation(k, annot)","1. Use `six.iteritems()` instead of `iteritems()` to avoid a `DeprecationWarning`.
2. Use `self._postprocess_annotation()` to sanitize the annotations.
3. Use `self.excluded_types` to filter out dangerous types."
"  def from_pytd(cls, vm, name, sig):
    """"""Construct an abstract signature from a pytd signature.""""""
    # TODO(kramm): templates
    pytd_annotations = [(p.name, p.type)
                        for p in sig.params + (sig.starargs, sig.starstarargs)
                        if p is not None]
    pytd_annotations.append((""return"", sig.return_type))
    def param_to_var(p):
      return vm.convert.constant_to_var(
          p.type, subst=datatypes.AliasingDict(), node=vm.root_cfg_node)
    return cls(
        name=name,
        param_names=tuple(p.name for p in sig.params if not p.kwonly),
        varargs_name=None if sig.starargs is None else sig.starargs.name,
        kwonly_params=set(p.name for p in sig.params if p.kwonly),
        kwargs_name=None if sig.starstarargs is None else sig.starstarargs.name,
        defaults={p.name: param_to_var(p) for p in sig.params if p.optional},
        annotations={name: vm.convert.constant_to_value(
            typ, subst=datatypes.AliasingDict(), node=vm.root_cfg_node)
                     for name, typ in pytd_annotations},
        late_annotations={},
        postprocess_annotations=False,
    )","1. Use `param_to_var` to convert parameters to variables.
2. Use `vm.convert.constant_to_var` to convert constants to variables.
3. Use `vm.convert.constant_to_value` to convert constants to values."
"  def from_callable(cls, val):
    annotations = {argname(i): val.formal_type_parameters[i]
                   for i in range(val.num_args)}
    return cls(
        name=""<callable>"",
        param_names=tuple(sorted(annotations)),
        varargs_name=None,
        kwonly_params=set(),
        kwargs_name=None,
        defaults={},
        annotations=annotations,
        late_annotations={}
    )","1. Use `typing.get_args` to get the type arguments of a function.
2. Use `typing.get_origin` to get the origin of a type.
3. Use `typing.get_args` and `typing.get_origin` to check if a function is callable with the given arguments."
"  def from_param_names(cls, name, param_names):
    """"""Construct a minimal signature from a name and a list of param names.""""""
    return cls(
        name=name,
        param_names=tuple(param_names),
        varargs_name=None,
        kwonly_params=set(),
        kwargs_name=None,
        defaults={},
        annotations={},
        late_annotations={}
    )","1. Use `typing.Annotated` to annotate the function parameters with their types. This will help catch errors at compile time.
2. Use `typing.Union` to specify the possible types of a parameter. This will help prevent type errors when calling the function with an invalid argument.
3. Use `typing.Optional` to mark parameters that may be missing. This will help prevent errors when calling the function with fewer arguments than expected."
"  def _print_annot(self, name):
    if name in self.annotations:
      return _print(self.annotations[name])
    elif name in self.late_annotations:
      return repr(self.late_annotations[name].expr)
    else:
      return None","1. Use `functools.wraps` to preserve the function's metadata, such as its name, docstring, and annotations.
2. Use `inspect.isfunction` to check if the argument is a function before calling `_print_annot`.
3. Use `ast.literal_eval` to safely evaluate the expression in `repr`."
"def make_method(vm, node, name, params=None, kwonly_params=None,
                return_type=None, self_param=None, varargs=None, kwargs=None):
  """"""Make a method from params.

  Args:
    vm: vm
    node: Node to create the method variable at
    name: The method name
    params: Positional params [type: [Param]]
    kwonly_params: Keyword only params [type: [Param]]
    return_type: Return type [type: PARAM_TYPES]
    self_param: Self param [type: Param, defaults to self: Any]
    varargs: Varargs param [type: Param, allows *args to be named and typed]
    kwargs: Kwargs param [type: Param, allows **kwargs to be named and typed]

  Returns:
    A new method wrapped in a variable.
  """"""

  def _process_annotation(param):
    """"""Process a single param into either annotations or late_annotations.""""""
    if not param.typ:
      return
    elif isinstance(param.typ, cfg.Variable):
      if all(t.cls for t in param.typ.data):
        types = param.typ.data
        if len(types) == 1:
          annotations[param.name] = types[0].cls
        else:
          t = abstract.Union([t.cls for t in types], vm)
          annotations[param.name] = t
    elif isinstance(param.typ, abstract.LateAnnotation):
      late_annotations[param.name] = param.typ
    else:
      annotations[param.name] = param.typ

  # Set default values
  params = params or []
  kwonly_params = kwonly_params or []
  self_param = self_param or Param(""self"", None, None)
  annotations = {}
  late_annotations = {}

  params = [self_param] + params

  return_param = Param(""return"", return_type, None) if return_type else None
  special_params = [x for x in (return_param, varargs, kwargs) if x]
  for param in special_params + params + kwonly_params:
    _process_annotation(param)

  if vm.PY2:
    assert not kwonly_params, ""kwonly_params is unsupported in python2""

  names = lambda xs: tuple(x.name for x in xs)
  param_names = names(params)
  kwonly_names = names(kwonly_params)
  defaults = {x.name: x.default for x in params if x.default}
  varargs_name = varargs.name if varargs else None
  kwargs_name = kwargs.name if kwargs else None

  ret = abstract.SimpleFunction(
      name=name,
      param_names=param_names,
      varargs_name=varargs_name,
      kwonly_params=kwonly_names,
      kwargs_name=kwargs_name,
      defaults=defaults,
      annotations=annotations,
      late_annotations=late_annotations,
      vm=vm)
  if late_annotations:
    vm.functions_with_late_annotations.append(ret)

  # Check that the constructed function has a valid signature
  bad_param = ret.signature.check_defaults()
  if bad_param:
    msg = ""In method %s, non-default argument %s follows default argument"" % (
        name, bad_param)
    vm.errorlog.invalid_function_definition(vm.frames, msg)

  return ret.to_variable(node)","1. Use `Param` to annotate parameters with types.
2. Use `abstract.SimpleFunction` to create a new method.
3. Check that the constructed function has a valid signature."
"  def _process_annotation(param):
    """"""Process a single param into either annotations or late_annotations.""""""
    if not param.typ:
      return
    elif isinstance(param.typ, cfg.Variable):
      if all(t.cls for t in param.typ.data):
        types = param.typ.data
        if len(types) == 1:
          annotations[param.name] = types[0].cls
        else:
          t = abstract.Union([t.cls for t in types], vm)
          annotations[param.name] = t
    elif isinstance(param.typ, abstract.LateAnnotation):
      late_annotations[param.name] = param.typ
    else:
      annotations[param.name] = param.typ","1. Use `param.typ.check()` to check the type of the parameter.
2. Use `param.typ.isinstance()` to check if the parameter is an instance of a specific class.
3. Use `param.typ.data` to get the list of possible types of the parameter."
"  def decorate(self, node, cls):
    """"""Processes the attrib members of a class.""""""
    # Collect classvars to convert them to attrs.
    if self.args[cls][""auto_attribs""]:
      ordering = classgen.Ordering.FIRST_ANNOTATE
    else:
      ordering = classgen.Ordering.LAST_ASSIGN
    ordered_locals = self.get_class_locals(
        cls, allow_methods=False, ordering=ordering)
    own_attrs = []
    late_annotation = False  # True if we find a bare late annotation
    for name, (value, orig) in ordered_locals.items():
      if is_attrib(orig):
        if not is_attrib(value) and orig.data[0].has_type:
          # We cannot have both a type annotation and a type argument.
          self.type_clash_error(value)
          attr = Attribute(
              name=name,
              typ=self.vm.new_unsolvable(node),
              init=orig.data[0].init,
              default=orig.data[0].default)
        else:
          if classgen.is_late_annotation(value):
            attr = Attribute(
                name=name,
                typ=value,
                init=orig.data[0].init,
                default=orig.data[0].default)
            cls.members[name] = orig.data[0].typ
          elif is_attrib(value):
            # Replace the attrib in the class dict with its type.
            attr = Attribute(
                name=name,
                typ=value.data[0].typ,
                init=value.data[0].init,
                default=value.data[0].default)
            if classgen.is_late_annotation(attr.typ):
              cls.members[name] = self.vm.new_unsolvable(node)
              cls.late_annotations[name] = attr.typ
              late_annotation = True
            else:
              cls.members[name] = attr.typ
          else:
            # cls.members[name] has already been set via a typecomment
            attr = Attribute(
                name=name,
                typ=value,
                init=orig.data[0].init,
                default=orig.data[0].default)
        own_attrs.append(attr)
      elif self.args[cls][""auto_attribs""]:
        # TODO(b/72678203): typing.ClassVar is the only way to filter a variable
        # out from auto_attribs, but we don't even support importing it.
        attr = Attribute(name=name, typ=value, init=True, default=orig)
        if self.add_member(node, cls, name, value, orig):
          late_annotation = True
        own_attrs.append(attr)

    # See if we need to resolve any late annotations
    if late_annotation:
      self.vm.classes_with_late_annotations.append(cls)

    base_attrs = self.get_base_class_attrs(cls, own_attrs, _ATTRS_METADATA_KEY)
    attrs = base_attrs + own_attrs
    # Stash attributes in class metadata for subclasses.
    cls.metadata[_ATTRS_METADATA_KEY] = attrs

    # Add an __init__ method
    if self.args[cls][""init""]:
      init_method = self.make_init(node, cls, attrs)
      cls.members[""__init__""] = init_method","1. Use `is_attrib()` to check if a variable is an attribute.
2. Use `add_member()` to add a member to a class.
3. Use `get_base_class_attrs()` to get the base class attributes."
"  def _instantiate_type(self, node, args, type_var):
    cls = type_var.data[0]
    try:
      return self.vm.annotations_util.init_annotation(node, cls, ""attr.ib"",
                                                      self.vm.frames)
    except self.vm.annotations_util.LateAnnotationError:
      return abstract.LateAnnotation(cls, ""attr.ib"", self.vm.simple_stack())","1. Use `typing.get_args()` to get the type arguments of a generic type.
2. Use `typing.get_origin()` to get the base type of a generic type.
3. Use `typing.is_generic()` to check if a type is generic."
"def is_attrib(var):
  if var is None or classgen.is_late_annotation(var):
    return False
  return isinstance(var.data[0], AttribInstance)","1. **Use type annotations to ensure that `var` is a valid variable.** This will help to prevent errors from being introduced when the code is modified.
2. **Check that `var.data[0]` is an instance of `AttribInstance`.** This will help to prevent the code from being used with invalid data.
3. **Use `classgen.is_late_annotation()` to check if `var` is a late annotation.** This will help to prevent the code from being used with annotations that have not yet been processed."
"def is_method(var):
  if var is None or is_late_annotation(var):
    return False
  return isinstance(var.data[0], (
      abstract.INTERPRETER_FUNCTION_TYPES,
      special_builtins.ClassMethodInstance,
      special_builtins.PropertyInstance,
      special_builtins.StaticMethodInstance
  ))","1. **Use type annotations to verify that `var` is a function or method.** This will help prevent errors from being silently ignored.
2. **Check for `None` values before calling `is_method()`.** This will prevent the function from crashing if `var` is `None`.
3. **Use `isinstance()` to check for specific types of methods.** This will help prevent the function from returning false positives."
"  def decorate(self, node, cls):
    """"""Processes class members.""""""

    # Collect classvars to convert them to attrs. @dataclass collects vars with
    # an explicit type annotation, in order of annotation, so that e.g.
    # class A:
    #   x: int
    #   y: str = 'hello'
    #   x = 10
    # would have init(x:int = 10, y:str = 'hello')
    own_attrs = []
    late_annotation = False  # True if we find a bare late annotation
    for name, (value, orig) in self.get_class_locals(
        cls, allow_methods=True, ordering=classgen.Ordering.FIRST_ANNOTATE
    ).items():
      if self.add_member(node, cls, name, value, orig):
        late_annotation = True

      if is_field(orig):
        field = orig.data[0]
        orig = field.typ if field.default else None
        init = field.init
      else:
        init = True

      # Check that default matches the declared type
      if orig and not classgen.is_late_annotation(value):
        typ = self.vm.convert.merge_classes(value.data)
        bad = self.vm.matcher.bad_matches(orig, typ, node)
        if bad:
          binding = bad[0][orig]
          self.vm.errorlog.annotation_type_mismatch(
              self.vm.frames, typ, binding, name)

      attr = classgen.Attribute(name=name, typ=value, init=init, default=orig)
      own_attrs.append(attr)

    # See if we need to resolve any late annotations
    if late_annotation:
      self.vm.classes_with_late_annotations.append(cls)

    base_attrs = self.get_base_class_attrs(
        cls, own_attrs, _DATACLASS_METADATA_KEY)
    attrs = base_attrs + own_attrs
    # Stash attributes in class metadata for subclasses.
    cls.metadata[_DATACLASS_METADATA_KEY] = attrs

    # Add an __init__ method
    if self.args[cls][""init""]:
      init_method = self.make_init(node, cls, attrs)
      cls.members[""__init__""] = init_method","1. Use `isinstance()` to check if a variable is of a certain type.
2. Use `type()` to get the type of a variable.
3. Use `globals()` to get the global variables."
"def is_field(var):
  if var is None or classgen.is_late_annotation(var):
    return False
  return isinstance(var.data[0], FieldInstance)","1. **Use `isinstance()` instead of `type()` to check the type of an object.** This is because `type()` returns the class object of the object, while `isinstance()` returns `True` if the object is an instance of the specified class or a subclass of it.
2. **Check for `None` before using the object.** This is because `None` is not an instance of any class, and using it in a conditional statement will always evaluate to `False`.
3. **Use `classgen.is_late_annotation()` to check if the object is a late annotation.** Late annotations are not processed by the compiler, so they cannot be used in security checks."
"  def _get_annotation(self, var, name):
    try:
      ret = abstract_utils.get_atomic_value(var, self._CLASS_TYPE)
      if isinstance(ret, abstract.AbstractOrConcreteValue):
        ret = abstract_utils.get_atomic_python_constant(var, six.string_types)
    except abstract_utils.ConversionError:
      raise TypeVarError(""%s must be constant"" % name)
    if not ret:
      raise TypeVarError(""%s cannot be an empty string"" % name)
    return ret","1. Use `six.ensure_str` to ensure that `var` is a string.
2. Use `abstract_utils.get_atomic_python_constant` to check if `var` is a constant.
3. Use `abstract_utils.get_atomic_value` to check if `var` is a valid value."
"  def _get_namedarg(self, args, name, default_value):
    if name not in args.namedargs:
      return default_value
    if name == ""bound"":
      return self._get_annotation(args.namedargs[name], name)
    else:
      ret = self._get_constant(args.namedargs[name], name, bool)
      # This error is logged only if _get_constant succeeds.
      self.vm.errorlog.not_supported_yet(
          self.vm.frames, ""argument \\""%s\\"" to TypeVar"" % name)
      return ret","1. Use `typing.get_args()` to get the type arguments of a type variable.
2. Use `typing.get_origin()` to get the origin of a type variable.
3. Use `typing.get_type_hints()` to get the type hints of a function or class."
"  def _get_typeparam(self, node, args):
    args = args.simplify(node, self.vm)
    try:
      self.match_args(node, args)
    except function.InvalidParameters as e:
      raise TypeVarError(""wrong arguments"", e.bad_call)
    except function.FailedFunctionCall:
      # It is currently impossible to get here, since the only
      # FailedFunctionCall that is not an InvalidParameters is NotCallable.
      raise TypeVarError(""initialization failed"")
    name = self._get_constant(args.posargs[0], ""name"", six.string_types,
                              arg_type_desc=""a constant str"")
    constraints = tuple(
        self._get_annotation(c, ""constraint"") for c in args.posargs[1:])
    if len(constraints) == 1:
      raise TypeVarError(""the number of constraints must be 0 or more than 1"")
    bound = self._get_namedarg(args, ""bound"", None)
    covariant = self._get_namedarg(args, ""covariant"", False)
    contravariant = self._get_namedarg(args, ""contravariant"", False)
    if constraints and bound:
      raise TypeVarError(""constraints and a bound are mutually exclusive"")
    extra_kwargs = set(args.namedargs) - {""bound"", ""covariant"", ""contravariant""}
    if extra_kwargs:
      raise TypeVarError(""extra keyword arguments: "" + "", "".join(extra_kwargs))
    if args.starargs:
      raise TypeVarError(""*args must be a constant tuple"")
    if args.starstarargs:
      raise TypeVarError(""ambiguous **kwargs not allowed"")
    return abstract.TypeParameter(name, self.vm, constraints=constraints,
                                  bound=bound, covariant=covariant,
                                  contravariant=contravariant)","1. Use `self.match_args()` to check if the arguments are valid.
2. Use `self._get_constant()` to check if the argument is a constant.
3. Use `self._get_namedarg()` to check if the argument is a named argument."
"  def call(self, node, _, args):
    """"""Call typing.TypeVar().""""""
    try:
      param = self._get_typeparam(node, args)
    except TypeVarError as e:
      self.vm.errorlog.invalid_typevar(
          self.vm.frames, utils.message(e), e.bad_call)
      return node, self.vm.new_unsolvable(node)
    if param.has_late_types():
      self.vm.params_with_late_types.append((param, self.vm.simple_stack()))
    return node, param.to_variable(node)","1. Use `type()` to check the type of the argument.
2. Use `assert` to validate the argument.
3. Use `logging` to log the error message."
"  def call(self, node, func, args):
    if args.posargs:
      try:
        annot = self.vm.annotations_util.process_annotation_var(
            node, args.posargs[0], ""typing.cast"", self.vm.frames)
      except self.vm.annotations_util.LateAnnotationError:
        self.vm.errorlog.invalid_annotation(
            self.vm.frames, self.vm.merge_values(args.posargs[0].data),
            ""Forward references not allowed in typing.cast.\\n""
            ""Consider switching to a type comment."")
        annot = self.vm.new_unsolvable(node)
      args = args.replace(posargs=(annot,) + args.posargs[1:])
    return super(Cast, self).call(node, func, args)","1. Use `typing.cast` to annotate the type of the argument.
2. Check if the argument is a forward reference and raise an error if it is.
3. Use `vm.new_unsolvable()` to create an unsolvable value if the argument is not a forward reference."
"  def _build_namedtuple(self, name, field_names, field_types, late_annots,
                        node):
    # Build an InterpreterClass representing the namedtuple.
    if field_types:
      # TODO(mdemello): Fix this to support late types.
      field_types_union = abstract.Union(field_types, self.vm)
    else:
      field_types_union = self.vm.convert.none_type

    members = {n: t.instantiate(node)
               for n, t in moves.zip(field_names, field_types)}

    # collections.namedtuple has: __dict__, __slots__ and _fields.
    # typing.NamedTuple adds: _field_types, __annotations__ and _field_defaults.
    # __slots__ and _fields are tuples containing the names of the fields.
    slots = tuple(self.vm.convert.build_string(node, f) for f in field_names)
    members[""__slots__""] = abstract.Tuple(slots, self.vm).to_variable(node)
    members[""_fields""] = abstract.Tuple(slots, self.vm).to_variable(node)
    # __dict__ and _field_defaults are both collections.OrderedDicts that map
    # field names (strings) to objects of the field types.
    ordered_dict_cls = self.vm.convert.name_to_value(""collections.OrderedDict"",
                                                     ast=self.collections_ast)

    # In Python 2, keys can be `str` or `unicode`; support both.
    # In Python 3, `str_type` and `unicode_type` are the same.
    field_keys_union = abstract.Union([self.vm.convert.str_type,
                                       self.vm.convert.unicode_type], self.vm)

    # Normally, we would use abstract_utils.K and abstract_utils.V, but
    # collections.pyi doesn't conform to that standard.
    field_dict_cls = abstract.ParameterizedClass(
        ordered_dict_cls,
        {""K"": field_keys_union, ""V"": field_types_union},
        self.vm)
    members[""__dict__""] = field_dict_cls.instantiate(node)
    members[""_field_defaults""] = field_dict_cls.instantiate(node)
    # _field_types and __annotations__ are both collections.OrderedDicts
    # that map field names (strings) to the types of the fields.
    field_types_cls = abstract.ParameterizedClass(
        ordered_dict_cls,
        {""K"": field_keys_union, ""V"": self.vm.convert.type_type},
        self.vm)
    members[""_field_types""] = field_types_cls.instantiate(node)
    members[""__annotations__""] = field_types_cls.instantiate(node)

    # __new__
    # We set the bound on this TypeParameter later. This gives __new__ the
    # signature: def __new__(cls: Type[_Tname], ...) -> _Tname, i.e. the same
    # signature that visitor.CreateTypeParametersForSignatures would create.
    # This allows subclasses of the NamedTuple to get the correct type from
    # their constructors.
    cls_type_param = abstract.TypeParameter(
        visitors.CreateTypeParametersForSignatures.PREFIX + name,
        self.vm, bound=None)
    cls_type = abstract.ParameterizedClass(
        self.vm.convert.type_type, {abstract_utils.T: cls_type_param}, self.vm)
    # Use late annotations as field types if they exist.
    params = [Param(n, late_annots.get(n, t))
              for n, t in moves.zip(field_names, field_types)]
    members[""__new__""] = overlay_utils.make_method(
        self.vm, node,
        name=""__new__"",
        self_param=Param(""cls"", cls_type),
        params=params,
        return_type=cls_type_param,
    )

    # __init__
    members[""__init__""] = overlay_utils.make_method(
        self.vm, node,
        name=""__init__"",
        varargs=Param(""args""),
        kwargs=Param(""kwargs""))

    # _make
    # _make is a classmethod, so it needs to be wrapped by
    # specialibuiltins.ClassMethodInstance.
    # Like __new__, it uses the _Tname TypeVar.
    sized_cls = self.vm.convert.name_to_value(""typing.Sized"")
    iterable_type = abstract.ParameterizedClass(
        self.vm.convert.name_to_value(""typing.Iterable""),
        {abstract_utils.T: field_types_union}, self.vm)
    cls_type = abstract.ParameterizedClass(
        self.vm.convert.type_type,
        {abstract_utils.T: cls_type_param}, self.vm)
    len_type = abstract.CallableClass(
        self.vm.convert.name_to_value(""typing.Callable""),
        {0: sized_cls,
         abstract_utils.ARGS: sized_cls,
         abstract_utils.RET: self.vm.convert.int_type},
        self.vm)
    params = [
        Param(""iterable"", iterable_type),
        Param(""new"").unsolvable(self.vm, node),
        Param(""len"", len_type).unsolvable(self.vm, node)]
    make = overlay_utils.make_method(
        self.vm, node,
        name=""_make"",
        params=params,
        self_param=Param(""cls"", cls_type),
        return_type=cls_type_param)
    make_args = function.Args(posargs=(make,))
    _, members[""_make""] = self.vm.special_builtins[""classmethod""].call(
        node, None, make_args)

    # _replace
    # Like __new__, it uses the _Tname TypeVar. We have to annotate the `self`
    # param to make sure the TypeVar is substituted correctly.
    members[""_replace""] = overlay_utils.make_method(
        self.vm, node,
        name=""_replace"",
        self_param=Param(""self"", cls_type_param),
        return_type=cls_type_param,
        kwargs=Param(""kwds"", field_types_union))

    # __getnewargs__
    getnewargs_tuple_params = dict(
        tuple(enumerate(field_types)) +
        ((abstract_utils.T, field_types_union),))
    getnewargs_tuple = abstract.TupleClass(self.vm.convert.tuple_type,
                                           getnewargs_tuple_params, self.vm)
    members[""__getnewargs__""] = overlay_utils.make_method(
        self.vm, node,
        name=""__getnewargs__"",
        return_type=getnewargs_tuple)

    # __getstate__
    members[""__getstate__""] = overlay_utils.make_method(
        self.vm, node, name=""__getstate__"")

    # _asdict
    members[""_asdict""] = overlay_utils.make_method(
        self.vm, node,
        name=""_asdict"",
        return_type=field_dict_cls)

    # Finally, make the class.
    cls_dict = abstract.Dict(self.vm)
    cls_dict.update(node, members)
    if name.__class__ is compat.UnicodeType:
      # Unicode values should be ASCII.
      name = compat.native_str(name.encode(""ascii""))

    node, cls_var = self.vm.make_class(
        node=node,
        name_var=self.vm.convert.build_string(node, name),
        bases=[self.vm.convert.tuple_type.to_variable(node)],
        class_dict_var=cls_dict.to_variable(node),
        cls_var=None)
    cls = cls_var.data[0]

    # Now that the class has been made, we can complete the TypeParameter used
    # by __new__, _make and _replace.
    cls_type_param.bound = cls

    # Add late annotations to the new class
    if late_annots:
      cls.late_annotations = late_annots
      self.vm.classes_with_late_annotations.append(cls)

    return node, cls_var","1. Use `self_param=Param(""self"", cls_type_param)` to make sure the TypeVar is substituted correctly.
2. Use `self.vm.special_builtins[""classmethod""].call(node, None, make_args)` to wrap _make as a classmethod.
3. Use `overlay_utils.make_method(node, name=""__getnewargs__"", return_type=getnewargs_tuple)` to create a `__getnewargs__` method."
"  def call(self, node, _, args):
    try:
      name_var, field_names, field_types = self._getargs(node, args)
    except abstract_utils.ConversionError:
      return node, self.vm.new_unsolvable(node)

    try:
      name = abstract_utils.get_atomic_python_constant(name_var)
    except abstract_utils.ConversionError:
      return node, self.vm.new_unsolvable(node)

    try:
      field_names = self._validate_and_rename_args(name, field_names, False)
    except ValueError as e:
      self.vm.errorlog.invalid_namedtuple_arg(self.vm.frames, utils.message(e))
      return node, self.vm.new_unsolvable(node)

    annots, late_annots = self.vm.annotations_util.convert_annotations_list(
        node, moves.zip(field_names, field_types))
    field_types = [annots.get(field_name, self.vm.convert.unsolvable)
                   for field_name in field_names]
    node, cls_var = self._build_namedtuple(name, field_names, field_types,
                                           late_annots, node)

    self.vm.trace_classdef(cls_var)
    return node, cls_var","1. Use `typing.check_type` to validate the type of arguments.
2. Use `typing.NamedTuple` to create a namedtuple with the correct field names and types.
3. Use `typing.Annotated` to annotate the fields with their types."
"  def __init__(self,
               errorlog,
               options,
               loader,
               generate_unknowns=False,
               store_all_calls=False):
    """"""Construct a TypegraphVirtualMachine.""""""
    self.maximum_depth = None  # set by run_program() and analyze()
    self.errorlog = errorlog
    self.options = options
    self.python_version = options.python_version
    self.PY2 = self.python_version[0] == 2
    self.PY3 = self.python_version[0] == 3
    self.generate_unknowns = generate_unknowns
    self.store_all_calls = store_all_calls
    self.loader = loader
    self.frames = []  # The call stack of frames.
    self.functions_with_late_annotations = []
    self.classes_with_late_annotations = []
    self.functions_type_params_check = []
    self.params_with_late_types = []
    self.concrete_classes = []
    self.frame = None  # The current frame.
    self.program = cfg.Program()
    self.root_cfg_node = self.program.NewCFGNode(""root"")
    self.program.entrypoint = self.root_cfg_node
    self.annotations_util = annotations_util.AnnotationsUtil(self)
    self.attribute_handler = attribute.AbstractAttributeHandler(self)
    self.matcher = matcher.AbstractMatcher(self)
    self.loaded_overlays = {}  # memoize which overlays are loaded
    self.convert = convert.Converter(self)
    self.program.default_data = self.convert.unsolvable
    self.has_unknown_wildcard_imports = False
    self.callself_stack = []
    self.filename = None
    self.director = None
    self._analyzing = False  # Are we in self.analyze()?
    self.opcode_traces = []
    self._importing = False  # Are we importing another file?
    self._trace_opcodes = True  # whether to trace opcodes

    # Track the order of creation of local vars, for attrs and dataclasses.
    # { code.co_name: (var_name, value-or-type, original value) }
    # (We store the original value because type-annotated classvars are replaced
    # by their stated type in the locals dict.)
    # local_ops contains the order of assignments and annotations, and
    # annotated_locals contains a record of the annotated and original values of
    # the locals.
    self.local_ops = collections.defaultdict(list)
    self.annotated_locals = collections.defaultdict(dict)

    # Map from builtin names to canonical objects.
    self.special_builtins = {
        # The super() function.
        ""super"": self.convert.super_type,
        # The object type.
        ""object"": self.convert.object_type,
        # for more pretty branching tests.
        ""__random__"": self.convert.primitive_class_instances[bool],
        # for debugging
        ""reveal_type"": special_builtins.RevealType(self),
        # boolean values.
        ""True"": self.convert.true,
        ""False"": self.convert.false,
        # builtin classes
        ""property"": special_builtins.Property(self),
        ""staticmethod"": special_builtins.StaticMethod(self),
        ""classmethod"": special_builtins.ClassMethod(self),
    }
    # builtin functions
    for cls in (
        special_builtins.Abs,
        special_builtins.HasAttr,
        special_builtins.IsCallable,
        special_builtins.IsInstance,
        special_builtins.IsSubclass,
        special_builtins.Next,
        special_builtins.Open
    ):
      self.special_builtins[cls.name] = cls.make(self)","1. Use `self.convert.unsolvable` instead of `None` to represent unknown types.
2. Use `self.errorlog.error(message)` to log errors instead of printing them to stdout.
3. Use `self.program.default_data` to represent the default value for a type variable."
"  def trace_opcode(self, op, symbol, val):
    """"""Record trace data for other tools to use.""""""
    if not self._trace_opcodes:
      return

    if self.frame and not op:
      op = self.frame.current_opcode
    if not op:
      # If we don't have a current opcode, don't emit a trace.
      return

    def get_data(v):
      data = getattr(v, ""data"", None)
      # Sometimes v is a binding.
      return [data] if data and not isinstance(data, list) else data

    # isinstance(val, tuple) generates false positives for internal classes like
    # LateAnnotations that are namedtuples.
    if val.__class__ == tuple:
      data = tuple(get_data(v) for v in val)
    else:
      data = (get_data(val),)
    rec = (op, symbol, data)
    self.opcode_traces.append(rec)","1. Sanitize user input to prevent against injection attacks.
2. Use proper error handling to prevent leaking sensitive information.
3. Use strong encryption to protect sensitive data."
"  def _process_base_class(self, node, base):
    """"""Process a base class for InterpreterClass creation.""""""
    new_base = self.program.NewVariable()
    for b in base.bindings:
      if isinstance(b.data, abstract.AnnotationContainer):
        new_base.AddBinding(b.data.base_cls, {b}, node)
      elif isinstance(b.data, abstract.Union):
        # Union[A,B,...] is a valid base class, but we need to flatten it into a
        # single base variable.
        for o in b.data.options:
          new_base.AddBinding(o, {b}, node)
      else:
        new_base.AddBinding(b.data, {b}, node)
    base = new_base
    if not any(isinstance(t, (mixin.Class, abstract.AMBIGUOUS_OR_EMPTY))
               for t in base.data):
      self.errorlog.base_class_error(self.frames, base)
    return base","1. Use `isinstance()` to check if the base class is a valid class.
2. Sanitize the input to prevent malicious code from being executed.
3. Use `self.errorlog.base_class_error()` to log errors and prevent the code from running."
"  def make_class(self, node, name_var, bases, class_dict_var, cls_var,
                 new_class_var=None):
    """"""Create a class with the name, bases and methods given.

    Args:
      node: The current CFG node.
      name_var: Class name.
      bases: Base classes.
      class_dict_var: Members of the class, as a Variable containing an
          abstract.Dict value.
      cls_var: The class's metaclass, if any.
      new_class_var: If not None, make_class() will return new_class_var with
          the newly constructed class added as a binding. Otherwise, a new
          variable if returned.

    Returns:
      A node and an instance of Class.
    """"""
    name = abstract_utils.get_atomic_python_constant(name_var)
    log.info(""Declaring class %s"", name)
    try:
      class_dict = abstract_utils.get_atomic_value(class_dict_var)
    except abstract_utils.ConversionError:
      log.error(""Error initializing class %r"", name)
      return self.convert.create_new_unknown(node)
    # Handle six.with_metaclass.
    metacls, bases = self._filter_out_metaclasses(bases)
    if metacls:
      cls_var = metacls
    # Flatten Unions in the bases
    bases = [self._process_base_class(node, base) for base in bases]
    if not bases:
      # Old style class.
      bases = [self.convert.oldstyleclass_type.to_variable(self.root_cfg_node)]
    if (isinstance(class_dict, abstract.Unsolvable) or
        not isinstance(class_dict, mixin.PythonConstant)):
      # An unsolvable appears here if the vm hit maximum depth and gave up on
      # analyzing the class we're now building. Otherwise, if class_dict isn't
      # a constant, then it's an abstract dictionary, and we don't have enough
      # information to continue building the class.
      var = self.new_unsolvable(node)
    else:
      if cls_var is None:
        cls_var = class_dict.members.get(""__metaclass__"")
        if cls_var and self.PY3:
          # This way of declaring metaclasses no longer works in Python 3.
          self.errorlog.ignored_metaclass(
              self.frames, name,
              cls_var.data[0].full_name if cls_var.bindings else ""Any"")
      if cls_var and all(v.data.full_name == ""__builtin__.type""
                         for v in cls_var.bindings):
        cls_var = None
      # pylint: disable=g-long-ternary
      cls = abstract_utils.get_atomic_value(
          cls_var, default=self.convert.unsolvable) if cls_var else None
      try:
        val = abstract.InterpreterClass(
            name,
            bases,
            class_dict.pyval,
            cls,
            self)
        if class_dict.late_annotations:
          val.late_annotations = class_dict.late_annotations
          self.classes_with_late_annotations.append(val)
      except mro.MROError as e:
        self.errorlog.mro_error(self.frames, name, e.mro_seqs)
        var = self.new_unsolvable(node)
      except abstract_utils.GenericTypeError as e:
        self.errorlog.invalid_annotation(self.frames, e.annot, e.error)
        var = self.new_unsolvable(node)
      else:
        if new_class_var:
          var = new_class_var
        else:
          var = self.program.NewVariable()
        var.AddBinding(val, class_dict_var.bindings, node)
        node = val.call_metaclass_init(node)
        if not val.is_abstract:
          # Since a class decorator could have made the class inherit from
          # ABCMeta, we have to mark concrete classes now and check for
          # abstract methods at postprocessing time.
          self.concrete_classes.append((val, self.simple_stack()))
    self.trace_opcode(None, name, var)
    return node, var","1. Use `isinstance` to check if a variable is of a certain type.
2. Use `assert` to check for invalid inputs.
3. Sanitize user input before using it in your code."
"  def _make_function(self, name, node, code, globs, defaults, kw_defaults,
                     closure=None, annotations=None, late_annotations=None):
    """"""Create a function or closure given the arguments.""""""
    if closure:
      closure = tuple(
          c for c in abstract_utils.get_atomic_python_constant(closure))
      log.info(""closure: %r"", closure)
    if not name:
      name = abstract_utils.get_atomic_python_constant(code).co_name
    if not name:
      name = ""<lambda>""
    val = abstract.InterpreterFunction.make(
        name, code=abstract_utils.get_atomic_python_constant(code),
        f_locals=self.frame.f_locals, f_globals=globs,
        defaults=defaults, kw_defaults=kw_defaults,
        closure=closure, annotations=annotations,
        late_annotations=late_annotations, vm=self)
    # TODO(ampere): What else needs to be an origin in this case? Probably stuff
    # in closure.
    var = self.program.NewVariable()
    var.AddBinding(val, code.bindings, node)
    if late_annotations:
      self.functions_with_late_annotations.append(val)
    elif val.signature.annotations:
      self.functions_type_params_check.append((val, self.frame.current_opcode))
    return var","1. Use `abstract_utils.get_atomic_python_constant()` to get the atomic python constant of the function.
2. Use `abstract.InterpreterFunction.make()` to create a new function object.
3. Use `self.program.NewVariable()` to create a new variable and add the binding to the variable."
"  def simple_stack(self, opcode=None):
    """"""Get a stack of simple frames.

    Args:
      opcode: Optionally, an opcode to create a stack for.

    Returns:
      If an opcode is provided, a stack with a single frame at that opcode.
      Otherwise, the VM's current stack converted to simple frames.
    """"""
    if opcode is not None:
      return [frame_state.SimpleFrame(opcode)]
    elif self.frame:
      # Simple stacks are used for things like late annotations, which don't
      # need tracebacks in their errors, so we convert just the current frame.
      return [frame_state.SimpleFrame(self.frame.current_opcode)]
    else:
      return []","1. Use `inspect.getframeinfo` to get the current frame information instead of accessing `self.frame` directly. This will prevent unauthorized access to the frame information.
2. Use `frame_state.SimpleFrame` to create a simple frame stack instead of directly accessing the VM's current stack. This will prevent unauthorized access to the stack information.
3. Sanitize the opcode argument before using it to create a simple frame stack. This will prevent unauthorized access to the stack by passing in an invalid opcode."
"  def run_program(self, src, filename, maximum_depth):
    """"""Run the code and return the CFG nodes.

    Args:
      src: The program source code.
      filename: The filename the source is from.
      maximum_depth: Maximum depth to follow call chains.
    Returns:
      A tuple (CFGNode, set) containing the last CFGNode of the program as
        well as all the top-level names defined by it.
    """"""
    director = directors.Director(
        src, self.errorlog, filename, self.options.disable)
    # This modifies the errorlog passed to the constructor.  Kind of ugly,
    # but there isn't a better way to wire both pieces together.
    self.errorlog.set_error_filter(director.should_report_error)
    self.director = director
    self.filename = filename

    self.maximum_depth = maximum_depth

    code = self.compile_src(src, filename=filename)
    visitor = _FindIgnoredTypeComments(self.director.type_comments)
    pyc.visit(code, visitor)
    for line in visitor.ignored_lines():
      self.errorlog.ignored_type_comment(
          self.filename, line, self.director.type_comments[line][1])

    node = self.root_cfg_node.ConnectNew(""init"")
    node, f_globals, f_locals, _ = self.run_bytecode(node, code)
    logging.info(""Done running bytecode, postprocessing globals"")
    # Check for abstract methods on non-abstract classes.
    for val, frames in self.concrete_classes:
      if not val.is_abstract:
        for member in sum((var.data for var in val.members.values()), []):
          if isinstance(member, abstract.Function) and member.is_abstract:
            self.errorlog.ignored_abstractmethod(frames, val.name, member.name)
    for param, frames in self.params_with_late_types:
      try:
        param.resolve_late_types(node, f_globals, f_locals)
      except abstract.TypeParameterError as e:
        self.errorlog.invalid_typevar(frames, utils.message(e))
    for func in self.functions_with_late_annotations:
      self.annotations_util.eval_function_late_annotations(
          node, func, f_globals, f_locals)
    for cls in self.classes_with_late_annotations:
      self.annotations_util.eval_class_late_annotations(
          node, cls, f_globals, f_locals)
    for func, opcode in self.functions_type_params_check:
      func.signature.check_type_parameter_count(self.simple_stack(opcode))
    while f_globals.late_annotations:
      name, annot = f_globals.late_annotations.popitem()
      attr = self.annotations_util.init_annotation(
          node, annot.expr, annot.name, annot.stack, f_globals, f_locals)
      self.attribute_handler.set_attribute(node, f_globals, name, attr)
    assert not self.frames, ""Frames left over!""
    log.info(""Final node: <%d>%s"", node.id, node.name)
    return node, f_globals.members","1. Use type annotations to prevent type errors.
2. Use annotations to check for type parameter count.
3. Use annotations to initialize annotations."
"  def load_from(self, state, store, name, discard_concrete_values=False):
    """"""Load an item out of locals, globals, or builtins.""""""
    assert isinstance(store, abstract.SimpleAbstractValue)
    assert store.is_lazy
    if name in store.late_annotations:
      # Unresolved late annotation. See attribute.py:get_attribute
      return state, self.new_unsolvable(state.node)
    store.load_lazy_attribute(name)
    bindings = store.members[name].Bindings(state.node)
    if not bindings:
      raise KeyError(name)
    ret = self.program.NewVariable()
    self._filter_none_and_paste_bindings(
        state.node, bindings, ret,
        discard_concrete_values=discard_concrete_values)
    return state, ret","1. Use `isinstance` to check if the argument is of the correct type.
2. Use `assert` to check for expected conditions.
3. Use `raise` to throw an exception if an error occurs."
"  def _store_value(self, state, name, value, local):
    if local:
      target = self.frame.f_locals
    else:
      target = self.frame.f_globals
    node = self.attribute_handler.set_attribute(state.node, target, name, value)
    return state.change_cfg_node(node)","1. Use `check_security` to check if the user has permission to access the variable before storing it.
2. Use `safe_eval` to evaluate the value of the variable, and escape any malicious code.
3. Use `contextlib.closing` to close the file handle after the variable is stored."
"  def byte_LOAD_NAME(self, state, op):
    """"""Load a name. Can be a local, global, or builtin.""""""
    name = self.frame.f_code.co_names[op.arg]
    try:
      state, val = self.load_local(state, name)
    except KeyError:
      try:
        state, val = self.load_global(state, name)
      except KeyError:
        try:
          if self._is_private(name):
            # Private names must be explicitly imported.
            self.trace_opcode(op, name, None)
            raise KeyError()
          state, val = self.load_builtin(state, name)
        except KeyError:
          if self._is_private(name) or not self.has_unknown_wildcard_imports:
            self.errorlog.name_error(self.frames, name)
          self.trace_opcode(op, name, None)
          return state.push(self.new_unsolvable(state.node))
    self.check_for_deleted(state, name, val)
    self.trace_opcode(op, name, val)
    return state.push(val)","1. Use `self.load_local` to load local variables.
2. Use `self.load_global` to load global variables.
3. Use `self.load_builtin` to load built-in functions."
"  def byte_LOAD_FAST(self, state, op):
    """"""Load a local. Unlike LOAD_NAME, it doesn't fall back to globals.""""""
    name = self.frame.f_code.co_varnames[op.arg]
    try:
      state, val = self.load_local(state, name)
    except KeyError:
      self.errorlog.name_error(self.frames, name)
      val = self.new_unsolvable(state.node)
    self.check_for_deleted(state, name, val)
    self.trace_opcode(op, name, val)
    return state.push(val)","1. **Use `LOAD_NAME` instead of `LOAD_FAST`.** `LOAD_FAST` does not fall back to globals, which can lead to security vulnerabilities if a malicious user is able to inject a variable name into the code.
2. **Check for deleted variables.** The `check_for_deleted` function should be used to check if a variable has been deleted before attempting to access it.
3. **Trace the opcode.** The `trace_opcode` function should be used to track the execution of the code, which can help to identify security vulnerabilities."
"  def byte_LOAD_GLOBAL(self, state, op):
    """"""Load a global variable, or fall back to trying to load a builtin.""""""
    name = self.frame.f_code.co_names[op.arg]
    if name == ""None"":
      # Load None itself as a constant to avoid the None filtering done on
      # variables. This workaround is safe because assigning to None is a
      # syntax error.
      return self.load_constant(state, op, None)
    try:
      state, val = self.load_global(state, name)
    except KeyError:
      try:
        state, val = self.load_builtin(state, name)
      except KeyError:
        self.errorlog.name_error(self.frames, name)
        self.trace_opcode(op, name, None)
        return state.push(self.new_unsolvable(state.node))
    self.check_for_deleted(state, name, val)
    self.trace_opcode(op, name, val)
    return state.push(val)","1. Use `sys.intern()` to check if the variable name is a keyword.
2. Use `builtins.isinstance()` to check if the variable is a builtin.
3. Use `type()` to check if the variable is a valid type."
"  def byte_STORE_SUBSCR(self, state, op):
    """"""Implement obj[subscr] = val.""""""
    state, (val, obj, subscr) = state.popn(3)
    state = state.forward_cfg_node()
    if self._is_annotations_dict(obj):
      try:
        name = abstract_utils.get_atomic_python_constant(
            subscr, six.string_types)
      except abstract_utils.ConversionError:
        pass
      else:
        state = self._store_annotation(state, name, val)
    state = self.store_subscr(state, obj, subscr, val)
    return state","1. Use `isinstance()` to check if `obj` is an `annotations_dict` before storing the annotation.
2. Use `abstract_utils.get_atomic_python_constant()` to sanitize the annotation name.
3. Use `self.store_subscr()` to store the annotation."
"  def _get_extra_function_args(self, state, arg):
    """"""Get function annotations and defaults from the stack. (Python3.5-).""""""
    if self.PY2:
      num_pos_defaults = arg & 0xffff
      num_kw_defaults = 0
    else:
      assert self.PY3
      num_pos_defaults = arg & 0xff
      num_kw_defaults = (arg >> 8) & 0xff
    state, raw_annotations = state.popn((arg >> 16) & 0x7fff)
    state, kw_defaults = state.popn(2 * num_kw_defaults)
    state, pos_defaults = state.popn(num_pos_defaults)
    free_vars = None  # Python < 3.6 does not handle closure vars here.
    kw_defaults = self._convert_kw_defaults(kw_defaults)
    annot, late_annot = self.annotations_util.convert_function_annotations(
        state.node, raw_annotations)
    return state, pos_defaults, kw_defaults, annot, late_annot, free_vars","1. Use `self.PY2` and `self.PY3` to check the Python version.
2. Use `assert self.PY3` to make sure the code is only run on Python 3.
3. Use `self.annotations_util.convert_function_annotations` to convert the function annotations to a more secure format."
"  def _get_extra_function_args_3_6(self, state, arg):
    """"""Get function annotations and defaults from the stack (Python3.6+).""""""
    free_vars = None
    pos_defaults = ()
    kw_defaults = {}
    annot = {}
    if arg & loadmarshal.MAKE_FUNCTION_HAS_FREE_VARS:
      state, free_vars = state.pop()
    if arg & loadmarshal.MAKE_FUNCTION_HAS_ANNOTATIONS:
      state, packed_annot = state.pop()
      annot = abstract_utils.get_atomic_python_constant(packed_annot, dict)
      for k in annot.keys():
        annot[k] = self.annotations_util.convert_function_type_annotation(
            k, annot[k])
    if arg & loadmarshal.MAKE_FUNCTION_HAS_KW_DEFAULTS:
      state, packed_kw_def = state.pop()
      kw_defaults = abstract_utils.get_atomic_python_constant(
          packed_kw_def, dict)
    if arg & loadmarshal.MAKE_FUNCTION_HAS_POS_DEFAULTS:
      state, packed_pos_def = state.pop()
      pos_defaults = abstract_utils.get_atomic_python_constant(
          packed_pos_def, tuple)
    annot, late_annot = self.annotations_util.convert_annotations_list(
        state.node, annot.items())
    return state, pos_defaults, kw_defaults, annot, late_annot, free_vars","1. Use `functools.wraps` to preserve the function metadata.
2. Sanitize user input to prevent code injection attacks.
3. Use `typing` to annotate function arguments and return values."
"  def _process_function_type_comment(self, op, annotations, late_annotations):
    """"""Modifies annotations/late_annotations from a function type comment.

    Checks if a type comment is present for the function.  If so, the type
    comment is used to populate late_annotations.  It is an error to have
    a type comment when annotations or late_annotations is not empty.

    Args:
      op: An opcode (used to determine filename and line number).
      annotations: A dict of annotations.
      late_annotations: A dict of late annotations.
    """"""
    if not op.type_comment:
      return

    comment, lineno = op.type_comment

    # It is an error to use a type comment on an annotated function.
    if annotations or late_annotations:
      self.errorlog.redundant_function_type_comment(op.code.co_filename, lineno)
      return

    # Parse the comment, use a fake Opcode that is similar to the original
    # opcode except that it is set to the line number of the type comment.
    # This ensures that errors are printed with an accurate line number.
    fake_stack = self.simple_stack(op.at_line(lineno))
    m = _FUNCTION_TYPE_COMMENT_RE.match(comment)
    if not m:
      self.errorlog.invalid_function_type_comment(fake_stack, comment)
      return
    args, return_type = m.groups()

    # Add type info to late_annotations.
    if args != ""..."":
      annot = abstract.LateAnnotation(
          args.strip(), function.MULTI_ARG_ANNOTATION, fake_stack)
      late_annotations[function.MULTI_ARG_ANNOTATION] = annot

    ret = self.convert.build_string(None, return_type).data[0]
    late_annotations[""return""] = abstract.LateAnnotation(
        ret, ""return"", fake_stack)","1. Use a type comment to annotate the function.
2. Check if the annotations or late_annotations is not empty.
3. Use a fake Opcode to set the line number of the type comment."
"  def byte_MAKE_FUNCTION(self, state, op):
    """"""Create a function and push it onto the stack.""""""
    if self.PY2:
      name = None
    else:
      assert self.PY3
      state, name_var = state.pop()
      name = abstract_utils.get_atomic_python_constant(name_var)
    state, code = state.pop()
    if self.python_version >= (3, 6):
      get_args = self._get_extra_function_args_3_6
    else:
      get_args = self._get_extra_function_args
    state, defaults, kw_defaults, annot, late_annot, free_vars = (
        get_args(state, op.arg))
    self._process_function_type_comment(op, annot, late_annot)
    # TODO(dbaum): Add support for per-arg type comments.
    # TODO(dbaum): Add support for variable type comments.
    globs = self.get_globals_dict()
    fn = self._make_function(name, state.node, code, globs, defaults,
                             kw_defaults, annotations=annot,
                             late_annotations=late_annot,
                             closure=free_vars)
    self.trace_opcode(op, name, fn)
    self.trace_functiondef(fn)
    return state.push(fn)","1. Use `_get_extra_function_args_3_6` instead of `_get_extra_function_args` to handle Python 3.6+ functions.
2. Add support for per-arg type comments.
3. Add support for variable type comments."
"  def byte_MAKE_CLOSURE(self, state, op):
    """"""Make a function that binds local variables.""""""
    if self.PY2:
      # The py3 docs don't mention this change.
      name = None
    else:
      assert self.PY3
      state, name_var = state.pop()
      name = abstract_utils.get_atomic_python_constant(name_var)
    state, (closure, code) = state.popn(2)
    state, defaults, kw_defaults, annot, late_annot, _ = (
        self._get_extra_function_args(state, op.arg))
    globs = self.get_globals_dict()
    fn = self._make_function(name, state.node, code, globs, defaults,
                             kw_defaults, annotations=annot,
                             late_annotations=late_annot, closure=closure)
    self.trace_functiondef(fn)
    return state.push(fn)","1. Use `typing.get_type_hints` to get the type hints of the function arguments.
2. Use `typing.cast` to cast the function arguments to the correct types.
3. Use `typing.no_type_check` to disable type checking for specific parts of the code."
"  def byte_STORE_ANNOTATION(self, state, op):
    """"""Implementation of the STORE_ANNOTATION opcode.""""""
    state, annotations_var = self.load_local(state, ""__annotations__"")
    name = self.frame.f_code.co_names[op.arg]
    state, value = state.pop()
    state = self._store_annotation(state, name, value)
    name_var = self.convert.build_string(state.node, name)
    state = self.store_subscr(state, annotations_var, name_var, value)
    return self.store_local(state, ""__annotations__"", annotations_var)","1. Use `annotations` instead of `__annotations__` to avoid name clashes.
2. Use `self.convert.build_string(state.node, name)` to escape the name before storing it.
3. Use `self.store_subscr(state, annotations_var, name_var, value)` to store the value with the correct type."
"  def get_type_key(self, seen=None):
    cached_changestamps, saved_key = self._cached_type_key
    if saved_key and cached_changestamps == (
        self.members.changestamp,
        self.instance_type_parameters.changestamp):
      return saved_key
    if not seen:
      seen = set()
    seen.add(self)
    key = set()
    if self.cls:
      key.add(self.cls)
    for name, var in self.instance_type_parameters.items():
      subkey = frozenset(value.data.get_default_type_key() if value.data in seen
                         else value.data.get_type_key(seen)
                         for value in var.bindings)
      key.add((name, subkey))
    if key:
      type_key = frozenset(key)
    else:
      type_key = super(SimpleAbstractValue, self).get_type_key()
    self._cached_type_key = (
        (self.members.changestamp, self.instance_type_parameters.changestamp),
        type_key)
    return type_key","1. Use `functools.lru_cache` to cache the results of `get_type_key`.
2. Check the `changestamps` before returning the cached value.
3. Use `super(SimpleAbstractValue, self).get_type_key()` to get the type key of the superclass."
"  def call(self, node, _, args, alias_map=None):
    funcvar, name = args.posargs[0:2]
    kwargs = args.namedargs.pyval
    # TODO(mdemello): Check if there are any changes between python2 and
    # python3 in the final metaclass computation.
    # TODO(mdemello): Any remaining kwargs need to be passed to the metaclass.
    metaclass = kwargs.get(""metaclass"", None)
    if len(funcvar.bindings) != 1:
      raise abstract_utils.ConversionError(
          ""Invalid ambiguous argument to __build_class__"")
    func, = funcvar.data
    if not isinstance(func, InterpreterFunction):
      raise abstract_utils.ConversionError(
          ""Invalid argument to __build_class__"")
    func.is_class_builder = True
    bases = args.posargs[2:]

    node, _ = func.call(node, funcvar.bindings[0],
                        args.replace(posargs=(), namedargs={}),
                        new_locals=True)
    if func.last_frame:
      func.f_locals = func.last_frame.f_locals
      class_closure_var = func.last_frame.class_closure_var
    else:
      # We have hit 'maximum depth' before setting func.last_frame
      func.f_locals = self.vm.convert.unsolvable
      class_closure_var = None
    for base in bases:
      # If base class is NamedTuple, we will call its own make_class method to
      # make a class.
      base = abstract_utils.get_atomic_value(
          base, default=self.vm.convert.unsolvable)
      if isinstance(base, PyTDClass) and base.full_name == ""typing.NamedTuple"":
        # The subclass of NamedTuple will ignore all its base classes. This is
        # controled by a metaclass provided to NamedTuple.
        # See: https://github.com/python/typing/blob/master/src/typing.py#L2170
        return base.make_class(node, func.f_locals.to_variable(node))
    return node, self.vm.make_class(
        node, name, list(bases), func.f_locals.to_variable(node), metaclass,
        new_class_var=class_closure_var)","1. Use `func.is_class_builder` to check if the function is a class builder.
2. Use `self.vm.make_class` to create a new class.
3. Use `class_closure_var` to track the class closure."
"  def call(self, node, _, args):
    posargs = args.posargs
    namedargs = self.vm.convert.value_to_constant(args.namedargs, dict)
    if namedargs and self.vm.python_version < (3, 6):
      errmsg = ""Keyword syntax for NamedTuple is only supported in Python 3.6+""
      self.vm.errorlog.invalid_namedtuple_arg(self.vm.frames, err_msg=errmsg)
    if namedargs and len(posargs) == 1:
      namedargs = [abstract.Tuple(
          (self.vm.convert.build_string(node, k), v), self.vm).to_variable(node)
                   for k, v in namedargs.items()]
      namedargs = abstract.List(namedargs, self.vm).to_variable(node)
      posargs += (namedargs,)
      args = function.Args(posargs)
    elif namedargs:
      errmsg = (""Either list of fields or keywords can be provided to ""
                ""NamedTuple, not both"")
      self.vm.errorlog.invalid_namedtuple_arg(self.vm.frames, err_msg=errmsg)
    return self.namedtuple.call(node, None, args)","1. **Use `self.vm.convert.value_to_constant` to check if the argument is a constant.** This will help prevent against malicious code being passed as an argument.
2. **Use `self.vm.errorlog.invalid_namedtuple_arg` to log errors if the argument is invalid.** This will help you track down any problems that occur.
3. **Use `self.vm.python_version` to check if the Python version is supported.** This will prevent the code from running on older versions of Python that don't support the NamedTuple keyword argument."
"  def byte_LOAD_METHOD(self, state, op):
    # We don't support this 3.7 opcode yet; simply don't crash.
    # TODO(rechen): Implement
    # https://docs.python.org/3/library/dis.html#opcode-LOAD_METHOD.
    unused_name = self.frame.f_code.co_names[op.arg]
    state, unused_self_obj = state.pop()
    return state","1. **Use `dis.opname` to check for unsupported opcodes.** This will help to prevent crashes caused by unknown opcodes.
2. **Implement the `LOAD_METHOD` opcode.** This opcode is used to load a method from an object, and it is necessary for supporting 3.7 bytecode.
3. **Test the code with a variety of inputs.** This will help to catch any bugs that may be introduced by the changes."
"  def byte_CALL_METHOD(self, state, op):
    # We don't support this 3.7 opcode yet; simply don't crash.
    # TODO(rechen): Implement
    # https://docs.python.org/3/library/dis.html#opcode-CALL_METHOD.
    for _ in range(op.arg):
      state = state.pop_and_discard()
    return state.push(self.new_unsolvable(state.node))","1. The code should be updated to support the CALL_METHOD opcode.
2. The code should be validated to ensure that it is not crashing.
3. The code should be reviewed to ensure that it is secure."
"  def __init__(self,
               base_module,
               python_version,
               pythonpath=(),
               imports_map=None,
               use_typeshed=True,
               modules=None):
    self._modules = modules or self._base_modules(python_version)
    if self._modules[""__builtin__""].needs_unpickling():
      self._unpickle_module(self._modules[""__builtin__""])
    if self._modules[""typing""].needs_unpickling():
      self._unpickle_module(self._modules[""typing""])
    self.builtins = self._modules[""__builtin__""].ast
    self.typing = self._modules[""typing""].ast
    self.base_module = base_module
    self.python_version = python_version
    self.pythonpath = pythonpath
    self.imports_map = imports_map
    self.use_typeshed = use_typeshed
    self._concatenated = None
    self._import_name_cache = {}  # performance cache
    self._aliases = {}
    # Paranoid verification that pytype.main properly checked the flags:
    if imports_map is not None:
      assert pythonpath == [""""], pythonpath","1. Use `importlib.import_module` instead of `__import__` to avoid the possibility of importing malicious modules.
2. Use `sys.path_importer_cache` to cache imported modules to avoid the possibility of importing modules from malicious paths.
3. Use `inspect.getfile` to get the absolute path of a module to verify that it is from a trusted location."
"  def _process_module(self, module_name, filename, ast):
    """"""Create a module from a loaded ast and save it to the loader cache.

    Args:
      module_name: The fully qualified name of the module being imported.
      filename: The file the ast was generated from.
      ast: The pytd.TypeDeclUnit representing the module.

    Returns:
      The ast (pytd.TypeDeclUnit) as represented in this loader.
    """"""
    module = Module(module_name, filename, ast)
    self._modules[module_name] = module
    try:
      module.ast = self._postprocess_pyi(module.ast)
      # Now that any imported TypeVar instances have been resolved, adjust type
      # parameters in classes and functions.
      module.ast = module.ast.Visit(visitors.AdjustTypeParameters())
      # Now we can fill in internal cls pointers to ClassType nodes in the
      # module. This code executes when the module is first loaded, which
      # happens before any others use it to resolve dependencies, so there are
      # no external pointers into the module at this point.
      module.ast.Visit(
          visitors.FillInLocalPointers({"""": module.ast,
                                        module_name: module.ast}))
    except:
      # don't leave half-resolved modules around
      del self._modules[module_name]
      raise
    return module.ast","1. Use `ast.Visit` to traverse the AST and check for security vulnerabilities.
2. Use `ast.Fix` to fix security vulnerabilities found in the AST.
3. Use `ast.dump` to print the AST in a human-readable format for debugging."
"  def __init__(self,
               errorlog,
               options,
               loader,
               generate_unknowns=False,
               store_all_calls=False):
    """"""Construct a TypegraphVirtualMachine.""""""
    self.maximum_depth = None  # set by run_program() and analyze()
    self.errorlog = errorlog
    self.options = options
    self.python_version = options.python_version
    self.PY2 = self.python_version[0] == 2
    self.PY3 = self.python_version[0] == 3
    self.generate_unknowns = generate_unknowns
    self.store_all_calls = store_all_calls
    self.loader = loader
    self.frames = []  # The call stack of frames.
    self.functions_with_late_annotations = []
    self.functions_type_params_check = []
    self.concrete_classes = []
    self.frame = None  # The current frame.
    self.program = cfg.Program()
    self.root_cfg_node = self.program.NewCFGNode(""root"")
    self.program.entrypoint = self.root_cfg_node
    self.annotations_util = annotations_util.AnnotationsUtil(self)
    self.attribute_handler = attribute.AbstractAttributeHandler(self)
    self.matcher = matcher.AbstractMatcher(self)
    self.convert = convert.Converter(self)
    self.program.default_data = self.convert.unsolvable
    self.has_unknown_wildcard_imports = False
    self.callself_stack = []
    self.filename = None
    self.director = None
    self._analyzing = False  # Are we in self.analyze()?
    self.opcode_traces = []

    # Map from builtin names to canonical objects.
    self.special_builtins = {
        # The super() function.
        ""super"": self.convert.super_type,
        # The object type.
        ""object"": self.convert.object_type,
        # for more pretty branching tests.
        ""__random__"": self.convert.primitive_class_instances[bool],
        # for debugging
        ""reveal_type"": special_builtins.RevealType(self),
        # boolean values.
        ""True"": self.convert.true,
        ""False"": self.convert.false,
        ""isinstance"": special_builtins.IsInstance(self),
        ""issubclass"": special_builtins.IsSubclass(self),
        ""hasattr"": special_builtins.HasAttr(self),
        ""callable"": special_builtins.IsCallable(self),
        ""abs"": special_builtins.Abs(self),
        ""next"": special_builtins.Next(self),
        ""open"": special_builtins.Open(self),
        ""property"": special_builtins.Property(self),
        ""staticmethod"": special_builtins.StaticMethod(self),
        ""classmethod"": special_builtins.ClassMethod(self),
    }

    # Memoize which overlays are loaded.
    self.loaded_overlays = {}","1. Use `self.convert.unsolvable` instead of `None` to represent unknown values. This will help to catch errors in type annotations.
2. Use `self.store_all_calls=True` to track all function calls. This will help to find bugs in call graphs.
3. Use `self.generate_unknowns=True` to generate unknown types for variables that are not annotated. This will help to find bugs in type inference."
"  def run_instruction(self, op, state):
    """"""Run a single bytecode instruction.

    Args:
      op: An opcode, instance of pyc.opcodes.Opcode
      state: An instance of state.FrameState, the state just before running
        this instruction.
    Returns:
      A tuple (why, state). ""why"" is the reason (if any) that this opcode aborts
      this function (e.g. through a 'raise'), or None otherwise. ""state"" is the
      FrameState right after this instruction that should roll over to the
      subsequent instruction.
    """"""
    _opcode_counter.inc(op.name)
    self.frame.current_opcode = op
    if log.isEnabledFor(logging.INFO):
      self.log_opcode(op, state)
    try:
      # dispatch
      bytecode_fn = getattr(self, ""byte_%s"" % op.name, None)
      if bytecode_fn is None:
        raise VirtualMachineError(""Unknown opcode: %s"" % op.name)
      state = bytecode_fn(state, op)
    except RecursionException:
      # This is not an error - it just means that the block we're analyzing
      # goes into a recursion, and we're already two levels deep.
      state = state.set_why(""recursion"")
    if state.why in (""reraise"", ""NoReturn""):
      state = state.set_why(""exception"")
    self.frame.current_opcode = None
    return state","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `inspect.getfullargspec` to get the argument names and default values.
3. Use `functools.partial` to create a new function with a subset of the original function's arguments."
"  def import_module(self, name, full_name, level):
    try:
      module = self._import_module(name, level)
    except (parser.ParseError, load_pytd.BadDependencyError,
            visitors.ContainerError, visitors.SymbolLookupError) as e:
      self.errorlog.pyi_error(self.frames, full_name, e)
      module = self.convert.unsolvable
    return module","1. Use `type()` to check if the argument is a string.
2. Use `os.path.isfile()` to check if the file exists.
3. Use `os.access()` to check if the user has permission to read the file."
"    def fix(self, parsed: BaseSegment, config: Optional[FluffConfig] = None):
        """"""Fix a parsed file object.""""""
        # Set up our config
        config = config or self.config
        # If we're in fix mode, then we need to progressively call and reconstruct
        working = parsed
        # Keep a set of previous versions to catch infinite loops.
        previous_versions = {working.raw}
        # A placeholder for the fixes we had on the previous loop
        last_fixes = None
        # How many loops have we had
        fix_loop_idx = 0
        # How many loops are we allowed
        loop_limit = config.get(""runaway_limit"")
        # Keep track of the errors from round 1
        linting_errors = []
        initial_linting_errors = []
        # Enter into the main fix loop. Some fixes may introduce other
        # problems and so we loop around this until we reach stability
        # or we reach the limit.
        while fix_loop_idx < loop_limit:
            fix_loop_idx += 1
            changed = False
            # Iterate through each rule.
            for crawler in self.get_ruleset(config=config):
                # fixes should be a dict {} with keys edit, delete, create
                # delete is just a list of segments to delete
                # edit and create are list of tuples. The first element is the
                # ""anchor"", the segment to look for either to edit or to insert BEFORE.
                # The second is the element to insert or create.
                lerrs, _, fixes, _ = crawler.crawl(
                    working, dialect=config.get(""dialect_obj""), fix=True
                )
                linting_errors += lerrs
                # Are there fixes to apply?
                if fixes:
                    linter_logger.info(""Applying Fixes: %s"", fixes)
                    # Do some sanity checks on the fixes before applying.
                    if last_fixes and fixes == last_fixes:
                        linter_logger.warning(
                            ""One fix for %s not applied, it would re-cause the same error."",
                            crawler.code,
                        )
                    else:
                        last_fixes = fixes
                        # Actually apply fixes.
                        new_working, _ = working.apply_fixes(fixes)
                        # Check for infinite loops
                        if new_working.raw not in previous_versions:
                            # We've not seen this version of the file so far. Continue.
                            working = new_working
                            previous_versions.add(working.raw)
                            changed = True
                            continue
                        # Applying these fixes took us back to a state which we've
                        # seen before. Abort.
                        linter_logger.warning(
                            ""One fix for %s not applied, it would re-cause the same error."",
                            crawler.code,
                        )
            # Keep track of initial errors for reporting.
            if fix_loop_idx == 1:
                initial_linting_errors = linting_errors.copy()
            # We did not change the file. Either the file is clean (no fixes), or
            # any fixes which are present will take us back to a previous state.
            if not changed:
                linter_logger.info(
                    ""Fix loop complete. Stability achieved after %s/%s loops."",
                    fix_loop_idx,
                    loop_limit,
                )
                break
        else:
            linter_logger.warning(
                ""Loop limit on fixes reached [%s]. Some fixes may be overdone."",
                loop_limit,
            )
        return working, initial_linting_errors","1. Use `last_fixes` to check if the fixes are the same as the previous ones. If they are, do not apply the fixes.
2. Check for infinite loops by using `previous_versions`. If the new working file is the same as any of the previous versions, abort the fix loop.
3. Keep track of the initial errors for reporting."
"    def lint(
        self, parsed: BaseSegment, config: Optional[FluffConfig] = None
    ) -> List[SQLLintError]:
        """"""Lint a parsed file object.""""""
        config = config or self.config
        linting_errors = []
        for crawler in self.get_ruleset(config=config):
            lerrs, _, _, _ = crawler.crawl(parsed, dialect=config.get(""dialect_obj""))
            linting_errors += lerrs

        # Filter out any linting errors in templated sections if relevant.
        if config.get(""ignore_templated_areas"", default=True):
            linting_errors = list(
                filter(
                    lambda e: getattr(e.segment.pos_marker, ""is_literal"", True),
                    linting_errors,
                )
            )

        return linting_errors","1. Use `config.get(""ignore_templated_areas"", default=True)` to filter out linting errors in templated sections.
2. Use `filter(lambda e: getattr(e.segment.pos_marker, ""is_literal"", True), linting_errors)` to filter out linting errors in literal sections.
3. Use `config.get(""dialect_obj"")` to get the dialect object for the parsed file."
"    def lint_string(
        self,
        in_str: str,
        fname: str = ""<string input>"",
        fix: bool = False,
        config: Optional[FluffConfig] = None,
    ) -> LintedFile:
        """"""Lint a string.

        Returns:
            :obj:`LintedFile`: an object representing that linted file.

        """"""
        # Sort out config, defaulting to the built in config if no override
        config = config or self.config

        # Using the new parser, read the file object.
        parsed = self.parse_string(in_str=in_str, fname=fname, config=config)
        time_dict = parsed.time_dict
        vs = parsed.violations
        tree = parsed.tree

        # Look for comment segments which might indicate lines to ignore.
        ignore_buff = []
        if tree:
            for comment in tree.recursive_crawl(""comment""):
                if comment.name == ""inline_comment"":
                    ignore_entry = self.extract_ignore_from_comment(comment)
                    if isinstance(ignore_entry, SQLParseError):
                        vs.append(ignore_entry)
                    elif ignore_entry:
                        ignore_buff.append(ignore_entry)
            if ignore_buff:
                linter_logger.info(""Parsed noqa directives from file: %r"", ignore_buff)

        if tree:
            t0 = time.monotonic()
            linter_logger.info(""LINTING (%s)"", fname)
            # If we're in fix mode, apply those fixes.
            # NB: We don't pass in the linting errors, because the fix function
            # regenerates them on each loop.
            if fix:
                tree, initial_linting_errors = self.fix(tree, config=config)
            else:
                initial_linting_errors = self.lint(tree, config=config)

            # Update the timing dict
            t1 = time.monotonic()
            time_dict[""linting""] = t1 - t0

            # We're only going to return the *initial* errors, rather
            # than any generated during the fixing cycle.
            vs += initial_linting_errors

        # We process the ignore config here if appropriate
        if config:
            for violation in vs:
                violation.ignore_if_in(config.get(""ignore""))

        linted_file = LintedFile(
            fname,
            vs,
            time_dict,
            tree,
            ignore_mask=ignore_buff,
            templated_file=parsed.templated_file,
        )

        # This is the main command line output from linting.
        if self.formatter:
            self.formatter.dispatch_file_violations(
                fname, linted_file, only_fixable=fix
            )

        # Safety flag for unset dialects
        if config.get(""dialect"") == ""ansi"" and linted_file.get_violations(
            fixable=True if fix else None, types=SQLParseError
        ):
            if self.formatter:
                self.formatter.dispatch_dialect_warning()

        return linted_file","1. Use `config=config or self.config` instead of `config or self.config` to avoid accidentally using the global `config` variable.
2. Use `ignore_buff.append(ignore_entry)` instead of `ignore_buff += ignore_entry` to avoid accidentally appending multiple copies of the same value to the list.
3. Use `time.monotonic()` instead of `time.time()` to get a more accurate measurement of the time spent linting the file."
"    def crawl(
        self,
        segment,
        dialect,
        parent_stack=None,
        siblings_pre=None,
        siblings_post=None,
        raw_stack=None,
        fix=False,
        memory=None,
    ):
        """"""Recursively perform the crawl operation on a given segment.

        Returns:
            A tuple of (vs, raw_stack, fixes, memory)

        """"""
        # parent stack should be a tuple if it exists

        # crawlers, should evaluate on segments FIRST, before evaluating on their
        # children. They should also return a list of violations.

        parent_stack = parent_stack or ()
        raw_stack = raw_stack or ()
        siblings_post = siblings_post or ()
        siblings_pre = siblings_pre or ()
        memory = memory or {}
        vs = []
        fixes = []

        # First, check whether we're looking at an unparsable and whether
        # this rule will still operate on that.
        if not self._works_on_unparsable and segment.is_type(""unparsable""):
            # Abort here if it doesn't. Otherwise we'll get odd results.
            return vs, raw_stack, [], memory

        # TODO: Document what options are available to the evaluation function.
        try:
            res = self._eval(
                segment=segment,
                parent_stack=parent_stack,
                siblings_pre=siblings_pre,
                siblings_post=siblings_post,
                raw_stack=raw_stack,
                memory=memory,
                dialect=dialect,
            )
        # Any exception at this point would halt the linter and
        # cause the user to get no results
        except Exception as e:
            self.logger.critical(
                f""Applying rule {self.code} threw an Exception: {e}"", exc_info=True
            )
            vs.append(
                SQLLintError(
                    rule=self,
                    segment=segment,
                    fixes=[],
                    description=(
                        f""""""Unexpected exception: {str(e)};
                        Could you open an issue at https://github.com/sqlfluff/sqlfluff/issues ?
                        You can ignore this exception for now, by adding '--noqa: {self.code}' at the end
                        of line {segment.pos_marker.line_no}
                        """"""
                    ),
                )
            )
            return vs, raw_stack, fixes, memory

        if res is None:
            # Assume this means no problems (also means no memory)
            pass
        elif isinstance(res, LintResult):
            # Extract any memory
            memory = res.memory
            lerr = res.to_linting_error(rule=self)
            if lerr:
                vs.append(lerr)
            fixes += res.fixes
        elif isinstance(res, list) and all(
            isinstance(elem, LintResult) for elem in res
        ):
            # Extract any memory from the *last* one, assuming
            # it was the last to be added
            memory = res[-1].memory
            for elem in res:
                lerr = elem.to_linting_error(rule=self)
                if lerr:
                    vs.append(lerr)
                fixes += elem.fixes
        else:
            raise TypeError(
                ""Got unexpected result [{0!r}] back from linting rule: {1!r}"".format(
                    res, self.code
                )
            )

        # The raw stack only keeps track of the previous raw segments
        if len(segment.segments) == 0:
            raw_stack += (segment,)
        # Parent stack keeps track of all the parent segments
        parent_stack += (segment,)

        for idx, child in enumerate(segment.segments):
            dvs, raw_stack, child_fixes, memory = self.crawl(
                segment=child,
                parent_stack=parent_stack,
                siblings_pre=segment.segments[:idx],
                siblings_post=segment.segments[idx + 1 :],
                raw_stack=raw_stack,
                fix=fix,
                memory=memory,
                dialect=dialect,
            )
            vs += dvs
            fixes += child_fixes
        return vs, raw_stack, fixes, memory","1. Use `try/except` to catch exceptions and log them.
2. Use `raise` to throw exceptions when errors occur.
3. Use `assert` to check for errors and abort the program if they occur."
"    def _unsafe_process(self, fname, in_str=None, config=None):
        if not config:
            raise ValueError(
                ""For the dbt templater, the `process()` method requires a config object.""
            )
        if not fname:
            raise ValueError(
                ""For the dbt templater, the `process()` method requires a file name""
            )
        elif fname == ""stdin"":
            raise ValueError(
                ""The dbt templater does not support stdin input, provide a path instead""
            )
        self.sqlfluff_config = config

        selected = self.dbt_selector_method.search(
            included_nodes=self.dbt_manifest.nodes,
            # Selector needs to be a relative path
            selector=os.path.relpath(fname, start=os.getcwd()),
        )
        results = [self.dbt_manifest.expect(uid) for uid in selected]

        if not results:
            raise RuntimeError(""File %s was not found in dbt project"" % fname)

        node = self.dbt_compiler.compile_node(
            node=results[0],
            manifest=self.dbt_manifest,
        )

        if not node.compiled_sql:
            raise SQLTemplaterError(
                ""dbt templater compilation failed silently, check your configuration ""
                ""by running `dbt compile` directly.""
            )

        raw_sliced, sliced_file = self.slice_file(node.raw_sql, node.compiled_sql)
        return (
            TemplatedFile(
                source_str=node.raw_sql,
                templated_str=node.compiled_sql,
                fname=fname,
                sliced_file=sliced_file,
                raw_sliced=raw_sliced,
            ),
            # No violations returned in this way.
            [],
        )","1. **Use prepared statements instead of string concatenation.** This will help to prevent SQL injection attacks.
2. **Sanitize user input.** Make sure to validate and sanitize any input from users before using it in your code.
3. **Use a secure password hashing algorithm.** This will help to protect your passwords from being cracked."
"    def process(
        self, *, in_str: str, fname: Optional[str] = None, config=None
    ) -> Tuple[Optional[TemplatedFile], list]:
        """"""Process a string and return the new string.

        Note that the arguments are enforced as keywords
        because Templaters can have differences in their
        `process` method signature.
        A Templater that only supports reading from a file
        would need the following signature:
            process(*, fname, in_str=None, config=None)
        (arguments are swapped)

        Args:
            in_str (:obj:`str`): The input string.
            fname (:obj:`str`, optional): The filename of this string. This is
                mostly for loading config files at runtime.
            config (:obj:`FluffConfig`): A specific config to use for this
                templating operation. Only necessary for some templaters.

        """"""
        if not config:
            raise ValueError(
                ""For the jinja templater, the `process()` method requires a config object.""
            )

        # Load the context
        live_context = self.get_context(fname=fname, config=config)
        # Apply dbt builtin functions if we're allowed.
        apply_dbt_builtins = config.get_section(
            (self.templater_selector, self.name, ""apply_dbt_builtins"")
        )
        if apply_dbt_builtins:
            # This feels a bit wrong defining these here, they should probably
            # be configurable somewhere sensible. But for now they're not.
            # TODO: Come up with a better solution.
            dbt_builtins = self._generate_dbt_builtins()
            for name in dbt_builtins:
                # Only apply if it hasn't already been set at this stage.
                if name not in live_context:
                    live_context[name] = dbt_builtins[name]

        # Load config macros
        env = self._get_jinja_env()
        ctx = self._extract_macros_from_config(config=config, env=env, ctx=live_context)
        # Load macros from path (if applicable)
        macros_path = config.get_section(
            (self.templater_selector, self.name, ""load_macros_from_path"")
        )
        if macros_path:
            ctx.update(
                self._extract_macros_from_path(macros_path, env=env, ctx=live_context)
            )
        live_context.update(ctx)

        # Load the template, passing the global context.
        try:
            template = env.from_string(in_str, globals=live_context)
        except TemplateSyntaxError as err:
            # Something in the template didn't parse, return the original
            # and a violation around what happened.
            (len(line) for line in in_str.split(""\\n"")[: err.lineno])
            return (
                TemplatedFile(source_str=in_str, fname=fname),
                [
                    SQLTemplaterError(
                        ""Failure to parse jinja template: {0}."".format(err),
                        pos=FilePositionMarker(
                            None,
                            err.lineno,
                            None,
                            # Calculate the charpos for sorting.
                            sum(
                                len(line)
                                for line in in_str.split(""\\n"")[: err.lineno - 1]
                            ),
                        ),
                    )
                ],
            )

        violations = []

        # Attempt to identify any undeclared variables. The majority
        # will be found during the _crawl_tree step rather than this
        # first Exception which serves only to catch catastrophic errors.
        try:
            syntax_tree = env.parse(in_str)
            undefined_variables = meta.find_undeclared_variables(syntax_tree)
        except Exception as err:
            # TODO: Add a url here so people can get more help.
            raise SQLTemplaterError(
                ""Failure in identifying Jinja variables: {0}."".format(err)
            )

        # Get rid of any that *are* actually defined.
        for val in live_context:
            if val in undefined_variables:
                undefined_variables.remove(val)

        if undefined_variables:
            # Lets go through and find out where they are:
            for val in self._crawl_tree(syntax_tree, undefined_variables, in_str):
                violations.append(val)

        try:
            # NB: Passing no context. Everything is loaded when the template is loaded.
            out_str = template.render()
            # Slice the file once rendered.
            raw_sliced, sliced_file = self.slice_file(in_str, out_str)
            return (
                TemplatedFile(
                    source_str=in_str,
                    templated_str=out_str,
                    fname=fname,
                    sliced_file=sliced_file,
                    raw_sliced=raw_sliced,
                ),
                violations,
            )
        except (TemplateError, TypeError) as err:
            templater_logger.info(""Unrecoverable Jinja Error: %s"", err)
            violations.append(
                SQLTemplaterError(
                    (
                        ""Unrecoverable failure in Jinja templating: {0}. Have you configured ""
                        ""your variables? https://docs.sqlfluff.com/en/latest/configuration.html""
                    ).format(err)
                )
            )
            return None, violations","1. Use `get_context` to load the context instead of directly accessing the `live_context` variable. This will help to prevent unauthorized access to sensitive data.
2. Use `_extract_macros_from_config` and `_extract_macros_from_path` to load macros instead of directly accessing the `macros` variable. This will help to prevent unauthorized access to sensitive data.
3. Use `_crawl_tree` to identify undeclared variables instead of directly accessing the `undefined_variables` variable. This will help to prevent errors caused by undeclared variables."
"    def process(
        self, *, in_str: str, fname: Optional[str] = None, config=None
    ) -> Tuple[Optional[TemplatedFile], list]:
        """"""Process a string and return a TemplatedFile.

        Note that the arguments are enforced as keywords
        because Templaters can have differences in their
        `process` method signature.
        A Templater that only supports reading from a file
        would need the following signature:
            process(*, fname, in_str=None, config=None)
        (arguments are swapped)

        Args:
            in_str (:obj:`str`): The input string.
            fname (:obj:`str`, optional): The filename of this string. This is
                mostly for loading config files at runtime.
            config (:obj:`FluffConfig`): A specific config to use for this
                templating operation. Only necessary for some templaters.

        """"""
        live_context = self.get_context(fname=fname, config=config)
        try:
            new_str = in_str.format(**live_context)
        except KeyError as err:
            # TODO: Add a url here so people can get more help.
            raise SQLTemplaterError(
                ""Failure in Python templating: {0}. Have you configured your variables?"".format(
                    err
                )
            )
        raw_sliced, sliced_file = self.slice_file(in_str, new_str)
        return (
            TemplatedFile(
                source_str=in_str,
                templated_str=new_str,
                fname=fname,
                sliced_file=sliced_file,
                raw_sliced=raw_sliced,
            ),
            [],
        )","1. Use `f-strings` instead of `format()` to avoid errors caused by incorrect positional arguments.
2. Validate the user input before using it in the template.
3. Use a secure hashing algorithm to hash the user's password."
"    def slice_file(
        cls, raw_str: str, templated_str: str
    ) -> Tuple[List[RawFileSlice], List[TemplatedFileSlice]]:
        """"""Slice the file to determine regions where we can fix.""""""
        templater_logger.info(""Slicing File Template"")
        templater_logger.debug(""    Raw String: %r"", raw_str)
        templater_logger.debug(""    Templated String: %r"", templated_str)
        # Slice the raw file
        raw_sliced = list(cls._slice_template(raw_str))
        # Find the literals
        literals = [
            raw_slice.raw
            for raw_slice in raw_sliced
            if raw_slice.slice_type == ""literal""
        ]
        templater_logger.debug(""    Literals: %s"", literals)
        # Calculate occurrences
        raw_occurrences = cls._substring_occurances(raw_str, literals)
        templated_occurances = cls._substring_occurances(templated_str, literals)
        templater_logger.debug(
            ""    Occurances: Raw: %s, Templated: %s"",
            raw_occurrences,
            templated_occurances,
        )
        # Split on invariants
        split_sliced = list(
            cls._split_invariants(
                raw_sliced,
                literals,
                raw_occurrences,
                templated_occurances,
                templated_str,
            )
        )
        templater_logger.debug(""    Split Sliced: %s"", split_sliced)
        # Deal with uniques and coalesce the rest
        sliced_file = list(
            cls._split_uniques_coalesce_rest(
                split_sliced, raw_occurrences, templated_occurances, templated_str
            )
        )
        templater_logger.debug(""    Fully Sliced: %s"", sliced_file)
        return raw_sliced, sliced_file","1. Use `logging.warning` instead of `logging.info` to log sensitive information.
2. Use `_substring_occurances` to calculate the occurrences of literals instead of manually counting them.
3. Use `_split_uniques_coalesce_rest` to split the slices on invariants and coalesce the rest."
"    def _lint_references_and_aliases(self, aliases, references, using_cols, parent_select):
        """"""Check whether any aliases are duplicates.

        NB: Subclasses of this error should override this function.

        """"""
        # Are any of the aliases the same?
        for a1, a2 in itertools.combinations(aliases, 2):
            # Compare the strings
            if a1[0] == a2[0] and a1[0]:
                # If there are any, then the rest of the code
                # won't make sense so just return here.
                return [LintResult(
                    # Reference the element, not the string.
                    anchor=a2[1],
                    description=(""Duplicate table alias {0!r}. Table ""
                                 ""aliases should be unique."").format(a2.raw)
                )]
        return None","1. Use `.format()` to sanitize user input.
2. Use `isinstance()` to check the type of input.
3. Use `assert()` to validate the input."
"    def stats(self):
        """"""Return a stats dictionary of this result.""""""
        all_stats = dict(files=0, clean=0, unclean=0, violations=0)
        for path in self.paths:
            all_stats = self.sum_dicts(path.stats(), all_stats)
        all_stats['avg per file'] = all_stats['violations'] * 1.0 / all_stats['files']
        all_stats['unclean rate'] = all_stats['unclean'] * 1.0 / all_stats['files']
        all_stats['clean files'] = all_stats['clean']
        all_stats['unclean files'] = all_stats['unclean']
        all_stats['exit code'] = 65 if all_stats['violations'] > 0 else 0
        all_stats['status'] = 'FAIL' if all_stats['violations'] > 0 else 'PASS'
        return all_stats","1. Use `pathlib.Path` instead of `os.path` to avoid path traversal vulnerabilities.
2. Use `subprocess.run` instead of `os.system` to avoid code injection vulnerabilities.
3. Use `json.dumps` with `default=str` to avoid JSON vulnerabilities."
"def fix(force, paths, **kwargs):
    """"""Fix SQL files.

    PATH is the path to a sql file or directory to lint. This can be either a
    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')
    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will
    be interpreted like passing the current working directory as a path argument.
    """"""
    c = get_config(**kwargs)
    lnt = get_linter(c)
    verbose = c.get('verbose')

    config_string = format_config(lnt, verbose=verbose)
    if len(config_string) > 0:
        lnt.log(config_string)
    # Check that if fix is specified, that we have picked only a subset of rules
    if lnt.config.get('rule_whitelist') is None:
        lnt.log((""The fix option is only available in combination""
                 "" with --rules. This is for your own safety!""))
        sys.exit(1)

    # handle stdin case. should output formatted sql to stdout and nothing else.
    if ('-',) == paths:
        stdin = sys.stdin.read()
        result = lnt.lint_string_wrapped(stdin, fname='stdin', verbosity=verbose, fix=True)
        stdout = result.paths[0].files[0].fix_string()
        click.echo(stdout, nl=False)
        sys.exit()

    # Lint the paths (not with the fix argument at this stage), outputting as we go.
    lnt.log(""==== finding violations ===="")
    try:
        result = lnt.lint_paths(paths, verbosity=verbose)
    except IOError:
        click.echo(colorize('The path(s) {0!r} could not be accessed. Check it/they exist(s).'.format(paths), 'red'))
        sys.exit(1)

    if result.num_violations() > 0:
        click.echo(""==== fixing violations ===="")
        click.echo(""{0} violations found"".format(
            result.num_violations()))
        if force:
            click.echo('FORCE MODE: Attempting fixes...')
            result = lnt.lint_paths(paths, fix=True)
            click.echo('Persisting Changes...')
            result.persist_changes()
            click.echo('Done. Please check your files to confirm.')
        else:
            click.echo('Are you sure you wish to attempt to fix these? [Y/n] ', nl=False)
            c = click.getchar().lower()
            click.echo('...')
            if c == 'y':
                click.echo('Attempting fixes...')
                result = lnt.lint_paths(paths, fix=True)
                click.echo('Persisting Changes...')
                result.persist_changes(verbosity=verbose)
                click.echo('Done. Please check your files to confirm.')
            elif c == 'n':
                click.echo('Aborting...')
            else:
                click.echo('Invalid input :(')
                click.echo('Aborting...')
    else:
        click.echo(""==== no violations found ===="")
    sys.exit(0)","1. **Use `get_config()` to get the config instead of passing it directly.** This will prevent accidentally passing a malicious config to the linter.
2. **Check that `rule_whitelist` is set before using the `fix` option.** This will prevent users from accidentally fixing files with rules that they didn't intend to use.
3. **Prompt the user to confirm before fixing files.** This will give the user a chance to review the changes that will be made and decide if they want to proceed."
"    def num_violations(self):
        """"""Count the number of violations.""""""
        return len(self.violations)","1. Use `set` instead of `list` to store violations to avoid duplicate entries.
2. Use `hashlib` to generate a unique hash for each violation and store it in the `violations` set.
3. Use `enumerate` to iterate over the violations and print the index and violation message."
"    def fix_string(self, verbosity=0):
        """"""Obtain the changes to a path as a string.

        We use the file_mask to do a safe merge, avoiding any templated
        sections. First we need to detect where there have been changes
        between the fixed and templated versions.

        We use difflib.SequenceMatcher.get_opcodes
        See: https://docs.python.org/3.7/library/difflib.html#difflib.SequenceMatcher.get_opcodes
        It returns a list of tuples ('equal|replace', ia1, ia2, ib1, ib2).

        """"""
        verbosity_logger(""Persisting file masks: {0}"".format(self.file_mask), verbosity=verbosity)
        # Compare Templated with Raw
        diff_templ = SequenceMatcher(autojunk=None, a=self.file_mask[0], b=self.file_mask[1])
        diff_templ_codes = diff_templ.get_opcodes()
        verbosity_logger(""Templater diff codes: {0}"".format(diff_templ_codes), verbosity=verbosity)

        # Compare Fixed with Templated
        diff_fix = SequenceMatcher(autojunk=None, a=self.file_mask[1], b=self.file_mask[2])
        # diff_fix = SequenceMatcher(autojunk=None, a=self.file_mask[1][0], b=self.file_mask[2][0])
        diff_fix_codes = diff_fix.get_opcodes()
        verbosity_logger(""Fixing diff codes: {0}"".format(diff_fix_codes), verbosity=verbosity)

        # If diff_templ isn't the same then we should just keep the template. If there *was*
        # a fix in that space, then we should raise an issue
        # If it is the same, then we can apply fixes as expected.
        write_buff = ''
        fixed_block = None
        templ_block = None
        # index in raw, templ and fix
        idx = (0, 0, 0)
        loop_idx = 0
        while True:
            loop_idx += 1
            verbosity_logger(
                ""{0:04d}: Write Loop: idx:{1}, buff:{2!r}"".format(loop_idx, idx, write_buff),
                verbosity=verbosity)

            if templ_block is None:
                if diff_templ_codes:
                    templ_block = diff_templ_codes.pop(0)
                # We've exhausted the template. Have we exhausted the fixes?
                elif fixed_block is None:
                    # Yes - excellent. DONE
                    break
                else:
                    raise NotImplementedError(""Fix Block left over! DOn't know how to handle this! aeflf8wh"")
            if fixed_block is None:
                if diff_fix_codes:
                    fixed_block = diff_fix_codes.pop(0)
                else:
                    raise NotImplementedError(""Unexpectedly depleted the fixes. Panic!"")
            verbosity_logger(
                ""{0:04d}: Blocks: template:{1}, fix:{2}"".format(loop_idx, templ_block, fixed_block),
                verbosity=verbosity)

            if templ_block[0] == 'equal':
                if fixed_block[0] == 'equal':
                    # No templating, no fixes, go with middle and advance indexes
                    # Find out how far we can advance (we use the middle version because it's common)
                    if templ_block[4] == fixed_block[2]:
                        buff = self.file_mask[1][idx[1]:fixed_block[2]]
                        # consume both blocks
                        fixed_block = None
                        templ_block = None
                    elif templ_block[4] > fixed_block[2]:
                        buff = self.file_mask[1][idx[1]:fixed_block[2]]
                        # consume fixed block
                        fixed_block = None
                    elif templ_block[4] < fixed_block[2]:
                        buff = self.file_mask[1][idx[1]:templ_block[4]]
                        # consume templ block
                        templ_block = None
                    idx = (idx[0] + len(buff), idx[1] + len(buff), idx[2] + len(buff))
                    write_buff += buff
                    continue
                elif fixed_block[0] == 'replace':
                    # Consider how to apply fixes.
                    # Can we implement the fix while staying in the equal segment?
                    if fixed_block[2] <= templ_block[4]:
                        # Yes! Write from the fixed version.
                        write_buff += self.file_mask[2][idx[2]:fixed_block[4]]
                        idx = (idx[0] + (fixed_block[2] - fixed_block[1]), fixed_block[2], fixed_block[4])
                        # Consume the fixed block because we've written the whole thing.
                        fixed_block = None
                        continue
                    else:
                        raise NotImplementedError(""DEF"")
                elif fixed_block[0] == 'delete':
                    # We're deleting items, nothing to write but we can consume some
                    # blocks and advance some indexes.
                    idx = (idx[0] + (fixed_block[2] - fixed_block[1]), fixed_block[2], fixed_block[4])
                    fixed_block = None
                elif fixed_block[0] == 'insert':
                    # We're inserting items, Write from the fix block, but only that index moves.
                    write_buff += self.file_mask[2][idx[2]:fixed_block[4]]
                    idx = (idx[0], idx[1], fixed_block[4])
                    fixed_block = None
                else:
                    raise ValueError(
                        (""Unexpected opcode {0} for fix block! Please report this ""
                         ""issue on github with the query and rules you're trying to ""
                         ""fix."").format(fixed_block[0]))
            elif templ_block[0] == 'replace':
                # We're in a templated section - we should write the templated version.
                # we should consume the whole replce block and then deal with where
                # we end up.
                buff = self.file_mask[0][idx[0]:templ_block[2]]
                new_templ_idx = templ_block[4]
                while True:
                    if fixed_block[2] > new_templ_idx >= fixed_block[1]:
                        # this block contains the end point
                        break
                    else:
                        if fixed_block[0] != 'equal':
                            print(""WARNING: Skipping edit block: {0}"".format(fixed_block))
                        fixed_block = None
                # Are we exaclty on a join?
                if new_templ_idx == fixed_block[1]:
                    # GREAT - this makes things easy because we have an equality point already
                    idx = (templ_block[2], new_templ_idx, fixed_block[3])
                else:
                    if fixed_block[0] == 'equal':
                        # If it's in an equal block, we can use the same offset from the end.
                        idx = (templ_block[2], new_templ_idx, fixed_block[3] + (new_templ_idx - fixed_block[1]))
                    else:
                        # TODO: We're trying to move through an templated section, but end up
                        # in a fixed section. We've lost track of indexes.
                        # We might need to panic if this happens...
                        print(""UMMMMMM!"")
                        print(new_templ_idx)
                        print(fixed_block)
                        raise NotImplementedError(""ABC"")
                write_buff += buff
                # consume template block
                templ_block = None
            elif templ_block[0] == 'delete':
                # The comparison, things that the templater has deleted
                # some characters. This is just a quirk of the differ.
                # In reality this means we just write these characters
                # and don't worry about advancing the other indexes.
                buff = self.file_mask[0][idx[0]:templ_block[2]]
                # consume templ block
                templ_block = None
                idx = (idx[0] + len(buff), idx[1], idx[2])
                write_buff += buff
            else:
                raise ValueError(
                    (""Unexpected opcode {0} for template block! Please report this ""
                     ""issue on github with the query and rules you're trying to ""
                     ""fix."").format(templ_block[0]))

        return write_buff","1. Use `assert` statements to validate the input parameters.
2. Use `sanitize_filename` to sanitize the filename before using it.
3. Use `os.path.join` to join the path components instead of concatenating them with `+`."
"    def persist_tree(self, verbosity=0):
        """"""Persist changes to the given path.

        We use the file_mask to do a safe merge, avoiding any templated
        sections. First we need to detect where there have been changes
        between the fixed and templated versions.

        We use difflib.SequenceMatcher.get_opcodes
        See: https://docs.python.org/3.7/library/difflib.html#difflib.SequenceMatcher.get_opcodes
        It returns a list of tuples ('equal|replace', ia1, ia2, ib1, ib2).

        """"""
        write_buff = self.fix_string(verbosity=verbosity)

        # Actually write the file.
        with open(self.path, 'w') as f:
            f.write(write_buff)

        # TODO: Make return value of persist_changes() a more interesting result and then format it
        # click.echo(format_linting_fixes(result, verbose=verbose), color=color)
        return True","1. Use `Path.write_text` instead of `open()` to write to a file. This will prevent arbitrary code execution if the file is opened by a malicious user.
2. Use `difflib.SequenceMatcher.find_longest_match` to find the longest common substring between the fixed and templated versions of the file. This will prevent malicious changes from being introduced into the file.
3. Use `click.echo` to format the results of the linting fixes. This will make it easier to identify any errors that were introduced."
"    def num_violations(self):
        """"""Count the number of violations in the path.""""""
        return sum(file.num_violations() for file in self.files)","1. Use `functools.lru_cache` to cache the results of `num_violations` to prevent repeated computation.
2. Validate the type of `file` argument to `num_violations` to prevent a denial-of-service attack.
3. Use `f-strings` to format the error message to prevent injection attacks."
"    def persist_changes(self, verbosity=0):
        """"""Persist changes to files in the given path.""""""
        # Run all the fixes for all the files and return a dict
        return {file.path: file.persist_tree(verbosity=verbosity) for file in self.files}","1. Use `os.path.expanduser()` to expand the user's home directory, rather than hardcoding it.
2. Use `os.makedirs()` to create directories, rather than `os.mkdir()`.
3. Use `chmod()` to set the permissions of files and directories, rather than relying on the default permissions."
"    def num_violations(self):
        """"""Count the number of violations in thie result.""""""
        return sum(path.num_violations() for path in self.paths)","1. Use `functools.lru_cache` to cache the results of `num_violations` to improve performance.
2. Use `pathlib.Path` to create paths instead of strings to avoid path injection attacks.
3. Validate the input of `num_violations` to prevent attackers from passing invalid paths."
"    def persist_changes(self, verbosity=0):
        """"""Run all the fixes for all the files and return a dict.""""""
        return self.combine_dicts(*[path.persist_changes(verbosity=verbosity) for path in self.paths])","1. Use `pathlib.Path` instead of `os.path` to avoid path traversal vulnerabilities.
2. Use `tempfile.NamedTemporaryFile` to create temporary files instead of opening files directly.
3. Use `shutil.copyfile` to copy files instead of `os.copyfile`."
"    def match(self, segments, parse_context):
        """"""Match a specific sequence of elements.""""""
        if isinstance(segments, BaseSegment):
            segments = tuple(segments)

        matched_segments = MatchResult.from_empty()
        unmatched_segments = segments

        for idx, elem in enumerate(self._elements):
            while True:
                # Is it an indent or dedent?
                if elem.is_meta:
                    matched_segments += elem()
                    break

                if len(unmatched_segments) == 0:
                    # We've run our of sequence without matching everyting.
                    # Do only optional elements remain.
                    if all(e.is_optional() for e in self._elements[idx:]):
                        # then it's ok, and we can return what we've got so far.
                        # No need to deal with anything left over because we're at the end.
                        return matched_segments
                    else:
                        # we've got to the end of the sequence without matching all
                        # required elements.
                        return MatchResult.from_unmatched(segments)
                else:
                    # We're not at the end, first detect whitespace and then try to match.
                    if self.code_only and not unmatched_segments[0].is_code:
                        # We should add this one to the match and carry on
                        matched_segments += (unmatched_segments[0],)
                        unmatched_segments = unmatched_segments[1:]
                        check_still_complete(segments, matched_segments.matched_segments, unmatched_segments)
                        continue

                    # It's not whitespace, so carry on to matching
                    elem_match = elem._match(
                        unmatched_segments, parse_context=parse_context.copy(incr='match_depth'))

                    if elem_match.has_match():
                        # We're expecting mostly partial matches here, but complete
                        # matches are possible.
                        matched_segments += elem_match.matched_segments
                        unmatched_segments = elem_match.unmatched_segments
                        # Each time we do this, we do a sense check to make sure we haven't
                        # dropped anything. (Because it's happened before!).
                        check_still_complete(segments, matched_segments.matched_segments, unmatched_segments)

                        # Break out of the while loop and move to the next element.
                        break
                    else:
                        # If we can't match an element, we should ascertain whether it's
                        # required. If so then fine, move on, but otherwise we should crash
                        # out without a match. We have not matched the sequence.
                        if elem.is_optional():
                            # This will crash us out of the while loop and move us
                            # onto the next matching element
                            break
                        else:
                            return MatchResult.from_unmatched(segments)

        # If we get to here, we've matched all of the elements (or skipped them)
        # but still have some segments left (or perhaps have precisely zero left).
        # In either case, we're golden. Return successfully, with any leftovers as
        # the unmatched elements. UNLESS they're whitespace and we should be greedy.
        if self.code_only:
            while unmatched_segments and not unmatched_segments[0].is_code:
                # We should add this one to the match and carry on
                matched_segments += (unmatched_segments[0],)
                unmatched_segments = unmatched_segments[1:]
                check_still_complete(segments, matched_segments.matched_segments, unmatched_segments)

        return MatchResult(matched_segments.matched_segments, unmatched_segments)","1. Use `check_still_complete` to make sure that no segments are dropped.
2. Use `is_optional` to check if an element is optional and skip it if it is.
3. Use `is_code` to check if an element is code and add it to the match if it is."
"    def match(self, segments, parse_context):
        """"""Match if this is a bracketed sequence, with content that matches one of the elements.

        1. work forwards to find the first bracket.
           If we find something other that whitespace, then fail out.
        2. Once we have the first bracket, we need to bracket count forward to find it's partner.
        3. Assuming we find it's partner then we try and match what goes between them.
           If we match, great. If not, then we return an empty match.
           If we never find it's partner then we return an empty match but should probably
           log a parsing warning, or error?

        """"""
        seg_buff = segments
        matched_segs = ()

        # Look for the first bracket
        start_match = self._code_only_sensitive_match(
            seg_buff, self.start_bracket,
            parse_context=parse_context.copy(incr='match_depth'),
            code_only=self.code_only)
        if start_match:
            seg_buff = start_match.unmatched_segments
        else:
            # Can't find the opening bracket. No Match.
            return MatchResult.from_unmatched(segments)

        # Look for the closing bracket
        pre, end_match, _ = self._bracket_sensitive_look_ahead_match(
            segments=seg_buff, matchers=[self.end_bracket],
            parse_context=parse_context, code_only=self.code_only
        )
        if not end_match:
            raise SQLParseError(
                ""Couldn't find closing bracket for opening bracket."",
                segment=matched_segs)

        # Match the content now we've confirmed the brackets. We use the
        # _longest helper function mostly just because it deals with multiple
        # matchers.
        content_match, _ = self._longest_code_only_sensitive_match(
            pre, self._elements,
            parse_context=parse_context.copy(incr='match_depth'),
            code_only=self.code_only)

        # We require a complete match for the content (hopefully for obvious reasons)
        if content_match.is_complete():
            # We don't want to add metas if they're already there, so check
            if content_match.matched_segments and content_match.matched_segments[0].is_meta:
                pre_meta = ()
            else:
                pre_meta = (Indent(),)
            if end_match.matched_segments and end_match.matched_segments[0].is_meta:
                post_meta = ()
            else:
                post_meta = (Dedent(),)

            return MatchResult(
                start_match.matched_segments
                + pre_meta  # Add a meta indent here
                + content_match.matched_segments
                + post_meta  # Add a meta indent here
                + end_match.matched_segments,
                end_match.unmatched_segments)
        else:
            # Now if we've not matched there's a final option. If the content is optional
            # and we allow non-code, then if the content is all non-code then it could be
            # empty brackets and still match.
            # NB: We don't add indents here, because there's nothing to indent
            if (
                all(e.is_optional() for e in self._elements)
                and self.code_only
                and all(not e.is_code for e in pre)
            ):
                # It worked!
                return MatchResult(
                    start_match.matched_segments
                    + pre
                    + end_match.matched_segments,
                    end_match.unmatched_segments)
            else:
                return MatchResult.from_unmatched(segments)","1. Use `white_space_only` to check for whitespace only.
2. Use `bracket_sensitive_look_ahead_match` to find the closing bracket.
3. Use `is_complete()` to check if the match is complete."
"    def validate_segments(self, text=""constructing""):
        """"""Check the elements of the `segments` attribute are all themselves segments.""""""
        for elem in self.segments:
            if not isinstance(elem, BaseSegment):
                raise TypeError(
                    ""In {0} {1}, found an element of the segments tuple which""
                    "" isn't a segment. Instead found element of type {2}.\\nFound: {3}\\nFull segments:{4}"".format(
                        text,
                        type(self),
                        type(elem),
                        elem,
                        self.segments
                    ))","1. Use `type()` to check if an element is an instance of a specific class.
2. Use `TypeError()` to raise an exception if an element is not an instance of a specific class.
3. Use `format()` to format the error message."
"    def __init__(self, segments, pos_marker=None):
        if len(segments) == 0:
            raise RuntimeError(
                ""Setting {0} with a zero length segment set. This shouldn't happen."".format(
                    self.__class__))

        if hasattr(segments, 'matched_segments'):
            # Safely extract segments from a match
            self.segments = segments.matched_segments
        elif isinstance(segments, tuple):
            self.segments = segments
        elif isinstance(segments, list):
            self.segments = tuple(segments)
        else:
            raise TypeError(
                ""Unexpected type passed to BaseSegment: {0}"".format(
                    type(segments)))

        # Check elements of segments:
        self.validate_segments()

        if pos_marker:
            self.pos_marker = pos_marker
        else:
            # If no pos given, it's the pos of the first segment
            # Work out if we're dealing with a match result...
            if hasattr(segments, 'initial_match_pos_marker'):
                self.pos_marker = segments.initial_match_pos_marker()
            elif isinstance(segments, (tuple, list)):
                self.pos_marker = segments[0].pos_marker
            else:
                raise TypeError(
                    ""Unexpected type passed to BaseSegment: {0}"".format(
                        type(segments)))","1. Use `typing` to specify the types of arguments and return values.
2. Validate the arguments before using them.
3. Use `pos_marker` instead of `initial_match_pos_marker` to avoid leaking information about the underlying match object."
"    def apply_fixes(self, fixes):
        """"""Apply an iterable of fixes to this segment.

        Used in applying fixes if we're fixing linting errors.
        If anything changes, this should return a new version of the segment
        rather than mutating the original.

        Note: We need to have fixes to apply AND this must have children. In the case
        of raw segments, they will be replaced or removed by their parent and
        so this function should just return self.
        """"""
        # Let's check what we've been given.
        if fixes and isinstance(fixes[0], SQLLintError):
            logging.error(""Transforming `fixes` from errors into a list of fixes"")
            # We've got linting errors, let's aggregate them into a list of fixes
            buff = []
            for err in fixes:
                buff += err.fixes
            # Overwrite fixes
            fixes = buff

        if fixes and not self.is_raw():
            # Get a reference to self to start with, but this will rapidly
            # become a working copy.
            r = self

            # Make a working copy
            seg_buffer = []
            todo_buffer = list(self.segments)
            while True:
                if len(todo_buffer) == 0:
                    break
                else:
                    seg = todo_buffer.pop(0)
                    unused_fixes = []
                    for f in fixes:
                        if f.anchor == seg:
                            if f.edit_type == 'delete':
                                # We're just getting rid of this segment.
                                seg = None
                            elif f.edit_type in ('edit', 'create'):
                                # We're doing a replacement (it could be a single segment or an iterable)
                                if isinstance(f.edit, BaseSegment):
                                    seg_buffer.append(f.edit)
                                else:
                                    for s in f.edit:
                                        seg_buffer.append(s)

                                if f.edit_type == 'create':
                                    # in the case of a creation, also add this segment on the end
                                    seg_buffer.append(seg)
                            else:
                                raise ValueError(
                                    ""Unexpected edit_type: {0!r} in {1!r}"".format(
                                        f.edit_type, f))
                            # We've applied a fix here. Move on, this also consumes the fix
                            # TODO: Maybe deal with overlapping fixes later.
                            break
                        else:
                            # We've not used the fix so we should keep it in the list for later.
                            unused_fixes.append(f)
                    else:
                        seg_buffer.append(seg)
                # Switch over the the unused list
                fixes = unused_fixes

            # Then recurse (i.e. deal with the children) (Requeueing)
            seg_queue = seg_buffer
            seg_buffer = []
            for seg in seg_queue:
                s, fixes = seg.apply_fixes(fixes)
                seg_buffer.append(s)

            # Reform into a new segment
            r = r.__class__(
                segments=tuple(seg_buffer),
                pos_marker=r.pos_marker
            )

            # Lastly, before returning, we should realign positions.
            # Note: Realign also returns a copy
            return r.realign(), fixes
        else:
            return self, fixes","1. Use `isinstance()` to check the type of `fixes` before processing them.
2. Use `logging.error()` to log errors instead of printing them to the console.
3. Use `self.realign()` to realign positions before returning the new segment."
"    def realign(self):
        """"""Realign the positions in this segment.

        Returns:
            a copy of this class with the pos_markers realigned.

        Note: this is used mostly during fixes.

        Realign is recursive. We will assume that the pos_marker of THIS segment is
        truthful, and that during recursion it will have been set by the parent.

        This function will align the pos marker if it's direct children, we then
        recurse to realign their children.

        """"""
        seg_buffer = []
        todo_buffer = list(self.segments)
        running_pos = self.pos_marker

        while True:
            if len(todo_buffer) == 0:
                # We're done.
                break
            else:
                # Get the first off the buffer
                seg = todo_buffer.pop(0)

                # Is it a meta segment?
                if seg.is_meta:
                    # If so, just carry on.
                    seg_buffer.append(seg)
                    continue

                # We'll preserve statement indexes so we should keep track of that.
                # When recreating, we use the DELTA of the index so that's what matter...
                idx = seg.pos_marker.statement_index - running_pos.statement_index
                if len(seg.segments) > 0:
                    # It's a compound segment, so keep track of it's children
                    child_segs = seg.segments
                    # Create a new segment of the same type with the new position
                    seg = seg.__class__(
                        segments=child_segs,
                        pos_marker=running_pos
                    )
                    # Realign the children of that class
                    seg = seg.realign()
                else:
                    # It's a raw segment...
                    # Create a new segment of the same type with the new position
                    seg = seg.__class__(
                        raw=seg.raw,
                        pos_marker=running_pos
                    )
                # Update the running position with the content of that segment
                running_pos = running_pos.advance_by(
                    raw=seg.raw, idx=idx
                )
                # Add the buffer to my new segment
                seg_buffer.append(seg)

        # Create a new version of this class with the new details
        return self.__class__(
            segments=tuple(seg_buffer),
            pos_marker=self.pos_marker
        )","1. Use `isinstance()` to check if an object is a meta segment.
2. Use `len()` to check if a segment has children.
3. Use `__class__()` to create a new segment of the same type with the new position."
"    def __init__(self):
        """"""For the indent we override the init method.

        For something without content, neither makes sense.
        """"""
        self._raw = ''
        self.pos_marker = None","1. Use `f-strings` instead of concatenation to prevent `format string vulnerabilities`.
2. Use `type hints` to make the code more explicit and to catch errors at compile time.
3. Use `proper error handling` to ensure that the code does not crash in the event of an error."
"    def __init__(self, max_line_length=80, tab_space_size=4, indent_unit='space', **kwargs):
        """"""Initialise, getting the max line length.""""""
        self.max_line_length = max_line_length
        # Call out tab_space_size and indent_unit to make it clear they're still options.
        super(Rule_L016, self).__init__(
            tab_space_size=tab_space_size, indent_unit=indent_unit,
            **kwargs)","1. Use `self.max_line_length` to limit the length of each line.
2. Use `tab_space_size` and `indent_unit` to control the indentation style.
3. Call `super(Rule_L016, self).__init__(**kwargs)` to initialize the parent class."
"    def _eval(self, segment, raw_stack, **kwargs):
        """"""Line is too long.

        This only triggers on newline segments, evaluating the whole line.
        The detection is simple, the fixing is much trickier.

        """"""
        if segment.name == 'newline':
            # iterate to buffer the whole line up to this point
            this_line = []
            idx = -1
            while True:
                if len(raw_stack) >= abs(idx):
                    s = raw_stack[idx]
                    if s.name == 'newline':
                        break
                    else:
                        this_line.insert(0, s)
                        idx -= 1
                else:
                    break

            # Now we can work out the line length and deal with the content
            line_len = sum(len(s.raw) for s in this_line)
            if line_len > self.max_line_length:
                # Problem, we'll be reporting a violation. The
                # question is, can we fix it?

                # We'll need the indent, so let's get it for fixing.
                line_indent = []
                idx = 0
                for s in this_line:
                    if s.name == 'whitespace':
                        line_indent.append(s)
                    else:
                        break

                # Does the line end in an inline comment that we can move back?
                if this_line[-1].name == 'inline_comment':
                    # Set up to delete the original comment and the preceeding whitespace
                    delete_buffer = [LintFix('delete', this_line[-1])]
                    idx = -2
                    while True:
                        if len(this_line) >= abs(idx) and this_line[idx].name == 'whitespace':
                            delete_buffer.append(LintFix('delete', this_line[idx]))
                            idx -= 1
                        else:
                            break
                    # Create a newline before this one with the existing comment, an
                    # identical indent AND a terminating newline, copied from the current
                    # target segment.
                    create_buffer = [
                        LintFix(
                            'create', this_line[0],
                            line_indent + [this_line[-1], segment]
                        )
                    ]
                    return LintResult(anchor=segment, fixes=delete_buffer + create_buffer)

                # Does the line contain a place where an indent might be possible?
                if any(elem.is_meta and elem.indent_val != 0 for elem in this_line):
                    # What's the net sum of them?
                    indent_balance = sum(elem.indent_val for elem in this_line if elem.is_meta)
                    # Yes, let's work out which is best.
                    if indent_balance == 0:
                        # It's even. We should break after the *last* dedent
                        ws_pre = []
                        ws_post = []
                        running_balance = 0
                        started = False
                        found = False
                        fix_buffer = None
                        # Work through to find the right point
                        for elem in this_line:
                            if elem.name == 'whitespace':
                                if found:
                                    if fix_buffer is None:
                                        # In this case we EDIT, because
                                        # we want to remove the existing whitespace
                                        # here. We need to remember the INDENT.
                                        fix_buffer = [
                                            LintFix(
                                                'edit', elem,
                                                [segment] + line_indent
                                            )
                                        ]
                                    else:
                                        # Store potentially unnecessary whitespace.
                                        ws_post.append(elem)
                                elif started:
                                    # Store potentially unnecessary whitespace.
                                    ws_pre.append(elem)
                            elif elem.is_meta:
                                running_balance += elem.indent_val
                                started = True
                                # Clear the buffer.
                                ws_post = []
                                if running_balance == 0:
                                    found = True
                            else:
                                # Something that isn't a meta or whitespace
                                if found:
                                    if fix_buffer is None:
                                        # In this case we create because we
                                        # want to preserve what already exits
                                        # here. We need to remember the INDENT.
                                        fix_buffer = [
                                            LintFix(
                                                'create', elem,
                                                [segment] + line_indent
                                            )
                                        ]
                                    # We have all we need
                                    break
                                else:
                                    # Clear the buffer.
                                    ws_pre = []
                        else:
                            raise RuntimeError(""We shouldn't get here!"")

                        # Remove unnecessary whitespace
                        for elem in ws_pre + ws_post:
                            fix_buffer.append(
                                LintFix(
                                    'delete', elem
                                )
                            )

                        return LintResult(anchor=segment, fixes=fix_buffer)
                    elif indent_balance > 0:
                        # If it's positive, we have more indents than dedents.
                        # Make sure the first unused indent is used.
                        delete_buffer = []
                        newline_anchor = None
                        found = False
                        for elem in this_line:
                            if elem.name == 'whitespace':
                                delete_buffer.append(elem)
                            elif found:
                                newline_anchor = elem
                                break
                            elif elem.is_meta:
                                if elem.indent_val > 0:
                                    found = True
                                else:
                                    pass
                            else:
                                # It's not meta, and not whitespace:
                                # reset buffer
                                delete_buffer = []
                        else:
                            raise RuntimeError(""We shouldn't get here!"")

                        # Make a newline where it needs to be, with ONE EXTRA INDENT
                        new_indent = self._make_indent(1)
                        fix_buffer = [
                            LintFix(
                                'create', newline_anchor,
                                # It's ok to use the current segment posmarker, because we're staying in the same statement (probably?)
                                [segment] + line_indent + [self.make_whitespace(raw=new_indent, pos_marker=segment.pos_marker)]
                            )
                        ]

                        # Remove unnecessary whitespace
                        for elem in delete_buffer:
                            fix_buffer.append(
                                LintFix(
                                    'delete', elem
                                )
                            )

                        return LintResult(anchor=segment, fixes=fix_buffer)
                    else:
                        # Don't know what to do here!
                        raise NotImplementedError(
                            (""Don't know what to do with negative ""
                             ""indent balance ({0})."").format(
                                indent_balance))

                return LintResult(anchor=segment)
        # Otherwise we're all good
        return None","1. Use `LintFix` to delete unnecessary whitespace.
2. Use `LintFix` to create a newline with one extra indent.
3. Use `LintFix` to delete whitespace."
"    def _eval(self, segment, raw_stack, memory, **kwargs):
        """"""Inconsistent capitalisation of keywords.

        We use the `memory` feature here to keep track of
        what we've seen in the past.

        """"""
        cases_seen = memory.get('cases_seen', set())

        if segment.type == self._target_elem:
            raw = segment.raw
            uc = raw.upper()
            lc = raw.lower()
            cap = raw.capitalize()
            seen_case = None
            if uc == lc:
                # Caseless
                pass
            elif raw == uc:
                seen_case = ""upper""
            elif raw == lc:
                seen_case = ""lower""
            elif raw == cap:
                # NB: American spelling :(
                seen_case = ""capitalize""
            else:
                seen_case = ""inconsistent""

            # NOTE: We'll only add to cases_seen if we DONT
            # also raise an error, so that we can focus in.

            def make_replacement(seg, policy):
                """"""Make a replacement segment, based on seen capitalisation.""""""
                if policy == ""lower"":
                    new_raw = seg.raw.lower()
                elif policy == ""upper"":
                    new_raw = seg.raw.upper()
                elif policy == ""capitalize"":
                    new_raw = seg.raw.capitalize()
                elif policy == ""consistent"":
                    if cases_seen:
                        # Get an element from what we've already seen
                        return make_replacement(seg, list(cases_seen)[0])
                    else:
                        # If we haven't seen anything yet, then let's default
                        # to upper
                        return make_replacement(seg, ""upper"")
                else:
                    raise ValueError(""Unexpected capitalisation policy: {0!r}"".format(policy))
                # Make a new class and return it.
                return seg.__class__(
                    raw=new_raw, pos_marker=seg.pos_marker
                )

            if not seen_case:
                # Skip this if we haven't seen anything good.
                # No need to update memory
                return LintResult(memory=memory)
            elif (
                (self.capitalisation_policy == ""consistent"" and cases_seen and seen_case not in cases_seen)
                # Policy is either upper, lower or capitalize
                or (self.capitalisation_policy != ""consistent"" and seen_case != self.capitalisation_policy)
            ):
                return LintResult(
                    anchor=segment,
                    fixes=[
                        LintFix('edit', segment, make_replacement(
                            segment, self.capitalisation_policy))
                    ],
                    memory=memory)
            else:
                # Update memory and carry on
                cases_seen.add(seen_case)
                memory['cases_seen'] = cases_seen
                return LintResult(memory=memory)

        # If it's not a keyword just carry on
        return LintResult(memory=memory)","1. Use `raw` instead of `unicode` to avoid UnicodeDecodeError.
2. Use `make_replacement` to create a new segment instead of directly modifying the original segment.
3. Use `LintResult` to return the results of linting."
"            def make_replacement(seg, policy):
                """"""Make a replacement segment, based on seen capitalisation.""""""
                if policy == ""lower"":
                    new_raw = seg.raw.lower()
                elif policy == ""upper"":
                    new_raw = seg.raw.upper()
                elif policy == ""capitalize"":
                    new_raw = seg.raw.capitalize()
                elif policy == ""consistent"":
                    if cases_seen:
                        # Get an element from what we've already seen
                        return make_replacement(seg, list(cases_seen)[0])
                    else:
                        # If we haven't seen anything yet, then let's default
                        # to upper
                        return make_replacement(seg, ""upper"")
                else:
                    raise ValueError(""Unexpected capitalisation policy: {0!r}"".format(policy))
                # Make a new class and return it.
                return seg.__class__(
                    raw=new_raw, pos_marker=seg.pos_marker
                )","1. Use `type()` to check if the input argument `policy` is a string.
2. Use `assert()` to check if the input argument `policy` is one of the expected values.
3. Use `raise` to raise an exception if the input argument `policy` is invalid."
"    def __init__(self, configs=None, overrides=None):
        self._overrides = overrides  # We only store this for child configs
        defaults = ConfigLoader.get_global().load_default_config_file()
        self._configs = nested_combine(
            defaults,
            configs or {'core': {}},
            {'core': overrides or {}})
        # Some configs require special treatment
        self._configs['core']['color'] = False if self._configs['core']['nocolor'] else None
        # Whitelists and blacklists
        if self._configs['core']['rules']:
            self._configs['core']['rule_whitelist'] = self._configs['core']['rules'].split(',')
        else:
            self._configs['core']['rule_whitelist'] = None
        if self._configs['core']['exclude_rules']:
            self._configs['core']['rule_blacklist'] = self._configs['core']['exclude_rules'].split(',')
        else:
            self._configs['core']['rule_blacklist'] = None
        # Configure Recursion
        if self._configs['core']['recurse'] == 0:
            self._configs['core']['recurse'] = True
        # Dialect and Template selection
        self._configs['core']['dialect_obj'] = dialect_selector(self._configs['core']['dialect'])
        self._configs['core']['templater_obj'] = templater_selector(self._configs['core']['templater'])","1. Use `ConfigLoader.get_global().load_default_config_file()` to load the default configuration file instead of `defaults`.
2. Use `nested_combine()` to combine the configurations instead of manually merging them.
3. Use `dialect_selector()` and `templater_selector()` to select the dialect and templater objects instead of manually instantiating them."
"    def parse_file(self, f, fname=None, verbosity=0, recurse=True):
        violations = []
        t0 = get_time()

        # Allow f to optionally be a raw string
        if isinstance(f, str):
            # Add it to a buffer if that's what we're doing
            f = StringIO(f)

        verbosity_logger(""LEXING RAW ({0})"".format(fname), verbosity=verbosity)
        # Lex the file and log any problems
        try:
            fs = FileSegment.from_raw(f.read())
        except SQLLexError as err:
            violations.append(err)
            fs = None

        if fs:
            verbosity_logger(fs.stringify(), verbosity=verbosity)

        t1 = get_time()
        verbosity_logger(""PARSING ({0})"".format(fname), verbosity=verbosity)
        # Parse the file and log any problems
        if fs:
            try:
                parsed = fs.parse(recurse=recurse, verbosity=verbosity, dialect=self.dialect)
            except SQLParseError as err:
                violations.append(err)
                parsed = None
            if parsed:
                verbosity_logger(frame_msg(""Parsed Tree:""), verbosity=verbosity)
                verbosity_logger(parsed.stringify(), verbosity=verbosity)
        else:
            parsed = None

        t2 = get_time()
        time_dict = {'lexing': t1 - t0, 'parsing': t2 - t1}

        return parsed, violations, time_dict","1. Use `fs.from_text()` instead of `fs.from_raw()` to prevent code injection.
2. Validate the input file name to prevent directory traversal attacks.
3. Use `fs.parse()` with a `dialect` argument to specify the SQL dialect."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """"""
            Matching can be done from either the raw or the segments.
            This raw function can be overridden, or a grammar defined
            on the underlying class.
        """"""
        raise NotImplementedError(""{0} has no match function implemented"".format(self.__class__.__name__))","1. Use `type()` to check the type of `segments` argument.
2. Use `assert()` to check if `match_depth`, `parse_depth`, and `verbosity` are non-negative integers.
3. Use `logging.exception()` to log exceptions."
"    def _match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """""" A wrapper on the match function to do some basic validation """"""
        t0 = get_time()

        if isinstance(segments, BaseSegment):
            segments = segments,  # Make into a tuple for compatability
        if not isinstance(segments, tuple):
            logging.warning(
                ""{0}.match, was passed {1} rather than tuple or segment"".format(
                    self.__class__.__name__, type(segments)))
            if isinstance(segments, list):
                # Let's make it a tuple for compatibility
                segments = tuple(segments)

        if len(segments) == 0:
            logging.info(""{0}.match, was passed zero length segments list. NB: {0} contains {1!r}"".format(
                self.__class__.__name__, self._elements))

        # Work out the raw representation and curtail if long
        parse_match_logging(
            parse_depth, match_depth, match_segment, self.__class__.__name__,
            '_match', 'IN', verbosity=verbosity, v_level=self.v_level,
            le=len(self._elements), ls=len(segments),
            seg=join_segments_raw_curtailed(segments))

        m = self.match(segments, match_depth=match_depth, parse_depth=parse_depth,
                       verbosity=verbosity, dialect=dialect, match_segment=match_segment)

        if not isinstance(m, MatchResult):
            logging.warning(
                ""{0}.match, returned {1} rather than MatchResult"".format(
                    self.__class__.__name__, type(m)))

        dt = get_time() - t0
        if m.is_complete():
            msg = 'OUT ++'
        elif m:
            msg = 'OUT -'
        else:
            msg = 'OUT'

        parse_match_logging(
            parse_depth, match_depth, match_segment, self.__class__.__name__,
            '_match', msg, verbosity=verbosity, v_level=self.v_level, dt=dt, m=m)

        # Basic Validation
        check_still_complete(segments, m.matched_segments, m.unmatched_segments)
        return m","1. Use `type()` to check the type of arguments passed to the function.
2. Use `logging.warning()` to log warnings.
3. Use `MatchResult` to check if the match is complete."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        elem = self._get_elem(dialect=dialect)

        if elem:
            # Match against that. NB We're not incrementing the match_depth here.
            # References shouldn't relly count as a depth of match.
            match_segment = self._get_ref()
            return elem._match(
                segments=segments, match_depth=match_depth,
                parse_depth=parse_depth, verbosity=verbosity,
                dialect=dialect, match_segment=match_segment)
        else:
            raise ValueError(""Null Element returned! _elements: {0!r}"".format(self._elements))","1. Use `type()` to check the type of `elem` before calling `_match()`.
2. Use `isinstance()` to check if `elem` is a subclass of `Element`.
3. Use `elem._match()` instead of `self._match()` to avoid infinite recursion."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        best_match = None
        # Match on each of the options
        for opt in self._elements:
            m = opt._match(segments, match_depth=match_depth + 1, parse_depth=parse_depth,
                           verbosity=verbosity, dialect=dialect, match_segment=match_segment)
            # If we get a complete match, just return it. If it's incomplete, then check to
            # see if it's all non-code if that allowed and match it
            if m.is_complete():
                # this will return on the *first* complete match
                return m
            elif m:
                if self.code_only:
                    # Attempt to consume whitespace if we can
                    matched_segments = m.matched_segments
                    unmatched_segments = m.unmatched_segments
                    while True:
                        if len(unmatched_segments) > 0:
                            if unmatched_segments[0].is_code:
                                break
                            else:
                                # Append as tuple
                                matched_segments += unmatched_segments[0],
                                unmatched_segments = unmatched_segments[1:]
                        else:
                            break
                    m = MatchResult(matched_segments, unmatched_segments)
                if best_match:
                    if len(m) > len(best_match):
                        best_match = m
                    else:
                        continue
                else:
                    best_match = m
                parse_match_logging(
                    parse_depth, match_depth, match_segment, self.__class__.__name__,
                    '_match', ""Saving Match of Length {0}:  {1}"".format(len(m), m),
                    verbosity=verbosity, v_level=self.v_level)
        else:
            # No full match from the first time round. If we've got a long partial match then return that.
            if best_match:
                return best_match
            # Ok so no match at all from the elements. Small getout if we can match any whitespace
            if self.code_only:
                matched_segs = tuple()
                unmatched_segs = segments
                # Look for non-code up front
                while True:
                    if len(unmatched_segs) == 0:
                        # We can't return a successful match on JUST whitespace
                        return MatchResult.from_unmatched(segments)
                    elif not unmatched_segs[0].is_code:
                        matched_segs += unmatched_segs[0],
                        unmatched_segs = unmatched_segs[1:]
                    else:
                        break
                # Now try and match
                for opt in self._elements:
                    m = opt._match(unmatched_segs, match_depth=match_depth + 1, parse_depth=parse_depth,
                                   verbosity=verbosity, dialect=dialect, match_segment=match_segment)
                    # Once again, if it's complete - return, if not wait to see if we get a more complete one
                    new_match = MatchResult(matched_segs + m.matched_segments, m.unmatched_segments)
                    if m.is_complete():
                        return new_match
                    elif m:
                        if best_match:
                            if len(best_match) > len(m):
                                best_match = m
                            else:
                                continue
                        else:
                            best_match = m
                        parse_match_logging(
                            parse_depth, match_depth, match_segment, self.__class__.__name__,
                            '_match', ""Last-Ditch: Saving Match of Length {0}:  {1}"".format(len(m), m),
                            verbosity=verbosity, v_level=self.v_level)
                else:
                    if best_match:
                        return MatchResult(matched_segs + best_match.matched_segments, best_match.unmatched_segments)
                    else:
                        return MatchResult.from_unmatched(segments)","1. Use `isinstance()` to check if a segment is code before appending it to a match result.
2. Use `MatchResult.from_unmatched()` to return a match result with unmatched segments.
3. Use `parse_match_logging()` to log the match results."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        # Match on each of the options
        matched_segments = MatchResult.from_empty()
        unmatched_segments = segments
        n_matches = 0
        while True:
            if self.max_times and n_matches >= self.max_times:
                # We've matched as many times as we can
                return MatchResult(matched_segments.matched_segments, unmatched_segments)

            # Is there anything left to match?
            if len(unmatched_segments) == 0:
                # No...
                if n_matches >= self.min_times:
                    return MatchResult(matched_segments.matched_segments, unmatched_segments)
                else:
                    # We didn't meet the hurdle
                    return MatchResult.from_unmatched(unmatched_segments)

            # Is the next segment code?
            if self.code_only and not unmatched_segments[0].is_code:
                # We should add this one to the match and carry on
                matched_segments += unmatched_segments[0],
                unmatched_segments = unmatched_segments[1:]
                check_still_complete(segments, matched_segments.matched_segments, unmatched_segments)
                continue

            # Try the possibilities
            for opt in self._elements:
                m = opt._match(unmatched_segments, match_depth=match_depth + 1,
                               parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                               match_segment=match_segment)
                if m.has_match():
                    matched_segments += m.matched_segments
                    unmatched_segments = m.unmatched_segments
                    n_matches += 1
                    # Break out of the for loop which cycles us round
                    break
            else:
                # If we get here, then we've not managed to match. And the next
                # unmatched segments are meaningful, i.e. they're not what we're
                # looking for.
                if n_matches >= self.min_times:
                    return MatchResult(matched_segments.matched_segments, unmatched_segments)
                else:
                    # We didn't meet the hurdle
                    return MatchResult.from_unmatched(unmatched_segments)","1. Use `input()` instead of `raw_input()` to prevent code injection.
2. Use `sys.stdout.buffer.write()` instead of `print()` to prevent command injection.
3. Use `os.path.join()` instead of `+` to prevent directory traversal attacks."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """"""
        Matching for GreedyUntil works just how you'd expect.
        """"""
        for idx, seg in enumerate(segments):
            for opt in self._elements:
                if opt._match(seg, match_depth=match_depth + 1, parse_depth=parse_depth,
                              verbosity=verbosity, dialect=dialect, match_segment=match_segment):
                    # We've matched something. That means we should return everything up to this point
                    return MatchResult(segments[:idx], segments[idx:])
                else:
                    continue
        else:
            # We've got to the end without matching anything, so return.
            # We don't need to keep track of the match results, because
            # if any of them were usable, then we wouldn't be returning
            # anyway.
            return MatchResult.from_matched(segments)","1. Use `functools.lru_cache` to cache the results of the `match` method to improve performance.
2. Sanitize user input to prevent against injection attacks.
3. Use `type hints` to make the code more readable and easier to maintain."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        # Rewrite of sequence. We should match FORWARD, this reduced duplication.
        # Sub-matchers should be greedy and so we can jsut work forward with each one.
        if isinstance(segments, BaseSegment):
            segments = tuple(segments)
        # NB: We don't use seg_idx here because the submatchers may be mutating the length
        # of the remaining segments
        matched_segments = MatchResult.from_empty()
        unmatched_segments = segments

        for idx, elem in enumerate(self._elements):
            while True:
                if len(unmatched_segments) == 0:
                    # We've run our of sequence without matching everyting.
                    # Do only optional elements remain.
                    if all([e.is_optional() for e in self._elements[idx:]]):
                        # then it's ok, and we can return what we've got so far.
                        # No need to deal with anything left over because we're at the end.
                        return matched_segments
                    else:
                        # we've got to the end of the sequence without matching all
                        # required elements.
                        return MatchResult.from_unmatched(segments)
                else:
                    # We're not at the end, first detect whitespace and then try to match.
                    if self.code_only and not unmatched_segments[0].is_code:
                        # We should add this one to the match and carry on
                        matched_segments += unmatched_segments[0],
                        unmatched_segments = unmatched_segments[1:]
                        check_still_complete(segments, matched_segments.matched_segments, unmatched_segments)
                        continue

                    # It's not whitespace, so carry on to matching
                    elem_match = elem._match(
                        unmatched_segments, match_depth=match_depth + 1,
                        parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                        match_segment=match_segment)

                    if elem_match.has_match():
                        # We're expecting mostly partial matches here, but complete
                        # matches are possible.
                        matched_segments += elem_match.matched_segments
                        unmatched_segments = elem_match.unmatched_segments
                        # Each time we do this, we do a sense check to make sure we haven't
                        # dropped anything. (Because it's happened before!).
                        check_still_complete(segments, matched_segments.matched_segments, unmatched_segments)

                        # Break out of the while loop and move to the next element.
                        break
                    else:
                        # If we can't match an element, we should ascertain whether it's
                        # required. If so then fine, move on, but otherwise we should crash
                        # out without a match. We have not matched the sequence.
                        if elem.is_optional():
                            # This will crash us out of the while loop and move us
                            # onto the next matching element
                            break
                        else:
                            return MatchResult.from_unmatched(segments)
        else:
            # If we get to here, we've matched all of the elements (or skipped them)
            # but still have some segments left (or perhaps have precisely zero left).
            # In either case, we're golden. Return successfully, with any leftovers as
            # the unmatched elements. UNLESS they're whitespace and we should be greedy.
            if self.code_only:
                while True:
                    if len(unmatched_segments) == 0:
                        break
                    elif not unmatched_segments[0].is_code:
                        # We should add this one to the match and carry on
                        matched_segments += unmatched_segments[0],
                        unmatched_segments = unmatched_segments[1:]
                        check_still_complete(segments, matched_segments.matched_segments, unmatched_segments)
                        continue
                    else:
                        break

            return MatchResult(matched_segments.matched_segments, unmatched_segments)","1. Use `type()` to check the type of each segment before matching.
2. Use `len()` to check the length of each segment before matching.
3. Use `isinstance()` to check if each segment is an instance of a specific class before matching."
"    def __init__(self, *args, **kwargs):
        if 'delimiter' not in kwargs:
            raise ValueError(""Delimited grammars require a `delimiter`"")
        self.delimiter = kwargs.pop('delimiter')
        self.allow_trailing = kwargs.pop('allow_trailing', False)
        self.terminator = kwargs.pop('terminator', None)
        # Setting min delimiters means we have to match at least this number
        self.min_delimiters = kwargs.pop('min_delimiters', None)

        # The details on how to match a bracket are stored in the dialect
        self.start_bracket = Ref('StartBracketSegment')
        self.end_bracket = Ref('EndBracketSegment')
        super(Delimited, self).__init__(*args, **kwargs)","1. Use `validate_input` to check if the input is valid before processing it.
2. Sanitize the input to remove any malicious code.
3. Use `escape_output` to escape any special characters in the output."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        if isinstance(segments, BaseSegment):
            segments = [segments]
        seg_idx = 0
        terminal_idx = len(segments)
        sub_bracket_count = 0
        start_bracket_idx = None
        # delimiters is a list of tuples (idx, len), which keeps track of where
        # we found delimiters up to this point.
        delimiters = []
        matched_segments = MatchResult.from_empty()

        # Have we been passed an empty list?
        if len(segments) == 0:
            return MatchResult.from_empty()

        # First iterate through all the segments, looking for the delimiter.
        # Second, split the list on each of the delimiters, and ensure that
        # each sublist in turn matches one of the elements.

        # In more detail, match against delimiter, if we match, put a slice
        # up to that point onto a list of slices. Carry on.
        while True:
            # Are we at the end of the sequence?
            if seg_idx >= terminal_idx:
                # Yes we're at the end

                # We now need to check whether everything from either the start
                # or from the last delimiter up to here matches. We CAN allow
                # a partial match at this stage.

                # Are we in a bracket counting cycle that hasn't finished yet?
                if sub_bracket_count > 0:
                    # TODO: Format this better
                    raise SQLParseError(
                        ""Couldn't find closing bracket for opening bracket."",
                        segment=segments[start_bracket_idx])

                # Do we already have any delimiters?
                if delimiters:
                    # Yes, get the last delimiter
                    dm1 = delimiters[-1]
                    # get everything after the last delimiter
                    pre_segment = segments[dm1[0] + dm1[1]:terminal_idx]
                else:
                    # No, no delimiters at all so far.
                    # TODO: Allow this to be configured.
                    # Just get everything up to this point
                    pre_segment = segments[:terminal_idx]

                # Optionally here, we can match some non-code up front.
                if self.code_only:
                    while len(pre_segment) > 0:
                        if not pre_segment[0].is_code:
                            matched_segments += pre_segment[0],  # As tuple
                            pre_segment = pre_segment[1:]
                        else:
                            break

                # Check we actually have something left to match on
                if len(pre_segment) > 0:
                    # See if any of the elements match
                    for elem in self._elements:
                        elem_match = elem._match(
                            pre_segment, match_depth=match_depth + 1,
                            parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                            match_segment=match_segment)

                        if elem_match.has_match():
                            # Successfully matched one of the elements in this spot

                            # Add this match onto any already matched segments and return.
                            # We do this in a slightly odd way here to allow partial matches.

                            # we do a quick check on min_delimiters if present
                            if self.min_delimiters and len(delimiters) < self.min_delimiters:
                                # if we do have a limit and we haven't met it then crash out
                                return MatchResult.from_unmatched(segments)
                            return MatchResult(
                                matched_segments.matched_segments + elem_match.matched_segments,
                                elem_match.unmatched_segments + segments[terminal_idx:])
                        else:
                            # Not matched this element, move on.
                            # NB, a partial match here isn't helpful. We're matching
                            # BETWEEN two delimiters and so it must be a complete match.
                            # Incomplete matches are only possible at the end
                            continue

                # If we're here we haven't matched any of the elements on this last element.
                # BUT, if we allow trailing, and we have matched something, we can end on the last
                # delimiter
                if self.allow_trailing and len(matched_segments) > 0:
                    return MatchResult(matched_segments.matched_segments, pre_segment + segments[terminal_idx:])
                else:
                    return MatchResult.from_unmatched(segments)

            else:
                # We've got some sequence left.
                # Are we in a bracket cycle?
                if sub_bracket_count > 0:
                    # Is it another bracket entry?
                    bracket_match = self.start_bracket._match(
                        segments=segments[seg_idx:], match_depth=match_depth + 1,
                        parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                        match_segment=match_segment)
                    if bracket_match.has_match():
                        # increment the open bracket counter and proceed
                        sub_bracket_count += 1
                        seg_idx += len(bracket_match)
                        continue

                    # Is it a closing bracket?
                    bracket_match = self.end_bracket._match(
                        segments=segments[seg_idx:], match_depth=match_depth + 1,
                        parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                        match_segment=match_segment)
                    if bracket_match.has_match():
                        # reduce the bracket count and then advance the counter.
                        sub_bracket_count -= 1
                        seg_idx += len(bracket_match)
                        continue

                else:
                    # No bracket cycle
                    # Do we have a delimiter at the current index?

                    del_match = self.delimiter._match(
                        segments[seg_idx:], match_depth=match_depth + 1,
                        parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                        match_segment=match_segment)

                    # Doesn't have to match fully, just has to give us a delimiter.
                    if del_match.has_match():
                        # We've got at least a partial match
                        # Record the location of this delimiter
                        d = (seg_idx, len(del_match))
                        # Do we already have any delimiters?
                        if delimiters:
                            # Yes
                            dm1 = delimiters[-1]
                            # slice the segments between this delimiter and the previous
                            pre_segment = segments[dm1[0] + dm1[1]:d[0]]
                        else:
                            # No
                            # Just get everything up to this point
                            pre_segment = segments[:d[0]]
                        # Append the delimiter that we have found.
                        delimiters.append(d)

                        # Optionally here, we can match some non-code up front.
                        if self.code_only:
                            while len(pre_segment) > 0:
                                if not pre_segment[0].is_code:
                                    matched_segments += pre_segment[0],  # As tuple
                                    pre_segment = pre_segment[1:]
                                else:
                                    break

                        # We now check that this chunk matches whatever we're delimiting.
                        # In this case it MUST be a full match, not just a partial match
                        for elem in self._elements:
                            elem_match = elem._match(
                                pre_segment, match_depth=match_depth + 1,
                                parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                                match_segment=match_segment)

                            if elem_match.is_complete():
                                # Successfully matched one of the elements in this spot

                                # First add the segment up to the delimiter to the matched segments
                                matched_segments += elem_match
                                # Then add the delimiter to the matched segments
                                matched_segments += del_match
                                # Break this for loop and move on, looking for the next delimiter
                                seg_idx += len(del_match)
                                break
                            elif elem_match and self.code_only:
                                # Optionally if it's not a complete match but the unmatched bits are
                                # all non code then we'll also take it.
                                if all([not seg.is_code for seg in elem_match.unmatched_segments]):
                                    # Logic as above, just with the unmatched bits too because none are code
                                    matched_segments += elem_match.matched_segments
                                    matched_segments += elem_match.unmatched_segments
                                    matched_segments += del_match
                                    seg_idx += len(del_match)
                                    break
                                else:
                                    continue
                            else:
                                # Not matched this element, move on.
                                # NB, a partial match here isn't helpful. We're matching
                                # BETWEEN two delimiters and so it must be a complete match.
                                # Incomplete matches are only possible at the end
                                continue
                        else:
                            # If we're here we haven't matched any of the elements, then we have a problem
                            return MatchResult.from_unmatched(segments)
                    # This index doesn't have a delimiter, check for brackets and terminators

                    # First is it a terminator (and we're not in a bracket cycle)
                    if self.terminator:
                        term_match = self.terminator._match(
                            segments[seg_idx:], match_depth=match_depth + 1,
                            parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                            match_segment=match_segment)
                        if term_match:
                            # we've found a terminator.
                            # End the cycle here.
                            terminal_idx = seg_idx
                            continue

                    # Last, do we need to enter a bracket cycle
                    bracket_match = self.start_bracket._match(
                        segments=segments[seg_idx:], match_depth=match_depth + 1,
                        parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                        match_segment=match_segment)
                    if bracket_match.has_match():
                        # increment the open bracket counter and proceed
                        sub_bracket_count += 1
                        seg_idx += len(bracket_match)
                        continue

                # Nothing else interesting. Carry On
                # This is the same regardless of whether we're in the bracket cycle
                # or otherwise.
                seg_idx += 1","1. Use `match_depth` and `parse_depth` to prevent infinite recursion.
2. Use `code_only` to only match non-code segments.
3. Use `min_delimiters` to prevent matching empty segments."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        matched_buffer = tuple()
        forward_buffer = segments
        while True:
            if len(forward_buffer) == 0:
                # We're all good
                return MatchResult.from_matched(matched_buffer)
            elif self.code_only and not forward_buffer[0].is_code:
                matched_buffer += forward_buffer[0],
                forward_buffer = forward_buffer[1:]
            else:
                # Try and match it
                for opt in self._elements:
                    if isinstance(opt, str):
                        if forward_buffer[0].type == opt:
                            matched_buffer += forward_buffer[0],
                            forward_buffer = forward_buffer[1:]
                            break
                    else:
                        m = opt._match(
                            forward_buffer, match_depth=match_depth + 1, parse_depth=parse_depth,
                            verbosity=verbosity, dialect=dialect, match_segment=match_segment)
                        if m:
                            matched_buffer += m.matched_segments
                            forward_buffer = m.unmatched_segments
                            break
                else:
                    # Unable to match the forward buffer. We must have found something
                    # which isn't on our element list. Crash out.
                    return MatchResult.from_unmatched(segments)","1. Use `type()` to check if the element is a code before matching it.
2. Use `isinstance()` to check if the element is an instance of a class before matching it.
3. Use `MatchResult.from_unmatched()` to return an unmatched result if the element is not on the element list."
"    def __init__(self, target, *args, **kwargs):
        self.target = target
        self.terminator = kwargs.pop('terminator', None)
        # The details on how to match a bracket are stored in the dialect
        self.start_bracket = Ref('StartBracketSegment')
        self.end_bracket = Ref('EndBracketSegment')
        super(StartsWith, self).__init__(*args, **kwargs)","1. Use `self.target` instead of `target` to avoid accidental modification of the argument.
2. Use `self.terminator` instead of `kwargs.pop('terminator', None)` to avoid typos.
3. Use `Ref('StartBracketSegment')` instead of `self.start_bracket = Ref('StartBracketSegment')` to avoid creating a reference to `self.start_bracket`."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        if self.code_only:
            first_code_idx = None
            # Work through to find the first code segment...
            for idx, seg in enumerate(segments):
                if seg.is_code:
                    first_code_idx = idx
                    break
            else:
                # We've trying to match on a sequence of segments which contain no code.
                # That means this isn't a match.
                return MatchResult.from_unmatched(segments)

            match = self.target._match(
                segments=segments[first_code_idx:], match_depth=match_depth + 1,
                parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                match_segment=match_segment)
            if match:
                # The match will probably have returned a mutated version rather
                # that the raw segment sent for matching. We need to reinsert it
                # back into the sequence in place of the raw one, but we can't
                # just assign at the index because it's a tuple and not a list.
                # to get around that we do this slightly more elaborate construction.

                # NB: This match may be partial or full, either is cool. In the case
                # of a partial match, given that we're only interested in what it STARTS
                # with, then we can still used the unmatched parts on the end.
                # We still need to deal with any non-code segments at the start.
                if self.terminator:
                    # We have an optional terminator. We should only match up to when
                    # this matches. This should also respect bracket counting.
                    match_segments = match.matched_segments
                    trailing_segments = match.unmatched_segments

                    # Given a set of segments, iterate through looking for
                    # a terminator.
                    term_match = self.bracket_sensitive_forward_match(
                        segments=trailing_segments,
                        start_bracket=self.start_bracket,
                        end_bracket=self.end_bracket,
                        match_depth=match_depth,
                        parse_depth=parse_depth,
                        verbosity=verbosity,
                        terminator=self.terminator,
                        dialect=dialect,
                        match_segment=match_segment
                    )
                    return MatchResult(
                        segments[:first_code_idx]
                        + match_segments
                        + term_match.matched_segments,
                        term_match.unmatched_segments,
                    )
                else:
                    return MatchResult.from_matched(
                        segments[:first_code_idx]
                        + match.matched_segments
                        + match.unmatched_segments)
            else:
                return MatchResult.from_unmatched(segments)
        else:
            raise NotImplementedError(""Not expecting to match StartsWith and also not just code!?"")","1. Use `match.matched_segments` instead of `match.segments` to avoid leaking information about unmatched segments.
2. Use `match_segment` to avoid mutating the original segments.
3. Use `bracket_sensitive_forward_match` to correctly handle brackets."
"    def match(self, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """""" The match function for `bracketed` implements bracket counting. """"""

        # 1. work forwards to find the first bracket.
        #    If we find something other that whitespace, then fail out.
        # 2. Once we have the first bracket, we need to bracket count forward to find it's partner.
        # 3. Assuming we find it's partner then we try and match what goes between them.
        #    If we match, great. If not, then we return an empty match.
        #    If we never find it's partner then we return an empty match but should probably
        #    log a parsing warning, or error?

        sub_bracket_count = 0
        pre_content_segments = tuple()
        unmatched_segs = segments
        matched_segs = tuple()
        current_bracket_segment = None

        # Step 1. Find the first useful segment
        # Work through to find the first code segment...
        if self.code_only:
            for idx, seg in enumerate(segments):
                if seg.is_code:
                    break
                else:
                    matched_segs += seg,
                    unmatched_segs = unmatched_segs[1:]
            else:
                # We've trying to match on a sequence of segments which contain no code.
                # That means this isn't a match.
                return MatchResult.from_unmatched(segments)

        # is it a bracket?
        m = self.start_bracket._match(
            segments=unmatched_segs, match_depth=match_depth + 1,
            parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
            match_segment=match_segment)

        if m.has_match():
            # We've got the first bracket.
            # Update the seg_idx by the length of the match
            current_bracket_segment = m.matched_segments[0]
            # No indexing to allow mutation
            matched_segs += m.matched_segments
            unmatched_segs = m.unmatched_segments
        else:
            # Whatever we have, it doesn't start with a bracket.
            return MatchResult.from_unmatched(segments)

        # Step 2: Bracket count forward to find it's pair
        content_segments = tuple()
        pre_content_segments = matched_segs

        while True:
            # Are we at the end of the sequence?
            if len(unmatched_segs) == 0:
                # We've got to the end without finding the closing bracket
                # this isn't just parsing issue this is probably a syntax
                # error.
                # TODO: Format this better
                raise SQLParseError(
                    ""Couldn't find closing bracket for opening bracket."",
                    segment=current_bracket_segment)

            # Is it a closing bracket?
            m = self.end_bracket._match(
                segments=unmatched_segs, match_depth=match_depth + 1,
                parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                match_segment=match_segment)
            if m.has_match():
                if sub_bracket_count == 0:
                    # We're back to the bracket pair!
                    matched_segs += m.matched_segments
                    unmatched_segs = m.unmatched_segments
                    closing_bracket_segs = m.matched_segments
                    break
                else:
                    # reduce the bracket count and then advance the counter.
                    sub_bracket_count -= 1
                    matched_segs += m.matched_segments
                    unmatched_segs = m.unmatched_segments
                    continue

            # Is it an opening bracket?
            m = self.start_bracket._match(
                segments=unmatched_segs, match_depth=match_depth + 1,
                parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                match_segment=match_segment)
            if m.has_match():
                # increment the open bracket counter and proceed
                sub_bracket_count += 1
                matched_segs += m.matched_segments
                unmatched_segs = m.unmatched_segments
                continue

            # If we get here it's not an opening bracket or a closing bracket
            # so we should carry on our merry way
            matched_segs += unmatched_segs[0],
            content_segments += unmatched_segs[0],
            unmatched_segs = unmatched_segs[1:]

        # If we get to here then we've found our closing bracket.
        # Let's identify the section to match for our content matchers
        # and then try it against each of them.

        for elem in self._elements:
            elem_match = elem._match(
                content_segments, match_depth=match_depth + 1,
                parse_depth=parse_depth, verbosity=verbosity, dialect=dialect,
                match_segment=match_segment)
            # Matches at this stage must be complete, because we've got nothing
            # to do with any leftovers within the brackets.
            if elem_match.is_complete():
                # We're also returning the *mutated* versions from the sub-matcher
                return MatchResult(
                    pre_content_segments
                    + elem_match.matched_segments
                    + closing_bracket_segs,
                    unmatched_segs)
            else:
                # Not matched this element, move on.
                # NB, a partial match here isn't helpful. We're matching
                # BETWEEN two delimiters and so it must be a complete match.
                # Incomplete matches are only possible at the end
                continue
        else:
            # If we're here we haven't matched any of the elements, then we have a problem
            return MatchResult.from_unmatched(segments)","1. Use `isinstance()` to check if a segment is a code segment.
2. Use `MatchResult.from_unmatched()` to return an empty match if no match is found.
3. Use `SQLParseError()` to raise an error if the closing bracket is not found."
"    def __init__(self, config=None):
        self.config = config or default_config
        self.matcher = RepeatedMultiMatcher(
            RegexMatcher.from_shorthand(""whitespace"", r""[\\t ]*""),
            RegexMatcher.from_shorthand(""inline_comment"", r""(-- |#)[^\\n]*"", is_comment=True),
            RegexMatcher.from_shorthand(""block_comment"", r""\\/\\*([^\\*]|\\*[^\\/])*\\*\\/"", is_comment=True),
            RegexMatcher.from_shorthand(""single_quote"", r""'[^']*'"", is_code=True),
            RegexMatcher.from_shorthand(""double_quote"", r'""[^""]*""', is_code=True),
            RegexMatcher.from_shorthand(""back_quote"", r""`[^`]*`"", is_code=True),
            RegexMatcher.from_shorthand(""numeric_literal"", r""(-?[0-9]+(\\.[0-9]+)?)"", is_code=True),
            RegexMatcher.from_shorthand(""greater_than_or_equal"", r"">="", is_code=True),
            RegexMatcher.from_shorthand(""less_than_or_equal"", r""<="", is_code=True),
            RegexMatcher.from_shorthand(""newline"", r""\\r\\n""),
            RegexMatcher.from_shorthand(""casting_operator"", r""::""),
            RegexMatcher.from_shorthand(""not_equals"", r""!=""),
            SingletonMatcher.from_shorthand(""newline"", ""\\n""),
            SingletonMatcher.from_shorthand(""equals"", ""="", is_code=True),
            SingletonMatcher.from_shorthand(""greater_than"", "">"", is_code=True),
            SingletonMatcher.from_shorthand(""less_than"", ""<"", is_code=True),
            SingletonMatcher.from_shorthand(""dot"", ""."", is_code=True),
            SingletonMatcher.from_shorthand(""comma"", "","", is_code=True),
            SingletonMatcher.from_shorthand(""plus"", ""+"", is_code=True),
            SingletonMatcher.from_shorthand(""minus"", ""-"", is_code=True),
            SingletonMatcher.from_shorthand(""divide"", ""/"", is_code=True),
            SingletonMatcher.from_shorthand(""star"", ""*"", is_code=True),
            SingletonMatcher.from_shorthand(""bracket_open"", ""("", is_code=True),
            SingletonMatcher.from_shorthand(""bracket_close"", "")"", is_code=True),
            SingletonMatcher.from_shorthand(""semicolon"", "";"", is_code=True),
            RegexMatcher.from_shorthand(""code"", r""[0-9a-zA-Z_]*"", is_code=True)
        )","1. Use `prepared statements` to prevent SQL injection.
2. Use `user-defined functions` to prevent code injection.
3. Use `secure hashing algorithms` to protect passwords."
"def parse_match_logging(parse_depth, match_depth, match_segment, grammar,
                        func, msg, verbosity, v_level, **kwargs):
    s = ""[PD:{0} MD:{1}]\\t{2:<50}\\t{3:<20}"".format(
        parse_depth, match_depth, ('.' * match_depth) + str(match_segment),
        ""{0}.{1} {2}"".format(grammar, func, msg)
    )
    if kwargs:
        s += ""\\t[{0}]"".format(
            ', '.join([""{0}={1}"".format(
                k, repr(v) if isinstance(v, str) else v) for k, v in kwargs.items()])
        )
    verbosity_logger(s, verbosity, v_level=v_level)","1. Use `functools.wraps` to preserve the function signature of the decorated function.
2. Use `inspect.getfullargspec` to get the full argument spec of the decorated function.
3. Use `inspect.ismethod` to check if the decorated function is a method."
"    def parse(self, recurse=True, parse_depth=0, verbosity=0, dialect=None):
        """""" Use the parse kwarg for testing, mostly to check how deep to go.
        True/False for yes or no, an integer allows a certain number of levels """"""

        # We should call the parse grammar on this segment, which calls
        # the match grammar on all it's children.

        if not dialect:
            raise RuntimeError(""No dialect provided to {0!r}!"".format(self))

        # the parse_depth and recurse kwargs control how deep we will recurse for testing.
        if not self.segments:
            # This means we're a root segment, just return an unmutated self
            return self

        # Get the Parse Grammar
        g = self._parse_grammar()
        if g is None:
            logging.debug(""{0}.parse: no grammar. returning"".format(self.__class__.__name__))
            return self
        # Use the Parse Grammar (and the private method)
        # NOTE: No match_depth kwarg, because this is the start of the matching.
        m = g._match(
            segments=self.segments, parse_depth=parse_depth, verbosity=verbosity,
            dialect=dialect, match_segment=self.__class__.__name__)

        # Calling unify here, allows the MatchResult class to do all the type checking.
        try:
            m = MatchResult.unify(m)
        except TypeError as err:
            logging.error(
                ""[PD:{0}] {1}.parse. Error on unifying result of match grammar!"".format(
                    parse_depth, self.__class__.__name__))
            raise err

        # Basic Validation, that we haven't dropped anything.
        check_still_complete(self.segments, m.matched_segments, m.unmatched_segments)

        if m.has_match():
            if m.is_complete():
                # Complete match, happy days!
                self.segments = m.matched_segments
            else:
                # Incomplete match.
                # For now this means the parsing has failed. Lets add the unmatched bit at the
                # end as something unparsable.
                # TODO: Do something more intelligent here.
                self.segments = m.matched_segments + (UnparsableSegment(
                    segments=m.unmatched_segments, expected=""Nothing...""),)
        else:
            # If there's no match at this stage, then it's unparsable. That's
            # a problem at this stage so wrap it in an unparable segment and carry on.
            self.segments = UnparsableSegment(segments=self.segments, expected=g.expected_string(dialect=dialect)),  # NB: tuple

        # Validate new segments
        self.validate_segments(text=""parsing"")

        # Recurse if allowed (using the expand method to deal with the expansion)
        logging.debug(
            ""{0}.parse: Done Parse. Plotting Recursion. Recurse={1!r}"".format(
                self.__class__.__name__, recurse))
        parse_depth_msg = ""###\\n#\\n# Beginning Parse Depth {0}: {1}\\n#\\n###\\nInitial Structure:\\n{2}"".format(
            parse_depth + 1, self.__class__.__name__, self.stringify())
        if recurse is True:
            logging.debug(parse_depth_msg)
            self.segments = self.expand(self.segments, recurse=True, parse_depth=parse_depth + 1,
                                        verbosity=verbosity, dialect=dialect)
        elif isinstance(recurse, int):
            if recurse > 1:
                logging.debug(parse_depth_msg)
                self.segments = self.expand(self.segments, recurse=recurse - 1, parse_depth=parse_depth + 1,
                                            verbosity=verbosity, dialect=dialect)

        # Validate new segments
        self.validate_segments(text=""expanding"")

        return self","1. Use `validate_segments()` to check if the segments are valid.
2. Use `MatchResult.unify()` to unify the match result.
3. Use `check_still_complete()` to check if the segments are still complete."
"    def match(cls, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """"""
            Matching can be done from either the raw or the segments.
            This raw function can be overridden, or a grammar defined
            on the underlying class.
        """"""
        if cls._match_grammar():
            # Call the private method
            m = cls._match_grammar()._match(segments=segments, match_depth=match_depth + 1,
                                            parse_depth=parse_depth, verbosity=verbosity,
                                            dialect=dialect, match_segment=match_segment)

            # Calling unify here, allows the MatchResult class to do all the type checking.
            try:
                m = MatchResult.unify(m)
            except TypeError as err:
                logging.error(
                    ""[PD:{0} MD:{1}] {2}.match. Error on unifying result of match grammar!"".format(
                        parse_depth, match_depth, cls.__name__))
                raise err

            # Once unified we can deal with it just as a MatchResult
            if m.has_match():
                return MatchResult((cls(segments=m.matched_segments),), m.unmatched_segments)
            else:
                return MatchResult.from_unmatched(segments)
        else:
            raise NotImplementedError(""{0} has no match function implemented"".format(cls.__name__))","1. Use `type()` to check the type of arguments passed to the function.
2. Use `logging.exception()` to log errors.
3. Use `MatchResult.from_unmatched()` to return a MatchResult object when there is no match."
"    def _match(cls, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """""" A wrapper on the match function to do some basic validation and logging """"""
        parse_match_logging(
            parse_depth, match_depth, match_segment, cls.__name__,
            '_match', 'IN', verbosity=verbosity, v_level=4, ls=len(segments))

        if isinstance(segments, BaseSegment):
            segments = segments,  # Make into a tuple for compatability

        if not isinstance(segments, tuple):
            logging.warning(
                ""{0}.match, was passed {1} rather than tuple or segment"".format(
                    cls.__name__, type(segments)))
            if isinstance(segments, list):
                # Let's make it a tuple for compatibility
                segments = tuple(segments)

        if len(segments) == 0:
            logging.info(""{0}._match, was passed zero length segments list"".format(cls.__name__))

        m = cls.match(segments, match_depth=match_depth, parse_depth=parse_depth,
                      verbosity=verbosity, dialect=dialect, match_segment=match_segment)

        if not isinstance(m, tuple) and m is not None:
            logging.warning(
                ""{0}.match, returned {1} rather than tuple"".format(
                    cls.__name__, type(m)))

        parse_match_logging(
            parse_depth, match_depth, match_segment, cls.__name__,
            '_match', 'OUT', verbosity=verbosity, v_level=4, m=m)
        # Basic Validation
        check_still_complete(segments, m.matched_segments, m.unmatched_segments)
        return m","1. Use `type()` to check if the input is a tuple or BaseSegment.
2. Use `logging.warning()` to log warnings when the input is invalid.
3. Use `check_still_complete()` to check if the segments are still complete after matching."
"    def expand(segments, recurse=True, parse_depth=0, verbosity=0, dialect=None):
        segs = tuple()
        for stmt in segments:
            try:
                if not stmt.is_expandable:
                    logging.info(""[PD:{0}] Skipping expansion of {1}..."".format(parse_depth, stmt))
                    segs += stmt,
                    continue
            except Exception as err:
                # raise ValueError(""{0} has no attribute `is_expandable`. This segment appears poorly constructed."".format(stmt))
                logging.error(""{0} has no attribute `is_expandable`. This segment appears poorly constructed."".format(stmt))
                raise err
            if not hasattr(stmt, 'parse'):
                raise ValueError(""{0} has no method `parse`. This segment appears poorly constructed."".format(stmt))
            parse_depth_msg = ""Parse Depth {0}. Expanding: {1}: {2!r}"".format(
                parse_depth, stmt.__class__.__name__,
                curtail_string(stmt.raw, length=40))
            verbosity_logger(frame_msg(parse_depth_msg), verbosity=verbosity)
            res = stmt.parse(recurse=recurse, parse_depth=parse_depth, verbosity=verbosity, dialect=dialect)
            if isinstance(res, BaseSegment):
                segs += (res,)
            else:
                # We might get back an iterable of segments
                segs += tuple(res)
        # Basic Validation
        check_still_complete(segments, segs, tuple())
        return segs","1. Use `assert` statements to verify that the input data is valid.
2. Sanitize user input to prevent against injection attacks.
3. Use a secure password hashing function to protect user passwords."
"    def match(cls, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """""" Keyword implements it's own matching function """"""
        # If we've been passed the singular, make it a list
        if isinstance(segments, BaseSegment):
            segments = [segments]

        # We're only going to match against the first element
        if len(segments) >= 1:
            raw = segments[0].raw
            pos = segments[0].pos_marker
            if cls._case_sensitive:
                raw_comp = raw
            else:
                raw_comp = raw.upper()
            logging.debug(""[PD:{0} MD:{1}] (KW) {2}.match considering {3!r} against {4!r}"".format(
                parse_depth, match_depth, cls.__name__, raw_comp, cls._template))
            if cls._template == raw_comp:
                m = cls(raw=raw, pos_marker=pos),  # Return as a tuple
                return MatchResult(m, segments[1:])
        else:
            logging.debug(""{1} will not match sequence of length {0}"".format(len(segments), cls.__name__))
        return MatchResult.from_unmatched(segments)","1. Use `type()` to check if the argument is a `BaseSegment` object.
2. Use `logging.critical()` to log errors.
3. Use `return MatchResult.from_unmatched(segments)` to return an error message if the match fails."
"    def match(cls, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """""" ReSegment implements it's own matching function,
        we assume that ._template is a r"""" string, and is formatted
        for use directly as a regex. This only matches on a single segment.""""""
        # If we've been passed the singular, make it a list
        if isinstance(segments, BaseSegment):
            segments = [segments]
        # Regardless of what we're passed, make a string.
        # NB: We only match on the first element of a set of segments.
        s = segments[0].raw
        # Deal with case sentitivity
        if not cls._case_sensitive:
            sc = s.upper()
        else:
            sc = s
        if len(s) == 0:
            raise ValueError(""Zero length string passed to ReSegment!?"")
        logging.debug(""[PD:{0} MD:{1}] (RE) {2}.match considering {3!r} against {4!r}"".format(
            parse_depth, match_depth, cls.__name__, sc, cls._template))
        # Try the regex
        result = re.match(cls._template, sc)
        if result:
            r = result.group(0)
            # Check that we've fully matched
            if r == sc:
                m = cls(raw=s, pos_marker=segments[0].pos_marker),  # Return a tuple
                return MatchResult(m, segments[1:])
        return MatchResult.from_unmatched(segments)","1. Use `re.compile()` to compile the regular expression once, and then use the compiled object to match strings. This will improve performance and prevent against malicious regular expressions.
2. Use `re.IGNORECASE` to make the regular expression case-insensitive. This will make the code more robust against typos.
3. Use `re.DOTALL` to match the `.` character to include newlines. This will make the code more robust against input that contains newlines."
"    def match(cls, segments, match_depth=0, parse_depth=0, verbosity=0, dialect=None, match_segment=None):
        """""" NamedSegment implements it's own matching function,
        we assume that ._template is the `name` of a segment""""""
        # If we've been passed the singular, make it a list
        if isinstance(segments, BaseSegment):
            segments = [segments]

        # We only match on the first element of a set of segments
        if len(segments) >= 1:
            s = segments[0]
            if not cls._case_sensitive:
                n = s.name.upper()
            else:
                n = s.name
            logging.debug(""[PD:{0} MD:{1}] (KW) {2}.match considering {3!r} against {4!r}"".format(
                parse_depth, match_depth, cls.__name__, n, cls._template))
            if cls._template == n:
                m = cls(raw=s.raw, pos_marker=segments[0].pos_marker),  # Return a tuple
                return MatchResult(m, segments[1:])
        else:
            logging.debug(""{1} will not match sequence of length {0}"".format(len(segments), cls.__name__))
        return MatchResult.from_unmatched(segments)","1. Use `type()` to check if the input is a `BaseSegment` object.
2. Use `logging.critical()` to log errors.
3. Use `return` to exit the function early when a condition is not met."
"    def expected_string(cls, dialect=None, called_from=None):
        return ""["" + cls._template + ""]""","1. **Use prepared statements** to prevent SQL injection attacks.
2. **Escape all user input** before using it in SQL queries.
3. **Sanitize all user input** before displaying it to the user."
"def ls(long: bool, dropbox_path: str, include_deleted: bool, config_name: str) -> None:

    from datetime import datetime
    from .utils import natural_size

    if not dropbox_path.startswith(""/""):
        dropbox_path = ""/"" + dropbox_path

    with MaestralProxy(config_name, fallback=True) as m:

        entries = m.list_folder(
            dropbox_path, recursive=False, include_deleted=include_deleted
        )
        entries.sort(key=lambda x: cast(str, x[""name""]).lower())

    if long:

        names = []
        types = []
        sizes = []
        shared = []
        last_modified = []
        excluded = []

        to_short_type = {
            ""FileMetadata"": ""file"",
            ""FolderMetadata"": ""folder"",
            ""DeletedMetadata"": ""deleted"",
        }

        for e in entries:

            long_type = cast(str, e[""type""])
            name = cast(str, e[""name""])
            path_lower = cast(str, e[""path_lower""])

            types.append(to_short_type[long_type])
            names.append(name)

            shared.append(""shared"" if ""sharing_info"" in e else ""private"")
            excluded.append(m.excluded_status(path_lower))

            if ""size"" in e:
                size = cast(float, e[""size""])
                sizes.append(natural_size(size))
            else:
                sizes.append(""-"")

            if ""client_modified"" in e:
                cm = cast(str, e[""client_modified""])
                dt = datetime.strptime(cm, ""%Y-%m-%dT%H:%M:%S%z"").astimezone()
                last_modified.append(dt.strftime(""%d %b %Y %H:%M""))
            else:
                last_modified.append(""-"")

        click.echo("""")
        click.echo(
            format_table(
                headers=[""Name"", ""Type"", ""Size"", ""Shared"", ""Syncing"", ""Last modified""],
                columns=[names, types, sizes, shared, excluded, last_modified],
                alignment=[LEFT, LEFT, RIGHT, LEFT, LEFT, LEFT],
                wrap=False,
            ),
        )
        click.echo("""")

    else:

        from .utils import chunks

        names = []
        colors = []
        formatted_names = []
        max_len = 0

        for e in entries:
            name = cast(str, e[""name""])

            max_len = max(max_len, len(name))
            names.append(name)
            colors.append(""blue"" if e[""type""] == ""DeletedMetadata"" else None)

        max_len += 2  # add 2 spaces padding

        for name, color in zip(names, colors):
            formatted_names.append(click.style(name.ljust(max_len), fg=color))

        width, height = click.get_terminal_size()
        n_columns = max(width // max_len, 1)

        rows = chunks(formatted_names, n_columns)

        for row in rows:
            click.echo("""".join(row))","1. **Use proper casing**. The code is not using proper casing for variable names and function names. This can make it difficult to read and understand the code, and it can also lead to errors.
2. **Use secure functions**. The code is using insecure functions such as `strptime` and `astimezone`. These functions can be vulnerable to attacks, so it is important to use secure alternatives.
3. **Sanitize user input**. The code is not sanitizing user input. This can allow attackers to inject malicious code into the system, which can lead to security breaches."
"def check_for_updates():
    """"""
    Checks if updates are available by reading the cached release number from the
    config file and notifies the user. Prints an update note to the command line.
    """"""
    from packaging.version import Version
    from maestral import __version__

    state = MaestralState('maestral')
    latest_release = state.get('app', 'latest_release')

    has_update = Version(__version__) < Version(latest_release)

    if has_update:
        click.secho(
            f'Maestral v{latest_release} has been released, you have v{__version__}. '
            f'Please use your package manager to update.', fg='orange'
        )","1. Use `click.echo` instead of `click.secho` to avoid printing sensitive information in the terminal.
2. Use `state.get('app', 'latest_release', None)` to check if the latest release is set, and only print a message if it is.
3. Use `Version.parse(latest_release)` to parse the latest release version string, and compare it to the current version using `Version.__le__`."
"def catch_sync_issues(func):
    """"""
    Decorator that catches all SyncErrors and logs them.
    """"""

    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            res = func(self, *args, **kwargs)
            if res is None:
                res = True
        except SyncError as exc:
            file_name = os.path.basename(exc.dbx_path)
            logger.warning('Could not sync %s', file_name, exc_info=True)
            if exc.dbx_path is not None:
                if exc.local_path is None:
                    exc.local_path = self.to_local_path(exc.dbx_path)
                self.sync_errors.put(exc)
                if any(isinstance(a, Metadata) for a in args):
                    self.download_errors.add(exc.dbx_path)

            res = False

        return res

    return wrapper","1. Use `functools.wraps` to preserve the metadata of the decorated function.
2. Add a `logger.exception` to log the stack trace of the error.
3. Add a `self.sync_errors.put(exc)` to add the error to a list of errors."
"    def wrapper(self, *args, **kwargs):
        try:
            res = func(self, *args, **kwargs)
            if res is None:
                res = True
        except SyncError as exc:
            file_name = os.path.basename(exc.dbx_path)
            logger.warning('Could not sync %s', file_name, exc_info=True)
            if exc.dbx_path is not None:
                if exc.local_path is None:
                    exc.local_path = self.to_local_path(exc.dbx_path)
                self.sync_errors.put(exc)
                if any(isinstance(a, Metadata) for a in args):
                    self.download_errors.add(exc.dbx_path)

            res = False

        return res","1. Use `try-except` to catch errors and log them.
2. Set `exc.local_path` if it is `None` to avoid undefined variable errors.
3. Use `any()` to check if any of the arguments is an instance of `Metadata` to avoid `KeyError`."
"    def _handle_case_conflict(self, event):
        """"""
        Checks for other items in the same directory with same name but a different
        case. Renames items if necessary.Only needed for case sensitive file systems.

        :param FileSystemEvent event: Created or moved event.
        :returns: ``True`` or ``False``.
        :rtype: bool
        """"""

        if not IS_FS_CASE_SENSITIVE:
            return False

        if event.event_type not in (EVENT_TYPE_CREATED, EVENT_TYPE_MOVED):
            return False

        # get the created path (src_path or dest_path)
        dest_path = get_dest_path(event)
        dirname, basename = osp.split(dest_path)

        # check number of paths with the same case
        if len(path_exists_case_insensitive(basename, root=dirname)) > 1:

            dest_path_cc = generate_cc_name(dest_path, suffix='case conflict')
            with self.fs_events.ignore(dest_path, recursive=osp.isdir(dest_path),
                                       event_types=(EVENT_TYPE_DELETED,
                                                    EVENT_TYPE_MOVED)):
                exc = move(dest_path, dest_path_cc)
                if exc:
                    raise os_to_maestral_error(exc, local_path=dest_path_cc)

            logger.info('Case conflict: renamed ""%s"" to ""%s""', dest_path, dest_path_cc)

            return True
        else:
            return False","1. Use `pathlib` instead of `os.path` to avoid path traversal vulnerabilities.
2. Use `os.makedirs` with the `exist_ok` flag to avoid creating directories that already exist.
3. Use `shutil.move` instead of `os.rename` to avoid race conditions."
"    def __init__(self, run=True):

        self.client = MaestralApiClient()

        # periodically check for updates and refresh account info
        self.update_thread = Thread(
            name=""Maestral update check"",
            target=self._periodic_refresh,
            daemon=True,
        )
        self.update_thread.start()

        # monitor needs to be created before any decorators are called
        self.monitor = MaestralMonitor(self.client)
        self.sync = self.monitor.sync

        if NOTIFY_SOCKET and system_notifier:
            # notify systemd that we have successfully started
            system_notifier.notify(""READY=1"")

        if WATCHDOG_USEC and int(WATCHDOG_PID) == os.getpid() and system_notifier:
            # notify systemd periodically that we are still alive
            self.watchdog_thread = Thread(
                name=""Maestral watchdog"",
                target=self._periodic_watchdog,
                daemon=True,
            )
            self.update_thread.start()

        if run:
            # if `run == False`, make sure that you manually run the setup
            # before calling `start_sync`
            if self.pending_dropbox_folder():
                self.create_dropbox_directory()
                self.set_excluded_folders()

                self.sync.last_cursor = """"
                self.sync.last_sync = 0

            self.start_sync()","1. Use `threading.Lock` to protect shared state between threads.
2. Use `os.fchmod` to set the file mode to 0o600 for files created by the daemon.
3. Use `pwd.getpwuid` to get the user name of the current process and use it as the owner of the files created by the daemon."
"    def __str__(self):
        return ""{0}: {1}"".format(self.title, self.message)","1. Use `html.escape()` to escape HTML characters in the message.
2. Use `assert isinstance(title, str)` and `assert isinstance(message, str)` to assert that the title and message are strings.
3. Use `logging.warning()` instead of `print()` to log warnings."
"    def bulk_history_create(self, objs, batch_size=None):
        """"""Bulk create the history for the objects specified by objs""""""

        historical_instances = [
            self.model(
                history_date=getattr(instance, '_history_date', now()),
                history_user=getattr(instance, '_history_user', None),
                **{
                    field.attname: getattr(instance, field.attname)
                    for field in instance._meta.fields
                }
            ) for instance in objs]

        return self.model.objects.bulk_create(historical_instances,
                                              batch_size=batch_size)","1. Use `django.utils.timezone.now()` instead of `now()` to get the current date and time.
2. Use `django.contrib.auth.models.User.objects.get_or_create()` to get the user who created the object.
3. Use `django.db.models.F()` to update the history date and user fields."
"    def create_history_model(self, model, inherited):
        """"""
        Creates a historical model to associate with the model provided.
        """"""
        attrs = {'__module__': self.module}

        app_module = '%s.models' % model._meta.app_label

        if inherited:
            # inherited use models module
            attrs['__module__'] = model.__module__
        elif model.__module__ != self.module:
            # registered under different app
            attrs['__module__'] = self.module
        elif app_module != self.module:
            # Abuse an internal API because the app registry is loading.
            app = apps.app_configs[model._meta.app_label]
            models_module = app.name
            attrs['__module__'] = models_module

        fields = self.copy_fields(model)
        attrs.update(fields)
        attrs.update(self.get_extra_fields(model, fields))
        # type in python2 wants str as a first argument
        attrs.update(Meta=type(str('Meta'), (), self.get_meta_options(model)))
        if self.table_name is not None:
            attrs['Meta'].db_table = self.table_name
        name = 'Historical%s' % model._meta.object_name
        registered_models[model._meta.db_table] = model
        return python_2_unicode_compatible(
            type(str(name), self.bases, attrs))","1. Use `django.utils.safestring.mark_safe()` to escape HTML strings.
2. Use `django.contrib.auth.models.User` as the default user model.
3. Use `django.contrib.auth.models.Group` to manage user permissions."
"    def get_extra_fields(self, model, fields):
        """"""Return dict of extra fields added to the historical record model""""""

        user_model = getattr(settings, 'AUTH_USER_MODEL', 'auth.User')

        def revert_url(self):
            """"""URL for this change in the default admin site.""""""
            opts = model._meta
            app_label, model_name = opts.app_label, opts.model_name
            return reverse(
                '%s:%s_%s_simple_history' % (
                    admin.site.name,
                    app_label,
                    model_name
                ),
                args=[getattr(self, opts.pk.attname), self.history_id]
            )

        def get_instance(self):
            return model(**{
                field.attname: getattr(self, field.attname)
                for field in fields.values()
            })

        def get_next_record(self):
            """"""
            Get the next history record for the instance. `None` if last.
            """"""
            return self.instance.history.filter(
                Q(history_date__gt=self.history_date)
            ).order_by('history_date').first()

        def get_prev_record(self):
            """"""
            Get the previous history record for the instance. `None` if first.
            """"""
            return self.instance.history.filter(
                Q(history_date__lt=self.history_date)
            ).order_by('history_date').last()

        if self.history_id_field:
            history_id_field = self.history_id_field
            history_id_field.primary_key = True
            history_id_field.editable = False
        elif getattr(settings, 'SIMPLE_HISTORY_HISTORY_ID_USE_UUID', False):
            history_id_field = models.UUIDField(
                primary_key=True, default=uuid.uuid4, editable=False
            )
        else:
            history_id_field = models.AutoField(primary_key=True)

        if self.history_change_reason_field:
            # User specific field from init
            history_change_reason_field = self.history_change_reason_field
        elif getattr(
            settings, 'SIMPLE_HISTORY_HISTORY_CHANGE_REASON_USE_TEXT_FIELD',
            False
        ):
            # Use text field with no max length, not enforced by DB anyways
            history_change_reason_field = models.TextField(null=True)
        else:
            # Current default, with max length
            history_change_reason_field = models.CharField(
                max_length=100, null=True
            )

        return {
            'history_id': history_id_field,
            'history_date': models.DateTimeField(),
            'history_change_reason': history_change_reason_field,
            'history_user': models.ForeignKey(
                user_model, null=True, related_name=self.user_related_name,
                on_delete=models.SET_NULL),
            'history_type': models.CharField(max_length=1, choices=(
                ('+', _('Created')),
                ('~', _('Changed')),
                ('-', _('Deleted')),
            )),
            'history_object': HistoricalObjectDescriptor(
                model,
                self.fields_included(model)
            ),
            'instance': property(get_instance),
            'instance_type': model,
            'next_record': property(get_next_record),
            'prev_record': property(get_prev_record),
            'revert_url': revert_url,
            '__str__': lambda self: '%s as of %s' % (self.history_object,
                                                     self.history_date)
        }","1. Use `django.contrib.auth.models.User` as the default user model.
2. Use `models.CharField(max_length=100, null=True)` for the `history_change_reason` field.
3. Use `models.ForeignKey(user_model, null=True, related_name=self.user_related_name, on_delete=models.SET_NULL)` for the `history_user` field."
"        def get_instance(self):
            return model(**{
                field.attname: getattr(self, field.attname)
                for field in fields.values()
            })","1. Use `django.utils.six.text_type` instead of `str` to avoid casting errors.
2. Use `django.utils.six.ensure_str` to ensure that values are strings.
3. Use `django.utils.six.moves.filterfalse` to filter out falsy values."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the HomeMatic binary sensor platform.""""""
    if discovery_info is None:
        return

    devices = []
    for conf in discovery_info[ATTR_DISCOVER_DEVICES]:
        if discovery_info[ATTR_DISCOVERY_TYPE] == DISCOVER_BATTERY:
            devices.append(HMBatterySensor(conf))
        else:
            devices.append(HMBinarySensor(conf))

    add_entities(devices)","1. Use `hass.async_add_job()` to run blocking code in the background.
2. Use `asyncio.run()` to run blocking code in a coroutine.
3. Use `cryptography` to encrypt sensitive data."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the Homematic thermostat platform.""""""
    if discovery_info is None:
        return

    devices = []
    for conf in discovery_info[ATTR_DISCOVER_DEVICES]:
        new_device = HMThermostat(conf)
        devices.append(new_device)

    add_entities(devices)","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `cryptography` to generate and store secure random numbers.
3. Use `pydantic` to validate user input."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the platform.""""""
    if discovery_info is None:
        return

    devices = []
    for conf in discovery_info[ATTR_DISCOVER_DEVICES]:
        new_device = HMCover(conf)
        devices.append(new_device)

    add_entities(devices)","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `cryptography` to generate and verify secure random numbers.
3. Use `pydantic` to validate user input."
"    def _init_data_struct(self):
        """"""Generate a data dictionary (self._data) from metadata.""""""
        self._state = ""LEVEL""
        self._data.update({self._state: STATE_UNKNOWN})
        if ""LEVEL_2"" in self._hmdevice.WRITENODE:
            self._data.update({""LEVEL_2"": STATE_UNKNOWN})","1. Use `SECRET_KEY` instead of `""secret""` as the secret key for the JWTs.
2. Use `os.urandom(32)` to generate a random secret key instead of hard-coding it.
3. Use `jwt.decode()` to decode the JWTs instead of `json.loads()`."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the Homematic light platform.""""""
    if discovery_info is None:
        return

    devices = []
    for conf in discovery_info[ATTR_DISCOVER_DEVICES]:
        new_device = HMLight(conf)
        devices.append(new_device)

    add_entities(devices)","1. Use `asyncio` to avoid blocking the main thread.
2. Use `cryptography` to securely generate random numbers.
3. Use `pydantic` to validate user input."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the Homematic lock platform.""""""
    if discovery_info is None:
        return

    devices = []
    for conf in discovery_info[ATTR_DISCOVER_DEVICES]:
        devices.append(HMLock(conf))

    add_entities(devices)","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `cryptography` to securely generate random numbers.
3. Use `pydantic` to validate user input."
"    def _init_data_struct(self):
        """"""Generate the data dictionary (self._data) from metadata.""""""
        self._state = ""STATE""
        self._data.update({self._state: STATE_UNKNOWN})","1. Use a constant for the state variable instead of a string. This will prevent typos and make it easier to track changes.
2. Use a dictionary comprehension to initialize the data dictionary. This will prevent the need to manually create the dictionary and reduce the risk of errors.
3. Use a more descriptive name for the data dictionary, such as `state_data`. This will make it easier to understand the code and identify potential problems."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the HomeMatic sensor platform.""""""
    if discovery_info is None:
        return

    devices = []
    for conf in discovery_info[ATTR_DISCOVER_DEVICES]:
        new_device = HMSensor(conf)
        devices.append(new_device)

    add_entities(devices)","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `cryptography` to securely generate random numbers.
3. Use `hashlib` to securely hash passwords."
"    def _init_data_struct(self):
        """"""Generate a data dictionary (self._data) from metadata.""""""
        if self._state:
            self._data.update({self._state: STATE_UNKNOWN})
        else:
            _LOGGER.critical(""Unable to initialize sensor: %s"", self._name)","1. Use `logging.exception` instead of `logging.critical` to log errors.
2. Use `dict.get()` instead of `dict.update()` to avoid overwriting existing keys.
3. Check if the `state` attribute is set before using it in the `_init_data_struct()` method."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the HomeMatic switch platform.""""""
    if discovery_info is None:
        return

    devices = []
    for conf in discovery_info[ATTR_DISCOVER_DEVICES]:
        new_device = HMSwitch(conf)
        devices.append(new_device)

    add_entities(devices)","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `cryptography` to generate and verify secure random numbers.
3. Use `pydantic` to validate the configuration data."
"    def _init_data_struct(self):
        """"""Generate the data dictionary (self._data) from metadata.""""""
        self._state = ""STATE""
        self._data.update({self._state: STATE_UNKNOWN})

        # Need sensor values for SwitchPowermeter
        for node in self._hmdevice.SENSORNODE:
            self._data.update({node: STATE_UNKNOWN})","1. Use `SECRET_KEY` instead of `SECRET` in the code.
2. Use `os.getenv()` to get the environment variable instead of hardcoding it.
3. Use `json.dumps()` to serialize the data instead of `str()`."
"async def async_setup_entry(hass, config_entry, async_add_entities):
    """"""Set up PS4 from a config entry.""""""
    config = config_entry
    await async_setup_platform(hass, config, async_add_entities, discovery_info=None)","1. Use `asyncio.run()` instead of `asyncio.get_event_loop().run_until_complete()` to prevent the blocking of the event loop.
2. Use `asyncio.wait()` instead of `asyncio.gather()` to avoid creating a new task for each awaitable.
3. Use `asyncio.Timeout()` to prevent long-running tasks from blocking the event loop."
"async def async_setup_platform(hass, config, async_add_entities, discovery_info=None):
    """"""Set up PS4 Platform.""""""
    creds = config.data[CONF_TOKEN]
    device_list = []
    for device in config.data[""devices""]:
        host = device[CONF_HOST]
        region = device[CONF_REGION]
        name = device[CONF_NAME]
        ps4 = pyps4.Ps4Async(host, creds, device_name=DEFAULT_ALIAS)
        device_list.append(PS4Device(config, name, host, region, ps4, creds))
    async_add_entities(device_list, update_before_add=True)","1. Use secure credentials.
2. Use HTTPS.
3. Validate inputs."
"async def async_setup_entry(hass, config_entry):
    """"""Set up TPLink from a config entry.""""""
    from pyHS100 import SmartBulb, SmartPlug, SmartDeviceException

    devices = {}

    config_data = hass.data[DOMAIN].get(ATTR_CONFIG)

    # These will contain the initialized devices
    lights = hass.data[DOMAIN][CONF_LIGHT] = []
    switches = hass.data[DOMAIN][CONF_SWITCH] = []

    # If discovery is defined and not disabled, discover devices
    # If initialized from configure integrations, there's no config
    # so we default here to True
    if config_data is None or config_data[CONF_DISCOVERY]:
        devs = await _async_has_devices(hass)
        _LOGGER.info(""Discovered %s TP-Link smart home device(s)"", len(devs))
        devices.update(devs)

    def _device_for_type(host, type_):
        dev = None
        if type_ == CONF_LIGHT:
            dev = SmartBulb(host)
        elif type_ == CONF_SWITCH:
            dev = SmartPlug(host)

        return dev

    # When arriving from configure integrations, we have no config data.
    if config_data is not None:
        for type_ in [CONF_LIGHT, CONF_SWITCH]:
            for entry in config_data[type_]:
                try:
                    host = entry['host']
                    dev = _device_for_type(host, type_)
                    devices[host] = dev
                    _LOGGER.debug(""Succesfully added %s %s: %s"",
                                  type_, host, dev)
                except SmartDeviceException as ex:
                    _LOGGER.error(""Unable to initialize %s %s: %s"",
                                  type_, host, ex)

    # This is necessary to avoid I/O blocking on is_dimmable
    def _fill_device_lists():
        for dev in devices.values():
            if isinstance(dev, SmartPlug):
                if dev.is_dimmable:  # Dimmers act as lights
                    lights.append(dev)
                else:
                    switches.append(dev)
            elif isinstance(dev, SmartBulb):
                lights.append(dev)
            else:
                _LOGGER.error(""Unknown smart device type: %s"", type(dev))

    # Avoid blocking on is_dimmable
    await hass.async_add_executor_job(_fill_device_lists)

    forward_setup = hass.config_entries.async_forward_entry_setup
    if lights:
        _LOGGER.debug(""Got %s lights: %s"", len(lights), lights)
        hass.async_create_task(forward_setup(config_entry, 'light'))
    if switches:
        _LOGGER.debug(""Got %s switches: %s"", len(switches), switches)
        hass.async_create_task(forward_setup(config_entry, 'switch'))

    return True","1. Use `asyncio.run()` instead of `hass.async_add_executor_job()` to avoid blocking the event loop.
2. Use `async with await hass.config_entries.async_forward_entry_setup(config_entry, 'light')` instead of `hass.async_create_task(forward_setup(config_entry, 'light'))` to avoid race conditions.
3. Use `hass.data[DOMAIN][ATTR_CONFIG][CONF_DISCOVERY]` instead of `config_data[CONF_DISCOVERY]` to get the discovery setting from the config entry."
"    def _fill_device_lists():
        for dev in devices.values():
            if isinstance(dev, SmartPlug):
                if dev.is_dimmable:  # Dimmers act as lights
                    lights.append(dev)
                else:
                    switches.append(dev)
            elif isinstance(dev, SmartBulb):
                lights.append(dev)
            else:
                _LOGGER.error(""Unknown smart device type: %s"", type(dev))","1. Use `isinstance()` to check the type of a variable before casting it. This will help to prevent errors and security vulnerabilities.
2. Use `logging.exception()` to log errors and exceptions. This will help to troubleshoot problems and identify security vulnerabilities.
3. Use `type()` to get the type of a variable. This will help to identify the type of a variable and prevent errors."
"    def __init__(self, device, device_type, xiaomi_hub):
        """"""Initialize the Xiaomi device.""""""
        self._state = None
        self._is_available = True
        self._sid = device['sid']
        self._name = '{}_{}'.format(device_type, self._sid)
        self._type = device_type
        self._write_to_hub = xiaomi_hub.write_to_hub
        self._get_from_hub = xiaomi_hub.get_from_hub
        self._device_state_attributes = {}
        self._remove_unavailability_tracker = None
        self._xiaomi_hub = xiaomi_hub
        self.parse_data(device['data'], device['raw_data'])
        self.parse_voltage(device['data'])

        if hasattr(self, '_data_key') \\
                and self._data_key:  # pylint: disable=no-member
            self._unique_id = slugify(""{}-{}"".format(
                self._data_key,  # pylint: disable=no-member
                self._sid))
        else:
            self._unique_id = slugify(""{}-{}"".format(self._type, self._sid))","1. Use `functools.lru_cache` to cache the data from the hub.
2. Use `asyncio.Timeout` to handle timeouts when communicating with the hub.
3. Use `cryptography` to encrypt the data sent to the hub."
"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Perform the setup for Xiaomi devices.""""""
    devices = []
    for (_, gateway) in hass.data[PY_XIAOMI_GATEWAY].gateways.items():
        for device in gateway.devices['cover']:
            model = device['model']
            if model == 'curtain':
                devices.append(XiaomiGenericCover(device, ""Curtain"",
                                                  {'status': 'status',
                                                   'pos': 'curtain_level'},
                                                  gateway))
    add_entities(devices)","1. Use `hass.data[PY_XIAOMI_GATEWAY]` instead of `hass.data['xiaomi_gateway']` to access the data. This will prevent the data from being leaked if the `xiaomi_gateway` key is accidentally exposed.
2. Use `gateway.devices['cover']` instead of `gateway.devices` to access the devices. This will prevent the user from accessing devices that they do not have access to.
3. Use `XiaomiGenericCover(device, ""Curtain"", {'status': 'status', 'pos': 'curtain_level'}, gateway)` instead of `XiaomiGenericCover(device, ""Curtain"", {'status': 'status', 'pos': 'curtain_level'})` to pass the gateway object to the constructor. This will ensure that the gateway object is available when the cover is created."
"    def close_cover(self, **kwargs):
        """"""Close the cover.""""""
        self._write_to_hub(self._sid, **{self._data_key['status']: 'close'})","1. Use proper casing for function names (e.g. `close_cover()` instead of `close_cover()`).
2. Use `**kwargs` instead of `**{self._data_key['status']: 'close'}` to avoid typos.
3. Use `self._sid` instead of `self._data_key['status']` to avoid leaking sensitive information."
"    def open_cover(self, **kwargs):
        """"""Open the cover.""""""
        self._write_to_hub(self._sid, **{self._data_key['status']: 'open'})","1. Use proper authorization checks to ensure that only authorized users can open the cover.
2. Sanitize all user input to prevent malicious code from being executed.
3. Use strong encryption to protect sensitive data, such as the cover's status."
"    def stop_cover(self, **kwargs):
        """"""Stop the cover.""""""
        self._write_to_hub(self._sid, **{self._data_key['status']: 'stop'})","1. Use proper encryption for sensitive data.
2. Sanitize user input to prevent against injection attacks.
3. Use access control to restrict who can access sensitive data."
"    def set_cover_position(self, **kwargs):
        """"""Move the cover to a specific position.""""""
        position = kwargs.get(ATTR_POSITION)
        self._write_to_hub(self._sid, **{self._data_key['pos']: str(position)})","1. Use `cryptography` to generate a random secret key and use it to encrypt the data sent to the hub.
2. Use `HTTP Basic Auth` to authenticate the requests to the hub.
3. Use `HTTPS` to secure the communication between the hub and the device."
"def setup_cors(app, origins):
    """"""Setup cors.""""""
    import aiohttp_cors

    cors = aiohttp_cors.setup(app, defaults={
        host: aiohttp_cors.ResourceOptions(
            allow_headers=ALLOWED_CORS_HEADERS,
            allow_methods='*',
        ) for host in origins
    })

    def allow_cors(route, methods):
        """"""Allow cors on a route.""""""
        cors.add(route, {
            '*': aiohttp_cors.ResourceOptions(
                allow_headers=ALLOWED_CORS_HEADERS,
                allow_methods=methods,
            )
        })

    app['allow_cors'] = allow_cors

    if not origins:
        return

    async def cors_startup(app):
        """"""Initialize cors when app starts up.""""""
        cors_added = set()

        for route in list(app.router.routes()):
            if hasattr(route, 'resource'):
                route = route.resource
            if route in cors_added:
                continue
            cors.add(route)
            cors_added.add(route)

    app.on_startup.append(cors_startup)","1. Use `aiohttp_cors.setup()` to configure CORS.
2. Use `aiohttp_cors.ResourceOptions()` to specify the allowed headers and methods.
3. Use `aiohttp_cors.add()` to add CORS support to individual routes."
"    async def cors_startup(app):
        """"""Initialize cors when app starts up.""""""
        cors_added = set()

        for route in list(app.router.routes()):
            if hasattr(route, 'resource'):
                route = route.resource
            if route in cors_added:
                continue
            cors.add(route)
            cors_added.add(route)","1. Use `app.add_middleware(cors)` instead of `cors_startup` to initialize CORS.
2. Use `cors.add_resource()` to add routes to the CORS middleware.
3. Use `cors.set_origins()` to set the allowed origins."
"    def register(self, app, router):
        """"""Register the view with a router.""""""
        assert self.url is not None, 'No url set for view'
        urls = [self.url] + self.extra_urls
        routes = []

        for method in ('get', 'post', 'delete', 'put'):
            handler = getattr(self, method, None)

            if not handler:
                continue

            handler = request_handler_factory(self, handler)

            for url in urls:
                routes.append(
                    (method, router.add_route(method, url, handler))
                )

        if not self.cors_allowed:
            return

        for method, route in routes:
            app['allow_cors'](route, [method.upper()])","1. Use `functools.wraps` to preserve the original function's metadata, such as its name and docstring.
2. Use `inspect.iscoroutinefunction` to check if the decorated function is a coroutine function.
3. Use `asyncio.coroutine` to annotate the decorated function as a coroutine function."
"    def __init__(self, hass, config):
        """"""Initialize.""""""
        super().__init__()
        self._extra_arguments = config.get(CONF_FFMPEG_ARGUMENTS)
        self._ftp = None
        self._last_image = None
        self._last_url = None
        self._manager = hass.data[DATA_FFMPEG]
        self._name = config[CONF_NAME]
        self.host = config[CONF_HOST]
        self.port = config[CONF_PORT]
        self.path = config[CONF_PATH]
        self.user = config[CONF_USERNAME]
        self.passwd = config[CONF_PASSWORD]

        hass.async_add_job(self._connect_to_client)","1. Use `SECRET_KEY` instead of `config[CONF_PASSWORD]` to store the password.
2. Use `async_create_client()` instead of `async_add_job(self._connect_to_client)` to connect to the FTP server.
3. Use `async_read()` to read the image from the FTP server instead of `async_read()`."
"def _recursive_merge(pack_name, comp_name, config, conf, package):
    """"""Merge package into conf, recursively.""""""
    for key, pack_conf in package.items():
        if isinstance(pack_conf, dict):
            if not pack_conf:
                continue
            conf[key] = conf.get(key, OrderedDict())
            _recursive_merge(pack_name, comp_name, config,
                             conf=conf[key], package=pack_conf)

        elif isinstance(pack_conf, list):
            if not pack_conf:
                continue
            conf[key] = cv.ensure_list(conf.get(key))
            conf[key].extend(cv.ensure_list(pack_conf))

        else:
            if conf.get(key) is not None:
                _log_pkg_error(
                    pack_name, comp_name, config,
                    'has keys that are defined multiple times')
            else:
                conf[key] = pack_conf","1. Use `dict.get()` to check for the existence of a key before accessing it.
2. Use `cv.ensure_list()` to convert a list-like object to a list.
3. Use `logging.error()` to log errors."
"def merge_packages_config(hass, config, packages,
                          _log_pkg_error=_log_pkg_error):
    """"""Merge packages into the top-level configuration. Mutate config.""""""
    # pylint: disable=too-many-nested-blocks
    PACKAGES_CONFIG_SCHEMA(packages)
    for pack_name, pack_conf in packages.items():
        for comp_name, comp_conf in pack_conf.items():
            if comp_name == CONF_CORE:
                continue
            component = get_component(hass, comp_name)

            if component is None:
                _log_pkg_error(pack_name, comp_name, config, ""does not exist"")
                continue

            if hasattr(component, 'PLATFORM_SCHEMA'):
                if not comp_conf:
                    continue  # Ensure we dont add Falsy items to list
                config[comp_name] = cv.ensure_list(config.get(comp_name))
                config[comp_name].extend(cv.ensure_list(comp_conf))
                continue

            if hasattr(component, 'CONFIG_SCHEMA'):
                merge_type, _ = _identify_config_schema(component)

                if merge_type == 'list':
                    if not comp_conf:
                        continue  # Ensure we dont add Falsy items to list
                    config[comp_name] = cv.ensure_list(config.get(comp_name))
                    config[comp_name].extend(cv.ensure_list(comp_conf))
                    continue

                if merge_type == 'dict':
                    if comp_conf is None:
                        comp_conf = OrderedDict()

                    if not isinstance(comp_conf, dict):
                        _log_pkg_error(
                            pack_name, comp_name, config,
                            ""cannot be merged. Expected a dict."")
                        continue

                    if comp_name not in config:
                        config[comp_name] = OrderedDict()

                    if not isinstance(config[comp_name], dict):
                        _log_pkg_error(
                            pack_name, comp_name, config,
                            ""cannot be merged. Dict expected in main config."")
                        continue

                    for key, val in comp_conf.items():
                        if key in config[comp_name]:
                            _log_pkg_error(pack_name, comp_name, config,
                                           ""duplicate key '{}'"".format(key))
                            continue
                        config[comp_name][key] = val
                    continue

            # The last merge type are sections that require recursive merging
            if comp_name in config:
                _recursive_merge(pack_name, comp_name, config,
                                 conf=config[comp_name], package=comp_conf)
                continue
            config[comp_name] = comp_conf

    return config","1. Use `cv.ensure_list` to sanitize the input list.
2. Use `_recursive_merge` to recursively merge the configuration.
3. Use `_log_pkg_error` to log errors when merging the configuration."
"async def async_setup(hass, config):
    """"""Setup configured zones as well as home assistant zone if necessary.""""""
    if DOMAIN not in hass.data:
        hass.data[DOMAIN] = {}
    zone_entries = configured_zones(hass)
    for _, entry in config_per_platform(config, DOMAIN):
        name = slugify(entry[CONF_NAME])
        if name not in zone_entries:
            zone = Zone(hass, entry[CONF_NAME], entry[CONF_LATITUDE],
                        entry[CONF_LONGITUDE], entry.get(CONF_RADIUS),
                        entry.get(CONF_ICON), entry.get(CONF_PASSIVE))
            zone.entity_id = async_generate_entity_id(
                ENTITY_ID_FORMAT, entry[CONF_NAME], None, hass)
            hass.async_add_job(zone.async_update_ha_state())
            hass.data[DOMAIN][name] = zone

    if HOME_ZONE not in hass.data[DOMAIN] and HOME_ZONE not in zone_entries:
        name = hass.config.location_name
        zone = Zone(hass, name, hass.config.latitude, hass.config.longitude,
                    DEFAULT_RADIUS, ICON_HOME, False)
        zone.entity_id = ENTITY_ID_HOME
        hass.async_add_job(zone.async_update_ha_state())
        hass.data[DOMAIN][slugify(name)] = zone

    return True","1. Use `async_setup_entry` instead of `async_setup` to avoid race conditions.
2. Use `async_add_executor_job` to run tasks in a separate thread.
3. Use `async_contextmanager` to ensure that resources are closed properly."
"    def update_as_of(self, utc_point_in_time):
        """"""Calculate sun state at a point in UTC time.""""""
        mod = -1
        while True:
            next_rising_dt = self.location.sunrise(
                utc_point_in_time + timedelta(days=mod), local=False)
            if next_rising_dt > utc_point_in_time:
                break
            mod += 1

        mod = -1
        while True:
            next_setting_dt = (self.location.sunset(
                utc_point_in_time + timedelta(days=mod), local=False))
            if next_setting_dt > utc_point_in_time:
                break
            mod += 1

        self.next_rising = next_rising_dt
        self.next_setting = next_setting_dt","1. Use `utcnow()` instead of `utc_point_in_time` to avoid
                    time zone issues.
2. Use `datetime.now()` instead of `utc_point_in_time` to avoid
                    time zone issues.
3. Use `datetime.utcnow()` instead of `utc_point_in_time` to avoid
                    time zone issues."
"def detect_location_info():
    """""" Detect location information. """"""
    try:
        raw_info = requests.get(
            'https://freegeoip.net/json/', timeout=5).json()
    except requests.RequestException:
        return

    data = {key: raw_info.get(key) for key in LocationInfo._fields}

    # From Wikipedia: Fahrenheit is used in the Bahamas, Belize,
    # the Cayman Islands, Palau, and the United States and associated
    # territories of American Samoa and the U.S. Virgin Islands
    data['use_fahrenheit'] = data['country_code'] in (
        'BS', 'BZ', 'KY', 'PW', 'US', 'AS', 'VI')

    return LocationInfo(**data)","1. Use `requests.get()` with a `verify=False` parameter to avoid validating the TLS certificate of the remote server.
2. Use `requests.get()` with a `timeout` parameter to avoid waiting indefinitely for a response from the remote server.
3. Use `requests.get()` with a `headers` parameter to send the `User-Agent` header, which helps to identify the application that is sending the request."
"def setup_platform(hass, config, add_devices_callback, discovery_info=None):
    """""" Setup the RFXtrx platform. """"""
    import RFXtrx as rfxtrxmod

    lights = []
    devices = config.get('devices', None)

    if devices:
        for entity_id, entity_info in devices.items():
            if entity_id not in rfxtrx.RFX_DEVICES:
                _LOGGER.info(""Add %s rfxtrx.light"", entity_info[ATTR_NAME])

                # Check if i must fire event
                fire_event = entity_info.get(ATTR_FIREEVENT, False)
                datas = {ATTR_STATE: False, ATTR_FIREEVENT: fire_event}

                rfxobject = rfxtrx.get_rfx_object(entity_info[ATTR_PACKETID])
                new_light = RfxtrxLight(
                    entity_info[ATTR_NAME], rfxobject, datas
                )
                rfxtrx.RFX_DEVICES[entity_id] = new_light
                lights.append(new_light)

    add_devices_callback(lights)

    def light_update(event):
        """""" Callback for light updates from the RFXtrx gateway. """"""
        if not isinstance(event.device, rfxtrxmod.LightingDevice):
            return

        # Add entity if not exist and the automatic_add is True
        entity_id = slugify(event.device.id_string.lower())
        if entity_id not in rfxtrx.RFX_DEVICES:
            automatic_add = config.get('automatic_add', False)
            if not automatic_add:
                return

            _LOGGER.info(
                ""Automatic add %s rfxtrx.light (Class: %s Sub: %s)"",
                entity_id,
                event.device.__class__.__name__,
                event.device.subtype
            )
            pkt_id = """".join(""{0:02x}"".format(x) for x in event.data)
            entity_name = ""%s : %s"" % (entity_id, pkt_id)
            datas = {ATTR_STATE: False, ATTR_FIREEVENT: False}
            new_light = RfxtrxLight(entity_name, event, datas)
            rfxtrx.RFX_DEVICES[entity_id] = new_light
            add_devices_callback([new_light])

        # Check if entity exists or previously added automatically
        if entity_id in rfxtrx.RFX_DEVICES \\
                and isinstance(rfxtrx.RFX_DEVICES[entity_id], RfxtrxLight):
            _LOGGER.debug(
                ""EntityID: %s light_update. Command: %s"",
                entity_id,
                event.values['Command']
            )
            if event.values['Command'] == 'On'\\
                    or event.values['Command'] == 'Off':

                # Update the rfxtrx device state
                is_on = event.values['Command'] == 'On'
                # pylint: disable=protected-access
                rfxtrx.RFX_DEVICES[entity_id]._state = is_on
                rfxtrx.RFX_DEVICES[entity_id].update_ha_state()

                # Fire event
                if rfxtrx.RFX_DEVICES[entity_id].should_fire_event:
                    rfxtrx.RFX_DEVICES[entity_id].hass.bus.fire(
                        EVENT_BUTTON_PRESSED, {
                            ATTR_ENTITY_ID:
                                rfxtrx.RFX_DEVICES[entity_id].entity_id,
                            ATTR_STATE: event.values['Command'].lower()
                        }
                    )

    # Subscribe to main rfxtrx events
    if light_update not in rfxtrx.RECEIVED_EVT_SUBSCRIBERS:
        rfxtrx.RECEIVED_EVT_SUBSCRIBERS.append(light_update)","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `cryptography` to generate secure random numbers.
3. Use `TYPE_CHECKING` to catch errors early."
"    def light_update(event):
        """""" Callback for light updates from the RFXtrx gateway. """"""
        if not isinstance(event.device, rfxtrxmod.LightingDevice):
            return

        # Add entity if not exist and the automatic_add is True
        entity_id = slugify(event.device.id_string.lower())
        if entity_id not in rfxtrx.RFX_DEVICES:
            automatic_add = config.get('automatic_add', False)
            if not automatic_add:
                return

            _LOGGER.info(
                ""Automatic add %s rfxtrx.light (Class: %s Sub: %s)"",
                entity_id,
                event.device.__class__.__name__,
                event.device.subtype
            )
            pkt_id = """".join(""{0:02x}"".format(x) for x in event.data)
            entity_name = ""%s : %s"" % (entity_id, pkt_id)
            datas = {ATTR_STATE: False, ATTR_FIREEVENT: False}
            new_light = RfxtrxLight(entity_name, event, datas)
            rfxtrx.RFX_DEVICES[entity_id] = new_light
            add_devices_callback([new_light])

        # Check if entity exists or previously added automatically
        if entity_id in rfxtrx.RFX_DEVICES \\
                and isinstance(rfxtrx.RFX_DEVICES[entity_id], RfxtrxLight):
            _LOGGER.debug(
                ""EntityID: %s light_update. Command: %s"",
                entity_id,
                event.values['Command']
            )
            if event.values['Command'] == 'On'\\
                    or event.values['Command'] == 'Off':

                # Update the rfxtrx device state
                is_on = event.values['Command'] == 'On'
                # pylint: disable=protected-access
                rfxtrx.RFX_DEVICES[entity_id]._state = is_on
                rfxtrx.RFX_DEVICES[entity_id].update_ha_state()

                # Fire event
                if rfxtrx.RFX_DEVICES[entity_id].should_fire_event:
                    rfxtrx.RFX_DEVICES[entity_id].hass.bus.fire(
                        EVENT_BUTTON_PRESSED, {
                            ATTR_ENTITY_ID:
                                rfxtrx.RFX_DEVICES[entity_id].entity_id,
                            ATTR_STATE: event.values['Command'].lower()
                        }
                    )","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `type hints` to make the code more readable and reduce the chance of errors.
3. Use `unit tests` to catch bugs early and prevent them from being released into production."
"def setup(hass, config):
    """""" Setup the RFXtrx component. """"""

    # Declare the Handle event
    def handle_receive(event):
        """""" Callback all subscribers for RFXtrx gateway. """"""

        # Log RFXCOM event
        entity_id = slugify(event.device.id_string.lower())
        packet_id = """".join(""{0:02x}"".format(x) for x in event.data)
        entity_name = ""%s : %s"" % (entity_id, packet_id)
        _LOGGER.info(""Receive RFXCOM event from %s => %s"",
                     event.device, entity_name)

        # Callback to HA registered components
        for subscriber in RECEIVED_EVT_SUBSCRIBERS:
            subscriber(event)

    # Try to load the RFXtrx module
    import RFXtrx as rfxtrxmod

    # Init the rfxtrx module
    global RFXOBJECT

    if ATTR_DEVICE not in config[DOMAIN]:
        _LOGGER.exception(
            ""can found device parameter in %s YAML configuration section"",
            DOMAIN
        )
        return False

    device = config[DOMAIN][ATTR_DEVICE]
    debug = config[DOMAIN].get(ATTR_DEBUG, False)

    RFXOBJECT = rfxtrxmod.Core(device, handle_receive, debug=debug)

    return True","1. Use proper error handling to avoid leaking sensitive information.
2. Use a secure communication channel, such as HTTPS, to protect data from being intercepted.
3. Use strong passwords and security measures to protect your account from being compromised."
"    def handle_receive(event):
        """""" Callback all subscribers for RFXtrx gateway. """"""

        # Log RFXCOM event
        entity_id = slugify(event.device.id_string.lower())
        packet_id = """".join(""{0:02x}"".format(x) for x in event.data)
        entity_name = ""%s : %s"" % (entity_id, packet_id)
        _LOGGER.info(""Receive RFXCOM event from %s => %s"",
                     event.device, entity_name)

        # Callback to HA registered components
        for subscriber in RECEIVED_EVT_SUBSCRIBERS:
            subscriber(event)","1. Use `event.data` instead of `event.device` to avoid leaking sensitive information.
2. Use `slugify(event.device.id_string.lower())` to generate a unique entity_id.
3. Use `_LOGGER.debug()` instead of `_LOGGER.info()` to log sensitive information."
"def setup_platform(hass, config, add_devices_callback, discovery_info=None):
    """""" Setup the RFXtrx platform. """"""
    import RFXtrx as rfxtrxmod

    # Add switch from config file
    switchs = []
    devices = config.get('devices')
    if devices:
        for entity_id, entity_info in devices.items():
            if entity_id not in rfxtrx.RFX_DEVICES:
                _LOGGER.info(""Add %s rfxtrx.switch"", entity_info[ATTR_NAME])

                # Check if i must fire event
                fire_event = entity_info.get(ATTR_FIREEVENT, False)
                datas = {ATTR_STATE: False, ATTR_FIREEVENT: fire_event}

                rfxobject = rfxtrx.get_rfx_object(entity_info[ATTR_PACKETID])
                newswitch = RfxtrxSwitch(
                    entity_info[ATTR_NAME], rfxobject, datas)
                rfxtrx.RFX_DEVICES[entity_id] = newswitch
                switchs.append(newswitch)

    add_devices_callback(switchs)

    def switch_update(event):
        """""" Callback for sensor updates from the RFXtrx gateway. """"""
        if not isinstance(event.device, rfxtrxmod.LightingDevice):
            return

        # Add entity if not exist and the automatic_add is True
        entity_id = slugify(event.device.id_string.lower())
        if entity_id not in rfxtrx.RFX_DEVICES:
            automatic_add = config.get('automatic_add', False)
            if not automatic_add:
                return

            _LOGGER.info(
                ""Automatic add %s rfxtrx.switch (Class: %s Sub: %s)"",
                entity_id,
                event.device.__class__.__name__,
                event.device.subtype
            )
            pkt_id = """".join(""{0:02x}"".format(x) for x in event.data)
            entity_name = ""%s : %s"" % (entity_id, pkt_id)
            datas = {ATTR_STATE: False, ATTR_FIREEVENT: False}
            new_switch = RfxtrxSwitch(entity_name, event, datas)
            rfxtrx.RFX_DEVICES[entity_id] = new_switch
            add_devices_callback([new_switch])

        # Check if entity exists or previously added automatically
        if entity_id in rfxtrx.RFX_DEVICES \\
                and isinstance(rfxtrx.RFX_DEVICES[entity_id], RfxtrxSwitch):
            _LOGGER.debug(
                ""EntityID: %s switch_update. Command: %s"",
                entity_id,
                event.values['Command']
            )
            if event.values['Command'] == 'On'\\
                    or event.values['Command'] == 'Off':

                # Update the rfxtrx device state
                is_on = event.values['Command'] == 'On'
                # pylint: disable=protected-access
                rfxtrx.RFX_DEVICES[entity_id]._state = is_on
                rfxtrx.RFX_DEVICES[entity_id].update_ha_state()

                # Fire event
                if rfxtrx.RFX_DEVICES[entity_id].should_fire_event:
                    rfxtrx.RFX_DEVICES[entity_id].hass.bus.fire(
                        EVENT_BUTTON_PRESSED, {
                            ATTR_ENTITY_ID:
                                rfxtrx.RFX_DEVICES[entity_id].entity_id,
                            ATTR_STATE: event.values['Command'].lower()
                        }
                    )

    # Subscribe to main rfxtrx events
    if switch_update not in rfxtrx.RECEIVED_EVT_SUBSCRIBERS:
        rfxtrx.RECEIVED_EVT_SUBSCRIBERS.append(switch_update)","1. Use `asyncio` instead of `threading` to avoid potential deadlocks.
2. Use `cryptography` to generate secure random numbers.
3. Use `pydantic` to validate user input."
"    def switch_update(event):
        """""" Callback for sensor updates from the RFXtrx gateway. """"""
        if not isinstance(event.device, rfxtrxmod.LightingDevice):
            return

        # Add entity if not exist and the automatic_add is True
        entity_id = slugify(event.device.id_string.lower())
        if entity_id not in rfxtrx.RFX_DEVICES:
            automatic_add = config.get('automatic_add', False)
            if not automatic_add:
                return

            _LOGGER.info(
                ""Automatic add %s rfxtrx.switch (Class: %s Sub: %s)"",
                entity_id,
                event.device.__class__.__name__,
                event.device.subtype
            )
            pkt_id = """".join(""{0:02x}"".format(x) for x in event.data)
            entity_name = ""%s : %s"" % (entity_id, pkt_id)
            datas = {ATTR_STATE: False, ATTR_FIREEVENT: False}
            new_switch = RfxtrxSwitch(entity_name, event, datas)
            rfxtrx.RFX_DEVICES[entity_id] = new_switch
            add_devices_callback([new_switch])

        # Check if entity exists or previously added automatically
        if entity_id in rfxtrx.RFX_DEVICES \\
                and isinstance(rfxtrx.RFX_DEVICES[entity_id], RfxtrxSwitch):
            _LOGGER.debug(
                ""EntityID: %s switch_update. Command: %s"",
                entity_id,
                event.values['Command']
            )
            if event.values['Command'] == 'On'\\
                    or event.values['Command'] == 'Off':

                # Update the rfxtrx device state
                is_on = event.values['Command'] == 'On'
                # pylint: disable=protected-access
                rfxtrx.RFX_DEVICES[entity_id]._state = is_on
                rfxtrx.RFX_DEVICES[entity_id].update_ha_state()

                # Fire event
                if rfxtrx.RFX_DEVICES[entity_id].should_fire_event:
                    rfxtrx.RFX_DEVICES[entity_id].hass.bus.fire(
                        EVENT_BUTTON_PRESSED, {
                            ATTR_ENTITY_ID:
                                rfxtrx.RFX_DEVICES[entity_id].entity_id,
                            ATTR_STATE: event.values['Command'].lower()
                        }
                    )","1. Use `propercasing` for constants and attributes.
2. Use `TYPE_CHECKING` to validate the type of arguments passed to functions.
3. Use `@callback` decorator to annotate functions that are intended to be called from other components."
"        def run(self):
            """"""Do nothing so the command intentionally fails.""""""
            pass","1. **Use `assert` statements to validate the input.** This will help to ensure that the command is only run with valid arguments.
2. **Handle errors gracefully.** If an error occurs, the command should exit cleanly and not leave the system in an inconsistent state.
3. **Document the command's behavior.** This will help users understand how to use the command correctly and avoid making mistakes."
"    async def write(self, uuid):
        p = f""{self.path}/{uuid}.{self.count[uuid]}""
        async with AIOFile(p, mode='a') as fp:
            r = await fp.write(""\\n"".join(self.data[uuid]) + ""\\n"", offset=self.pointer[uuid])
            self.pointer[uuid] += len(r)
            self.data[uuid] = []

        if self.pointer[uuid] >= self.rotate:
            self.count[uuid] += 1
            self.pointer[uuid] = 0","1. Use `async with aiofiles.open()` instead of `async with open()` to ensure that the file is closed properly.
2. Use `os.fchmod()` to set the file mode to `0o600` to restrict access to the file.
3. Use `os.fchown()` to set the file owner to the current user to prevent other users from accessing the file."
"    def run_module(self):
        import runpy
        code = ""run_module(modname, run_name='__main__')""
        global_dict = {
            ""run_module"": runpy.run_module,
            ""modname"": self.options.module
        }
        sys.argv = [self.options.module] + self.command[:]
        sys.path.append(os.getcwd())
        return self.run_code(code, global_dict)","1. Use `importlib.import_module` instead of `runpy.run_module` to avoid running arbitrary code.
2. Use `sys.argv[0]` instead of `self.options.module` to get the module name, to prevent a malicious user from tricking the code into running a different module.
3. Use `sys.path.insert(0, os.getcwd())` to add the current working directory to the beginning of the `sys.path` list, to prevent a malicious user from tricking the code into importing a module from a different location."
"def ignore_function(method=None, tracer=None):
    if not tracer:
        tracer = get_tracer()
    
    def inner(func):

        @functools.wraps(func)
        def ignore_wrapper(*args, **kwargs):
            tracer.pause()
            ret = func(*args, **kwargs)
            tracer.resume()
            return ret

        return ignore_wrapper

    if method:
        return inner(method)
    return inner ","1. Use `functools.wraps` to preserve the function signature.
2. Use `@staticmethod` to make the function static.
3. Use `@abstractmethod` to make the function abstract."
"    def inner(func):

        @functools.wraps(func)
        def ignore_wrapper(*args, **kwargs):
            tracer.pause()
            ret = func(*args, **kwargs)
            tracer.resume()
            return ret

        return ignore_wrapper","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `@staticmethod` to mark the function as static.
3. Use `@classmethod` to mark the function as a class method."
"        def ignore_wrapper(*args, **kwargs):
            tracer.pause()
            ret = func(*args, **kwargs)
            tracer.resume()
            return ret","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `inspect.iscoroutinefunction` to check if the wrapped function is a coroutine.
3. Use `contextlib.closing` to ensure that the tracer is closed when the wrapped function exits."
"    def run_code(self, code, global_dict):
        options = self.options
        verbose = self.verbose
        ofile = self.ofile

        tracer = VizTracer(
            tracer_entries=options.tracer_entries,
            verbose=verbose,
            output_file=ofile,
            max_stack_depth=options.max_stack_depth,
            exclude_files=options.exclude_files,
            include_files=options.include_files,
            ignore_c_function=options.ignore_c_function,
            ignore_non_file=options.ignore_non_file,
            log_return_value=options.log_return_value,
            log_function_args=options.log_function_args,
            log_print=options.log_print,
            log_gc=options.log_gc,
            novdb=options.novdb,
            pid_suffix=options.pid_suffix
        )

        self.tracer = tracer

        builtins.__dict__[""__viz_tracer__""] = tracer

        self.parent_pid = os.getpid()
        if options.log_multiprocess:
            if get_start_method() != ""fork"":
                return False, ""Only fork based multiprocess is supported""
            self.patch_multiprocessing(tracer)

        def term_handler(signalnum, frame):
            self.exit_routine()
        signal.signal(signal.SIGTERM, term_handler)

        atexit.register(self.exit_routine)
        if options.log_sparse:
            tracer.enable = True
        else:
            tracer.start()
        exec(code, global_dict)
        tracer.stop()

        self.exit_routine()","1. Use `os.getpid()` to get the parent process ID instead of hardcoding it.
2. Use `atexit.register()` to register a function to be called when the program exits.
3. Use `signal.signal()` to register a handler for the SIGTERM signal."
"    def __init__(self,
                 tracer_entries=1000000,
                 verbose=1,
                 max_stack_depth=-1,
                 include_files=None,
                 exclude_files=None,
                 ignore_c_function=False,
                 ignore_non_file=False,
                 log_return_value=False,
                 log_function_args=False,
                 log_print=False,
                 log_gc=False,
                 novdb=False,
                 pid_suffix=False,
                 file_info=False,
                 output_file=""result.html""):
        super().__init__(
                tracer_entries=tracer_entries,
                max_stack_depth=max_stack_depth,
                include_files=include_files,
                exclude_files=exclude_files,
                ignore_c_function=ignore_c_function,
                ignore_non_file=ignore_non_file,
                log_return_value=log_return_value,
                log_print=log_print,
                log_gc=log_gc,
                novdb=novdb,
                log_function_args=log_function_args
        )
        self.verbose = verbose
        self.pid_suffix = pid_suffix
        self.file_info = file_info
        self.output_file = output_file
        self.system_print = None","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `inspect.getfullargspec` to get the full argument spec of the function.
3. Use `inspect.iscoroutinefunction` to check if the function is a coroutine."
"def post_delete_linked(sender, instance, **kwargs):
    # When removing project, the linked component might be already deleted now
    try:
        if instance.linked_component:
            instance.linked_component.update_alerts()
    except Component.DoesNotExist:
        pass","1. Use `instance.linked_component.id` instead of `instance.linked_component` to avoid a race condition.
2. Use `instance.linked_component.update_alerts(force=True)` to ensure that the alerts are updated even if the component has been deleted.
3. Add a `try`/`except` block to catch `Component.DoesNotExist` errors and log them appropriately."
"    def add_alert(self, alert, **details):
        obj, created = self.alert_set.get_or_create(
            name=alert, defaults={""details"": details}
        )

        # Automatically lock on error
        if created and self.auto_lock_error and alert in LOCKING_ALERTS:
            self.do_lock(user=None, lock=True)

        if not created:
            obj.details = details
            obj.save()
        if ALERTS[alert].link_wide:
            for component in self.linked_childs:
                component.add_alert(alert, **details)","1. Use `user.is_authenticated()` to check if the user is authenticated before adding an alert.
2. Use `ValidationError` to raise an error if the alert name is not a valid string.
3. Use `permissions.check()` to check if the user has permission to add the alert."
"    def update_alerts(self):
        if (
            self.project.access_control == self.project.ACCESS_PUBLIC
            and not self.license
            and not getattr(settings, ""LOGIN_REQUIRED_URLS"", None)
        ):
            self.add_alert(""MissingLicense"")
        else:
            self.delete_alert(""MissingLicense"")

        # Pick random translation with translated strings except source one
        translation = (
            self.translation_set.filter(unit__state__gte=STATE_TRANSLATED)
            .exclude(language=self.project.source_language)
            .first()
        )
        if translation:
            allunits = translation.unit_set
        else:
            allunits = self.source_translation.unit_set

        source_space = allunits.filter(source__contains="" "")
        target_space = allunits.filter(
            state__gte=STATE_TRANSLATED, target__contains="" ""
        )
        if (
            not self.template
            and allunits.count() > 3
            and not source_space.exists()
            and target_space.exists()
        ):
            self.add_alert(""MonolingualTranslation"")
        else:
            self.delete_alert(""MonolingualTranslation"")
        if not self.can_push():
            self.delete_alert(""PushFailure"")

        if self.vcs not in VCS_REGISTRY or self.file_format not in FILE_FORMATS:
            self.add_alert(
                ""UnsupportedConfiguration"",
                vcs=self.vcs not in VCS_REGISTRY,
                file_format=self.file_format not in FILE_FORMATS,
            )
        else:
            self.delete_alert(""UnsupportedConfiguration"")

        base = self.linked_component if self.is_repo_link else self
        masks = [base.filemask]
        masks.extend(base.linked_childs.values_list(""filemask"", flat=True))
        duplicates = [item for item, count in Counter(masks).items() if count > 1]
        if duplicates:
            self.add_alert(""DuplicateFilemask"", duplicates=duplicates)
        else:
            self.delete_alert(""DuplicateFilemask"")

        location_error = None
        location_link = None
        if self.repoweb:
            unit = allunits.exclude(location="""").first()
            if unit:
                for _location, filename, line in unit.get_locations():
                    location_link = self.get_repoweb_link(filename, line)
                    if location_link is None:
                        continue
                    # We only test first link
                    location_error = get_uri_error(location_link)
                    break
        if location_error:
            self.add_alert(""BrokenBrowserURL"", link=location_link, error=location_error)
        else:
            self.delete_alert(""BrokenBrowserURL"")
        if self.project.web:
            error = get_uri_error(self.project.web)
            if error is not None:
                self.add_alert(""BrokenProjectURL"", error=error)
            else:
                self.delete_alert(""BrokenProjectURL"")
        else:
            self.delete_alert(""BrokenProjectURL"")

        if self.screenshot_set.annotate(Count(""units"")).filter(units__count=0).exists():
            self.add_alert(""UnusedScreenshot"")
        else:
            self.delete_alert(""UnusedScreenshot"")","1. Use `get_uri_error()` to check for broken URLs.
2. Use `delete_alert()` to remove alerts that are no longer valid.
3. Use `add_alert()` to add new alerts as needed."
"def create_index(apps, schema_editor):
    if schema_editor.connection.vendor != ""postgresql"":
        return
    # This ensures that extensions are loaded into the session. Without that
    # the next ALTER database fails unless we're running as superuser (which
    # is allowed to set non existing parameters, so missing extension doesn't
    # matter)
    # See https://www.postgresql.org/message-id/6376.1533675236%40sss.pgh.pa.us
    schema_editor.execute(""SELECT show_limit()"")

    schema_editor.execute(
        ""ALTER DATABASE {} SET pg_trgm.similarity_threshold = 0.7"".format(
            schema_editor.connection.settings_dict[""NAME""]
        )
    )
    schema_editor.execute(""DROP INDEX memory_source_fulltext"")
    schema_editor.execute(
        ""CREATE INDEX memory_source_trgm ON memory_memory USING GIN (source gin_trgm_ops)""
    )","1. Use `schema_editor.execute_sql` instead of `schema_editor.execute` to prevent SQL injection.
2. Use `schema_editor.set_context` to set the connection's search path before running the `ALTER DATABASE` statement.
3. Use `schema_editor.delete_index` instead of `schema_editor.execute` to drop the index."
"def update_index(apps, schema_editor):
    if schema_editor.connection.vendor != ""postgresql"":
        return
    # This ensures that extensions are loaded into the session. Without that
    # the next ALTER database fails unless we're running as superuser (which
    # is allowed to set non existing parameters, so missing extension doesn't
    # matter)
    # See https://www.postgresql.org/message-id/6376.1533675236%40sss.pgh.pa.us
    schema_editor.execute(""SELECT show_limit()"")

    schema_editor.execute(
        ""ALTER DATABASE {} SET pg_trgm.similarity_threshold = 0.5"".format(
            schema_editor.connection.settings_dict[""NAME""]
        )
    )","1. Use `schema_editor.execute_sql` instead of `schema_editor.execute` to avoid SQL injection.
2. Use `schema_editor.connection.cursor()` to get a cursor and execute queries inside a transaction.
3. Use `schema_editor.close_connection` to close the connection after the transaction is committed."
"    def load_data(self):
        result = super().load_data()

        for key, vcs in list(result.items()):
            try:
                supported = vcs.is_supported()
            except Exception as error:
                supported = False
                self.errors[vcs.name] = str(error)

            if not supported:
                result.pop(key)

        return result","1. Use `try/except` to catch errors and log them.
2. Check if the `vcs` is supported before using it.
3. Remove the `vcs` from the result if it is not supported."
"    def build_unit(self, unit):
        output = super().build_unit(unit)
        try:
            converted_source = xliff_string_to_rich(unit.get_source_plurals())
            converted_target = xliff_string_to_rich(unit.get_target_plurals())
        except XMLSyntaxError:
            return output
        output.rich_source = converted_source
        output.set_rich_target(converted_target, self.language.code)
        return output","1. **Use proper error handling.** The code should catch and handle XMLSyntaxError exceptions.
2. **Sanitize user input.** The code should sanitize user input before using it to construct XML strings.
3. **Use secure coding practices.** The code should follow secure coding practices, such as using the least privilege principle and avoiding dangerous functions."
"    def build_unit(self, unit):
        try:
            converted_source = xliff_string_to_rich(unit.source)
            converted_target = xliff_string_to_rich(unit.target)
        except XMLSyntaxError:
            return super().build_unit(unit)
        output = self.storage.UnitClass("""")
        output.rich_source = converted_source
        output.set_rich_target(converted_target, self.language.code)
        return output","1. **Use proper error handling.** The code should catch and handle XMLSyntaxError exceptions, and return a default value instead of raising the exception.
2. **Sanitize user input.** The code should sanitize user input before converting it to rich text, to prevent cross-site scripting attacks.
3. **Use strong encryption.** The code should use strong encryption to protect sensitive data, such as user passwords."
"    def get_last_remote_commit(self):
        """"""Return latest locally known remote commit.""""""
        return self.repository.get_revision_info(self.repository.last_remote_revision)","1. Use `get_remote_revisions()` instead of `last_remote_revision` to get the latest remote commit.
2. Use `get_revision_info()` with a specific revision instead of `last_remote_revision`.
3. Check the return value of `get_revision_info()` to make sure it is not None."
"    def repo_needs_merge(self):
        """"""Check for unmerged commits from remote repository.""""""
        return self.repository.needs_merge()","1. Use `get_remote()` instead of `repository.remote` to avoid leaking the remote URL.
2. Use `repository.is_dirty()` instead of `repository.needs_merge()` to avoid leaking information about the repository state.
3. Use `repository.fetch()` to fetch the remote changes instead of calling `needs_merge()` directly."
"def get_queue_length(queue='celery'):
    with celery_app.connection_or_acquire() as conn:
        return conn.default_channel.queue_declare(
            queue=queue, durable=True, auto_delete=False
        ).message_count","1. Use `CELERY_RESULT_BACKEND` to store the queue length in a database instead of in memory.
2. Set `CELERY_QUEUE_MAX_LENGTH` to a limit that is appropriate for your application.
3. Use `CELERY_QUEUE_EXPIRES` to automatically delete messages from the queue after a certain amount of time."
"    def clean_repo_link(self):
        """"""Validate repository link.""""""
        try:
            repo = Component.objects.get_linked(self.repo)
            if repo is not None and repo.is_repo_link:
                raise ValidationError(
                    {
                        ""repo"": _(
                            ""Invalid link to a Weblate project, ""
                            ""cannot link to linked repository!""
                        )
                    }
                )
            if repo.pk == self.pk:
                raise ValidationError(
                    {
                        ""repo"": _(
                            ""Invalid link to a Weblate project, ""
                            ""cannot link it to itself!""
                        )
                    }
                )
        except (Component.DoesNotExist, ValueError):
            raise ValidationError(
                {
                    ""repo"": _(
                        ""Invalid link to a Weblate project, ""
                        ""use weblate://project/component.""
                    )
                }
            )
        for setting in (""push"", ""branch"", ""git_export""):
            if getattr(self, setting):
                raise ValidationError(
                    {setting: _(""Option is not available for linked repositories."")}
                )
        self.linked_component = Component.objects.get_linked(self.repo)","1. **Use `Component.get_linked()` instead of `Component.objects.get()` to avoid creating a new instance of the component.** This will prevent the user from creating a link to a component that is already linked to another component.
2. **Check if the `repo` field is a valid URL before trying to create the link.** This will prevent the user from creating a link to a non-existent repository.
3. **Raise a `ValidationError` if the user tries to set any of the following settings on a linked repository: `push`, `branch`, or `git_export`.** This will prevent the user from making changes to the linked repository."
"    def clean_repo_link(self):
        """"""Validate repository link.""""""
        try:
            repo = Component.objects.get_linked(self.repo)
            if repo is not None and repo.is_repo_link:
                raise ValidationError(
                    {
                        ""repo"": _(
                            ""Invalid link to a Weblate project, ""
                            ""cannot link to linked repository!""
                        )
                    }
                )
            if repo.pk == self.pk:
                raise ValidationError(
                    {
                        ""repo"": _(
                            ""Invalid link to a Weblate project, ""
                            ""cannot link it to itself!""
                        )
                    }
                )
        except (Component.DoesNotExist, ValueError):
            raise ValidationError(
                {
                    ""repo"": _(
                        ""Invalid link to a Weblate project, ""
                        ""use weblate://project/component.""
                    )
                }
            )
        for setting in (""push"", ""branch"", ""git_export""):
            # The git_export might be missing in case of form validation
            if getattr(self, setting, None):
                raise ValidationError(
                    {setting: _(""Option is not available for linked repositories."")}
                )
        self.linked_component = Component.objects.get_linked(self.repo)","1. **Use a more specific exception**. The current exception is too generic and doesn't provide much information about the error.
2. **Check for circular references**. This code could potentially create a circular reference if a component is linked to itself.
3. **Validate the settings**. Make sure that the settings for the linked repository are valid."
"def migrate_unitdata(apps, schema_editor):
    db_alias = schema_editor.connection.alias

    Unit = apps.get_model(""trans"", ""Unit"")

    for model in (apps.get_model(*args) for args in MODELS):
        # Create new objects for each related unit
        for obj in model.objects.using(db_alias).filter(unit=None).iterator():
            units = Unit.objects.using(db_alias).filter(
                content_hash=obj.content_hash,
                translation__component__project=obj.project,
            )
            if obj.language is None:
                units = units.filter(translation__language=obj.project.source_language)
            else:
                units = units.filter(translation__language=obj.language)
            state = obj.__getstate__()
            del state[""_state""]
            del state[""id""]
            del state[""unit_id""]
            for unit in units:
                if model.objects.using(db_alias).filter(unit=unit, **state).exists():
                    continue
                model.objects.using(db_alias).create(unit=unit, **state)

        # Remove old objects without unit link
        model.objects.using(db_alias).filter(unit=None).delete()","1. Use `get_model()` instead of `apps.get_model()` to avoid circular dependencies.
2. Use `with transaction.atomic()` to ensure that the migration is atomic.
3. Use `django.db.models.signals.post_save` to create the unit link after the object is saved."
"def migrate_unitdata(apps, schema_editor):
    db_alias = schema_editor.connection.alias

    Unit = apps.get_model(""trans"", ""Unit"")

    for model in (apps.get_model(*args) for args in MODELS):
        # Create new objects for each related unit
        for obj in model.objects.using(db_alias).filter(unit=None).iterator():
            units = Unit.objects.using(db_alias).filter(
                content_hash=obj.content_hash,
                translation__component__project=obj.project,
            )
            if obj.language is None:
                units = units.filter(translation__language=obj.project.source_language)
            else:
                units = units.filter(translation__language=obj.language)
            # Using __getstate__ would be cleaner, but needs Django 2.0
            state = dict(obj.__dict__)
            del state[""_state""]
            del state[""id""]
            del state[""unit_id""]
            for unit in units:
                if model.objects.using(db_alias).filter(unit=unit, **state).exists():
                    continue
                model.objects.using(db_alias).create(unit=unit, **state)

        # Remove old objects without unit link
        model.objects.using(db_alias).filter(unit=None).delete()","1. Use `get_model()` instead of `apps.get_model()` to avoid circular import.
2. Use `.filter()` instead of `.iterator()` to avoid leaking database connections.
3. Use `.create()` instead of `.update()` to avoid race conditions."
"    def discover(self):
        """"""Manual discovery.""""""
        # Accounts
        self.register(User, WeblateUserAdmin)
        self.register(Role, RoleAdmin)
        self.register(Group, WeblateGroupAdmin)
        self.register(AuditLog, AuditLogAdmin)
        self.register(AutoGroup, AutoGroupAdmin)
        self.register(Profile, ProfileAdmin)
        self.register(VerifiedEmail, VerifiedEmailAdmin)

        # Languages
        if settings.DEBUG:
            self.register(Language, LanguageAdmin)

        # Screenshots
        self.register(Screenshot, ScreenshotAdmin)

        # Fonts
        self.register(Font, FontAdmin)
        self.register(FontGroup, FontGroupAdmin)

        # Translations
        self.register(Project, ProjectAdmin)
        self.register(Component, ComponentAdmin)
        self.register(WhiteboardMessage, WhiteboardMessageAdmin)
        self.register(ComponentList, ComponentListAdmin)
        self.register(ContributorAgreement, ContributorAgreementAdmin)

        # Show some controls only in debug mode
        if settings.DEBUG:
            self.register(Translation, TranslationAdmin)
            self.register(Unit, UnitAdmin)
            self.register(Suggestion, SuggestionAdmin)
            self.register(Comment, CommentAdmin)
            self.register(Check, CheckAdmin)
            self.register(Dictionary, DictionaryAdmin)
            self.register(Change, ChangeAdmin)
            self.register(Source, SourceAdmin)

        if settings.BILLING_ADMIN:
            # Billing
            if 'weblate.billing' in settings.INSTALLED_APPS:
                # pylint: disable=wrong-import-position
                from weblate.billing.admin import (
                    PlanAdmin, BillingAdmin, InvoiceAdmin,
                )
                from weblate.billing.models import Plan, Billing, Invoice
                self.register(Plan, PlanAdmin)
                self.register(Billing, BillingAdmin)
                self.register(Invoice, InvoiceAdmin)

            # Hosted
            if 'wlhosted.integrations' in settings.INSTALLED_APPS:
                # pylint: disable=wrong-import-position
                from wlhosted.payments.admin import CustomerAdmin, PaymentAdmin
                from wlhosted.payments.models import Customer, Payment
                self.register(Customer, CustomerAdmin)
                self.register(Payment, PaymentAdmin)

        # Legal
        if 'weblate.legal' in settings.INSTALLED_APPS:
            # pylint: disable=wrong-import-position
            from weblate.legal.admin import AgreementAdmin
            from weblate.legal.models import Agreement
            self.register(Agreement, AgreementAdmin)

        # Python Social Auth
        self.register(UserSocialAuth, UserSocialAuthOption)
        self.register(Nonce, NonceOption)
        self.register(Association, AssociationOption)

        # Django REST Framework
        self.register(Token, TokenAdmin)

        # Django core
        self.register(Site, SiteAdmin)

        # Simple SSO
        if 'simple_sso.sso_server' in settings.INSTALLED_APPS:
            from simple_sso.sso_server.server import ConsumerAdmin
            from simple_sso.sso_server.models import Consumer
            self.register(Consumer, ConsumerAdmin)","1. Use `django.contrib.admin.sites.AdminSite` instead of `admin.site`.
2. Use `register()` to register models with the admin site.
3. Use `{% load admin_list %}` to render the list of models in the admin site."
"def create_profiles(apps, schema_editor):
    Profile = apps.get_model(""accounts"", ""Profile"")
    User = apps.get_model(""weblate_auth"", ""User"")
    for user in User.objects.iterator():
        Profile.objects.get_or_create(user=user)","1. Use `user.id` instead of `user` as the primary key for the `Profile` model.
2. Use `django.contrib.auth.models.User.objects.filter()` instead of `User.objects.iterator()` to get a list of all users.
3. Use `Profile.objects.create()` instead of `Profile.objects.get_or_create()` to create a new profile for each user."
"def migrate_subscriptions(apps, schema_editor):
    Profile = apps.get_model(""accounts"", ""Profile"")
    profiles = Profile.objects.all().select_related(""user"")
    profiles = profiles.exclude(user__username=settings.ANONYMOUS_USER_NAME)
    for profile in profiles:
        user = profile.user
        create_default_notifications(user)
        for flag, notifications in MAPPING:
            if getattr(profile, flag):
                for notification in notifications:
                    user.subscription_set.get_or_create(
                        scope=SCOPE_DEFAULT,
                        notification=notification,
                        defaults={""frequency"": FREQ_INSTANT},
                    )","1. Use `user.id` instead of `user.username` to avoid leaking the user's username.
2. Use `user.subscription_set.create()` instead of `user.subscription_set.get_or_create()` to avoid creating duplicate subscriptions.
3. Use `user.subscription_set.filter()` to filter subscriptions by scope and notification, instead of iterating over all subscriptions."
"def migrate_editor(apps, schema_editor):
    Profile = apps.get_model(""accounts"", ""Profile"")
    for profile in Profile.objects.exclude(editor_link=""""):
        profile.editor_link = weblate.utils.render.migrate_repoweb(profile.editor_link)
        profile.save()","1. Use `django.contrib.auth.models.User` instead of a custom `Profile` model.
2. Use `django.contrib.auth.mixins.LoginRequiredMixin` to protect views that should only be accessible to logged-in users.
3. Use `django.utils.crypto.get_random_string()` to generate a random salt for each user's password hash."
"def migrate_dashboard(apps, schema_editor):
    Profile = apps.get_model(""accounts"", ""Profile"")
    Profile.objects.filter(dashboard_view=2).update(dashboard_view=1)","1. Use `user.is_staff` instead of `dashboard_view` to check if a user can access the dashboard.
2. Use `django.contrib.auth.models.User` instead of a custom `Profile` model.
3. Use `django.db.transaction.atomic` to ensure that the update is performed atomically."
"def update_squash_addon(apps, schema_editor):
    """"""Update events setup for weblate.git.squash addon.""""""
    Addon = apps.get_model(""addons"", ""Addon"")
    Event = apps.get_model(""addons"", ""Event"")
    for addon in Addon.objects.filter(name=""weblate.git.squash""):
        Event.objects.get_or_create(addon=addon, event=EVENT_POST_COMMIT)
        addon.event_set.filter(event=EVENT_PRE_PUSH).delete()","1. Use `.get_or_create()` instead of `.create()` to avoid creating duplicate records.
2. Use `.filter()` to delete the records instead of `.delete()` to avoid deleting other records.
3. Use `.objects.filter(name=""weblate.git.squash"")` instead of `.filter(name=""weblate.git.squash"")` to avoid accidentally deleting other addons."
"def update_repo_scope(apps, schema_editor):
    """"""Update the repo_scope flag.""""""
    Addon = apps.get_model(""addons"", ""Addon"")
    Addon.objects.filter(name=""weblate.git.squash"").update(repo_scope=True)","1. Use `django.db.transaction.atomic()` to ensure that the update is performed atomically.
2. Use `django.contrib.auth.models.User.has_perm()` to check if the user has the required permission to update the flag.
3. Use `django.utils.timezone.now()` to set the updated timestamp."
"def fix_repo_scope(apps, schema_editor):
    Addon = apps.get_model(""addons"", ""Addon"")
    Addon.objects.filter(name=""weblate.git.squash"").update(repo_scope=True)","1. Use `django.contrib.auth.models.User.objects.get_by_natural_key()` to get the user by username instead of `django.contrib.auth.models.User.objects.get(username=username)`. This is more secure because it prevents SQL injection attacks.
2. Use `django.db.transaction.atomic()` to wrap the code that updates the `repo_scope` field. This is more secure because it prevents race conditions.
3. Use `django.utils.timezone.now()` to get the current time instead of `datetime.datetime.now()`. This is more secure because it prevents attacks that rely on the time being set to a specific value."
"def migrate_flags(apps, schema_editor):
    """"""Update the repo_scope flag.""""""
    Addon = apps.get_model(""addons"", ""Addon"")
    Addon.objects.filter(
        name__in=(""weblate.discovery.discovery"", ""weblate.git.squash"")
    ).update(repo_scope=True)
    Addon.objects.filter(
        name__in=(""weblate.removal.comments"", ""weblate.removal.suggestions"")
    ).update(project_scope=True)","1. Use `django.db.transaction.atomic()` to ensure that the updates are performed atomically.
2. Use `django.db.transaction.on_commit()` to run the code after the transaction is committed.
3. Use `django.db.transaction.on_rollback()` to run the code if the transaction is rolled back."
"def update_addon(apps, schema_editor):
    """"""Update the repo_scope flag.""""""
    Addon = apps.get_model(""addons"", ""Addon"")
    Event = apps.get_model(""addons"", ""Event"")
    for addon in Addon.objects.filter(name=""weblate.consistency.languages""):
        Event.objects.get_or_create(addon=addon, event=EVENT_DAILY)
        addon.event_set.filter(event=EVENT_POST_UPDATE).delete()","1. Use `.get_or_create()` instead of `.filter()` to avoid creating duplicate records.
2. Use `.delete()` instead of `.filter()` to delete records.
3. Use `.filter(name=""weblate.consistency.languages"")` instead of `.filter(name=""weblate.consistency.languages"")` to avoid typos."
"def resolve_auto_format(apps, schema_editor):
    Addon = apps.get_model(""addons"", ""Addon"")
    for addon in Addon.objects.filter(name=""weblate.discovery.discovery""):
        if addon.configuration[""file_format""] == ""auto"":
            detect = detect_filename(addon.configuration[""match""].replace(""\\\\."", "".""))
            if detect is None:
                raise Exception(
                    ""Existing component discovery with auto format, can not detect ""
                    ""file format. Please edit the format manually and rerun ""
                    ""migration. Affected component: {}/{}"".format(
                        addon.component.project.slug, addon.component.slug
                    )
                )

            addon.configuration[""file_format""] = detect.format_id
            addon.save()","1. Use `django.utils.regex.Regex` instead of `re.compile` to avoid potential security vulnerabilities.
2. Use `django.utils.safestring.mark_safe` to escape potentially dangerous strings.
3. Use `django.contrib.auth.models.User.objects.get_or_create` to create a new user if one does not exist."
"def update_resx_addon(apps, schema_editor):
    Addon = apps.get_model(""addons"", ""Addon"")
    Addon.objects.filter(
        component__file_format=""resx"", name=""weblate.cleanup.generic""
    ).update(name=""weblate.resx.update"")","1. Use `django.contrib.auth.models.User.objects.get_by_natural_key()` to get the user object by username. This is more secure than using `django.contrib.auth.models.User.objects.get(username=username)` because it protects against SQL injection attacks.
2. Use `django.db.transaction.atomic()` to wrap the code that updates the addon name. This will ensure that the update is either completed successfully or not at all, which prevents data corruption in the event of a database error.
3. Use `django.utils.timezone.now()` to get the current time. This will ensure that the update is always performed with the correct timestamp, which is important for maintaining the integrity of the database."
"def set_export_url(apps, schema_editor):
    Component = apps.get_model(""trans"", ""Component"")
    matching = Component.objects.filter(vcs__in=SUPPORTED_VCS).exclude(
        repo__startswith=""weblate:/""
    )
    for component in matching:
        new_url = get_export_url(component)
        if component.git_export != new_url:
            component.git_export = new_url
            component.save()","1. Use `get_component_by_id()` instead of `Component.objects.filter()` to avoid leaking information about the existence of components.
2. Use `get_export_url_for_vcs()` instead of `get_export_url()` to avoid leaking information about the supported VCS.
3. Use `update_component()` instead of `save()` to avoid triggering unnecessary signals."
"def update_linked(apps, schema_editor):
    """"""Clean branch for linked components.""""""
    Component = apps.get_model(""trans"", ""Component"")
    linked = Component.objects.filter(repo__startswith=""weblate:"")
    linked.update(branch="""")","1. **Use prepared statements instead of building queries manually.** This will help to prevent SQL injection attacks.
2. **Use `django.contrib.auth.models.User` instead of creating your own custom user model.** This will give you access to built-in security features such as user permissions and login sessions.
3. **Use `django.utils.crypto.get_random_string()` to generate random strings instead of hard-coding them.** This will help to make your code more secure against attacks such as brute force login attempts."
"def fix_alert_occurence(apps, schema_editor):
    Alert = apps.get_model(""trans"", ""Alert"")
    Alert.objects.filter(details__contains='""occurences""').update(
        details=Func(
            F(""details""),
            Value('""occurences""'),
            Value('""occurrences""'),
            function=""replace"",
        )
    )","1. Use `F()` instead of `__` to avoid SQL injection.
2. Use `Value()` to escape strings.
3. Use `func()` to specify the function to be used."
"def unfix_alert_occurence(apps, schema_editor):
    Alert = apps.get_model(""trans"", ""Alert"")
    Alert.objects.filter(details__contains='""occurrences""').update(
        details=Func(
            F(""details""),
            Value('""occurrences""'),
            Value('""occurences""'),
            function=""replace"",
        )
    )","1. Use `.filter()` instead of `.update()` to avoid accidentally updating all rows.
2. Use `F()` and `Value()` to avoid SQL injection.
3. Use `func()` to specify the exact replacement operation."
"def remove_unusednewbase_alert(apps, schema_editor):
    Alert = apps.get_model(""trans"", ""Alert"")
    Alert.objects.filter(name=""UnusedNewBase"").delete()","1. Use `.get_queryset()` instead of `.objects` to avoid creating a new queryset every time the method is called.
2. Use `.filter()` instead of `.delete()` to avoid deleting all instances of the model.
3. Use `.validate_delete_permission()` to check if the user has permission to delete the alert before deleting it."
"def resolve_auto_format(apps, schema_editor):
    Component = apps.get_model(""trans"", ""Component"")
    for component in Component.objects.filter(file_format=""auto""):
        path = get_path(component)
        template = None
        if component.template:
            template = AutodetectFormat.parse(os.path.join(path, component.template))
        try:
            translation = component.translation_set.all()[0]
        except IndexError:
            if template is None and component.new_base:
                template = AutodetectFormat.parse(
                    os.path.join(path, component.new_base)
                )
            if template is not None:
                update_format(component, template)
                continue
            raise Exception(
                ""Existing translation component with auto format and ""
                ""without any translations, can not detect file format. ""
                ""Please edit the format manually and rerun migration. ""
                ""Affected component: {}/{}"".format(
                    component.project.slug, component.slug
                )
            )
        store = AutodetectFormat.parse(
            os.path.join(path, translation.filename), template
        )
        update_format(component, store)","1. Use `get_path()` to get the path of the component, instead of hardcoding it. This will prevent the code from being vulnerable to path traversal attacks.
2. Use `try/except` to catch errors when getting the translation set. This will prevent the code from crashing if there are no translations for the component.
3. Use `raise Exception()` to raise an exception if the file format cannot be detected. This will prevent the code from continuing to run with an incorrect file format."
"def migrate_repoweb(apps, schema_editor):
    Component = apps.get_model(""trans"", ""Component"")
    for component in Component.objects.exclude(repoweb=""""):
        component.repoweb = weblate.utils.render.migrate_repoweb(component.repoweb)
        component.save()","1. Use `django.contrib.auth.models.User.objects.get_or_create()` instead of `get()` to avoid creating a new user if one does not exist.
2. Use `django.utils.html.escape()` to escape HTML in the `repoweb` field.
3. Use `django.db.transaction.atomic()` to ensure that the migration is performed atomically."
"def migrate_alert_change(apps, schema_editor):
    Change = apps.get_model(""trans"", ""Change"")
    for change in Change.objects.filter(action=47).exclude(target=""""):
        change.details = {""alert"": change.target}
        change.target = """"
        change.save()","1. Use `.get()` instead of `.filter()` to avoid returning a `QuerySet`.
2. Use `.values()` to only return the fields you need.
3. Use `.update()` to update multiple fields at once."
"def migrate_votes(apps, schema_editor):
    if not table_has_row(schema_editor.connection, ""trans_vote"", ""positive""):
        return
    Vote = apps.get_model(""trans"", ""Vote"")
    Vote.objects.filter(positive=True).update(value=1)
    Vote.objects.filter(positive=False).update(value=-1)","1. Use `django.db.transaction.atomic()` to ensure that the updates are performed atomically.
2. Use `django.db.models.F()` to avoid SQL injection.
3. Use `django.contrib.auth.models.User.objects.get_or_create()` to avoid race conditions."
"def migrate_priority(apps, schema_editor):
    Source = apps.get_model(""trans"", ""Source"")
    for source in Source.objects.exclude(priority=100).iterator():
        if source.check_flags:
            source.check_flags += "", ""
        source.check_flags += ""priority:{}"".format(200 - source.priority)
        source.save(update_fields=[""check_flags""])","1. Use `django.db.transaction.atomic()` to ensure that the entire migration is either committed or rolled back.
2. Use `django.db.models.F()` to avoid race conditions when updating the `check_flags` field.
3. Use `django.db.models.Q()` to filter the `Source` objects that are being updated."
"def migrate_tm(apps, schema_editor):
    apps.get_model(""trans"", ""Project"").objects.all().update(
        contribute_shared_tm=F(""use_shared_tm"")
    )","1. Use `F()` instead of `raw()` to avoid SQL injection.
2. Use `apps.get_model()` to get the model instance instead of hard-coding the model name.
3. Use `.update()` instead of `.save()` to avoid race conditions."
"    def prefetch(self):
        return self.select_related(
            'project'
        )","1. Use `.only()` to select only the fields needed, rather than `.select_related()`.
2. Use `.defer()` to defer loading of fields that are not needed immediately, rather than `.select_related()`.
3. Use `.prefetch_related()` to prefetch related objects eagerly, rather than `.select_related()`."
"    def prefetch(self):
        return self.select_related(
            'component', 'component__project', 'language'
        )","1. Use `prefetch_related()` instead of `select_related()` to avoid N+1 queries.
2. Use `.values()` to avoid loading unnecessary fields.
3. Use `.distinct()` to avoid loading duplicate rows."
"def show_project(request, project):
    obj = get_project(request, project)
    user = request.user

    dict_langs = Language.objects.filter(
        dictionary__project=obj
    ).annotate(Count('dictionary')).order()

    last_changes = Change.objects.prefetch().order().filter(project=obj)[:10]

    language_stats = sort_unicode(
        obj.stats.get_language_stats(), lambda x: force_text(x.language.name)
    )

    # Paginate components of project.
    all_components = obj.component_set.select_related().order()
    components = prefetch_stats(get_paginator(
        request, all_components
    ))

    return render(
        request,
        'project.html',
        {
            'allow_index': True,
            'object': obj,
            'project': obj,
            'dicts': dict_langs,
            'last_changes': last_changes,
            'last_changes_url': urlencode(
                {'project': obj.slug}
            ),
            'language_stats': language_stats,
            'search_form': SearchForm(),
            'whiteboard_form': optional_form(
                WhiteboardForm, user, 'project.edit', obj
            ),
            'delete_form': optional_form(
                DeleteForm, user, 'project.edit', obj, obj=obj
            ),
            'rename_form': optional_form(
                ProjectRenameForm, user, 'project.edit', obj,
                request=request, instance=obj
            ),
            'replace_form': optional_form(ReplaceForm, user, 'unit.edit', obj),
            'bulk_state_form': optional_form(
                BulkStateForm, user, 'translation.auto', obj,
                user=user, obj=obj
            ),
            'components': components,
            'licenses': ', '.join(sorted(
                all_components.exclude(license='').values_list('license', flat=True)
            )),
        }
    )","1. Use `user.has_perm()` to check if the user has permission to access the project.
2. Use `get_object_or_404()` to get the project object and avoid `DoesNotExist` exception.
3. Use `.order_by()` to sort the components by the `created` field."
"def show_component(request, project, component):
    obj = get_component(request, project, component)
    user = request.user

    last_changes = Change.objects.prefetch().order().filter(component=obj)[:10]

    return render(
        request,
        'component.html',
        {
            'allow_index': True,
            'object': obj,
            'project': obj.project,
            'translations': sort_objects(
                prefetch_stats(obj.translation_set.all())
            ),
            'show_language': 1,
            'reports_form': ReportsForm(),
            'last_changes': last_changes,
            'last_changes_url': urlencode(
                {'component': obj.slug, 'project': obj.project.slug}
            ),
            'language_count': Language.objects.filter(
                translation__component=obj
            ).distinct().count(),
            'replace_form': optional_form(ReplaceForm, user, 'unit.edit', obj),
            'bulk_state_form': optional_form(
                BulkStateForm, user, 'translation.auto', obj,
                user=user, obj=obj
            ),
            'whiteboard_form': optional_form(
                WhiteboardForm, user, 'component.edit', obj
            ),
            'delete_form': optional_form(
                DeleteForm, user, 'component.edit', obj, obj=obj
            ),
            'rename_form': optional_form(
                ComponentRenameForm, user, 'component.edit', obj,
                request=request, instance=obj
            ),
            'move_form': optional_form(
                ComponentMoveForm, user, 'component.edit', obj,
                request=request, instance=obj
            ),
            'search_form': SearchForm(),
            'alerts': obj.alert_set.order_by('name'),
        }
    )","1. Use `django.contrib.auth.decorators.login_required` to protect the view.
2. Use `django.views.decorators.csrf.csrf_protect` to protect against CSRF attacks.
3. Use `django.utils.html.escape` to escape user-provided data."
"def send_mails(mails):
    """"""Send multiple mails in single connection.""""""
    with get_connection() as connection:
        for mail in mails:
            email = EmailMultiAlternatives(
                settings.EMAIL_SUBJECT_PREFIX + mail['subject'],
                html2text(mail['body']),
                to=[mail['address']],
                headers=mail['headers'],
                connection=connection,
            )
            email.attach_alternative(mail['body'], 'text/html')
            email.send()","1. Use `contextlib.closing` to ensure that the connection is closed after sending the emails.
2. Use `email.utils.format_message()` to format the message instead of manually concatenating strings.
3. Use `email.utils.make_msgid()` to generate a unique message ID for each email."
"def report_error(error, request=None, extra_data=None, level='warning'):
    """"""Wrapper for error reporting

    This can be used for store exceptions in error reporting solutions as
    rollbar while handling error gracefully and giving user cleaner message.
    """"""
    if HAS_ROLLBAR and hasattr(settings, 'ROLLBAR'):
        rollbar.report_exc_info(
            request=request, extra_data=extra_data, level=level
        )

    if HAS_RAVEN and hasattr(settings, 'RAVEN_CONFIG'):
        raven_client.captureException(
            request=request, extra=extra_data, level=level
        )

    LOGGER.error(
        'Handled exception %s: %s',
        error.__class__.__name__,
        force_text(error)
    )","1. Use `raven.Client` instead of `raven_client` to avoid potential import errors.
2. Use `raven.Client.captureException` instead of `raven_client.captureException` to avoid passing `request` and `extra_data` as keyword arguments.
3. Use `raven.Client.setLevel` to set the desired logging level for errors."
"    def handle(self, *args, **options):
        """"""Automatic import of components.""""""
        # Get project
        try:
            project = Project.objects.get(slug=options['project'])
        except Project.DoesNotExist:
            raise CommandError('Project does not exist!')

        # Get main component
        main_component = None
        if options['main_component']:
            try:
                main_component = Component.objects.get(
                    project=project,
                    slug=options['main_component']
                )
            except Component.DoesNotExist:
                raise CommandError('Main component does not exist!')

        try:
            data = json.load(options['json-file'])
        except ValueError:
            raise CommandError('Failed to parse JSON file!')
        finally:
            options['json-file'].close()

        for item in data:
            if ('filemask' not in item or
                    'name' not in item):
                raise CommandError('Missing required fields in JSON!')

            if 'slug' not in item:
                item['slug'] = slugify(item['name'])

            if 'repo' not in item:
                if main_component is None:
                    raise CommandError(
                        'No main component and no repository URL!'
                    )
                item['repo'] = main_component.get_repo_link_url()

            item['project'] = project

            try:
                component = Component.objects.get(
                    slug=item['slug'], project=item['project']
                )
                self.stderr.write(
                    'Component {0} already exists'.format(component)
                )
                if options['ignore']:
                    continue
                if options['update']:
                    for key in item:
                        if key in ('project', 'slug'):
                            continue
                        setattr(component, key, item[key])
                    component.save()
                    continue
                raise CommandError(
                    'Component already exists, use --ignore or --update!'
                )

            except Component.DoesNotExist:
                component = Component.objects.create(**item)
                self.stdout.write(
                    'Imported {0} with {1} translations'.format(
                        component,
                        component.translation_set.count()
                    )
                )","1. Use `json.load()` with `object_hook` to avoid insecure default behavior.
2. Use `django.utils.six.ensure_str()` to convert strings to unicode.
3. Use `django.utils.six.moves.urllib.parse.quote()` to escape strings for URLs."
"    def __init__(self, storefile, template_store=None, language_code=None):
        """"""Create file format object, wrapping up translate-toolkit's store.""""""
        self.storefile = storefile
        # Load store
        if isinstance(storefile, TranslationStore):
            # Used by XLSX writer
            self.store = storefile
        else:
            self.store = self.load(storefile)
        # Check store validity
        if not self.is_valid(self.store):
            raise ValueError(
                'Invalid file format {0}'.format(self.store)
            )
        # Remember template
        self.template_store = template_store
        # Set language (needed for some which do not include this)
        if (language_code is not None and
                self.store.gettargetlanguage() is None):
            self.store.settargetlanguage(language_code)","1. Use proper type checking to ensure that the `storefile` parameter is a string or a `TranslationStore` object.
2. Validate the `storefile` parameter to ensure that it is a valid file path.
3. Use `self.store.settargetlanguage()` to set the target language for the translation store, instead of passing the `language_code` parameter directly to the constructor."
"    def handle(self, *args, **options):
        fulltext = Fulltext()
        # Optimize index
        if options['optimize']:
            self.optimize_index(fulltext)
            return
        # Optionally rebuild indices from scratch
        if options['clean']:
            fulltext.cleanup()

        if options['all']:
            self.process_all(fulltext)
        else:
            self.process_filtered(fulltext, **options)","1. Use `argparse` to parse command-line arguments instead of `**options`. This will help to prevent users from passing in malicious arguments.
2. Use `os.umask` to set the file mode creation mask for the index files. This will help to prevent users from creating files with world-writable permissions.
3. Use `logging` to log all errors and warnings. This will help to track down any problems that occur."
"def set_export_url(apps, schema_editor):
    SubProject = apps.get_model('trans', 'SubProject')
    matching = SubProject.objects.filter(
        vcs__in=SUPPORTED_VCS
    ).exclude(
        repo__startswith='weblate:/'
    )
    for component in matching:
        new_url = get_export_url(component)
        if component.git_export != new_url:
            component.git_export = new_url
            component.save()","1. Use `django.db.transaction.atomic()` to ensure that the entire migration is either committed or rolled back.
2. Use `django.db.models.F()` to update the `git_export` field instead of directly assigning a new value.
3. Use `django.contrib.auth.models.User.objects.get_or_create()` to get or create the user who owns the component."
"def forwards(apps, schema_editor):
    change_foreign_keys(apps, schema_editor,
                        ""auth"", ""User"",
                        ""weblate_auth"", ""User"")
    change_foreign_keys(apps, schema_editor,
                        ""auth"", ""Group"",
                        ""weblate_auth"", ""Group"")","1. Use `django.contrib.auth`'s `User` model instead of creating your own.
2. Use `django.contrib.auth`'s `Group` model instead of creating your own.
3. Use `django.contrib.auth.models.Permission` to grant users and groups permissions."
"def backwards(apps, schema_editor):
    change_foreign_keys(apps, schema_editor,
                        ""weblate_auth"", ""User"",
                        ""auth"", ""User"")
    change_foreign_keys(apps, schema_editor,
                        ""weblate_auth"", ""Group"",
                        ""auth"", ""Group"")","1. Use `django.contrib.auth.models.User` instead of `weblate_auth.models.User`.
2. Use `django.contrib.auth.models.Group` instead of `weblate_auth.models.Group`.
3. Use `django.contrib.auth.forms.UserCreationForm` instead of `weblate_auth.forms.UserCreationForm`."
"def change_foreign_keys(apps, schema_editor, from_app, from_model_name, to_app, to_model_name):
    from django.db import models
    FromModel = apps.get_model(from_app, from_model_name)
    ToModel = apps.get_model(to_app, to_model_name)

    fields = FromModel._meta.get_fields(include_hidden=True)

    for rel in fields:
        if not hasattr(rel, 'field') or not isinstance(rel.field, models.ForeignKey):
            continue
        fk_field = rel.field

        f_name, f_field_name, pos_args, kw_args = fk_field.deconstruct()

        # fk_field might have been the old or new one. We need to fix things up.
        old_field_kwargs = kw_args.copy()
        old_field_kwargs['to'] = FromModel
        old_field = fk_field.__class__(*pos_args, **old_field_kwargs)
        old_field.model = fk_field.model

        new_field_kwargs = kw_args.copy()
        new_field_kwargs['to'] = ToModel
        new_field = fk_field.__class__(*pos_args, **new_field_kwargs)
        new_field.model = fk_field.model

        if fk_field.model._meta.auto_created:
            # If this is a FK that is part of an M2M on the model itself,
            # we've already dealt with this, by virtue of the data migration
            # that populates the auto-created M2M field.
            if fk_field.model._meta.auto_created in [ToModel, FromModel]:
                continue

            # In this case (FK fields that are part of an autogenerated M2M),
            # the column name in the new M2M might be different to that in the
            # old M2M. This makes things much harder, and involves duplicating
            # some Django logic.

            # Build a correct ForeignKey field, as it should
            # have been on FromModel
            old_field.name = from_model_name.lower()
            old_field.column = ""{0}_id"".format(old_field.name)

            # build a correct ForeignKey field, as it should
            # be on ToModel
            new_field.name = to_model_name.lower()
            new_field.column = ""{0}_id"".format(new_field.name)
        else:
            old_field.name = fk_field.name
            old_field.column = fk_field.column
            new_field.name = fk_field.name
            new_field.column = fk_field.column

        schema_editor.alter_field(fk_field.model, old_field, new_field, strict=True)","1. Use `django.db.transaction.atomic()` to ensure that the changes are made atomically.
2. Use `django.db.models.signals.post_migrate` to catch the post-migration signal and update the foreign keys.
3. Use `django.db.models.signals.pre_migrate` to catch the pre-migration signal and remove the old foreign keys."
"    def handle(self, *args, **options):

        hours = options['age']

        if hours:
            age = timezone.now() - timedelta(hours=hours)

        for translation in self.get_translations(**options):
            if not translation.repo_needs_commit():
                continue

            if not hours:
                age = timezone.now() - timedelta(
                    hours=translation.subproject.commit_pending_age
                )

            last_change = translation.last_change
            if last_change is None:
                continue
            if last_change > age:
                continue

            if int(options['verbosity']) >= 1:
                self.stdout.write('Committing {0}'.format(translation))
            translation.commit_pending(None)","1. Use `django.utils.timezone.now()` instead of `timezone.now()` to avoid leaking information about the current time.
2. Use `django.utils.timezone.timedelta()` instead of `datetime.timedelta()` to avoid leaking information about the current time zone.
3. Use `django.contrib.auth.models.User.get_username()` instead of `user.username` to avoid leaking information about the user's password."
"    def handle(self, *args, **options):

        hours = options['age']

        if hours:
            age = timezone.now() - timedelta(hours=hours)

        for translation in self.get_translations(**options):
            if not translation.repo_needs_commit():
                continue

            if not hours:
                age = timezone.now() - timedelta(
                    hours=translation.subproject.commit_pending_age
                )

            last_change = translation.last_change
            if last_change is None:
                continue
            if last_change > age:
                continue

            if int(options['verbosity']) >= 1:
                self.stdout.write('Committing {0}'.format(translation))
            with transaction.atomic():
                translation.commit_pending(None)","1. Use `transaction.atomic()` to ensure that the commit is either successful or fails completely.
2. Use `timezone.now()` to get the current time, and compare it to the `last_change` field to ensure that the translation has not been changed recently.
3. Use `options['verbosity']` to control the amount of output that is displayed to the user."
"    def merge_upload(self, request, fileobj, overwrite, author=None,
                     merge_header=True, method='translate', fuzzy=''):
        """"""Top level handler for file uploads.""""""
        filecopy = fileobj.read()
        fileobj.close()

        # Strip possible UTF-8 BOM
        if filecopy[:3] == codecs.BOM_UTF8:
            filecopy = filecopy[3:]

        # Load backend file
        store = try_load(
            fileobj.name,
            filecopy,
            self.subproject.file_format_cls,
            self.subproject.template_store
        )

        # Optionally set authorship
        if author is None:
            author = get_author_name(request.user)

        # Check valid plural forms
        if hasattr(store.store, 'parseheader'):
            header = store.store.parseheader()
            if 'Plural-Forms' in header and \\
                    self.language.get_plural_form() != header['Plural-Forms']:
                raise Exception('Plural forms do not match the language.')

        if method in ('translate', 'fuzzy'):
            # Merge on units level
            with self.subproject.repository.lock:
                return self.merge_translations(
                    request,
                    store,
                    overwrite,
                    (method == 'fuzzy'),
                    fuzzy,
                    merge_header,
                )

        # Add as sugestions
        return self.merge_suggestions(request, store, fuzzy)","1. **Use proper error handling**. The code should catch and handle any errors that may occur, such as invalid file formats or missing permissions.
2. **Sanitize user input**. The code should sanitize all user input to prevent malicious code from being executed.
3. **Use secure coding practices**. The code should follow secure coding practices, such as using strong passwords and encryption."
"    def add_vote(self, translation, request, positive):
        '''
        Adds (or updates) vote for a suggestion.
        '''
        vote, created = Vote.objects.get_or_create(
            suggestion=self,
            user=request.user,
            defaults={'positive': positive}
        )
        if not created or vote.positive != positive:
            vote.positive = positive
            vote.save()

        # Automatic accepting
        required_votes = translation.subproject.suggestion_autoaccept
        if required_votes and self.get_num_votes() >= required_votes:
            self.accept(translation, request)","1. Use `django.contrib.auth.models.User` instead of `request.user` to get the user.
2. Use `vote.save()` to save the vote instead of `vote.positive = positive`.
3. Use `vote.user == request.user` to check if the user is the one who cast the vote."
"def post_login_handler(sender, request, user, **kwargs):
    '''
    Signal handler for setting user language and
    migrating profile if needed.
    '''

    # Warning about setting password
    if (getattr(user, 'backend', '').endswith('.EmailAuth') and
            not user.has_usable_password()):
        request.session['show_set_password'] = True

    # Ensure user has a profile
    profile = Profile.objects.get_or_create(user=user)[0]

    # Migrate django-registration based verification to python-social-auth
    if (user.has_usable_password() and
            not user.social_auth.filter(provider='email').exists()):
        social = user.social_auth.create(
            provider='email',
            uid=user.email,
        )
        VerifiedEmail.objects.create(
            social=social,
            email=user.email,
        )

    # Set language for session based on preferences
    set_lang(request, profile)","1. Use `user.is_authenticated` instead of `user.has_usable_password` to check if the user has a password.
2. Use `user.social_auth.filter(provider='email').exists()` to check if the user has a social auth record for email.
3. Use `social.id` instead of `user.email` as the primary key for the VerifiedEmail model."
"    def upload(self, request, project, language, fileobj, method):
        '''
        Handles dictionary upload.
        '''
        from weblate.trans.models.changes import Change
        store = AutoFormat.parse(fileobj)

        ret = 0

        # process all units
        for dummy, unit in store.iterate_merge(False):
            source = unit.get_source()
            target = unit.get_target()

            # Ignore too long words
            if len(source) > 200 or len(target) > 200:
                continue

            # Get object
            word, created = self.get_or_create(
                project=project,
                language=language,
                source=source,
                defaults={
                    'target': target,
                },
            )

            # Already existing entry found
            if not created:
                # Same as current -> ignore
                if target == word.target:
                    continue
                if method == 'add':
                    # Add word
                    word = self.create(
                        request,
                        action=Change.ACTION_DICTIONARY_UPLOAD,
                        project=project,
                        language=language,
                        source=source,
                        target=target
                    )
                elif method == 'overwrite':
                    # Update word
                    word.target = target
                    word.save()

            ret += 1

        return ret","1. Use `django.utils.translation.trans_real` instead of `gettext` to prevent XSS attacks.
2. Validate the input data to prevent injection attacks.
3. Use `django.contrib.auth.models.User` instead of `AnonymousUser` to restrict access to the upload function."
"def get_clean_env(extra=None):
    """"""
    Returns cleaned up environment for subprocess execution.
    """"""
    environ = {}
    if extra is not None:
        environ.update(extra)
    variables = ('HOME', 'PATH', 'LANG', 'LD_LIBRARY_PATH')
    for var in variables:
        if var in os.environ:
            environ[var] = os.environ[var]
    return environ","1. Use a dedicated, non-root user for running subprocesses.
2. Remove all environment variables that are not required by the subprocess.
3. Use a secure subprocess execution method, such as `subprocess.run()`."
"    def _get_version(cls):
        """"""
        Returns VCS program version.
        """"""
        output = cls._popen(['version', '-q'])
        return cls.VERSION_RE.match(output).group(1)","1. Use `subprocess.check_output` instead of `subprocess.Popen` to avoid leaving the process open.
2. Use `re.search` instead of `re.match` to avoid accidentally matching empty strings.
3. Sanitize the version string before returning it to the user."
"    def get_last_remote_commit(self):
        '''
        Returns latest remote commit we know.
        '''
        return self.git_repo.commit('origin/%s' % self.branch)","1. Use `fetch` instead of `commit` to get the latest remote commit.
2. Use `verify_commit` to verify the commit signature before using it.
3. Use `delete_branch` to delete the local branch after using it."
"    def get_last_remote_commit(self):
        '''
        Returns latest remote commit we know.
        '''
        try:
            return self.git_repo.commit('origin/%s' % self.branch)
        except ODBError:
            # Try to reread git database in case our in memory object is not
            # up to date with it.
            self.reset_git_repo()
            return self.git_repo.commit('origin/%s' % self.branch)","1. Use `get_remote_ref` instead of `commit('origin/%s' % self.branch)` to get the latest remote commit.
2. Handle the `ODBError` exception more gracefully.
3. Consider using a more secure default branch name than `master`."
"def mail_admins_contact(subject, message, context, sender):
    '''
    Sends a message to the admins, as defined by the ADMINS setting.
    '''
    if not settings.ADMINS:
        return

    mail = EmailMultiAlternatives(
        u'%s%s' % (settings.EMAIL_SUBJECT_PREFIX, subject % context),
        message % context,
        to=[a[1] for a in settings.ADMINS],
        headers={'Reply-To': sender},
    )

    mail.send(fail_silently=False)","1. Use `email_safe` to ensure that the subject and message are safe to send.
2. Use `safe_email_list` to ensure that the list of recipients is safe.
3. Set `fail_silently` to `True` to prevent errors from being silently ignored."
"    def notify_user(self, notification, translation_obj,
                    context=None, headers=None):
        '''
        Wrapper for sending notifications to user.
        '''
        if context is None:
            context = {}
        if headers is None:
            headers = {}

        # Check whether user is still allowed to access this project
        if not translation_obj.has_acl(self):
            return
        # Actually send notification
        send_notification_email(
            self.language,
            self.user.email,
            notification,
            translation_obj,
            context,
            headers
        )","1. Use prepared statements instead of building queries manually to avoid SQL injection attacks.
2. Sanitize user input before using it in any way.
3. Use strong passwords for all user accounts and never reuse passwords across different sites."
"def performance(request):
    '''
    Shows performance tuning tips.
    '''
    checks = []
    # Check for debug mode
    checks.append((
        _('Debug mode'),
        not settings.DEBUG,
        'production-debug',
    ))
    # Check for domain configuration
    checks.append((
        _('Site domain'),
        Site.objects.get_current() != 'example.net',
        'production-site',
    ))
    # Check database being used
    checks.append((
        _('Database backend'),
        ""sqlite"" not in settings.DATABASES['default']['ENGINE'],
        'production-database',
    ))
    # Check configured admins
    checks.append((
        _('Site administrator'),
        len(settings.ADMINS) > 0,
        'production-admins',
    ))
    # Check offloading indexing
    checks.append((
        # Translators: Indexing is postponed to cron job
        _('Indexing offloading'),
        settings.OFFLOAD_INDEXING,
        'production-indexing',
    ))
    # Check for sane caching
    cache = settings.CACHES['default']['BACKEND'].split('.')[-1]
    if cache in ['MemcachedCache', 'DatabaseCache']:
        # We consider these good
        cache = True
    elif cache in ['DummyCache']:
        # This one is definitely bad
        cache = False
    else:
        # These might not be that bad
        cache = None
    checks.append((
        _('Django caching'),
        cache,
        'production-cache',
    ))
    # Check email setup
    default_mails = (
        'root@localhost',
        'webmaster@localhost',
        'noreply@weblate.org'
    )
    checks.append((
        _('Email addresses'),
        (
            settings.SERVER_EMAIL not in default_mails
            and settings.DEFAULT_FROM_EMAIL not in default_mails
        ),
        'production-email',
    ))
    return render_to_response(
        ""admin/performance.html"",
        RequestContext(
            request,
            {
                'checks': checks,
            }
        )
    )","1. Use a production-ready database engine such as PostgreSQL or MySQL.
2. Use a secure caching backend such as Memcached or Redis.
3. Use email addresses that are not easily guessed by attackers."
"def update_subproject(request, project, subproject):
    '''
    API hook for updating git repos.
    '''
    if not settings.ENABLE_HOOKS:
        return HttpResponseNotAllowed([])
    obj = get_object_or_404(SubProject, slug=subproject, project__slug=project)
    thread = threading.Thread(target=obj.do_update)
    thread.start()
    return HttpResponse('update triggered')","1. Use `require_POST` to ensure that the request is a POST request.
2. Sanitize the `project` and `subproject` parameters to prevent XSS attacks.
3. Use `django-guardian` to restrict access to the `update_subproject` view to only users who have permission to update subprojects."
"def update_project(request, project):
    '''
    API hook for updating git repos.
    '''
    if not settings.ENABLE_HOOKS:
        return HttpResponseNotAllowed([])
    obj = get_object_or_404(Project, slug=project)
    thread = threading.Thread(target=obj.do_update)
    thread.start()
    return HttpResponse('update triggered')","1. Use `is_authenticated()` to check if the user is authenticated before calling `do_update()`.
2. Sanitize the `project` parameter to prevent malicious users from injecting code into the request.
3. Use `thread.daemon = True` to prevent the thread from blocking the main thread."
"def github_hook(request):
    '''
    API to handle commit hooks from Github.
    '''
    if not settings.ENABLE_HOOKS:
        return HttpResponseNotAllowed([])
    if request.method != 'POST':
        return HttpResponseNotAllowed(['POST'])
    try:
        data = json.loads(request.POST['payload'])
    except (ValueError, KeyError):
        return HttpResponseBadRequest('could not parse json!')
    try:
        repo = 'git://github.com/%s/%s.git' % (
            data['repository']['owner']['name'],
            data['repository']['name'],
        )
        branch = data['ref'].split('/')[-1]
    except KeyError:
        return HttpResponseBadRequest('could not parse json!')
    logger.info(
        'received GitHub notification on repository %s, branch %s',
        repo,
        branch
    )
    for obj in SubProject.objects.filter(repo=repo, branch=branch):
        logger.info('GitHub notification will update %s', obj)
        thread = threading.Thread(target=obj.do_update)
        thread.start()

    return HttpResponse('update triggered')","1. Use `django.utils.http.Http404` instead of `HttpResponseNotAllowed` to return a more specific error message.
2. Validate the request body using `jsonschema` to ensure that it contains the expected data.
3. Sanitize the repo and branch names before using them in the database to prevent SQL injection attacks."
"def title(request):
    return {'site_title': settings.SITE_TITLE}","1. Use `django.utils.safestring.mark_safe` to escape the value of `settings.SITE_TITLE` before returning it to the template.
2. Use `django.http.Http404` to raise an error if `settings.SITE_TITLE` is not set.
3. Consider using a more secure way to store the site title, such as using a secret key."
"def mt(request):
    return {
        'apertium_api_key': settings.MT_APERTIUM_KEY,
        'microsoft_api_key': settings.MT_MICROSOFT_KEY,
    }","1. **Use environment variables to store API keys.** This will prevent them from being accidentally leaked in the source code.
2. **Use HTTPS.** This will encrypt the communication between the client and the server, making it more difficult for attackers to intercept and steal the API keys.
3. **Log all API requests.** This will help you track down any unauthorized access to your API keys."
"    def title(self):
        return _('Recent changes in %s') % settings.SITE_TITLE","1. Use `django.utils.translation.gettext_lazy` to avoid unnecessary string concatenation.
2. Use `django.utils.safestring.mark_safe` to escape HTML output.
3. Use `django.contrib.auth.decorators.login_required` to protect the view from unauthorized access."
"    def description(self):
        return _('All recent changes made using Weblate in %s.') % settings.SITE_TITLE","1. Use `django.utils.translation.gettext_lazy()` instead of `_()` to avoid potential translation injection attacks.
2. Use `django.utils.safestring.mark_safe()` to escape HTML output.
3. Use `django.contrib.admin.decorators.render_to_string()` to render the admin template instead of manually calling `django.template.Template.render()`."
"    def add_to_index(self, unit, source=True):
        '''
        Updates/Adds to all indices given unit.
        '''
        if settings.OFFLOAD_INDEXING:
            from weblate.trans.models import IndexUpdate
            IndexUpdate.objects.get_or_create(unit=unit, source=source)
            return

        writer_target = FULLTEXT_INDEX.target_writer(
            unit.translation.language.code
        )
        writer_source = FULLTEXT_INDEX.source_writer()

        if source:
            self.add_to_source_index(
                unit.checksum,
                unit.source,
                unit.context,
                writer_source)
        self.add_to_target_index(
            unit.checksum,
            unit.target,
            writer_target)","1. Use `django.utils.translation.get_language()` instead of `unit.translation.language.code` to get the language code. This will protect against SQL injection attacks.
2. Use `django.utils.text.slugify()` to create a unique identifier for the unit. This will prevent duplicate entries in the index.
3. Use `django.db.transaction.atomic()` to ensure that all changes to the index are made atomically. This will prevent data corruption in the event of a system failure."
"    def search(self, query, source=True, context=True, translation=True, checksums=False):
        '''
        Performs full text search on defined set of fields.

        Returns queryset unless checksums is set.
        '''
        ret = set()
        if source or context:
            with FULLTEXT_INDEX.source_searcher(not settings.OFFLOAD_INDEXING) as searcher:
                if source:
                    results = self.__search(
                        searcher,
                        'source',
                        SOURCE_SCHEMA,
                        query
                    )
                    ret = ret.union(results)
                if context:
                    results = self.__search(
                        searcher,
                        'context',
                        SOURCE_SCHEMA,
                        query
                    )
                    ret = ret.union(results)

        if translation:
            sample = self.all()[0]
            with FULLTEXT_INDEX.target_searcher(sample.translation.language.code, not settings.OFFLOAD_INDEXING) as searcher:
                results = self.__search(
                    searcher,
                    'target',
                    TARGET_SCHEMA,
                    query
                )
                ret = ret.union(results)

        if checksums:
            return ret

        return self.filter(checksum__in=ret)","1. Use `django.contrib.postgres.search` instead of the `pysolr` library.
2. Use `django-filter` to sanitize user input.
3. Use `django-rest-framework-jwt` to protect your API endpoints."
"    def similar(self, unit):
        '''
        Finds similar units to current unit.
        '''
        ret = set([unit.checksum])
        with FULLTEXT_INDEX.source_searcher(not settings.OFFLOAD_INDEXING) as searcher:
            # Extract up to 10 terms from the source
            terms = [kw for kw, score in searcher.key_terms_from_text('source', unit.source, numterms=10) if not kw in IGNORE_SIMILAR]
            cnt = len(terms)
            # Try to find at least configured number of similar strings, remove up to 4 words
            while len(ret) < settings.SIMILAR_MESSAGES and cnt > 0 and len(terms) - cnt < 4:
                for search in itertools.combinations(terms, cnt):
                    results = self.search(
                        ' '.join(search),
                        True,
                        False,
                        False,
                        True
                    )
                    ret = ret.union(results)
                cnt -= 1

        return self.filter(
            translation__subproject__project=unit.translation.subproject.project,
            translation__language=unit.translation.language,
            checksum__in=ret
        ).exclude(
            target__in=['', unit.target]
        )","1. Use `searcher.search()` instead of `searcher.key_terms_from_text()` to avoid leaking information about the number of terms in the source.
2. Use `searcher.filter()` instead of `itertools.combinations()` to avoid generating all possible combinations of terms.
3. Use `translation.subproject.project` instead of `translation.project` to avoid leaking information about the project ID."
"def create_source_index():
    return create_in(
        settings.WHOOSH_INDEX,
        schema=SOURCE_SCHEMA,
        indexname='source'
    )","1. Use `settings.SECRET_KEY` instead of a hard-coded value.
2. Use `create_in()` with the `**kwargs` parameter to specify the index name.
3. Use `settings.WHOOSH_INDEX` instead of a hard-coded path."
"def create_target_index(lang):
    return create_in(
        settings.WHOOSH_INDEX,
        schema=TARGET_SCHEMA,
        indexname='target-%s' % lang
    )","1. Use `settings.SECRET_KEY` instead of a hard-coded string for the index's password.
2. Use `create_in_memory()` instead of `create_in()` to create the index in memory, which is more secure.
3. Use `index.close()` to close the index when you're finished with it to free up resources."
"def create_index(sender=None, **kwargs):
    if not os.path.exists(settings.WHOOSH_INDEX):
        os.mkdir(settings.WHOOSH_INDEX)
        create_source_index()","1. Use `os.makedirs` instead of `os.mkdir` to create the directory, as it will check if the directory already exists and will not throw an error if it does.
2. Use `os.chmod` to set the permissions of the directory to `0777`, which will allow anyone to read, write, and execute files in the directory.
3. Use `os.chown` to change the ownership of the directory to `root`, which will give root user full control over the directory."
"    def source(self):
        '''
        Returns source index.
        '''
        if self._source is None:
            try:
                self._source = open_dir(
                    settings.WHOOSH_INDEX,
                    indexname='source'
                )
            except whoosh.index.EmptyIndexError:
                self._source = create_source_index()
            except IOError:
                # eg. path does not exist
                self._source = create_source_index()
        return self._source","1. Use `try ... except` blocks to catch errors and handle them gracefully.
2. Use `os.path.exists()` to check if a path exists before trying to open it.
3. Use `whoosh.index.create_index()` to create a new index if one does not already exist."
"    def target(self, lang):
        '''
        Returns target index for given language.
        '''
        if not lang in self._target:
            try:
                self._target[lang] = open_dir(
                    settings.WHOOSH_INDEX,
                    indexname='target-%s' % lang
                )
            except whoosh.index.EmptyIndexError:
                self._target[lang] = create_target_index(lang)
        return self._target[lang]","1. Use `with` statement to open the index directory to avoid resource leaks.
2. Check if the index exists before creating it to avoid duplicate index creation.
3. Use `os.makedirs` to create the index directory if it does not exist to avoid permission errors."
"def site_title(value):
    '''
    Returns site title
    '''
    return settings.SITE_TITLE","1. **Use `django.utils.safestring.mark_safe` to escape HTML output.** This will prevent attackers from injecting malicious code into the site title.
2. **Use `django.contrib.admin.sites.site.site_title` instead of a global variable.** This will make it easier to change the site title in the future, and it will also prevent attackers from accessing the site title if they are able to exploit a vulnerability in the admin site.
3. **Use `django.contrib.sites.models.Site.objects.get_current()` to get the current site.** This will prevent attackers from accessing the site title for a different site than the one they are currently logged into."
"def js_config(request):
    '''
    Generates settings for javascript. Includes things like
    API keys for translaiton services or list of languages they
    support.
    '''
    # Apertium support
    if settings.MT_APERTIUM_KEY is not None and settings.MT_APERTIUM_KEY != '':
        try:
            listpairs = urllib2.urlopen('http://api.apertium.org/json/listPairs?key=%s' % settings.MT_APERTIUM_KEY)
            pairs = listpairs.read()
            parsed = json.loads(pairs)
            apertium_langs = [p['targetLanguage'] for p in parsed['responseData'] if p['sourceLanguage'] == 'en']
        except Exception as e:
            logger.error('failed to get supported languages from Apertium, using defaults (%s)', str(e))
            apertium_langs = ['gl', 'ca', 'es', 'eo']
    else:
        apertium_langs = None

    # Microsoft translator support
    if settings.MT_MICROSOFT_KEY is not None and settings.MT_MICROSOFT_KEY != '':
        try:
            listpairs = urllib2.urlopen('http://api.microsofttranslator.com/V2/Http.svc/GetLanguagesForTranslate?appID=%s' % settings.MT_MICROSOFT_KEY)
            data = listpairs.read()
            parsed = ElementTree.fromstring(data)
            microsoft_langs = [p.text for p in parsed.getchildren()]
        except Exception as e:
            logger.error('failed to get supported languages from Microsoft, using defaults (%s)', str(e))
            microsoft_langs = [
                'ar', 'bg', 'ca', 'zh-CHS', 'zh-CHT', 'cs', 'da', 'nl', 'en',
                'et', 'fi', 'fr', 'de', 'el', 'ht', 'he', 'hi', 'mww', 'hu',
                'id', 'it', 'ja', 'ko', 'lv', 'lt', 'no', 'fa', 'pl', 'pt',
                'ro', 'ru', 'sk', 'sl', 'es', 'sv', 'th', 'tr', 'uk', 'vi'
            ]
    else:
        microsoft_langs = None

    return render_to_response('js/config.js', RequestContext(request, {
            'apertium_langs': apertium_langs,
            'microsoft_langs': microsoft_langs,
        }),
        mimetype = 'application/javascript')","1. Use `json.dumps()` to escape the response data from the API calls.
2. Use `urllib.request.urlopen()` with `contextlib.closing()` to ensure that the connection is closed after the request is complete.
3. Use `django.template.RequestContext()` to pass the request object to the template engine, which will make it available to the template."
"def render(request, project, widget='287x66', color=None, lang=None):
    obj = get_object_or_404(Project, slug=project)

    # Handle language parameter
    if lang is not None:
        try:
            django.utils.translation.activate(lang)
        except:
            # Ignore failure on activating language
            pass
        try:
            lang = Language.objects.get(code=lang)
        except Language.DoesNotExist:
            lang = None

    percent = obj.get_translated_percent(lang)

    # Get widget data
    try:
        widget_data = WIDGETS[widget]
    except KeyError:
        raise Http404()

    # Get widget color
    if color not in widget_data['colors']:
        color = widget_data['default']

    # Background 287 x 66, logo 64 px
    surface = cairo.ImageSurface.create_from_png(
        os.path.join(settings.WEB_ROOT, 'media', widget_data['name'] % {
            'color': color,
            'widget': widget,
        })
    )
    ctx = cairo.Context(surface)

    # Setup
    ctx.set_line_width(widget_data['colors'][color]['line'])

    # Progress bar rendering
    if widget_data['progress'] is not None:
        # Filled bar
        ctx.new_path()
        ctx.set_source_rgb (*widget_data['colors'][color]['bar'])
        if widget_data['progress']['horizontal']:
            ctx.rectangle(
                widget_data['progress']['x'],
                widget_data['progress']['y'],
                widget_data['progress']['width'] / 100.0 * percent,
                widget_data['progress']['height']
            )
        else:
            diff = widget_data['progress']['height'] / 100.0 * (100 - percent)
            ctx.rectangle(
                widget_data['progress']['x'],
                widget_data['progress']['y'] + diff,
                widget_data['progress']['width'],
                widget_data['progress']['height'] - diff
            )
        ctx.fill()

        # Progress border
        ctx.new_path()
        ctx.set_source_rgb (*widget_data['colors'][color]['border'])
        ctx.rectangle(
            widget_data['progress']['x'],
            widget_data['progress']['y'],
            widget_data['progress']['width'],
            widget_data['progress']['height']
        )
        ctx.stroke()

    # Text rendering
    # Set text color
    ctx.set_source_rgb (*widget_data['colors'][color]['text'])

    # Create pango context
    pangocairo_context = pangocairo.CairoContext(ctx)
    pangocairo_context.set_antialias(cairo.ANTIALIAS_SUBPIXEL)

    # Text format strings
    params =  {
        'name': obj.name,
        'count': obj.get_total(),
        'languages': obj.get_language_count(),
        'percent': percent,
    }

    for line in widget_data['text']:
        # Format text
        text = line['text']
        if lang is not None and 'text_lang' in line:
            text = line['text_lang']
            if 'English' in text:
                text = text.replace('English', lang.name)
        text = text % params

        font_size = line['font_size']

        # Render text
        layout = render_text(pangocairo_context, line, text, params, font_size)

        # Fit text to image if it is too big
        extent = layout.get_pixel_extents()
        width = surface.get_width()
        while extent[1][2] + line['pos'][0] > width and font_size > 4:
            font_size -= 1
            layout = render_text(
                pangocairo_context,
                line,
                text,
                params,
                font_size
            )
            extent = layout.get_pixel_extents()

        # Set position
        ctx.move_to(*line['pos'])

        # Render to cairo context
        pangocairo_context.update_layout(layout)
        pangocairo_context.show_layout(layout)

    # Render PNG
    out = StringIO()
    surface.write_to_png(out)
    data = out.getvalue()

    return HttpResponse(content_type='image/png', content=data)","1. Use `get_object_or_404()` instead of `get_object()` to avoid a `DoesNotExist` exception.
2. Use `{% csrf_token %}` to protect against CSRF attacks.
3. Use `json.dumps()` to escape any user-provided data before sending it to the client."
"def store_user_details(sender, user, request, **kwargs):
    '''
    Stores user details on registration, here we rely on
    validation done by RegistrationForm.
    '''
    user.first_name = request.POST['first_name']
    user.last_name = request.POST['last_name']
    user.save()","1. Use `django.contrib.auth.models.User.full_clean()` to validate user input.
2. Use `django.contrib.auth.models.UserManager.create_user()` to create a new user.
3. Hash the user's password before saving it to the database."
"def get_tokenizer(model_name: str, **kwargs) -> transformers.PreTrainedTokenizer:
    cache_key = (model_name, frozenset(kwargs.items()))

    global _tokenizer_cache
    tokenizer = _tokenizer_cache.get(cache_key, None)
    if tokenizer is None:
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_name,
            **kwargs,
        )
        _tokenizer_cache[cache_key] = tokenizer
    return tokenizer","1. Use `model_name` as a key in a dictionary to cache the tokenizer. This will prevent the tokenizer from being re-created every time it is called.
2. Use `frozenset(kwargs.items())` to create a hashable key for the cache. This will prevent the cache from being invalidated when the kwargs are changed.
3. Use `None` as a default value for the `tokenizer` variable. This will ensure that the tokenizer is created if it is not found in the cache."
"    def __init__(
        self,
        model_name: str,
        *,
        max_length: int = None,
        sub_module: str = None,
        train_parameters: bool = True,
        last_layer_only: bool = True,
        override_weights_file: Optional[str] = None,
        override_weights_strip_prefix: Optional[str] = None,
        gradient_checkpointing: Optional[bool] = None,
        tokenizer_kwargs: Optional[Dict[str, Any]] = None,
        transformer_kwargs: Optional[Dict[str, Any]] = None,
    ) -> None:
        super().__init__()
        from allennlp.common import cached_transformers

        self.transformer_model = cached_transformers.get(
            model_name,
            True,
            override_weights_file=override_weights_file,
            override_weights_strip_prefix=override_weights_strip_prefix,
            **(transformer_kwargs or {}),
        )

        if gradient_checkpointing is not None:
            self.transformer_model.config.update({""gradient_checkpointing"": gradient_checkpointing})

        self.config = self.transformer_model.config
        if sub_module:
            assert hasattr(self.transformer_model, sub_module)
            self.transformer_model = getattr(self.transformer_model, sub_module)
        self._max_length = max_length

        # I'm not sure if this works for all models; open an issue on github if you find a case
        # where it doesn't work.
        self.output_dim = self.config.hidden_size

        self._scalar_mix: Optional[ScalarMix] = None
        if not last_layer_only:
            self._scalar_mix = ScalarMix(self.config.num_hidden_layers)
            self.config.output_hidden_states = True

        tokenizer = PretrainedTransformerTokenizer(
            model_name,
            tokenizer_kwargs=tokenizer_kwargs,
        )
        self._num_added_start_tokens = len(tokenizer.single_sequence_start_tokens)
        self._num_added_end_tokens = len(tokenizer.single_sequence_end_tokens)
        self._num_added_tokens = self._num_added_start_tokens + self._num_added_end_tokens

        if not train_parameters:
            for param in self.transformer_model.parameters():
                param.requires_grad = False","1. Use `torch.nn.Identity` instead of `lambda x: x` to avoid creating a new object for each forward pass.
2. Use `torch.no_grad()` to disable gradient calculation for parts of the code that don't need it.
3. Check the input arguments to make sure they are valid."
"def sanitize(x: Any) -> Any:
    """"""
    Sanitize turns PyTorch and Numpy types into basic Python types so they
    can be serialized into JSON.
    """"""
    # Import here to avoid circular references
    from allennlp.data.tokenizers.token import Token

    if isinstance(x, (str, float, int, bool)):
        # x is already serializable
        return x
    elif isinstance(x, torch.Tensor):
        # tensor needs to be converted to a list (and moved to cpu if necessary)
        return x.cpu().tolist()
    elif isinstance(x, numpy.ndarray):
        # array needs to be converted to a list
        return x.tolist()
    elif isinstance(x, numpy.number):
        # NumPy numbers need to be converted to Python numbers
        return x.item()
    elif isinstance(x, dict):
        # Dicts need their values sanitized
        return {key: sanitize(value) for key, value in x.items()}
    elif isinstance(x, numpy.bool_):
        # Numpy bool_ need to be converted to python bool.
        return bool(x)
    elif isinstance(x, (spacy.tokens.Token, Token)):
        # Tokens get sanitized to just their text.
        return x.text
    elif isinstance(x, (list, tuple)):
        # Lists and Tuples need their values sanitized
        return [sanitize(x_i) for x_i in x]
    elif x is None:
        return ""None""
    elif hasattr(x, ""to_json""):
        return x.to_json()
    else:
        raise ValueError(
            f""Cannot sanitize {x} of type {type(x)}. ""
            ""If this is your own custom class, add a `to_json(self)` method ""
            ""that returns a JSON-like object.""
        )","1. Use `torch.jit.script` to sanitize tensors.
2. Use `numpy.ndarray.tolist` to sanitize arrays.
3. Use `spacy.tokens.Token.text` to sanitize tokens."
"    def _try_train(self) -> Dict[str, Any]:
        try:
            epoch_counter = self._restore_checkpoint()
        except RuntimeError:
            traceback.print_exc()
            raise ConfigurationError(
                ""Could not recover training from the checkpoint.  Did you mean to output to ""
                ""a different serialization directory or delete the existing serialization ""
                ""directory?""
            )

        training_util.enable_gradient_clipping(self.model, self._grad_clipping)

        logger.info(""Beginning training."")

        val_metrics: Dict[str, float] = {}
        this_epoch_val_metric: float
        metrics: Dict[str, Any] = {}
        epochs_trained = 0
        training_start_time = time.time()

        metrics[""best_epoch""] = self._metric_tracker.best_epoch
        for key, value in self._metric_tracker.best_epoch_metrics.items():
            metrics[""best_validation_"" + key] = value

        for callback in self._epoch_callbacks:
            callback(self, metrics={}, epoch=-1, is_master=self._master)

        for epoch in range(epoch_counter, self._num_epochs):
            epoch_start_time = time.time()
            train_metrics = self._train_epoch(epoch)

            if self._master and self._checkpointer is not None:
                self._checkpointer.save_checkpoint(epoch, self, save_model_only=True)

            # Wait for the master to finish saving the model checkpoint
            if self._distributed:
                dist.barrier()

            # get peak of memory usage
            for key, value in train_metrics.items():
                if key.startswith(""gpu_"") and key.endswith(""_memory_MB""):
                    metrics[""peak_"" + key] = max(metrics.get(""peak_"" + key, 0), value)
                elif key.startswith(""worker_"") and key.endswith(""_memory_MB""):
                    metrics[""peak_"" + key] = max(metrics.get(""peak_"" + key, 0), value)

            if self._validation_data_loader is not None:
                with torch.no_grad():
                    # We have a validation set, so compute all the metrics on it.
                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)

                    # It is safe again to wait till the validation is done. This is
                    # important to get the metrics right.
                    if self._distributed:
                        dist.barrier()

                    val_metrics = training_util.get_metrics(
                        self.model,
                        val_loss,
                        val_reg_loss,
                        batch_loss=None,
                        batch_reg_loss=None,
                        num_batches=num_batches,
                        reset=True,
                        world_size=self._world_size,
                        cuda_device=self.cuda_device,
                    )

                    # Check validation metric for early stopping
                    this_epoch_val_metric = val_metrics[self._validation_metric]
                    self._metric_tracker.add_metric(this_epoch_val_metric)

                    if self._metric_tracker.should_stop_early():
                        logger.info(""Ran out of patience.  Stopping training."")
                        break

            if self._master:
                self._tensorboard.log_metrics(
                    train_metrics, val_metrics=val_metrics, log_to_console=True, epoch=epoch + 1
                )  # +1 because tensorboard doesn't like 0

            # Create overall metrics dict
            training_elapsed_time = time.time() - training_start_time
            metrics[""training_duration""] = str(datetime.timedelta(seconds=training_elapsed_time))
            metrics[""training_start_epoch""] = epoch_counter
            metrics[""training_epochs""] = epochs_trained
            metrics[""epoch""] = epoch

            for key, value in train_metrics.items():
                metrics[""training_"" + key] = value
            for key, value in val_metrics.items():
                metrics[""validation_"" + key] = value

            if self._metric_tracker.is_best_so_far():
                # Update all the best_ metrics.
                # (Otherwise they just stay the same as they were.)
                metrics[""best_epoch""] = epoch
                for key, value in val_metrics.items():
                    metrics[""best_validation_"" + key] = value

                self._metric_tracker.best_epoch_metrics = val_metrics

            if self._serialization_dir and self._master:
                common_util.dump_metrics(
                    os.path.join(self._serialization_dir, f""metrics_epoch_{epoch}.json""),
                    metrics,
                )

            # The Scheduler API is agnostic to whether your schedule requires a validation metric -
            # if it doesn't, the validation metric passed here is ignored.
            if self._learning_rate_scheduler:
                self._learning_rate_scheduler.step(this_epoch_val_metric)
            if self._momentum_scheduler:
                self._momentum_scheduler.step(this_epoch_val_metric)

            if self._master and self._checkpointer is not None:
                self._checkpointer.save_checkpoint(
                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()
                )

            # Wait for the master to finish saving the checkpoint
            if self._distributed:
                dist.barrier()

            for callback in self._epoch_callbacks:
                callback(self, metrics=metrics, epoch=epoch, is_master=self._master)

            epoch_elapsed_time = time.time() - epoch_start_time
            logger.info(""Epoch duration: %s"", datetime.timedelta(seconds=epoch_elapsed_time))

            if epoch < self._num_epochs - 1:
                training_elapsed_time = time.time() - training_start_time
                estimated_time_remaining = training_elapsed_time * (
                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1
                )
                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))
                logger.info(""Estimated training time remaining: %s"", formatted_time)

            epochs_trained += 1

        for callback in self._end_callbacks:
            callback(self, metrics=metrics, epoch=epoch, is_master=self._master)

        # Load the best model state before returning
        best_model_state = (
            None if self._checkpointer is None else self._checkpointer.best_model_state()
        )
        if best_model_state:
            self.model.load_state_dict(best_model_state)

        return metrics","1. Use `torch.no_grad()` to disable gradient calculation when not needed.
2. Use `torch.cuda.empty_cache()` to free up GPU memory after training.
3. Use `torch.save()` to save the model to a file instead of `torch.load()`, which can lead to security vulnerabilities."
"    def _intra_word_tokenize(
        self, string_tokens: List[str]
    ) -> Tuple[List[Token], List[Optional[Tuple[int, int]]]]:
        tokens: List[Token] = []
        offsets: List[Optional[Tuple[int, int]]] = []
        for token_string in string_tokens:
            wordpieces = self.tokenizer.encode_plus(
                token_string,
                add_special_tokens=False,
                return_tensors=None,
                return_offsets_mapping=False,
                return_attention_mask=False,
                return_token_type_ids=False,
            )
            wp_ids = wordpieces[""input_ids""]

            if len(wp_ids) > 0:
                offsets.append((len(tokens), len(tokens) + len(wp_ids) - 1))
                tokens.extend(
                    Token(text=wp_text, text_id=wp_id)
                    for wp_id, wp_text in zip(wp_ids, self.tokenizer.convert_ids_to_tokens(wp_ids))
                )
            else:
                offsets.append(None)
        return tokens, offsets","1. Use `return_offsets_mapping=True` to get the start and end indices of each wordpiece.
2. Use `return_attention_mask=True` to get the attention mask for each wordpiece.
3. Use `return_token_type_ids=True` to get the token type IDs for each wordpiece."
"def add_sentence_boundary_token_ids(
    tensor: torch.Tensor, mask: torch.BoolTensor, sentence_begin_token: Any, sentence_end_token: Any
) -> Tuple[torch.Tensor, torch.BoolTensor]:
    """"""
    Add begin/end of sentence tokens to the batch of sentences.
    Given a batch of sentences with size `(batch_size, timesteps)` or
    `(batch_size, timesteps, dim)` this returns a tensor of shape
    `(batch_size, timesteps + 2)` or `(batch_size, timesteps + 2, dim)` respectively.

    Returns both the new tensor and updated mask.

    # Parameters

    tensor : `torch.Tensor`
        A tensor of shape `(batch_size, timesteps)` or `(batch_size, timesteps, dim)`
    mask : `torch.BoolTensor`
         A tensor of shape `(batch_size, timesteps)`
    sentence_begin_token: `Any`
        Can be anything that can be broadcast in torch for assignment.
        For 2D input, a scalar with the `<S>` id. For 3D input, a tensor with length dim.
    sentence_end_token: `Any`
        Can be anything that can be broadcast in torch for assignment.
        For 2D input, a scalar with the `</S>` id. For 3D input, a tensor with length dim.

    # Returns

    tensor_with_boundary_tokens : `torch.Tensor`
        The tensor with the appended and prepended boundary tokens. If the input was 2D,
        it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape
        (batch_size, timesteps + 2, dim).
    new_mask : `torch.BoolTensor`
        The new mask for the tensor, taking into account the appended tokens
        marking the beginning and end of the sentence.
    """"""
    # TODO: matthewp, profile this transfer
    sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()
    tensor_shape = list(tensor.data.shape)
    new_shape = list(tensor_shape)
    new_shape[1] = tensor_shape[1] + 2
    tensor_with_boundary_tokens = tensor.new_zeros(*new_shape)
    if len(tensor_shape) == 2:
        tensor_with_boundary_tokens[:, 1:-1] = tensor
        tensor_with_boundary_tokens[:, 0] = sentence_begin_token
        for i, j in enumerate(sequence_lengths):
            tensor_with_boundary_tokens[i, j + 1] = sentence_end_token
        new_mask = tensor_with_boundary_tokens != 0
    elif len(tensor_shape) == 3:
        tensor_with_boundary_tokens[:, 1:-1, :] = tensor
        for i, j in enumerate(sequence_lengths):
            tensor_with_boundary_tokens[i, 0, :] = sentence_begin_token
            tensor_with_boundary_tokens[i, j + 1, :] = sentence_end_token
        new_mask = (tensor_with_boundary_tokens > 0).sum(dim=-1) > 0
    else:
        raise ValueError(""add_sentence_boundary_token_ids only accepts 2D and 3D input"")

    return tensor_with_boundary_tokens, new_mask","1. Use `torch.tensor()` instead of `torch.new_zeros()` to prevent data from being overwritten.
2. Use `torch.BoolTensor()` instead of `torch.ByteTensor()` to prevent data from being misinterpreted.
3. Use `torch.sum()` instead of `torch.detach().cpu().numpy()` to prevent data from being leaked."
"def import_plugins() -> None:
    """"""
    Imports the plugins found with `discover_plugins()`.
    """"""
    for module_name in DEFAULT_PLUGINS:
        try:
            # For default plugins we recursively import everything.
            import_module_and_submodules(module_name)
            logger.info(""Plugin %s available"", module_name)
        except ModuleNotFoundError:
            pass
    for module_name in discover_plugins():
        try:
            importlib.import_module(module_name)
            logger.info(""Plugin %s available"", module_name)
        except ModuleNotFoundError as e:
            logger.error(f""Plugin {module_name} could not be loaded: {e}"")","1. **Use a more secure import method.** The `importlib.import_module()` function does not perform any security checks, which can allow attackers to load malicious code into the system. Use the `importlib.util.module_from_spec()` function instead, which validates the module before importing it.
2. **Handle errors more gracefully.** The code currently logs errors to the console, but does not take any steps to prevent the malicious code from being loaded. Catch and handle errors more gracefully, and take steps to prevent the malicious code from being executed.
3. **Use a more secure default plugins list.** The default plugins list includes a number of well-known plugins that are likely to be safe. However, it is also possible for attackers to add their own malicious plugins to the list. Use a more secure default plugins list that only includes plugins that you trust."
"    def get_metric(
        self,
        reset: bool = False,
        cuda_device: Union[int, torch.device] = torch.device(""cpu""),
    ):
        """"""
        # Returns

        The accumulated metrics as a dictionary.
        """"""
        unlabeled_attachment_score = 0.0
        labeled_attachment_score = 0.0
        unlabeled_exact_match = 0.0
        labeled_exact_match = 0.0

        if self._total_words > 0.0:
            unlabeled_attachment_score = float(self._unlabeled_correct) / float(self._total_words)
            labeled_attachment_score = float(self._labeled_correct) / float(self._total_words)
        if self._total_sentences > 0:
            unlabeled_exact_match = float(self._exact_unlabeled_correct) / float(
                self._total_sentences
            )
            labeled_exact_match = float(self._exact_labeled_correct) / float(self._total_sentences)
        if reset:
            self.reset()
        metrics = {
            ""UAS"": unlabeled_attachment_score,
            ""LAS"": labeled_attachment_score,
            ""UEM"": unlabeled_exact_match,
            ""LEM"": labeled_exact_match,
        }
        return metrics","1. Use `torch.no_grad()` to disable gradient calculation when not needed.
2. Use `torch.cuda.empty_cache()` to free up GPU memory after training.
3. Use `torch.save()` to save the model to a file instead of printing it to the console."
"    def __call__(self, value):
        """"""
        # Parameters

        value : `float`
            The value to average.
        """"""
        _total_value = list(self.detach_tensors(value))[0]
        _count = 1
        if is_distributed():
            device = torch.device(""cpu"")
            count = torch.tensor(_count).to(device)
            total_value = torch.tensor(_total_value).to(device)
            dist.all_reduce(count, op=dist.ReduceOp.SUM)
            dist.all_reduce(total_value, op=dist.ReduceOp.SUM)
            _count = count.item()
            _total_value = total_value.item()
        self._count += _count
        self._total_value += _total_value","1. Use `torch.cuda.FloatTensor` instead of `torch.tensor` to avoid data copying.
2. Use `dist.all_reduce_async` instead of `dist.all_reduce` to avoid blocking the main thread.
3. Use `dist.is_available()` to check if distributed training is available before calling `dist.all_reduce`."
"    def __call__(self, predicted_trees: List[Tree], gold_trees: List[Tree]) -> None:  # type: ignore
        """"""
        # Parameters

        predicted_trees : `List[Tree]`
            A list of predicted NLTK Trees to compute score for.
        gold_trees : `List[Tree]`
            A list of gold NLTK Trees to use as a reference.
        """"""
        if not os.path.exists(self._evalb_program_path):
            logger.warning(
                f""EVALB not found at {self._evalb_program_path}.  Attempting to compile it.""
            )
            EvalbBracketingScorer.compile_evalb(self._evalb_directory_path)

            # If EVALB executable still doesn't exist, raise an error.
            if not os.path.exists(self._evalb_program_path):
                compile_command = (
                    f""python -c 'from allennlp.training.metrics import EvalbBracketingScorer; ""
                    f'EvalbBracketingScorer.compile_evalb(""{self._evalb_directory_path}"")\\''
                )
                raise ConfigurationError(
                    f""EVALB still not found at {self._evalb_program_path}. ""
                    ""You must compile the EVALB scorer before using it.""
                    "" Run 'make' in the '{}' directory or run: {}"".format(
                        self._evalb_program_path, compile_command
                    )
                )
        tempdir = tempfile.mkdtemp()
        gold_path = os.path.join(tempdir, ""gold.txt"")
        predicted_path = os.path.join(tempdir, ""predicted.txt"")
        with open(gold_path, ""w"") as gold_file:
            for tree in gold_trees:
                gold_file.write(f""{tree.pformat(margin=1000000)}\\n"")

        with open(predicted_path, ""w"") as predicted_file:
            for tree in predicted_trees:
                predicted_file.write(f""{tree.pformat(margin=1000000)}\\n"")

        command = [
            self._evalb_program_path,
            ""-p"",
            self._evalb_param_path,
            ""-e"",
            str(self._evalb_num_errors_to_kill),
            gold_path,
            predicted_path,
        ]
        completed_process = subprocess.run(
            command, stdout=subprocess.PIPE, universal_newlines=True, check=True
        )

        _correct_predicted_brackets = 0.0
        _gold_brackets = 0.0
        _predicted_brackets = 0.0

        for line in completed_process.stdout.split(""\\n""):
            stripped = line.strip().split()
            if len(stripped) == 12 and stripped != self._header_line:
                # This line contains results for a single tree.
                numeric_line = [float(x) for x in stripped]
                _correct_predicted_brackets += numeric_line[5]
                _gold_brackets += numeric_line[6]
                _predicted_brackets += numeric_line[7]

        shutil.rmtree(tempdir)

        if is_distributed():
            # Setting the device to CPU since this metric is not expected to run on GPUs.
            device = torch.device(""cpu"")
            correct_predicted_brackets = torch.tensor(_correct_predicted_brackets).to(device)
            predicted_brackets = torch.tensor(_predicted_brackets).to(device)
            gold_brackets = torch.tensor(_gold_brackets).to(device)
            dist.all_reduce(correct_predicted_brackets, op=dist.ReduceOp.SUM)
            dist.all_reduce(predicted_brackets, op=dist.ReduceOp.SUM)
            dist.all_reduce(gold_brackets, op=dist.ReduceOp.SUM)
            _correct_predicted_brackets = correct_predicted_brackets.item()
            _predicted_brackets = predicted_brackets.item()
            _gold_brackets = gold_brackets.item()

        self._correct_predicted_brackets += _correct_predicted_brackets
        self._gold_brackets += _gold_brackets
        self._predicted_brackets += _predicted_brackets","1. Use `subprocess.run` with the `check=True` argument to raise an exception if the subprocess returns a non-zero exit code. This will help catch errors in the EVALB executable.
2. Use `shutil.rmtree` with the `ignore_errors=True` argument to avoid raising an exception if the temporary directory cannot be deleted. This will prevent the training script from crashing if the temporary directory is not empty.
3. Use `torch.device(""cpu"")` to set the device to CPU for the distributed training. This will prevent the training script from crashing if the model is trained on GPUs."
"    def __init__(self) -> None:
        self._predictions_labels_covariance = Covariance()
        self._predictions_variance = Covariance()
        self._labels_variance = Covariance()
        self._device = torch.device(""cpu"")","1. Use `torch.jit.script` to create a compiled version of the model. This will make it more difficult for attackers to reverse engineer the model.
2. Use `torch.nn.functional.softmax` instead of `torch.nn.LogSoftmax`. This will make it more difficult for attackers to use gradient-based attacks.
3. Use `torch.utils.data.DataLoader` to load data in batches. This will make it more difficult for attackers to perform a denial-of-service attack."
"    def __call__(
        self,
        predictions: torch.Tensor,
        gold_labels: torch.Tensor,
        mask: Optional[torch.BoolTensor] = None,
    ):
        """"""
        # Parameters

        predictions : `torch.Tensor`, required.
            A tensor of predictions of shape (batch_size, ...).
        gold_labels : `torch.Tensor`, required.
            A tensor of the same shape as `predictions`.
        mask : `torch.BoolTensor`, optional (default = `None`).
            A tensor of the same shape as `predictions`.
        """"""
        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)
        self._device = gold_labels.device
        if not is_distributed():
            self._predictions_labels_covariance(predictions, gold_labels, mask)
            self._predictions_variance(predictions, predictions, mask)
            self._labels_variance(gold_labels, gold_labels, mask)","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to create a traced version of the model.
3. Use `torch.jit.save` to save the traced model to a file."
"def infer_params(cls: Type[T], constructor: Callable[..., T] = None):
    if constructor is None:
        constructor = cls.__init__

    signature = inspect.signature(constructor)
    parameters = dict(signature.parameters)

    has_kwargs = False
    for param in parameters.values():
        if param.kind == param.VAR_KEYWORD:
            has_kwargs = True

    if not has_kwargs:
        return parameters

    # ""mro"" is ""method resolution order"".  The first one is the current class, the next is the
    # first superclass, and so on.  We take the first superclass we find that inherits from
    # FromParams.
    super_class = None
    for super_class_candidate in cls.mro()[1:]:
        if issubclass(super_class_candidate, FromParams):
            super_class = super_class_candidate
            break
    if not super_class:
        raise RuntimeError(""found a kwargs parameter with no inspectable super class"")
    super_parameters = infer_params(super_class)

    return {**super_parameters, **parameters}  # Subclass parameters overwrite superclass ones","1. Use `inspect.isclass` to check if `cls` is a class before calling `cls.__init__`.
2. Use `inspect.issubclass` to check if `super_class_candidate` inherits from `FromParams` before calling `infer_params` on it.
3. Use `super_parameters` to initialize the parameters of the current class, and then overwrite them with the parameters of the subclass."
"def create_kwargs(
    constructor: Callable[..., T], cls: Type[T], params: Params, **extras
) -> Dict[str, Any]:
    """"""
    Given some class, a `Params` object, and potentially other keyword arguments,
    create a dict of keyword args suitable for passing to the class's constructor.

    The function does this by finding the class's constructor, matching the constructor
    arguments to entries in the `params` object, and instantiating values for the parameters
    using the type annotation and possibly a from_params method.

    Any values that are provided in the `extras` will just be used as is.
    For instance, you might provide an existing `Vocabulary` this way.
    """"""
    # Get the signature of the constructor.

    kwargs: Dict[str, Any] = {}

    parameters = infer_params(cls, constructor)

    # Iterate over all the constructor parameters and their annotations.
    for param_name, param in parameters.items():
        # Skip ""self"". You're not *required* to call the first parameter ""self"",
        # so in theory this logic is fragile, but if you don't call the self parameter
        # ""self"" you kind of deserve what happens.
        if param_name == ""self"":
            continue
        # Also skip **kwargs parameters; we handled them above.
        if param.kind == param.VAR_KEYWORD:
            continue

        # If the annotation is a compound type like typing.Dict[str, int],
        # it will have an __origin__ field indicating `typing.Dict`
        # and an __args__ field indicating `(str, int)`. We capture both.
        annotation = remove_optional(param.annotation)

        constructed_arg = pop_and_construct_arg(
            cls.__name__, param_name, annotation, param.default, params, **extras
        )

        # If we just ended up constructing the default value for the parameter, we can just omit it.
        # Leaving it in can cause issues with **kwargs in some corner cases, where you might end up
        # with multiple values for a single parameter (e.g., the default value gives you lazy=False
        # for a dataset reader inside **kwargs, but a particular dataset reader actually hard-codes
        # lazy=True - the superclass sees both lazy=True and lazy=False in its constructor).
        if constructed_arg is not param.default:
            kwargs[param_name] = constructed_arg

    params.assert_empty(cls.__name__)
    return kwargs","1. Use `typing.get_args()` to get the type arguments of a generic type.
2. Use `typing.get_origin()` to get the base type of a generic type.
3. Use `typing.get_type_hints()` to get the type hints of a function or class."
"    def from_params(
        cls: Type[T],
        params: Params,
        constructor_to_call: Callable[..., T] = None,
        constructor_to_inspect: Callable[..., T] = None,
        **extras,
    ) -> T:
        """"""
        This is the automatic implementation of `from_params`. Any class that subclasses
        `FromParams` (or `Registrable`, which itself subclasses `FromParams`) gets this
        implementation for free.  If you want your class to be instantiated from params in the
        ""obvious"" way -- pop off parameters and hand them to your constructor with the same names --
        this provides that functionality.

        If you need more complex logic in your from `from_params` method, you'll have to implement
        your own method that overrides this one.

        The `constructor_to_call` and `constructor_to_inspect` arguments deal with a bit of
        redirection that we do.  We allow you to register particular `@classmethods` on a class as
        the constructor to use for a registered name.  This lets you, e.g., have a single
        `Vocabulary` class that can be constructed in two different ways, with different names
        registered to each constructor.  In order to handle this, we need to know not just the class
        we're trying to construct (`cls`), but also what method we should inspect to find its
        arguments (`constructor_to_inspect`), and what method to call when we're done constructing
        arguments (`constructor_to_call`).  These two methods are the same when you've used a
        `@classmethod` as your constructor, but they are `different` when you use the default
        constructor (because you inspect `__init__`, but call `cls()`).
        """"""

        from allennlp.common.registrable import Registrable  # import here to avoid circular imports

        logger.debug(
            f""instantiating class {cls} from params {getattr(params, 'params', params)} ""
            f""and extras {set(extras.keys())}""
        )

        if params is None:
            return None

        if isinstance(params, str):
            params = Params({""type"": params})

        if not isinstance(params, Params):
            raise ConfigurationError(
                ""from_params was passed a `params` object that was not a `Params`. This probably ""
                ""indicates malformed parameters in a configuration file, where something that ""
                ""should have been a dictionary was actually a list, or something else. ""
                f""This happened when constructing an object of type {cls}.""
            )

        registered_subclasses = Registrable._registry.get(cls)

        if is_base_registrable(cls) and registered_subclasses is None:
            # NOTE(mattg): There are some potential corner cases in this logic if you have nested
            # Registrable types.  We don't currently have any of those, but if we ever get them,
            # adding some logic to check `constructor_to_call` should solve the issue.  Not
            # bothering to add that unnecessary complexity for now.
            raise ConfigurationError(
                ""Tried to construct an abstract Registrable base class that has no registered ""
                ""concrete types. This might mean that you need to use --include-package to get ""
                ""your concrete classes actually registered.""
            )

        if registered_subclasses is not None and not constructor_to_call:
            # We know `cls` inherits from Registrable, so we'll use a cast to make mypy happy.

            as_registrable = cast(Type[Registrable], cls)
            default_to_first_choice = as_registrable.default_implementation is not None
            choice = params.pop_choice(
                ""type"",
                choices=as_registrable.list_available(),
                default_to_first_choice=default_to_first_choice,
            )
            subclass, constructor_name = as_registrable.resolve_class_name(choice)
            # See the docstring for an explanation of what's going on here.
            if not constructor_name:
                constructor_to_inspect = subclass.__init__
                constructor_to_call = subclass  # type: ignore
            else:
                constructor_to_inspect = getattr(subclass, constructor_name)
                constructor_to_call = constructor_to_inspect

            if hasattr(subclass, ""from_params""):
                # We want to call subclass.from_params.
                extras = create_extras(subclass, extras)
                # mypy can't follow the typing redirection that we do, so we explicitly cast here.
                retyped_subclass = cast(Type[T], subclass)
                return retyped_subclass.from_params(
                    params=params,
                    constructor_to_call=constructor_to_call,
                    constructor_to_inspect=constructor_to_inspect,
                    **extras,
                )
            else:
                # In some rare cases, we get a registered subclass that does _not_ have a
                # from_params method (this happens with Activations, for instance, where we
                # register pytorch modules directly).  This is a bit of a hack to make those work,
                # instead of adding a `from_params` method for them somehow.  We just trust that
                # you've done the right thing in passing your parameters, and nothing else needs to
                # be recursively constructed.
                extras = create_extras(subclass, extras)
                constructor_args = {**params, **extras}
                return subclass(**constructor_args)  # type: ignore
        else:
            # This is not a base class, so convert our params and extras into a dict of kwargs.

            # See the docstring for an explanation of what's going on here.
            if not constructor_to_inspect:
                constructor_to_inspect = cls.__init__
            if not constructor_to_call:
                constructor_to_call = cls

            if constructor_to_inspect == object.__init__:
                # This class does not have an explicit constructor, so don't give it any kwargs.
                # Without this logic, create_kwargs will look at object.__init__ and see that
                # it takes *args and **kwargs and look for those.
                kwargs: Dict[str, Any] = {}
                params.assert_empty(cls.__name__)
            else:
                # This class has a constructor, so create kwargs for it.
                kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)

            return constructor_to_call(**kwargs)  # type: ignore","1. Use `mypy` to check for type errors.
2. Use `typeguard` to check for type hints.
3. Use `safety` to check for security vulnerabilities."
"def infer_params(cls: Type[T], constructor: Callable[..., T] = None) -> Dict[str, Any]:
    if cls == FromParams:
        return {}
    if constructor is None:
        constructor = cls.__init__

    signature = inspect.signature(constructor)
    parameters = dict(signature.parameters)

    has_kwargs = False
    for param in parameters.values():
        if param.kind == param.VAR_KEYWORD:
            has_kwargs = True

    if not has_kwargs:
        return parameters

    # ""mro"" is ""method resolution order"".  The first one is the current class, the next is the
    # first superclass, and so on.  We take the first superclass we find that inherits from
    # FromParams.
    super_class = None
    for super_class_candidate in cls.mro()[1:]:
        if issubclass(super_class_candidate, FromParams):
            super_class = super_class_candidate
            break
    if super_class:
        super_parameters = infer_params(super_class)
    else:
        super_parameters = {}

    return {**super_parameters, **parameters}  # Subclass parameters overwrite superclass ones","1. Use `inspect.signature.parameters` to get the parameters of a function.
2. Check if the function has a `**kwargs` parameter.
3. If the function has a `**kwargs` parameter, recursively call `infer_params` on the superclass."
"    def __call__(self, tensor: torch.Tensor, parameter_name: str, **kwargs) -> None:  # type: ignore
        # Select the new parameter name if it's being overridden
        if parameter_name in self.parameter_name_overrides:
            parameter_name = self.parameter_name_overrides[parameter_name]

        # If the size of the source and destination tensors are not the
        # same, then we need to raise an error
        source_weights = self.weights[parameter_name]
        if tensor.data.size() != source_weights.size():
            raise ConfigurationError(
                ""Incompatible sizes found for parameter %s. ""
                ""Found %s and %s"" % (parameter_name, tensor.data.size(), source_weights.size())
            )

        # Copy the parameters from the source to the destination
        tensor.data[:] = source_weights[:]","1. Use `torch.nn.Module.load_state_dict()` instead of manually copying parameters. This will ensure that the parameters are copied correctly and that the sizes are compatible.
2. Use `torch.jit.script()` to compile the model to a TorchScript module. This will make the model more secure by preventing it from being modified.
3. Use `torch.jit.save()` to save the model to a file. This will allow you to load the model without having to worry about the model being tampered with."
"    def from_path(
        cls,
        archive_path: str,
        predictor_name: str = None,
        cuda_device: int = -1,
        dataset_reader_to_load: str = ""validation"",
        frozen: bool = True,
    ) -> ""Predictor"":
        """"""
        Instantiate a `Predictor` from an archive path.

        If you need more detailed configuration options, such as overrides,
        please use `from_archive`.

        # Parameters

        archive_path : `str`
            The path to the archive.
        predictor_name : `str`, optional (default=`None`)
            Name that the predictor is registered as, or None to use the
            predictor associated with the model.
        cuda_device : `int`, optional (default=`-1`)
            If `cuda_device` is >= 0, the model will be loaded onto the
            corresponding GPU. Otherwise it will be loaded onto the CPU.
        dataset_reader_to_load : `str`, optional (default=`""validation""`)
            Which dataset reader to load from the archive, either ""train"" or
            ""validation"".
        frozen : `bool`, optional (default=`True`)
            If we should call `model.eval()` when building the predictor.

        # Returns

        `Predictor`
            A Predictor instance.
        """"""
        return Predictor.from_archive(
            load_archive(archive_path, cuda_device=cuda_device),
            predictor_name,
            dataset_reader_to_load=dataset_reader_to_load,
            frozen=frozen,
        )","1. Use `torch.jit.freeze` to prevent attackers from modifying the model's parameters.
2. Use `torch.jit.save` to save the model in a secure format.
3. Use `torch.jit.load` to load the model in a secure way."
"    def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:
        self._matched_indexer._add_encoding_to_vocabulary_if_needed(vocabulary)

        wordpieces, offsets = self._allennlp_tokenizer.intra_word_tokenize([t.text for t in tokens])
        output: IndexedTokenList = {
            ""token_ids"": [t.text_id for t in wordpieces],
            ""mask"": [True] * len(tokens),  # for original tokens (i.e. word-level)
            ""type_ids"": [t.type_id for t in wordpieces],
            ""offsets"": offsets,
            ""wordpiece_mask"": [True] * len(wordpieces),  # for wordpieces (i.e. subword-level)
        }

        return self._matched_indexer._postprocess_output(output)","1. Use `allennlp_tokenizer.intra_word_tokenize` to tokenize the input text into wordpieces.
2. Use `_matched_indexer._add_encoding_to_vocabulary_if_needed` to add the encoding to the vocabulary if needed.
3. Use `_matched_indexer._postprocess_output` to postprocess the output."
"    def forward(
        self,
        token_ids: torch.LongTensor,
        mask: torch.BoolTensor,
        offsets: torch.LongTensor,
        wordpiece_mask: torch.BoolTensor,
        type_ids: Optional[torch.LongTensor] = None,
        segment_concat_mask: Optional[torch.BoolTensor] = None,
    ) -> torch.Tensor:  # type: ignore
        """"""
        # Parameters

        token_ids: `torch.LongTensor`
            Shape: [batch_size, num_wordpieces] (for exception see `PretrainedTransformerEmbedder`).
        mask: `torch.BoolTensor`
            Shape: [batch_size, num_orig_tokens].
        offsets: `torch.LongTensor`
            Shape: [batch_size, num_orig_tokens, 2].
            Maps indices for the original tokens, i.e. those given as input to the indexer,
            to a span in token_ids. `token_ids[i][offsets[i][j][0]:offsets[i][j][1] + 1]`
            corresponds to the original j-th token from the i-th batch.
        wordpiece_mask: `torch.BoolTensor`
            Shape: [batch_size, num_wordpieces].
        type_ids: `Optional[torch.LongTensor]`
            Shape: [batch_size, num_wordpieces].
        segment_concat_mask: `Optional[torch.BoolTensor]`
            See `PretrainedTransformerEmbedder`.

        # Returns

        `torch.Tensor`
            Shape: [batch_size, num_orig_tokens, embedding_size].
        """"""
        # Shape: [batch_size, num_wordpieces, embedding_size].
        embeddings = self._matched_embedder(
            token_ids, wordpiece_mask, type_ids=type_ids, segment_concat_mask=segment_concat_mask
        )

        # span_embeddings: (batch_size, num_orig_tokens, max_span_length, embedding_size)
        # span_mask: (batch_size, num_orig_tokens, max_span_length)
        span_embeddings, span_mask = util.batched_span_select(embeddings.contiguous(), offsets)
        span_mask = span_mask.unsqueeze(-1)
        span_embeddings *= span_mask  # zero out paddings

        span_embeddings_sum = span_embeddings.sum(2)
        span_embeddings_len = span_mask.sum(2)
        # Shape: (batch_size, num_orig_tokens, embedding_size)
        orig_embeddings = span_embeddings_sum / span_embeddings_len

        return orig_embeddings","1. Use `torch.jit.script` to make the model's forward function more secure.
2. Check the input arguments for validity.
3. Use `torch.jit.trace` to create a traced model that is more efficient and secure."
"def get_posonlyargs(node: AnyFunctionDefAndLambda) -> List[ast.arg]:
    """"""
    Helper function to get posonlyargs in all version of python.

    This field was added in ``python3.8+``. And it was not present before.

    mypy also gives an error on this on older version of python::

        error: ""arguments"" has no attribute ""posonlyargs""; maybe ""kwonlyargs""?

    """"""
    return getattr(node.args, 'posonlyargs', [])","1. Use `ast.literal_eval` instead of `eval` to sanitize user input.
2. Use `ast.unparse` to validate the AST before executing it.
3. Use `ast.dump` to debug the AST and find any errors."
"def get_annotation_compexity(annotation_node: _Annotation) -> int:
    """"""
    Recursevly counts complexity of annotation nodes.

    When annotations are written as strings,
    we additionally parse them to ``ast`` nodes.
    """"""
    if isinstance(annotation_node, ast.Str):
        # try to parse string-wrapped annotations
        try:
            annotation_node = ast.parse(  # type: ignore
                annotation_node.s,
            ).body[0].value
        except (SyntaxError, IndexError):
            return 1

    if isinstance(annotation_node, ast.Subscript):
        return 1 + get_annotation_compexity(
            annotation_node.slice.value,  # type: ignore
        )
    elif isinstance(annotation_node, (ast.Tuple, ast.List)):
        return max(
            (get_annotation_compexity(node) for node in annotation_node.elts),
            default=1,
        )
    return 1","1. **Use `ast.literal_eval` instead of `ast.parse` to parse string-wrapped annotations.** This will prevent malicious code from being executed when an annotation is parsed.
2. **Check the type of the annotation node before calling `get_annotation_compexity`.** This will prevent errors from being thrown when an invalid annotation node is passed to the function.
3. **Return a default value of 1 if the annotation node is not a tuple, list, or subscript.** This will prevent the function from crashing when an invalid annotation node is passed to it."
"def is_same_slice(
    iterable: str,
    target: str,
    node: ast.Subscript,
) -> bool:
    """"""Used to tell when slice is identical to some pair of name/index.""""""
    return (
        source.node_to_string(node.value) == iterable and
        isinstance(node.slice, ast.Index) and  # mypy is unhappy
        source.node_to_string(node.slice.value) == target
    )","1. Use `ast.literal_eval` instead of `source.node_to_string` to parse strings into AST nodes. This will prevent malicious code from being injected into the program.
2. Use `ast.walk` to traverse the AST and check for dangerous constructs, such as `eval` and `exec`.
3. Use `ast.fix_missing_locations` to fix any missing location information in the AST. This will help to identify potential security vulnerabilities."
"    def _is_valid_final_value(self, format_value: ast.AST) -> bool:
        # Variable lookup is okay and a single attribute is okay
        if isinstance(format_value, (ast.Name, ast.Attribute)):
            return True
        # Function call with empty arguments is okay
        elif isinstance(format_value, ast.Call) and not format_value.args:
            return True
        # Named lookup, Index lookup & Dict key is okay
        elif isinstance(format_value, ast.Subscript):
            if isinstance(format_value.slice, ast.Index):
                return isinstance(
                    format_value.slice.value,
                    self._valid_format_index,
                )
        return False","1. Use `ast.literal_eval` to sanitize user input before using it in a format string.
2. Use `ast.unparse` to validate the format string before using it.
3. Use `ast.dump` to debug the format string and identify any potential security issues."
"    def _check_float_key(self, node: ast.Subscript) -> None:
        is_float_key = (
            isinstance(node.slice, ast.Index) and
            self._is_float_key(node.slice)
        )

        if is_float_key:
            self.add_violation(best_practices.FloatKeyViolation(node))","1. Use `isinstance()` to check if the `slice` node is an `ast.Index` node.
2. Use `self._is_float_key()` to check if the `Index` node contains a float key.
3. If the `Index` node contains a float key, add a `FloatKeyViolation` to the `violations` list."
"    def _check_len_call(self, node: ast.Subscript) -> None:
        is_len_call = (
            isinstance(node.slice, ast.Index) and
            isinstance(node.slice.value, ast.BinOp) and
            isinstance(node.slice.value.op, ast.Sub) and
            self._is_wrong_len(
                node.slice.value,
                source.node_to_string(node.value),
            )
        )

        if is_len_call:
            self.add_violation(
                refactoring.ImplicitNegativeIndexViolation(node),
            )","1. **Use `len()` instead of `-len()` to get the length of a list.** This will prevent negative indexing errors.
2. **Use `isinstance()` to check if a variable is a list before using `len()` on it.** This will prevent errors if the variable is not a list.
3. **Use `abs()` to get the absolute value of a number before using it in an index expression.** This will prevent errors if the number is negative."
"    def _is_float_key(self, node: ast.Index) -> bool:
        real_node = operators.unwrap_unary_node(node.value)
        return (
            isinstance(real_node, ast.Num) and
            isinstance(real_node.n, float)
        )","1. Use `ast.literal_eval` instead of `ast.literal_eval`, which is more secure.
2. Use `ast.unparse` to sanitize the string before evaluating it.
3. Use `ast.dump` to debug the code and find potential security issues."
"    def show_source(self, error: Violation) -> str:
        """"""Called when ``--show-source`` option is provided.""""""
        if not self._should_show_source(error):
            return ''

        formated_line = error.physical_line.lstrip()
        adjust = len(error.physical_line) - len(formated_line)

        code = highlight(
            formated_line,
            self._lexer,
            self._formatter,
        )

        return '  {code}  {pointer}^'.format(
            code=code,
            pointer=' ' * (error.column_number - 1 - adjust),
        )","1. Use `black` to format the code.
2. Use `f-strings` to format strings.
3. Use `type annotations` to make the code more readable."
"    def _check_useless_math_operator(
        self,
        op: ast.operator,
        left: ast.AST,
        right: Optional[ast.AST] = None,
    ) -> None:
        if isinstance(left, ast.Num) and right:
            if left.n == 1:
                left = None
        non_negative_numbers = self._get_non_negative_nodes(left, right)

        for number in non_negative_numbers:
            forbidden = self._meaningless_operations.get(number.n, None)
            if forbidden and isinstance(op, forbidden):
                self.add_violation(
                    consistency.MeaninglessNumberOperationViolation(number),
                )","1. Use `ast.literal_eval` instead of `eval` to parse strings into Python objects. This will prevent code injection attacks.
2. Use `ast.dump` to debug AST trees. This will help you identify potential security vulnerabilities.
3. Use a security scanner to identify potential security vulnerabilities in your code."
"    def _get_non_negative_nodes(
        self,
        left: ast.AST,
        right: Optional[ast.AST] = None,
    ):
        non_negative_numbers = []
        for node in filter(None, (left, right)):
            real_node = unwrap_unary_node(node)
            if not isinstance(real_node, ast.Num):
                continue

            if real_node.n not in self._meaningless_operations:
                continue

            if real_node.n == 1 and walk.is_contained(node, ast.USub):
                continue
            non_negative_numbers.append(real_node)
        return non_negative_numbers","1. Use `ast.literal_eval` instead of `eval` to sanitize user input.
2. Check if the node is a number and has a non-zero value.
3. Check if the node is a unary node and has a `USub` operation."
"    def visit_collection(self, node: AnyCollection) -> None:
        """"""Checks how collection items indentation.""""""
        elements = node.keys if isinstance(node, ast.Dict) else node.elts
        self._check_indentation(node, elements, extra_lines=1)
        self.generic_visit(node)","1. Use `ast.literal_eval` instead of `eval` to sanitize user input.
2. Validate the type of user input before using it.
3. Handle errors and exceptions gracefully."
"    def check_attribute_name(self, node: ast.ClassDef) -> None:
        top_level_assigns = [
            sub_node
            for sub_node in node.body
            if isinstance(sub_node, ast.Assign)
        ]

        for assignment in top_level_assigns:
            for target in assignment.targets:
                name = getattr(target, 'id', None)
                if logical.is_upper_case_name(name):
                    self._error_callback(
                        naming.UpperCaseAttributeViolation(target, text=name),
                    )","1. **Use a consistent casing style for attribute names.** This will help to prevent typos and make it easier to read and understand the code.
2. **Use descriptive attribute names.** This will help to make the code more readable and easier to understand.
3. **Avoid using reserved words as attribute names.** This can cause errors and make it difficult to read and understand the code."
"    def __init__(self, tree: Module, filename: str = constants.STDIN) -> None:
        """"""Creates new checker instance.""""""
        self.tree = maybe_set_parent(tree)
        self.filename = filename","1. Use `typing` to specify the types of arguments and return values. This will help catch errors early and prevent typecasting errors.
2. Use `isinstance()` to check the type of an object before using it. This will help prevent errors caused by using the wrong type of object.
3. Use `assert` statements to check for conditions that should always be true. This will help catch errors that might not be caught by other means."
"def fix_line_number(tree: ast.AST) -> ast.AST:
    """"""
    Adjusts line number for some nodes.

    They are set incorrectly for some collections.
    It might be either a bug or a feature.

    We do several checks here, to be sure that we won't get
    an incorrect line number. But, we basically check if there's
    a parent, so we can compare and adjust.

    Example::

        print((  # should start from here
            1, 2, 3,  # actually starts from here
        ))

    """"""
    affected = (ast.Tuple,)
    for node in ast.walk(tree):
        if isinstance(node, affected):
            parent_lineno = getattr(
                getattr(node, 'parent', None), 'lineno', None,
            )
            if parent_lineno and parent_lineno < node.lineno:
                node.lineno = node.lineno - 1
    return tree","1. Use `ast.copy_loc()` to copy the line number of the parent node to the child node.
2. Check if the parent node exists before getting its line number.
3. Use `ast.fix_missing_locations()` to fix the line numbers of nodes that do not have a parent node."
"def _set_parent(tree: ast.AST) -> ast.AST:
    """"""
    Sets parents for all nodes that do not have this prop.

    This step is required due to how `flake8` works.
    It does not set the same properties as `ast` module.

    This function was the cause of `issue-112`.

    .. versionchanged:: 0.0.11

    """"""
    for statement in ast.walk(tree):
        for child in ast.iter_child_nodes(statement):
            setattr(child, 'parent', statement)
    return tree","1. Use `ast.copy_location` to copy the location of the node being cloned. This will ensure that the cloned node has the same line number and column number as the original node.
2. Use `ast.fix_missing_locations` to fix any missing location information in the cloned node. This will ensure that the cloned node has valid line number and column number information.
3. Use `ast.dump` to print the AST of the cloned node. This will allow you to verify that the cloned node has been properly created."
"    def _get_real_parent(self, node: Optional[ast.AST]) -> Optional[ast.AST]:
        """"""
        Returns real number's parent.

        What can go wrong?

        1. Number can be negative: ``x = -1``,
          so ``1`` has ``UnaryOp`` as parent, but should return ``Assign``

        """"""
        parent = getattr(node, 'parent', None)
        if isinstance(parent, self._proxy_parents):
            return self._get_real_parent(parent)
        return parent","1. Use `ast.unparse()` to get the string representation of the AST node.
2. Use `ast.literal_eval()` to parse the string representation into an AST node.
3. Check if the AST node is a `Assign` node."
"    def _check_members_count(self, node: ModuleMembers) -> None:
        """"""This method increases the number of module members.""""""
        parent = getattr(node, 'parent', None)
        is_real_method = is_method(getattr(node, 'function_type', None))

        if isinstance(parent, ast.Module) and not is_real_method:
            self._public_items_count += 1","1. **Use `isinstance()` to check if the node is an `ast.Module` instance.** This will help to prevent a type error from being thrown if the node is not a module.
2. **Use `getattr()` to get the `function_type` attribute of the node.** This will help to determine if the node is a real method or not.
3. **Use `not` to negate the result of the `is_real_method()` function.** This will ensure that the code only increments the `_public_items_count` variable if the node is not a real method."
"    def _check_method(self, node: AnyFunctionDef) -> None:
        parent = getattr(node, 'parent', None)
        if isinstance(parent, ast.ClassDef):
            self._methods[parent] += 1","1. Use `ast.ClassDef` instead of `ast.AnyFunctionDef` to check for methods in a class.
2. Use `getattr(node, 'parent')` to get the parent of a node.
3. Use `isinstance(parent, ast.ClassDef)` to check if the parent is a class."
"    def _update_variables(
        self,
        function: AnyFunctionDef,
        variable_def: ast.Name,
    ) -> None:
        """"""
        Increases the counter of local variables.

        What is treated as a local variable?
        Check ``TooManyLocalsViolation`` documentation.
        """"""
        function_variables = self.variables[function]
        if variable_def.id not in function_variables:
            if variable_def.id == UNUSED_VARIABLE:
                return

            parent = getattr(variable_def, 'parent', None)
            if isinstance(parent, self._not_contain_locals):
                return

            function_variables.append(variable_def.id)","1. Use `isinstance()` to check if the variable is a local variable.
2. Check if the variable is defined in a function that is not supposed to contain locals.
3. Add `UNUSED_VARIABLE` to the list of variables if the variable is not used."
"    def _check_nested_function(self, node: AnyFunctionDef) -> None:
        parent = getattr(node, 'parent', None)
        is_inside_function = isinstance(parent, self._function_nodes)

        if is_inside_function and node.name not in NESTED_FUNCTIONS_WHITELIST:
            self.add_violation(NestedFunctionViolation(node, text=node.name))","1. **Use the `@staticmethod` decorator to mark functions that do not need an instance.** This will prevent them from being called from within other functions, which can reduce the risk of unintended side effects.
2. **Use the `@classmethod` decorator to mark functions that need access to the class's state.** This will prevent them from being called from outside the class, which can reduce the risk of unauthorized access to data.
3. **Use the `__init__` method to initialize the class's state.** This will ensure that the class is in a consistent state before it is used, which can reduce the risk of errors."
"    def _check_nested_classes(self, node: ast.ClassDef) -> None:
        parent = getattr(node, 'parent', None)
        is_inside_class = isinstance(parent, ast.ClassDef)
        is_inside_function = isinstance(parent, self._function_nodes)

        if is_inside_class and node.name not in NESTED_CLASSES_WHITELIST:
            self.add_violation(NestedClassViolation(node, text=node.name))
        elif is_inside_function:
            self.add_violation(NestedClassViolation(node, text=node.name))","1. **Use the `isinstance` function to check if a node is an instance of a specific class.** This will help you avoid accidentally violating the security policy.
2. **Add the `NestedClassViolation` violation to the list of violations if the node is a nested class.** This will alert you to potential security issues.
3. **Use the `text` attribute of the `NestedClassViolation` object to get the name of the nested class.** This will help you identify the source of the security issue."
"    def _check_nested_lambdas(self, node: ast.Lambda) -> None:
        parent = getattr(node, 'parent', None)
        if isinstance(parent, ast.Lambda):
            self.add_violation(NestedFunctionViolation(node))","1. **Use `ast.unparse()` to get the string representation of the AST.** This will make it easier to identify potential security issues.
2. **Use `ast.walk()` to recursively iterate over the AST.** This will allow you to check for security issues in all parts of the code.
3. **Use `ast.fix_missing_locations()` to fix any missing location information in the AST.** This will ensure that the security checks are accurate."
"    def check_nested_import(self, node: AnyImport) -> None:
        parent = getattr(node, 'parent', None)
        if parent is not None and not isinstance(parent, ast.Module):
            self._error_callback(NestedImportViolation(node))","1. **Use `ast.Module` as the parent of all imports.** This will prevent nested imports, which can be a security risk.
2. **Use `ast.ImportFrom` instead of `ast.Import`.** This will allow you to specify the specific module that you are importing from, which can help to prevent unauthorized access.
3. **Use `ast.check_module` to validate the imported modules.** This will help to ensure that the modules are safe to import and that they do not contain any malicious code."
"    def _check_ifs(self, node: ast.comprehension) -> None:
        if len(node.ifs) > self._max_ifs:
            # We are trying to fix line number in the report,
            # since `comprehension` does not have this property.
            parent = getattr(node, 'parent', node)
            self.add_violation(MultipleIfsInComprehensionViolation(parent))","1. Limit the number of `if` statements in a comprehension to reduce the risk of introducing security vulnerabilities.
2. Use `parent` to get the line number of the comprehension in the report.
3. Inherit from `MultipleIfsInComprehensionViolation` to create a custom violation for this issue."
"    def _check_fors(self, node: ast.comprehension) -> None:
        parent = getattr(node, 'parent', node)
        self._fors[parent] = len(parent.generators)","1. Use `ast.NodeVisitor` to traverse the AST and check for security vulnerabilities.
2. Use `ast.get_parent` to get the parent node of the current node.
3. Use `ast.len` to get the number of generators in the parent node."
"    def _check_metadata(self, node: ast.Assign) -> None:
        node_parent = getattr(node, 'parent', None)
        if not isinstance(node_parent, ast.Module):
            return

        for target_node in node.targets:
            target_node_id = getattr(target_node, 'id', None)
            if target_node_id in MODULE_METADATA_VARIABLES_BLACKLIST:
                self.add_violation(
                    WrongModuleMetadataViolation(node, text=target_node_id),
                )","1. Use `black` to lint your code and fix any style errors.
2. Use `f-strings` to format strings instead of concatenation.
3. Use `type annotations` to make your code more type-safe."
"    def _check_expression(
        self,
        node: ast.Expr,
        is_first: bool = False,
    ) -> None:
        if isinstance(node.value, self._have_effect):
            return

        if is_first and is_doc_string(node):
            parent = getattr(node, 'parent', None)
            if isinstance(parent, self._have_doc_strings):
                return

        self.add_violation(StatementHasNoEffectViolation(node))","1. **Use `isinstance()` to check if the node value has an effect.** This will help to avoid false positives.
2. **Check if the node is a doc string and ignore it if it is.** This will help to avoid false negatives.
3. **Add a `StatementHasNoEffectViolation` to the violation list if the node does not have an effect.** This will help to catch potential security issues."
"    def do_render(self, cr, widget, background_area, cell_area, flags):

        vw_tags = self.__count_viewable_tags()
        count = 0

        # Select source
        if self.tag_list is not None:
            tags = self.tag_list
        elif self.tag is not None:
            tags = [self.tag]
        else:
            return

        if self.config.get('dark_mode'):
            symbolic_color = Gdk.RGBA(0.9, 0.9, 0.9, 1)
        else:
            symbolic_color = Gdk.RGBA(0, 0, 0, 1)

        # Drawing context
        gdkcontext = cr
        gdkcontext.set_antialias(cairo.ANTIALIAS_NONE)

        # Coordinates of the origin point
        x_align = self.get_property(""xalign"")
        y_align = self.get_property(""yalign"")
        padding = self.PADDING
        orig_x = cell_area.x + int((cell_area.width - 16 * vw_tags -
                                    padding * 2 * (vw_tags - 1)) * x_align)
        orig_y = cell_area.y + int((cell_area.height - 16) * y_align)

        # We draw the icons & squares
        for my_tag in tags:

            my_tag_icon = my_tag.get_attribute(""icon"")
            my_tag_color = my_tag.get_attribute(""color"")

            rect_x = orig_x + self.PADDING * 2 * count + 16 * count
            rect_y = orig_y


            if my_tag_icon:
                if my_tag_icon in self.SYMBOLIC_ICONS:
                    icon_theme = Gtk.IconTheme.get_default()
                    info = icon_theme.lookup_icon(my_tag_icon, 16, 0)
                    load = info.load_symbolic(symbolic_color)
                    pixbuf = load[0]

                    Gdk.cairo_set_source_pixbuf(gdkcontext, pixbuf,
                                                rect_x, rect_y)
                    gdkcontext.paint()
                    count +=  1

                else:
                    layout = PangoCairo.create_layout(cr)
                    layout.set_markup(my_tag_icon, -1)
                    cr.move_to(rect_x - 2, rect_y - 1)
                    PangoCairo.show_layout(cr, layout)
                    count += 1

            elif my_tag_color:

                # Draw rounded rectangle
                my_color = Gdk.color_parse(my_tag_color)
                Gdk.cairo_set_source_color(gdkcontext, my_color)
                self.__roundedrec(gdkcontext, rect_x, rect_y, 16, 16, 8)
                gdkcontext.fill()
                count += 1

                # Outer line
                Gdk.cairo_set_source_rgba(gdkcontext, Gdk.RGBA(0, 0, 0, 0.20))
                gdkcontext.set_line_width(1.0)
                self.__roundedrec(gdkcontext, rect_x, rect_y, 16, 16, 8)
                gdkcontext.stroke()

        if self.tag and my_tag:

            my_tag_icon = my_tag.get_attribute(""icon"")
            my_tag_color = my_tag.get_attribute(""color"")

            if not my_tag_icon and not my_tag_color:
                # Draw rounded rectangle
                Gdk.cairo_set_source_rgba(gdkcontext,
                                          Gdk.RGBA(0.95, 0.95, 0.95, 1))
                self.__roundedrec(gdkcontext, rect_x, rect_y, 16, 16, 8)
                gdkcontext.fill()

                # Outer line
                Gdk.cairo_set_source_rgba(gdkcontext, Gdk.RGBA(0, 0, 0, 0.20))
                gdkcontext.set_line_width(1.0)
                self.__roundedrec(gdkcontext, rect_x, rect_y, 16, 16, 8)
                gdkcontext.stroke()","1. Use `cairo.ANTIALIAS_SUBPIXEL` instead of `cairo.ANTIALIAS_NONE` to improve the rendering quality.
2. Use `Gdk.cairo_set_source_rgba()` instead of `Gdk.cairo_set_source_color()` to set the source color.
3. Use `Gdk.cairo_set_line_width()` to set the line width."
"    def __draw(self, cr):
        """"""Draws the widget""""""
        alloc = self.get_allocation()
        # FIXME - why to use a special variables?
        alloc_w, alloc_h = alloc.width, alloc.height
        # Drawing context
        # cr_ctxt    = Gdk.cairo_create(self.window)
        # gdkcontext = Gdk.CairoContext(cr_ctxt)
        # FIXME
        gdkcontext = cr

        # Draw rectangle
        if self.color is not None:
            my_color = Gdk.color_parse(self.color)
            Gdk.cairo_set_source_color(gdkcontext, my_color)
        else:
            Gdk.cairo_set_source_rgba(gdkcontext, Gdk.RGBA(0, 0, 0, 0))
        gdkcontext.rectangle(0, 0, alloc_w, alloc_h)
        gdkcontext.fill()

        # Outer line
        Gdk.cairo_set_source_rgba(gdkcontext, Gdk.RGBA(0, 0, 0, 0.30))
        gdkcontext.set_line_width(2.0)
        gdkcontext.rectangle(0, 0, alloc_w, alloc_h)
        gdkcontext.stroke()

        # If selected draw a symbol
        if(self.selected):
            size = alloc_h * 0.50 - 3
            pos_x = math.floor((alloc_w - size) / 2)
            pos_y = math.floor((alloc_h - size) / 2)
            Gdk.cairo_set_source_rgba(gdkcontext,
                                      Gdk.RGBA(255, 255, 255, 0.80))
            gdkcontext.arc(
                alloc_w / 2, alloc_h / 2, size / 2 + 3, 0, 2 * math.pi)
            gdkcontext.fill()
            gdkcontext.set_line_width(1.0)
            Gdk.cairo_set_source_rgba(gdkcontext, Gdk.RGBA(0, 0, 0, 0.20))
            gdkcontext.arc(
                alloc_w / 2, alloc_h / 2, size / 2 + 3, 0, 2 * math.pi)
            gdkcontext.stroke()
            Gdk.cairo_set_source_rgba(gdkcontext, Gdk.RGBA(0, 0, 0, 0.50))
            gdkcontext.set_line_width(3.0)
            gdkcontext.move_to(pos_x, pos_y + size / 2)
            gdkcontext.line_to(pos_x + size / 2, pos_y + size)
            gdkcontext.line_to(pos_x + size, pos_y)
            gdkcontext.stroke()","1. Use Gdk.cairo_set_source_rgba() instead of Gdk.cairo_set_source_color() to prevent color parsing.
2. Use Gdk.cairo_set_line_width() to set the line width instead of hard-coding it.
3. Use Gdk.cairo_move_to() and Gdk.cairo_line_to() to draw lines instead of using Gdk.cairo_rectangle()."
"    def insert_existing_subtask(self, tid: str, line: int = None) -> None:
        """"""Insert an existing subtask in the buffer.""""""

        # Check if the task exists first
        if not self.req.has_task(tid):
            log.debug(f'Task {tid} not found')
            return

        if line is not None:
            start = self.buffer.get_iter_at_line(line)
        else:
            start = self.buffer.get_end_iter()
            self.buffer.insert(start, '\\n')
            start.forward_line()
            line = start.get_line()

        # Add subtask name
        task = self.req.get_task(tid)
        self.buffer.insert(start, task.get_title())

        # Reset iterator
        start = self.buffer.get_iter_at_line(line)

        # Add checkbox
        self.add_checkbox(tid, start)

        # Apply link to subtask text
        end = start.copy()
        end.forward_to_line_end()

        link_tag = InternalLinkTag(tid, task.get_status())
        self.table.add(link_tag)
        self.buffer.apply_tag(link_tag, start, end)
        self.tags_applied.append(link_tag)

        # Apply subtask tag to everything
        start.backward_char()
        subtask_tag = SubTaskTag(tid)
        self.table.add(subtask_tag)
        self.buffer.apply_tag(subtask_tag, start, end)

        self.subtasks['tags'].append(tid)

        # Make sure subtasks can be deleted when removed in the text editor
        task.can_be_deleted = True","1. Use `requests.get` instead of `requests.head` to get the response body.
2. Use `requests.post` instead of `requests.put` to send data.
3. Use `requests.delete` instead of `requests.patch` to delete data."
"    def delete_editor_task(self, action, params):
        """"""Callback to delete the task currently open.""""""

        editor = self.get_active_editor()
        task = editor.task

        if task.is_new():
            self.close_task(task.get_id(), editor.window)
        else:
            self.delete_tasks([task.get_id()], editor.window)","1. Use `task.is_saved()` instead of `task.is_new()` to check if the task has been saved.
2. Use `self.delete_task(task.get_id())` instead of `self.delete_tasks([task.get_id()])` to delete a single task.
3. Use `editor.window.close()` instead of `self.close_task(task.get_id(), editor.window)` to close the editor window."
"    def close_focused_task(self, action, params):
        """"""Callback to close currently focused task editor.""""""

        if self.open_tasks:
            tid = self.get_active_editor().task.get_id()
            self.close_task(tid)","1. Use `get_active_editor()` to get a reference to the currently focused task editor.
2. Use `task.get_id()` to get the ID of the task to be closed.
3. Use `close_task(tid)` to close the task with the given ID."
"    def close_task(self, tid):
        """"""Close a task editor window.""""""

        if tid in self.open_tasks:
            editor = self.open_tasks[tid]

            # We have to remove the tid first, otherwise
            # close_task would be called once again
            # by editor.close
            del self.open_tasks[tid]

            editor.close()

            open_tasks = self.config.get(""opened_tasks"")

            if tid in open_tasks:
                open_tasks.remove(tid)

            self.config.set(""opened_tasks"", open_tasks)

        else:
            log.warn(f'Tried to close tid {tid} but it is not open')","1. Use `tid` as a key in a dictionary to store the task editor window. This will prevent the `close_task` function from being called twice.
2. Check if the `tid` exists in the dictionary before calling the `close` function. This will prevent the function from being called for a task that is not open.
3. Use `log.error` instead of `log.warn` to log errors. This will make the errors more visible."
"    def destruction(self, a=None):
        # Save should be also called when buffer is modified
        self.pengine.onTaskClose(self.plugin_api)
        self.pengine.remove_api(self.plugin_api)
        tid = self.task.get_id()
        if self.task.is_new():
            self.req.delete_task(tid)
        else:
            self.save()
            for i in self.task.get_subtasks():
                if i:
                    i.set_to_keep()
        self.app.close_task(tid)","1. Use `self.req.delete_task(tid)` instead of `self.delete_task(tid)` to prevent unauthorized access.
2. Use `self.task.is_new()` to check if the task is new before deleting it.
3. Use `self.save()` to save the task before closing it."
"    def open_edit_backends(self, sender=None, backend_id=None):
        if not self.edit_backends_dialog:
            self.edit_backends_dialog = BackendsDialog(self.req)
            self.edit_backends_dialog.dialog.insert_action_group('app', self)

        self.edit_backends_dialog.activate()
        if backend_id is not None:
            self.edit_backends_dialog.show_config_for_backend(backend_id)","1. Use `django.utils.safestring. mark_safe()` to escape user input.
2. Use `django.contrib.auth.decorators.login_required()` to protect the view.
3. Use `django.views.decorators.csrf.csrf_protect()` to protect the view from CSRF attacks."
"    def get_added_date_string(self):
        if self.added_date:
            return self.added_date.strftime(""%Y-%m-%dT%H:%M:%S"")
        else:
            return Date.now()","1. Use `strftime` with a format string that does not include microseconds.
2. Use `now()` with a `timezone` argument to avoid time zone-related errors.
3. Use `isinstance()` to check if the `added_date` attribute is a `datetime.datetime` object before calling `strftime`."
"    def set_title(self, title, transaction_ids=[]):
        """"""Sets the task title""""""
        title = cgi.escape(title)
        result = self.rtm.tasks.setName(timeline=self.timeline,
                                        list_id=self.rtm_list.id,
                                        taskseries_id=self.rtm_taskseries.id,
                                        task_id=self.rtm_task.id,
                                        name=title)
        transaction_ids.append(result.transaction.id)","1. Use `json.dumps` to escape user input instead of `cgi.escape`.
2. Use `transaction_ids.append(result.transaction.id)` to track changes.
3. Use `self.rtm.tasks.setName(timeline=self.timeline, list_id=self.rtm_list.id, taskseries_id=self.rtm_taskseries.id, task_id=self.rtm_task.id, name=title)` to set the task title."
"    def set_text(self, text, transaction_ids=[]):
        """"""
        deletes all the old notes in a task and sets a single note with the
        given text
        """"""
        # delete old notes
        notes = self.rtm_taskseries.notes
        if notes:
            note_list = self.__getattr_the_rtm_way(notes, 'note')
            for note_id in [note.id for note in note_list]:
                result = self.rtm.tasksNotes.delete(timeline=self.timeline,
                                                    note_id=note_id)
                transaction_ids.append(result.transaction.id)

        if text == """":
            return
        text = cgi.escape(text)

        # RTM does not support well long notes (that is, it denies the request)
        # Thus, we split long text in chunks. To make them show in the correct
        # order on the website, we have to upload them from the last to the
        # first (they show the most recent on top)
        text_cursor_end = len(text)
        while True:
            text_cursor_start = text_cursor_end - 1000
            if text_cursor_start < 0:
                text_cursor_start = 0

            result = self.rtm.tasksNotes.add(timeline=self.timeline,
                                             list_id=self.rtm_list.id,
                                             taskseries_id=self.
                                             rtm_taskseries.id,
                                             task_id=self.rtm_task.id,
                                             note_title="""",
                                             note_text=text[text_cursor_start:
                                                            text_cursor_end])
            transaction_ids.append(result.transaction.id)
            if text_cursor_start <= 0:
                break
            text_cursor_end = text_cursor_start - 1","1. Use proper escaping to prevent XSS attacks.
2. Use prepared statements to prevent SQL injection attacks.
3. Sanitize user input to prevent other attacks."
"    def set_text(self, texte):
        self.can_be_deleted = False
        if texte != ""<content/>"":
            # defensive programmation to filter bad formatted tasks
            if not texte.startswith(""<content>""):
                texte = cgi.escape(texte, quote=True)
                texte = f""<content>{texte}""
            if not texte.endswith(""</content>""):
                texte = f""{texte}</content>""
            self.content = str(texte)
        else:
            self.content = ''","1. Use `html.escape()` to escape HTML entities in the input text.
2. Use `str.endswith()` to check if the input text ends with `</content>`.
3. Use `str.startswith()` to check if the input text starts with `<content>`."
"    def add_tag(self, tagname):
        ""Add a tag to the task and insert '@tag' into the task's content""
        if self.tag_added(tagname):
            c = self.content

            # strip <content>...</content> tags
            if c.startswith('<content>'):
                c = c[len('<content>'):]
            if c.endswith('</content>'):
                c = c[:-len('</content>')]

            if not c:
                # don't need a separator if it's the only text
                sep = ''
            elif c.startswith('<tag>'):
                # if content starts with a tag, make a comma-separated list
                sep = ', '
            else:
                # other text at the beginning, so put the tag on its own line
                sep = '\\n\\n'

            self.content = ""<content><tag>%s</tag>%s%s</content>"" % (
                cgi.escape(tagname), sep, c)
            # we modify the task internal state, thus we have to call for a
            # sync
            self.sync()","1. Use prepared statements instead of cgi.escape to prevent SQL injection attacks.
2. Sanitize user input to prevent cross-site scripting attacks.
3. Use proper error handling to prevent leaking sensitive information."
"def _get_tasks_from_blocks(task_blocks: Sequence) -> Generator:
    """"""Get list of tasks from list made of tasks and nested tasks.""""""
    NESTED_TASK_KEYS = [
        'block',
        'always',
        'rescue',
    ]

    def get_nested_tasks(task: Any) -> Generator[Any, None, None]:
        return (
            subtask
            for k in NESTED_TASK_KEYS if task and k in task
            for subtask in task[k]
        )

    for task in task_blocks:
        for sub_task in get_nested_tasks(task):
            yield sub_task
        yield task","1. Use `set` instead of `list` to avoid duplicate tasks.
2. Use `dict` instead of `tuple` to avoid key errors.
3. Use `enumerate` instead of `range` to avoid off-by-one errors."
"    def get_nested_tasks(task: Any) -> Generator[Any, None, None]:
        return (
            subtask
            for k in NESTED_TASK_KEYS if task and k in task
            for subtask in task[k]
        )","1. Use `type()` to check if `task` is a dict before iterating over its keys.
2. Use `filter()` to filter out tasks that are not dicts.
3. Use `list()` to convert the generator to a list before iterating over it."
"def _taskshandlers_children(basedir, k, v, parent_type: FileType) -> List:
    results = []
    for th in v:

        # ignore empty tasks, `-`
        if not th:
            continue

        with contextlib.suppress(LookupError):
            children = _get_task_handler_children_for_tasks_or_playbooks(
                th, basedir, k, parent_type,
            )
            results.append(children)
            continue

        if 'include_role' in th or 'import_role' in th:  # lgtm [py/unreachable-statement]
            th = normalize_task_v2(th)
            _validate_task_handler_action_for_role(th['action'])
            results.extend(_roles_children(basedir, k, [th['action'].get(""name"")],
                                           parent_type,
                                           main=th['action'].get('tasks_from', 'main')))
            continue

        if 'block' not in th:
            continue

        results.extend(_taskshandlers_children(basedir, k, th['block'], parent_type))
        if 'rescue' in th:
            results.extend(_taskshandlers_children(basedir, k, th['rescue'], parent_type))
        if 'always' in th:
            results.extend(_taskshandlers_children(basedir, k, th['always'], parent_type))

    return results","1. Use `normalize_task_v2()` to validate the task handler action.
2. Use `_validate_task_handler_action_for_role()` to validate the role name.
3. Use `_roles_children()` to get the children of the role."
"    def matchyaml(self, file: dict, text: str) -> List[MatchError]:
        matches: List[MatchError] = []
        if not self.matchplay:
            return matches

        yaml = ansiblelint.utils.parse_yaml_linenumbers(text, file['path'])
        # yaml returned can be an AnsibleUnicode (a string) when the yaml
        # file contains a single string. YAML spec allows this but we consider
        # this an fatal error.
        if isinstance(yaml, str):
            return [MatchError(
                filename=file['path'],
                rule=LoadingFailureRule()
            )]
        if not yaml:
            return matches

        if isinstance(yaml, dict):
            yaml = [yaml]

        yaml = ansiblelint.skip_utils.append_skipped_rules(yaml, text, file['type'])

        for play in yaml:
            if self.id in play.get('skipped_rules', ()):
                continue

            result = self.matchplay(file, play)
            if not result:
                continue

            if isinstance(result, tuple):
                result = [result]

            if not isinstance(result, list):
                raise TypeError(""{} is not a list"".format(result))

            for section, message, *optional_linenumber in result:
                linenumber = self._matchplay_linenumber(play, optional_linenumber)
                matches.append(self.create_matcherror(
                    message=message,
                    linenumber=linenumber,
                    details=str(section),
                    filename=file['path']
                    ))
        return matches","1. Use `ansiblelint.utils.parse_yaml_safe` to parse YAML input.
2. Check for `skipped_rules` in the YAML and skip the rules if necessary.
3. Use `self.create_matcherror` to create a MatchError object."
"def _playbook_items(pb_data: dict) -> ItemsView:
    if isinstance(pb_data, dict):
        return pb_data.items()
    elif not pb_data:
        return []
    else:
        return [item for play in pb_data for item in play.items()]","1. Use `pb_data.items()` instead of `pb_data.iteritems()` to avoid a security vulnerability in older Python versions.
2. Use `pb_data and pb_data.items()` instead of `bool(pb_data)` to avoid a potential `KeyError`.
3. Use `list(map(itemgetter(0), pb_data.values()))` instead of `[item[0] for item in pb_data]` to avoid a potential `KeyError`."
"def expand_path_vars(path: str) -> str:
    """"""Expand the environment or ~ variables in a path string.""""""
    path = path.strip()
    path = os.path.expanduser(path)
    path = os.path.expandvars(path)
    return path","1. Use `pathlib.Path` instead of `os.path` to avoid security vulnerabilities.
2. Sanitize user input before expanding environment variables to prevent malicious code execution.
3. Use a trusted library to expand environment variables, such as `subprocess.check_output`."
"    def focus_event(self):
        return self.focus.original_widget","1. Use `self.focus.widget` instead of `self.focus.original_widget` to avoid a [security vulnerability](https://bugs.python.org/issue34322).
2. Use [type annotations](https://docs.python.org/3/library/typing.html) to make the code more readable and easier to maintain.
3. Use [defensive programming](https://en.wikipedia.org/wiki/Defensive_programming) to catch errors and prevent security breaches."
"def config_checks(
        config,
        _get_color_from_vdir=get_color_from_vdir,
        _get_vdir_type=get_vdir_type):
    """"""do some tests on the config we cannot do with configobj's validator""""""
    if len(config['calendars'].keys()) < 1:
        logger.fatal('Found no calendar section in the config file')
        raise InvalidSettingsError()
    config['sqlite']['path'] = expand_db_path(config['sqlite']['path'])
    if not config['locale']['default_timezone']:
        config['locale']['default_timezone'] = is_timezone(
            config['locale']['default_timezone'])
    if not config['locale']['local_timezone']:
        config['locale']['local_timezone'] = is_timezone(
            config['locale']['local_timezone'])

    # expand calendars with type = discover
    vdirs_complete = list()
    vdir_colors_from_config = {}
    for calendar in list(config['calendars'].keys()):
        if not isinstance(config['calendars'][calendar], dict):
            logger.fatal('Invalid config file, probably missing calendar sections')
            raise InvalidSettingsError
        if config['calendars'][calendar]['type'] == 'discover':
            logger.debug(
                'discovering calendars in {}'.format(config['calendars'][calendar]['path'])
            )
            vdirs = get_all_vdirs(config['calendars'][calendar]['path'])
            vdirs_complete += vdirs
            if 'color' in config['calendars'][calendar]:
                for vdir in vdirs:
                    vdir_colors_from_config[vdir] = config['calendars'][calendar]['color']
            config['calendars'].pop(calendar)
    for vdir in sorted(vdirs_complete):
        calendar = {'path': vdir,
                    'color': _get_color_from_vdir(vdir),
                    'type': _get_vdir_type(vdir),
                    'readonly': False
                    }

        # get color from config if not defined in vdir

        if calendar['color'] is None and vdir in vdir_colors_from_config:
            logger.debug(""using collection's color for {}"".format(vdir))
            calendar['color'] = vdir_colors_from_config[vdir]

        name = get_unique_name(vdir, config['calendars'].keys())
        config['calendars'][name] = calendar

    test_default_calendar(config)
    for calendar in config['calendars']:
        if config['calendars'][calendar]['type'] == 'birthdays':
            config['calendars'][calendar]['readonly'] = True
        if config['calendars'][calendar]['color'] == 'auto':
            config['calendars'][calendar]['color'] = \\
                _get_color_from_vdir(config['calendars'][calendar]['path'])","1. Use `configparser` instead of `configobj` to validate the configuration file.
2. Sanitize user input before using it in the code.
3. Use `assert` statements to check for invalid or unexpected input."
"    def __init__(self, rrule, conf, startdt):
        self._conf = conf
        self._startdt = startdt
        self._rrule = rrule
        self.repeat = bool(rrule)
        self._allow_edit = not self.repeat or self.check_understood_rrule(rrule)
        self.repeat_box = urwid.CheckBox(
            'Repeat: ', state=self.repeat, on_state_change=self.check_repeat,
        )

        if ""UNTIL"" in self._rrule:
            self._until = ""Until""
        elif ""COUNT"" in self._rrule:
            self._until = ""Repetitions""
        else:
            self._until = ""Forever""

        recurrence = self._rrule['freq'][0].lower() if self._rrule else ""weekly""
        self.recurrence_choice = Choice(
            [""daily"", ""weekly"", ""monthly"", ""yearly""],
            recurrence,
            callback=self.rebuild,
        )
        self.interval_edit = PositiveIntEdit(
            caption='every:',
            edit_text=str(self._rrule.get('INTERVAL', [1])[0]),
        )
        self.until_choice = Choice(
            [""Forever"", ""Until"", ""Repetitions""], self._until, callback=self.rebuild,
        )

        count = str(self._rrule.get('COUNT', [1])[0])
        self.repetitions_edit = PositiveIntEdit(edit_text=count)

        until = self._rrule.get('UNTIL', [None])[0]
        if until is None and isinstance(self._startdt, datetime):
            until = self._startdt.date()
        elif until is None:
            until = self._startdt

        if isinstance(until, datetime):
            until = datetime.date()
        self.until_edit = DateEdit(
            until, self._conf['locale']['longdateformat'],
            lambda _: None, self._conf['locale']['weeknumbers'],
            self._conf['locale']['firstweekday'],
        )

        self._rebuild_weekday_checks()
        self._rebuild_monthly_choice()
        self._pile = pile = NPile([urwid.Text('')])
        urwid.WidgetWrap.__init__(self, pile)
        self.rebuild()","1. Use `urwid.CheckBox` instead of `urwid.Text` to prevent users from entering malicious input.
2. Use `urwid.PositiveIntEdit` to prevent users from entering non-integer values.
3. Use `urwid.DateEdit` to prevent users from entering invalid dates."
"    def fromVEvents(cls, events_list, ref=None, **kwargs):
        """"""
        :type events: list
        """"""
        assert isinstance(events_list, list)

        vevents = dict()
        if len(events_list) == 1:
            vevents['PROTO'] = events_list[0]  # TODO set mutable = False
        else:
            for event in events_list:
                if 'RECURRENCE-ID' in event:
                    if invalid_timezone(event['RECURRENCE-ID']):
                        default_timezone = kwargs['locale']['default_timezone']
                        recur_id = default_timezone.localize(event['RECURRENCE-ID'].dt)
                        ident = str(to_unix_time(recur_id))
                    else:
                        ident = str(to_unix_time(event['RECURRENCE-ID'].dt))
                    vevents[ident] = event
                else:
                    vevents['PROTO'] = event
        if ref is None:
            ref = 'PROTO'

        try:
            if type(vevents[ref]['DTSTART'].dt) != type(vevents[ref]['DTEND'].dt):  # flake8: noqa
                raise ValueError('DTSTART and DTEND should be of the same type (datetime or date)')
        except KeyError:
            pass

        if kwargs.get('start'):
            instcls = cls._get_type_from_date(kwargs.get('start'))
        else:
            instcls = cls._get_type_from_vDDD(vevents[ref]['DTSTART'])
        return instcls(vevents, ref=ref, **kwargs)","1. Use `from_dict` instead of `fromVEvents` to avoid potential security issues.
2. Validate the input data before using it.
3. Use a secure default timezone."
"def expand(vevent, href=''):
    """"""
    Constructs a list of start and end dates for all recurring instances of the
    event defined in vevent.

    It considers RRULE as well as RDATE and EXDATE properties. In case of
    unsupported recursion rules an UnsupportedRecurrence exception is thrown.
    If the timezone defined in vevent is not understood by icalendar,
    default_tz is used.

    :param vevent: vevent to be expanded
    :type vevent: icalendar.cal.Event
    :param href: the href of the vevent, used for more informative logging and
                 nothing else
    :type href: str
    :returns: list of start and end (date)times of the expanded event
    :rtype: list(tuple(datetime, datetime))
    """"""
    # we do this now and than never care about the ""real"" end time again
    if 'DURATION' in vevent:
        duration = vevent['DURATION'].dt
    else:
        duration = vevent['DTEND'].dt - vevent['DTSTART'].dt

    events_tz = getattr(vevent['DTSTART'].dt, 'tzinfo', None)
    allday = not isinstance(vevent['DTSTART'].dt, datetime)

    def sanitize_datetime(date):
        if allday and isinstance(date, datetime):
            date = date.date()
        if events_tz is not None:
            date = events_tz.localize(date)
        return date

    rrule_param = vevent.get('RRULE')
    if rrule_param is not None:
        vevent = sanitize_rrule(vevent)

        # dst causes problem while expanding the rrule, therefore we transform
        # everything to naive datetime objects and transform back after
        # expanding
        # See https://github.com/dateutil/dateutil/issues/102
        dtstart = vevent['DTSTART'].dt
        if events_tz:
            dtstart = dtstart.replace(tzinfo=None)

        rrule = dateutil.rrule.rrulestr(
            rrule_param.to_ical().decode(),
            dtstart=dtstart
        )

        if rrule._until is None:
            # rrule really doesn't like to calculate all recurrences until
            # eternity, so we only do it until 2037, because a) I'm not sure
            # if python can deal with larger datetime values yet and b) pytz
            # doesn't know any larger transition times
            rrule._until = datetime(2037, 12, 31)
        elif getattr(rrule._until, 'tzinfo', None):
            rrule._until = rrule._until \\
                .astimezone(events_tz) \\
                .replace(tzinfo=None)

        rrule = map(sanitize_datetime, rrule)

        logger.debug('calculating recurrence dates for {0}, '
                     'this might take some time.'.format(href))

        # RRULE and RDATE may specify the same date twice, it is recommended by
        # the RFC to consider this as only one instance
        dtstartl = set(rrule)
        if not dtstartl:
            raise UnsupportedRecurrence()
    else:
        dtstartl = {vevent['DTSTART'].dt}

    def get_dates(vevent, key):
        dates = vevent.get(key)
        if dates is None:
            return
        if not isinstance(dates, list):
            dates = [dates]

        dates = (leaf.dt for tree in dates for leaf in tree.dts)
        dates = localize_strip_tz(dates, events_tz)
        return map(sanitize_datetime, dates)

    # include explicitly specified recursion dates
    dtstartl.update(get_dates(vevent, 'RDATE') or ())

    # remove excluded dates
    for date in get_dates(vevent, 'EXDATE') or ():
        try:
            dtstartl.remove(date)
        except KeyError:
            logger.warning(
                'In event {}, excluded instance starting at {} not found, '
                'event might be invalid.'.format(href, date))

    dtstartend = [(start, start + duration) for start in dtstartl]
    # not necessary, but I prefer deterministic output
    dtstartend.sort()
    return dtstartend","1. Use `strptime` instead of `strftime` to avoid
    potential security issues.
2. Use `datetime.datetime.utcnow()` instead of `datetime.datetime.now()`
    to avoid leaking the local timezone.
3. Use `urllib.parse.quote()` to escape special characters in URLs."
"    def get_dates(vevent, key):
        dates = vevent.get(key)
        if dates is None:
            return
        if not isinstance(dates, list):
            dates = [dates]

        dates = (leaf.dt for tree in dates for leaf in tree.dts)
        dates = localize_strip_tz(dates, events_tz)
        return map(sanitize_datetime, dates)","1. Sanitize user input to prevent injection attacks.
2. Use proper error handling to prevent leaking sensitive information.
3. Use secure transport protocols such as HTTPS to protect data in transit."
"def delete_instance(vevent, instance):
    """"""remove a recurrence instance from a VEVENT's RRDATE list

    :type vevent: icalendar.cal.Event
    :type instance: datetime.datetime
    """"""

    if 'RDATE' in vevent and 'RRULE' in vevent:
        # TODO check where this instance is coming from and only call the
        # appropriate function
        _add_exdate(vevent, instance)
        _remove_instance(vevent, instance)
    elif 'RRULE' in vevent:
        _add_exdate(vevent, instance)
    elif 'RDATE' in vevent:
        _remove_instance(vevent, instance)","1. Use `functools.lru_cache` to memoize the function `_add_exdate` to improve performance.
2. Validate the input parameters of the function `delete_instance` to prevent malicious users from injecting invalid data.
3. Use `icalendar.cal.Event.validate()` to validate the VEVENT object before deleting an instance."
"    def check_understood_rrule(rrule):
        """"""test if we can reproduce `rrule`.""""""
        keys = set(rrule.keys())
        freq = rrule.get('FREQ', [None])[0]
        unsupported_rrule_parts = {
            'BYSECOND', 'BYMINUTE', 'BYHOUR', 'BYYEARDAY',
            'BYWEEKNO', 'BYMONTH', 'BYSETPOS',
        }
        if keys.intersection(unsupported_rrule_parts):
            return False
        # we don't support negative BYMONTHDAY numbers
        if rrule.get('BYMONTHDAY', ['1'])[0][0] == '-':
            return False
        if rrule.get('BYDAY', ['1'])[0][0] == '-':
            return False
        if freq not in ['DAILY', 'WEEKLY', 'MONTHLY', 'YEARLY']:
            return False
        if 'BYDAY' in keys and freq == 'YEARLY':
            return False
        return True","1. Validate the rrule parameters to ensure they are supported.
2. Sanitize the rrule parameters to prevent injection attacks.
3. Use a secure random number generator to generate the BYDAY values."
"def guessrangefstr(daterange, locale, default_timedelta=None, adjust_reasonably=False):
    """"""parses a range string

    :param daterange: date1 [date2 | timedelta]
    :type daterange: str or list
    :param locale:
    :rtype: (datetime, datetime, bool)

    """"""
    range_list = daterange
    if isinstance(daterange, str):
        range_list = daterange.split(' ')

    try:
        if default_timedelta is None or len(default_timedelta) == 0:
            default_timedelta = None
        else:
            default_timedelta = guesstimedeltafstr(default_timedelta)
    except ValueError:
        default_timedelta = None

    for i in reversed(range(1, len(range_list) + 1)):
        start = ' '.join(range_list[:i])
        end = ' '.join(range_list[i:])
        allday = False
        try:
            # figuring out start
            if len(start) == 0:
                start = datetime_fillin(end=False)
            elif start.lower() == 'week':
                    today_weekday = datetime.today().weekday()
                    start = datetime.today() - \\
                        timedelta(days=(today_weekday - locale['firstweekday']))
                    end = start + timedelta(days=7)
            else:
                split = start.split("" "")
                start, allday = guessdatetimefstr(split, locale)
                if len(split) != 0:
                    continue

            # and end
            if len(end) == 0:
                if default_timedelta is not None:
                    end = start + default_timedelta
                else:
                    end = datetime_fillin(day=start)
            elif end.lower() == 'eod':
                    end = datetime_fillin(day=start)
            elif end.lower() == 'week':
                start -= timedelta(days=(start.weekday() - locale['firstweekday']))
                end = start + timedelta(days=7)
            else:

                try:
                    delta = guesstimedeltafstr(end)
                    end = start + delta
                except ValueError:
                    split = end.split("" "")
                    end, end_allday = guessdatetimefstr(split, locale, default_day=start.date())
                    if len(split) != 0:
                        continue
                end = datetime_fillin(end)

            if adjust_reasonably:
                if allday:
                    # TODO move out of here, this is an icalendar peculiarity
                    end += timedelta(days=1)
                    # test if end's year is this year, but start's year is not
                    today = datetime.today()
                    if end.year == today.year and start.year != today.year:
                        end = datetime(start.year, *end.timetuple()[1:6])

                    if end < start:
                        end = datetime(end.year + 1, *end.timetuple()[1:6])

                if end < start:
                    end = datetime(*start.timetuple()[0:3] + end.timetuple()[3:5])
                if end < start:
                    end = end + timedelta(days=1)
            return start, end, allday
        except ValueError:
            pass

    raise ValueError('Could not parse `{}` as a daterange'.format(daterange))","1. Use `datetime.now()` instead of `datetime.today()` to avoid leaking the current user's timezone.
2. Use `datetime.strptime()` to parse dates instead of manually splitting the string.
3. Use `datetime.combine()` to create a `datetime` object from a date and time, rather than using `datetime()`."
"def expand(vevent, href=''):
    """"""
    Constructs a list of start and end dates for all recurring instances of the
    event defined in vevent.

    It considers RRULE as well as RDATE and EXDATE properties. In case of
    unsupported recursion rules an UnsupportedRecurrence exception is thrown.
    If the timezone defined in vevent is not understood by icalendar,
    default_tz is used.

    :param vevent: vevent to be expanded
    :type vevent: icalendar.cal.Event
    :param href: the href of the vevent, used for more informative logging and
                 nothing else
    :type href: str
    :returns: list of start and end (date)times of the expanded event
    :rtyped list(tuple(datetime, datetime))
    """"""
    # we do this now and than never care about the ""real"" end time again
    if 'DURATION' in vevent:
        duration = vevent['DURATION'].dt
    else:
        duration = vevent['DTEND'].dt - vevent['DTSTART'].dt

    events_tz = getattr(vevent['DTSTART'].dt, 'tzinfo', None)
    allday = not isinstance(vevent['DTSTART'].dt, datetime)

    def sanitize_datetime(date):
        if allday and isinstance(date, datetime):
            date = date.date()
        if events_tz is not None:
            date = events_tz.localize(date)
        return date

    rrule_param = vevent.get('RRULE')
    if rrule_param is not None:
        vevent = sanitize_rrule(vevent)

        # dst causes problem while expanding the rrule, therefore we transform
        # everything to naive datetime objects and tranform back after
        # expanding
        # See https://github.com/dateutil/dateutil/issues/102
        dtstart = vevent['DTSTART'].dt
        if events_tz:
            dtstart = dtstart.replace(tzinfo=None)

        rrule = dateutil.rrule.rrulestr(
            rrule_param.to_ical().decode(),
            dtstart=dtstart
        )

        if rrule._until is None:
            # rrule really doesn't like to calculate all recurrences until
            # eternity, so we only do it until 2037, because a) I'm not sure
            # if python can deal with larger datetime values yet and b) pytz
            # doesn't know any larger transition times
            rrule._until = datetime(2037, 12, 31)
        elif getattr(rrule._until, 'tzinfo', None):
            rrule._until = rrule._until \\
                .astimezone(events_tz) \\
                .replace(tzinfo=None)

        rrule = map(sanitize_datetime, rrule)

        logger.debug('calculating recurrence dates for {0}, '
                     'this might take some time.'.format(href))

        # RRULE and RDATE may specify the same date twice, it is recommended by
        # the RFC to consider this as only one instance
        dtstartl = set(rrule)
        if not dtstartl:
            raise UnsupportedRecurrence()
    else:
        dtstartl = {vevent['DTSTART'].dt}

    def get_dates(vevent, key):
        dates = vevent.get(key)
        if dates is None:
            return
        if not isinstance(dates, list):
            dates = [dates]

        dates = (leaf.dt for tree in dates for leaf in tree.dts)
        dates = localize_strip_tz(dates, events_tz)
        return map(sanitize_datetime, dates)

    # include explicitly specified recursion dates
    dtstartl.update(get_dates(vevent, 'RDATE') or ())

    # remove excluded dates
    for date in get_dates(vevent, 'EXDATE') or ():
        dtstartl.remove(date)

    dtstartend = [(start, start + duration) for start in dtstartl]
    # not necessary, but I prefer deterministic output
    dtstartend.sort()
    return dtstartend","1. Use `str.encode()` to convert the string to bytes before passing it to `dateutil.rrule.rrulestr()`.
2. Use `datetime.now()` instead of a hard-coded date to avoid potential security issues.
3. Use `datetime.utcnow()` instead of `datetime.now()` to ensure that the time is always in UTC."
"    def get_event(self, href, calendar):
        return self._cover_event(self._backend.get(href, calendar))","1. Use `self._backend.get_secure(href, calendar)` instead of `self._backend.get(href, calendar)` to prevent XSS attacks.
2. Sanitize the `href` and `calendar` parameters to prevent injection attacks.
3. Use `self._cover_event()` to wrap the returned event object and protect it from unauthorized access."
"    def __init__(self, eventcolumn):
        self.eventcolumn = eventcolumn
        self.list_walker = None
        pile = urwid.Filler(urwid.Pile([]))
        urwid.WidgetWrap.__init__(self, pile)
        self.update_by_date()","1. Use `eventcolumn.get_list()` to get the list of events instead of directly accessing `eventcolumn`. This will prevent unauthorized access to the list of events.
2. Use `urwid.ListWalker()` to iterate over the list of events instead of directly iterating over the list. This will prevent unauthorized modifications to the list of events.
3. Use `urwid.AttrMap()` to set the attributes of the events in the list. This will prevent unauthorized users from changing the appearance of the events."
"    def update_by_date(self, this_date=date.today()):
        if this_date is None:   # this_date might be None
            return

        date_text = urwid.Text(
            this_date.strftime(self.eventcolumn.pane.conf['locale']['longdateformat']))
        events = sorted(self.eventcolumn.pane.collection.get_events_on(this_date))

        event_list = [
            urwid.AttrMap(U_Event(event, this_date=this_date, eventcolumn=self.eventcolumn),
                          'calendar ' + event.calendar, 'reveal focus') for event in events]
        event_count = len(event_list)
        if not event_list:
            event_list = [urwid.Text('no scheduled events')]
        self.list_walker = urwid.SimpleFocusListWalker(event_list)
        self._w = urwid.Frame(urwid.ListBox(self.list_walker), header=date_text)
        return event_count","1. Sanitize user input to prevent injection attacks.
2. Use proper error handling to prevent leaking sensitive information.
3. Use strong encryption to protect sensitive data."
"    def fromVEvents(cls, events_list, ref=None, **kwargs):
        """"""
        :type events: list
        """"""
        assert isinstance(events_list, list)

        vevents = dict()
        if len(events_list) == 1:
            vevents['PROTO'] = events_list[0]  # TODO set mutable = False
        else:
            for event in events_list:
                if 'RECURRENCE-ID' in event:
                    if invalid_timezone(event['RECURRENCE-ID']):
                        default_timezone = kwargs['locale']['default_timezone']
                        recur_id = default_timezone.localize(event['RECURRENCE-ID'].dt)
                        ident = str(to_unix_time(recur_id))
                    else:
                        ident = str(to_unix_time(event['RECURRENCE-ID'].dt))
                    vevents[ident] = event
                else:
                    vevents['PROTO'] = event
        if ref is None:
            ref = 'PROTO'

        try:
            if type(vevents[ref]['DTSTART'].dt) != type(vevents[ref]['DTEND'].dt):  # flake8: noqa
                raise ValueError('DTSTART and DTEND should be of the same type (datetime or date)')
        except KeyError:
            pass

        instcls = cls._get_type_from_vDDD(vevents[ref]['DTSTART'])
        return instcls(vevents, ref=ref, **kwargs)","1. Use `from_vevents` to create an instance of the class. This will ensure that the data is validated and properly formatted.
2. Set `mutable=False` on the `PROTO` event to prevent it from being modified.
3. Use `invalid_timezone` to check if the timezone of the `RECURRENCE-ID` is valid. If it is not, use `default_timezone` to localize the value and generate a new identifier."
"    def toggle_delete(self):
        # TODO unify, either directly delete *normal* events as well
        # or stage recurring deletion as well
        def delete_this(_):
            if self.event.ref == u'PROTO':
                instance = self.event.start
            else:
                instance = self.event.ref
            self.event.delete_instance(instance)

            self.eventcolumn.pane.collection.update(self.event)
            self.eventcolumn.pane.window.backtrack()

        def delete_future(_):
            self.eventcolumn.pane.window.backtrack()

        def delete_all(_):
            if self.uid in self.eventcolumn.pane.deleted:
                self.eventcolumn.pane.deleted.remove(self.uid)
            else:
                self.eventcolumn.pane.deleted.append(self.uid)
            self.eventcolumn.pane.window.backtrack()

        if self.event.readonly:
            self.eventcolumn.pane.window.alert(
                ('light red',
                 'Calendar {} is read-only.'.format(self.event.calendar)))
            return
        if self.uid in self.eventcolumn.pane.deleted:
            self.eventcolumn.pane.deleted.remove(self.uid)
        else:
            if self.event.recurring:
                overlay = urwid.Overlay(
                    DeleteDialog(
                        delete_this,
                        delete_future,
                        delete_all,
                        self.eventcolumn.pane.window.backtrack,
                    ),
                    self.eventcolumn.pane,
                    'center', ('relative', 70), ('relative', 70), None)
                self.eventcolumn.pane.window.open(overlay)
            else:
                self.eventcolumn.pane.deleted.append(self.uid)
        self.set_title()","1. Use `event.readonly` to check if the event is read-only before deleting it.
2. Use `eventcolumn.pane.deleted.remove(self.uid)` to remove the event from the deleted list if it is already there.
3. Use `eventcolumn.pane.window.backtrack()` to close the overlay and return to the previous screen."
"        def delete_this(_):
            if self.event.ref == u'PROTO':
                instance = self.event.start
            else:
                instance = self.event.ref
            self.event.delete_instance(instance)

            self.eventcolumn.pane.collection.update(self.event)
            self.eventcolumn.pane.window.backtrack()","1. Use `instance.delete()` instead of `self.event.delete_instance()` to avoid leaking references.
2. Use `self.eventcolumn.pane.collection.delete()` instead of `self.eventcolumn.pane.collection.update()` to avoid leaving dangling references.
3. Use `self.eventcolumn.pane.window.backtrack()` to ensure that the user is returned to the previous state after the deletion."
"def vertical_month(month=datetime.date.today().month,
                   year=datetime.date.today().year,
                   today=datetime.date.today(),
                   weeknumber=False,
                   count=3,
                   firstweekday=0):
    """"""
    returns a list() of str() of weeks for a vertical arranged calendar

    :param month: first month of the calendar,
                  if non given, current month is assumed
    :type month: int
    :param year: year of the first month included,
                 if non given, current year is assumed
    :type year: int
    :param today: day highlighted, if non is given, current date is assumed
    :type today: datetime.date()
    :param weeknumber: if not False the iso weeknumber will be shown for each
                       week, if weeknumber is 'right' it will be shown in its
                       own column, if it is 'left' it will be shown interleaved
                       with the month names
    :type weeknumber: str/bool
    :returns: calendar strings,  may also include some
              ANSI (color) escape strings
    :rtype: list() of str()
    """"""

    khal = list()
    w_number = '    ' if weeknumber == 'right' else ''
    calendar.setfirstweekday(firstweekday)
    _calendar = calendar.Calendar(firstweekday)
    khal.append(bstring('    ' + calendar.weekheader(2) + ' ' + w_number))
    for _ in range(count):
        for week in _calendar.monthdatescalendar(year, month):
            new_month = len([day for day in week if day.day == 1])
            strweek = str_week(week, today)
            if new_month:
                m_name = bstring(calendar.month_abbr[week[6].month].ljust(4))
            elif weeknumber == 'left':
                m_name = bstring(' {:2} '.format(getweeknumber(week[0])))
            else:
                m_name = '    '
            if weeknumber == 'right':
                w_number = bstring(' {}'.format(getweeknumber(week[0])))
            else:
                w_number = ''

            sweek = m_name + strweek + w_number
            if sweek != khal[-1]:
                khal.append(sweek)
        month = month + 1
        if month > 12:
            month = 1
            year = year + 1
    return khal","1. Use `datetime.datetime.today()` instead of `datetime.date.today()` to avoid leaking the current year and month.
2. Use `calendar.setfirstweekday(firstweekday)` to set the first day of the week.
3. Use `calendar.monthdatescalendar(year, month)` to get the list of weeks in the specified month."
"def global_options(f):
    config = click.option('--config', '-c', default=None, metavar='PATH',
                          help='The config file to use.')
    verbose = click.option('--verbose', '-v', is_flag=True,
                           help='Output debugging information.')
    version = click.version_option(version=__version__)

    return config(verbose(version(f)))","1. Use click.argument() instead of click.option() to avoid accidentally leaking sensitive information.
2. Use click.password_option() to prompt users for passwords securely.
3. Use click.confirm() to confirm dangerous actions before executing them."
"def prepare_context(ctx, config, verbose):
    if ctx.obj is not None:
        return

    if verbose:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)

    ctx.obj = {}
    try:
        ctx.obj['conf'] = conf = get_config(config)
    except InvalidSettingsError:
        sys.exit(1)

    logger.debug('khal %s' % __version__)
    logger.debug('Using config:')
    logger.debug(to_unicode(stringify_conf(conf), 'utf-8'))

    if conf is None:
        raise click.UsageError('Invalid config file, exiting.')","1. Use `click.option` to parse command line arguments instead of `sys.argv`. This will help to prevent against injection attacks.
2. Use `logging.getLogger(__name__).setLevel(logging.DEBUG)` to set the logging level for the `khal` module to `DEBUG`. This will help to debug any security issues that may arise.
3. Use `conf is None` to check if the config file is valid. If it is not valid, raise a `click.UsageError` exception. This will help to prevent users from running the `khal` command with an invalid config file."
"def _get_cli():
    @click.group(invoke_without_command=True)
    @global_options
    @click.pass_context
    def cli(ctx, config, verbose):
        # setting the process title so it looks nicer in ps
        # shows up as 'khal' under linux and as 'python: khal (python2.7)'
        # under FreeBSD, which is still nicer than the default
        setproctitle('khal')
        prepare_context(ctx, config, verbose)

        if not ctx.invoked_subcommand:
            command = ctx.obj['conf']['default']['default_command']
            if command:
                ctx.invoke(cli.commands[command])
            else:
                click.echo(ctx.get_help())
                ctx.exit(1)

    @cli.command()
    @time_args
    @multi_calendar_option
    @click.pass_context
    def calendar(ctx, days, events, dates):
        '''Print calendar with agenda.'''
        controllers.calendar(
            build_collection(ctx),
            date=dates,
            firstweekday=ctx.obj['conf']['locale']['firstweekday'],
            encoding=ctx.obj['conf']['locale']['encoding'],
            locale=ctx.obj['conf']['locale'],
            weeknumber=ctx.obj['conf']['locale']['weeknumbers'],
            show_all_days=ctx.obj['conf']['default']['show_all_days'],
            days=days or ctx.obj['conf']['default']['days'],
            events=events
        )

    @cli.command()
    @time_args
    @multi_calendar_option
    @click.pass_context
    def agenda(ctx, days, events, dates):
        '''Print agenda.'''
        controllers.agenda(
            build_collection(ctx),
            date=dates,
            firstweekday=ctx.obj['conf']['locale']['firstweekday'],
            encoding=ctx.obj['conf']['locale']['encoding'],
            show_all_days=ctx.obj['conf']['default']['show_all_days'],
            locale=ctx.obj['conf']['locale'],
            days=days or ctx.obj['conf']['default']['days'],
            events=events,
        )

    @cli.command()
    @calendar_option
    @click.option('--location', '-l',
                  help=('The location of the new event.'))
    @click.option('--repeat', '-r',
                  help=('Repeat event: daily, weekly, monthly or yearly.'))
    @click.option('--until', '-u',
                  help=('Stop an event repeating on this date.'))
    @click.argument('description', nargs=-1, required=True)
    @click.pass_context
    def new(ctx, calendar, description, location, repeat, until):
        '''Create a new event.'''
        controllers.new_from_string(
            build_collection(ctx),
            calendar,
            ctx.obj['conf'],
            list(description),
            location=location,
            repeat=repeat,
            until=until.split(' ') if until is not None else None,
        )

    @cli.command('import')
    @click.option('--include-calendar', '-a', help=('The calendar to use.'),
                  expose_value=False, callback=_calendar_select_callback,
                  metavar='CAL')
    @click.option('--batch', help=('do not ask for any confirmation.'),
                  is_flag=True)
    @click.option('--random_uid', '-r', help=('Select a random uid.'),
                  is_flag=True)
    @click.argument('ics', type=click.File('rb'))
    @click.pass_context
    def import_ics(ctx, ics, batch, random_uid):
        '''Import events from an .ics file.

        If an event with the same UID is already present in the (implicitly)
        selected calendar import will ask before updating (i.e. overwriting)
        that old event with the imported one, unless --batch is given, than it
        will always update. If this behaviour is not desired, use the
        `--random-uid` flag to generate a new, random UID.
        If no calendar is specified (and not `--batch`), you will be asked
        to choose a calendar. You can either enter the number printed behind
        each calendar's name or any unique prefix of a calendar's name.

        '''
        ics_str = ics.read()
        controllers.import_ics(
            build_collection(ctx),
            ctx.obj['conf'],
            ics=ics_str,
            batch=batch,
            random_uid=random_uid
        )

    @cli.command()
    @multi_calendar_option
    @click.pass_context
    def interactive(ctx):
        '''Interactive UI. Also launchable via `ikhal`.'''
        controllers.interactive(build_collection(ctx), ctx.obj['conf'])

    @click.command()
    @multi_calendar_option
    @global_options
    @click.pass_context
    def interactive_cli(ctx, config, verbose):
        '''Interactive UI. Also launchable via `khal interactive`.'''
        prepare_context(ctx, config, verbose)
        controllers.interactive(build_collection(ctx), ctx.obj['conf'])

    @cli.command()
    @multi_calendar_option
    @click.pass_context
    def printcalendars(ctx):
        '''List all calendars.'''
        click.echo('\\n'.join(build_collection(ctx).names))

    @cli.command()
    @click.pass_context
    def printformats(ctx):
        '''Print a date in all formats.

        Print the date 2013-12-21 10:09 in all configured date(time)
        formats to check if these locale settings are configured to ones
        liking.'''
        from datetime import datetime
        time = datetime(2013, 12, 21, 10, 9)

        for strftime_format in [
                'longdatetimeformat', 'datetimeformat', 'longdateformat',
                'dateformat', 'timeformat']:
            dt_str = time.strftime(ctx.obj['conf']['locale'][strftime_format])
            click.echo('{}: {}'.format(strftime_format, dt_str))

    @cli.command()
    @multi_calendar_option
    @click.argument('search_string')
    @click.pass_context
    def search(ctx, search_string):
        '''Search for events matching SEARCH_STRING.

        For repetitive events only one event is currently shown.
        '''
        # TODO support for time ranges, location, description etc
        collection = build_collection(ctx)
        events = collection.search(search_string)
        event_column = list()
        term_width, _ = get_terminal_size()
        for event in events:
            desc = textwrap.wrap(event.event_description, term_width)
            event_column.extend([colored(d, event.color) for d in desc])
        click.echo(to_unicode(
            '\\n'.join(event_column),
            ctx.obj['conf']['locale']['encoding'])
        )

    @cli.command()
    @multi_calendar_option
    @click.argument('datetime', required=False, nargs=-1)
    @click.pass_context
    def at(ctx, datetime=None):
        '''Show all events scheduled for DATETIME.

        if DATETIME is given (or the string `now`) all events scheduled
        for this moment are shown, if only a time is given, the date is assumed
        to be today
        '''
        collection = build_collection(ctx)
        locale = ctx.obj['conf']['locale']
        dtime_list = list(datetime)
        if dtime_list == [] or dtime_list == ['now']:
            import datetime
            dtime = datetime.datetime.now()
        else:
            try:
                dtime, _ = aux.guessdatetimefstr(dtime_list, locale)
            except ValueError:
                logger.fatal(
                    '{} is not a valid datetime (matches neither {} nor {} nor'
                    ' {})'.format(
                        ' '.join(dtime_list),
                        locale['timeformat'],
                        locale['datetimeformat'],
                        locale['longdatetimeformat']))
                sys.exit(1)
        dtime = locale['local_timezone'].localize(dtime)
        dtime = dtime.astimezone(pytz.UTC)
        events = collection.get_events_at(dtime)
        event_column = list()
        term_width, _ = get_terminal_size()
        for event in events:
            desc = textwrap.wrap(event.event_description, term_width)
            event_column.extend([colored(d, event.color) for d in desc])
        click.echo(to_unicode(
            '\\n'.join(event_column),
            ctx.obj['conf']['locale']['encoding'])
        )

    return cli, interactive_cli","1. Use `click.option('--include-calendar', '-a', help=('The calendar to use.'), expose_value=False, callback=_calendar_select_callback, metavar='CAL')` to avoid XSS attacks.
2. Use `controllers.import_ics(build_collection(ctx), ctx.obj['conf'], ics=ics_str, batch=batch, random_uid=random_uid)` to prevent importing malicious ICS files.
3. Use `controllers.interactive(build_collection(ctx), ctx.obj['conf'])` to prevent interactively creating malicious events."
"    def cli(ctx, config, verbose):
        # setting the process title so it looks nicer in ps
        # shows up as 'khal' under linux and as 'python: khal (python2.7)'
        # under FreeBSD, which is still nicer than the default
        setproctitle('khal')
        prepare_context(ctx, config, verbose)

        if not ctx.invoked_subcommand:
            command = ctx.obj['conf']['default']['default_command']
            if command:
                ctx.invoke(cli.commands[command])
            else:
                click.echo(ctx.get_help())
                ctx.exit(1)","1. Use `click.argument` to sanitize user input.
2. Use `click.option` to validate user input.
3. Use `click.echo` to display helpful error messages."
"    def interactive_cli(ctx, config, verbose):
        '''Interactive UI. Also launchable via `khal interactive`.'''
        prepare_context(ctx, config, verbose)
        controllers.interactive(build_collection(ctx), ctx.obj['conf'])","1. Use `os.getenv()` to get the config file path instead of hardcoding it.
2. Use `pwdlib.getpass()` to get the password from the user instead of asking for it in the console.
3. Validate the input before using it to prevent against injection attacks."
"def import_event(vevent, collection, locale, batch, random_uid):
    """"""import one event into collection, let user choose the collection""""""

    # print all sub-events
    for sub_event in vevent:
        if not batch:
            event = Event.fromVEvents(
                [sub_event], calendar=collection.default_calendar_name, locale=locale)
            echo(event.event_description)

    # get the calendar to insert into
    if batch or len(collection.writable_names) == 1:
        calendar_name = collection.default_calendar_name
    else:
        choice = list()
        for num, name in enumerate(collection.writable_names):
            choice.append('{}({})'.format(name, num))
        choice = ', '.join(choice)
        while True:
            value = prompt('Which calendar do you want to import to? \\n'
                           '{}'.format(choice), default=collection.default_calendar_name)
            try:
                number = int(value)
                calendar_name = collection.writable_names[number]
                break
            except (ValueError, IndexError):
                matches = filter(lambda x: x.startswith(value), collection.writable_names)
                if len(matches) == 1:
                    calendar_name = matches[0]
                    break
            echo('invalid choice')

    if batch or confirm(""Do you want to import this event into `{}`?""
                        """".format(calendar_name)):
        ics = aux.ics_from_list(vevent, random_uid)
        try:
            collection.new(
                Item(ics.to_ical().decode('utf-8')),
                collection=calendar_name)
        except DuplicateUid:
            if batch or confirm(""An event with the same UID already exists. ""
                                ""Do you want to update it?""):
                collection.force_update(
                    Item(ics.to_ical().decode('utf-8')),
                    collection=calendar_name)
            else:
                logger.warn(""Not importing event with UID `{}`"".format(event.uid))","1. Use `input()` instead of `prompt()` to prevent users from entering arbitrary code.
2. Use `calendar.new()` instead of `collection.new()` to avoid overwriting existing events.
3. Use `calendar.force_update()` instead of `collection.update()` to prevent events from being overwritten by accident."
"def prepare_context(ctx, config, verbose):
    if ctx.obj is not None:
        return

    if verbose:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)

    ctx.obj = {}
    ctx.obj['conf'] = conf = get_config(config)

    out = StringIO()
    conf.write(out)
    logger.debug('Using config:')
    logger.debug(out.getvalue())

    if conf is None:
        raise click.UsageError('Invalid config file, exiting.')","1. Use `click.Option(envvar=...)` to make the config file path configurable via environment variable.
2. Validate the config file using `click.argument(...).validator(...)`.
3. Use `logging.getLogger(__name__).setLevel(logging.DEBUG)` to only log debug messages to the console when `verbose` is True."
"def get_config(config_path=None):
    """"""reads the config file, validates it and return a config dict

    :param config_path: path to a custom config file, if none is given the
                        default locations will be searched
    :type config_path: str
    :returns: configuration
    :rtype: dict
    """"""
    if config_path is None:
        config_path = _find_configuration_file()

    logger.debug('using the config file at {}'.format(config_path))
    config = ConfigObj(DEFAULTSPATH, interpolation=False)

    try:
        user_config = ConfigObj(config_path,
                                configspec=SPECPATH,
                                interpolation=False,
                                file_error=True,
                                )
    except ConfigObjError as error:
        logger.fatal('parsing the config file file with the following error: '
                     '{}'.format(error))
        logger.fatal('if you recently updated khal, the config file format '
                     'might have changed, in that case please consult the '
                     'CHANGELOG or other documentation')
        sys.exit(1)

    fdict = {'timezone': is_timezone,
             'expand_path': expand_path,
             }
    validator = Validator(fdict)
    results = user_config.validate(validator, preserve_errors=True)
    if not results:
        for entry in flatten_errors(config, results):
            # each entry is a tuple
            section_list, key, error = entry
            if key is not None:
                section_list.append(key)
            else:
                section_list.append('[missing section]')
            section_string = ', '.join(section_list)
            if error is False:
                error = 'Missing value or section.'
            print(section_string, ' = ', error)
        raise ValueError  # TODO own error class

    config.merge(user_config)
    config_checks(config)

    extras = get_extra_values(user_config)
    for section, value in extras:
        if section == ():
            logger.warn('unknown section ""{}"" in config file'.format(value))
        else:
            section = sectionize(section)
            logger.warn('unknown key or subsection ""{}"" in '
                        'section ""{}""'.format(value, section))
    return config","1. Use `ConfigObj(interpolation=False)` to disable interpolation.
2. Use `validator=Validator(fdict)` to validate the config file.
3. Use `config_checks(config)` to check the config file for errors."
"def config_checks(config):
    """"""do some tests on the config we cannot do with configobj's validator""""""
    if len(config['calendars'].keys()) < 1:
        raise ValueError('Found no calendar in the config file')
    test_default_calendar(config)
    config['sqlite']['path'] = expand_db_path(config['sqlite']['path'])
    if not config['locale']['default_timezone']:
        config['locale']['default_timezone'] = is_timezone(
            config['locale']['default_timezone'])
    if not config['locale']['local_timezone']:
        config['locale']['local_timezone'] = is_timezone(
            config['locale']['local_timezone'])","1. Use `configparser` instead of `configobj` to validate the configuration file.
2. Sanitize user input before using it to construct the database path.
3. Use `tzinfo` to validate the timezones."
"def expand(vevent, default_tz, href=''):
    """"""

    :param vevent: vevent to be expanded
    :type vevent: icalendar.cal.Event
    :param default_tz: the default timezone used when we (icalendar)
                       don't understand the embedded timezone
    :type default_tz: pytz.timezone
    :param href: the href of the vevent, used for more informative logging
    :type href: str
    :returns: list of start and end (date)times of the expanded event
    :rtyped list(tuple(datetime, datetime))
    """"""
    # we do this now and than never care about the ""real"" end time again
    if 'DURATION' in vevent:
        duration = vevent['DURATION'].dt
    else:
        duration = vevent['DTEND'].dt - vevent['DTSTART'].dt

    # icalendar did not understand the defined timezone
    if ('TZID' in vevent['DTSTART'].params and
            vevent['DTSTART'].dt.tzinfo is None):
        vevent['DTSTART'].dt = default_tz.localize(vevent['DTSTART'].dt)

    if 'RRULE' not in vevent.keys():
        return [(vevent['DTSTART'].dt, vevent['DTSTART'].dt + duration)]

    events_tz = None
    if (hasattr(vevent['DTSTART'].dt, 'tzinfo') and
            vevent['DTSTART'].dt.tzinfo is not None):
        events_tz = vevent['DTSTART'].dt.tzinfo
        vevent['DTSTART'].dt = vevent['DTSTART'].dt.astimezone(pytz.UTC)

    # dateutil.rrule converts everything to datetime
    allday = True if not isinstance(vevent['DTSTART'].dt, datetime) else False

    rrulestr = vevent['RRULE'].to_ical()
    rrule = dateutil.rrule.rrulestr(rrulestr, dtstart=vevent['DTSTART'].dt)
    if not set(['UNTIL', 'COUNT']).intersection(vevent['RRULE'].keys()):
        # rrule really doesn't like to calculate all recurrences until
        # eternity, so we only do it 15years into the future
        rrule._until = vevent['DTSTART'].dt + timedelta(days=15 * 365)
    if (rrule._until is not None and
            rrule._until.tzinfo is None and
            vevent['DTSTART'].dt.tzinfo is not None):
        rrule._until = vevent['DTSTART'].dt.tzinfo.localize(rrule._until)
    logging.debug('calculating recurrence dates for {0}, '
                  'this might take some time.'.format(href))
    dtstartl = list(rrule)

    if len(dtstartl) == 0:
        raise UnsupportedRecursion

    if events_tz is not None:
        dtstartl = [start.astimezone(events_tz) for start in dtstartl]
    elif allday:
        dtstartl = [start.date() for start in dtstartl]

    dtstartend = [(start, start + duration) for start in dtstartl]
    return dtstartend","1. Use `pytz.utc` instead of `pytz.timezone` to avoid timezone
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   "
"def expand(vevent, default_tz, href=''):
    """"""

    :param vevent: vevent to be expanded
    :type vevent: icalendar.cal.Event
    :param default_tz: the default timezone used when we (icalendar)
                       don't understand the embedded timezone
    :type default_tz: pytz.timezone
    :param href: the href of the vevent, used for more informative logging
    :type href: str
    :returns: list of start and end (date)times of the expanded event
    :rtyped list(tuple(datetime, datetime))
    """"""
    # we do this now and than never care about the ""real"" end time again
    if 'DURATION' in vevent:
        duration = vevent['DURATION'].dt
    else:
        duration = vevent['DTEND'].dt - vevent['DTSTART'].dt

    # dateutil.rrule converts everything to datetime
    allday = True if not isinstance(vevent['DTSTART'].dt, datetime) else False

    # icalendar did not understand the defined timezone
    if (not allday and 'TZID' in vevent['DTSTART'].params and
            vevent['DTSTART'].dt.tzinfo is None):
        vevent['DTSTART'].dt = default_tz.localize(vevent['DTSTART'].dt)

    if 'RRULE' not in vevent.keys():
        return [(vevent['DTSTART'].dt, vevent['DTSTART'].dt + duration)]

    events_tz = None
    if (hasattr(vevent['DTSTART'].dt, 'tzinfo') and
            vevent['DTSTART'].dt.tzinfo is not None):
        events_tz = vevent['DTSTART'].dt.tzinfo
        vevent['DTSTART'].dt = vevent['DTSTART'].dt.astimezone(pytz.UTC)

    rrulestr = vevent['RRULE'].to_ical()
    rrule = dateutil.rrule.rrulestr(rrulestr, dtstart=vevent['DTSTART'].dt)
    if not set(['UNTIL', 'COUNT']).intersection(vevent['RRULE'].keys()):
        # rrule really doesn't like to calculate all recurrences until
        # eternity, so we only do it 15years into the future
        rrule._until = vevent['DTSTART'].dt + timedelta(days=15 * 365)
    if (rrule._until is not None and
            rrule._until.tzinfo is None and
            vevent['DTSTART'].dt.tzinfo is not None):
        rrule._until = vevent['DTSTART'].dt.tzinfo.localize(rrule._until)
    logging.debug('calculating recurrence dates for {0}, '
                  'this might take some time.'.format(href))
    dtstartl = list(rrule)

    if len(dtstartl) == 0:
        raise UnsupportedRecursion

    if events_tz is not None:
        dtstartl = [start.astimezone(events_tz) for start in dtstartl]
    elif allday:
        dtstartl = [start.date() for start in dtstartl]

    dtstartend = [(start, start + duration) for start in dtstartl]
    return dtstartend","1. Use `pytz.timezone` to ensure that all datetime objects are timezone-aware.
2. Use `dateutil.rrule.rrulestr` to parse the RRULE property of the vevent object.
3. Use `dateutil.rrule.rrule` to iterate over the recurrence dates of the vevent object."
"    def update(self, vevent, account, href='', etag='', status=OK):
        """"""insert a new or update an existing card in the db

        This is mostly a wrapper around two SQL statements, doing some cleanup
        before.

        :param vcard: vcard to be inserted or updated
        :type vcard: unicode
        :param href: href of the card on the server, if this href already
                     exists in the db the card gets updated. If no href is
                     given, a random href is chosen and it is implied that this
                     card does not yet exist on the server, but will be
                     uploaded there on next sync.
        :type href: str()
        :param etag: the etga of the vcard, if this etag does not match the
                     remote etag on next sync, this card will be updated from
                     the server. For locally created vcards this should not be
                     set
        :type etag: str()
        :param status: status of the vcard
                       * OK: card is in sync with remote server
                       * NEW: card is not yet on the server, this needs to be
                              set for locally created vcards
                       * CHANGED: card locally changed, will be updated on the
                                  server on next sync (if remote card has not
                                  changed since last sync)
                       * DELETED: card locally delete, will also be deleted on
                                  one the server on next sync (if remote card
                                  has not changed)
        :type status: one of backend.OK, backend.NEW, backend.CHANGED,
                      backend.DELETED


        """"""
        self._check_account(account)
        if not isinstance(vevent, icalendar.cal.Event):
            ical = icalendar.Event.from_ical(vevent)
            for component in ical.walk():
                if component.name == 'VEVENT':
                    vevent = component
        all_day_event = False
        if href == '' or href is None:
            href = get_random_href()
        if 'VALUE' in vevent['DTSTART'].params:
            if vevent['DTSTART'].params['VALUE'] == 'DATE':
                all_day_event = True

        dtstart = vevent['DTSTART'].dt

        if 'RRULE' in vevent.keys():
            rrulestr = vevent['RRULE'].to_ical()
            rrule = dateutil.rrule.rrulestr(rrulestr, dtstart=dtstart)
            today = datetime.datetime.today()
            if hasattr(dtstart, 'tzinfo') and dtstart.tzinfo is not None:
                # would be better to check if self is all day event
                today = self.conf.default.default_timezone.localize(today)
            if not set(['UNTIL', 'COUNT']).intersection(vevent['RRULE'].keys()):
                # rrule really doesn't like to calculate all recurrences until
                # eternity
                rrule._until = today + datetime.timedelta(days=15 * 365)
            logging.debug('calculating recurrence dates for {0}, '
                          'this might take some time.'.format(href))
            dtstartl = list(rrule)
            if len(dtstartl) == 0:
                raise UpdateFailed('Unsupported recursion rule for event '
                                   '{0}:\\n{1}'.format(href, vevent.to_ical()))

            if 'DURATION' in vevent.keys():
                duration = vevent['DURATION'].dt
            else:
                duration = vevent['DTEND'].dt - vevent['DTSTART'].dt
            dtstartend = [(start, start + duration) for start in dtstartl]
        else:
            if 'DTEND' in vevent.keys():
                dtend = vevent['DTEND'].dt
            else:
                dtend = vevent['DTSTART'].dt + vevent['DURATION'].dt
            dtstartend = [(dtstart, dtend)]

        for dbname in [account + '_d', account + '_dt']:
            sql_s = ('DELETE FROM {0} WHERE href == ?'.format(dbname))
            self.sql_ex(sql_s, (href, ), commit=False)

        for dtstart, dtend in dtstartend:
            if all_day_event:
                dbstart = dtstart.strftime('%Y%m%d')
                dbend = dtend.strftime('%Y%m%d')
                dbname = account + '_d'
            else:
                # TODO: extract strange (aka non Olson) TZs from params['TZID']
                # perhaps better done in model/vevent
                if dtstart.tzinfo is None:
                    dtstart = self.conf.default.default_timezone.localize(dtstart)
                if dtend.tzinfo is None:
                    dtend = self.conf.default.default_timezone.localize(dtend)

                dtstart_utc = dtstart.astimezone(pytz.UTC)
                dtend_utc = dtend.astimezone(pytz.UTC)
                dbstart = calendar.timegm(dtstart_utc.timetuple())
                dbend = calendar.timegm(dtend_utc.timetuple())
                dbname = account + '_dt'

            sql_s = ('INSERT INTO {0} '
                     '(dtstart, dtend, href) '
                     'VALUES (?, ?, ?);'.format(dbname))
            stuple = (dbstart,
                      dbend,
                      href)
            self.sql_ex(sql_s, stuple, commit=False)

        sql_s = ('INSERT OR REPLACE INTO {0} '
                 '(status, vevent, etag, href) '
                 'VALUES (?, ?, ?, '
                 'COALESCE((SELECT href FROM {0} WHERE href = ?), ?)'
                 ');'.format(account + '_m'))

        stuple = (status,
                  vevent.to_ical().decode('utf-8'),
                  etag,
                  href,
                  href)
        self.sql_ex(sql_s, stuple, commit=False)
        self.conn.commit()","1. Use prepared statements instead of building SQL strings.
2. Sanitize user input to prevent SQL injection attacks.
3. Use a secure password hashing function to protect passwords."
"    def __init__(self, conf):

        db_path = conf.sqlite.path
        if db_path is None:
            db_path = xdg.BaseDirectory.save_data_path('khal') + '/khal.db'
        self.db_path = path.expanduser(db_path)
        self.conn = sqlite3.connect(self.db_path)
        self.cursor = self.conn.cursor()
        self.debug = conf.debug
        self._create_default_tables()
        self._check_table_version()
        self.conf = conf","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Set the appropriate permissions on the database file to prevent unauthorized access.
3. Use a secure password for the database."
"    def needs_update(self, account, href_etag_list):
        """"""checks if we need to update this vcard
        :param account: account
        :param account: string
        :param href_etag_list: list of tuples of (hrefs and etags)
        :return: list of hrefs that need an update
        """"""
        needs_update = list()
        for href, etag in href_etag_list:
            stuple = (href,)
            sql_s = 'SELECT etag FROM {0} WHERE href = ?'.format(account + '_m')
            result = self.sql_ex(sql_s, stuple)
            if not result or etag != result[0][0]:
                needs_update.append(href)
        return needs_update","1. Use prepared statements instead of building SQL strings in the code. This will prevent SQL injection attacks.
2. Use a secure hashing function to store the etag values.
3. Sanitize all input data before using it in the code."
"    def update(self, vevent, account, href='', etag='', status=OK):
        """"""insert a new or update an existing card in the db

        This is mostly a wrapper around two SQL statements, doing some cleanup
        before.

        :param vcard: vcard to be inserted or updated
        :type vcard: unicode
        :param href: href of the card on the server, if this href already
                     exists in the db the card gets updated. If no href is
                     given, a random href is chosen and it is implied that this
                     card does not yet exist on the server, but will be
                     uploaded there on next sync.
        :type href: str()
        :param etag: the etga of the vcard, if this etag does not match the
                     remote etag on next sync, this card will be updated from
                     the server. For locally created vcards this should not be
                     set
        :type etag: str()
        :param status: status of the vcard
                       * OK: card is in sync with remote server
                       * NEW: card is not yet on the server, this needs to be
                              set for locally created vcards
                       * CHANGED: card locally changed, will be updated on the
                                  server on next sync (if remote card has not
                                  changed since last sync)
                       * DELETED: card locally delete, will also be deleted on
                                  one the server on next sync (if remote card
                                  has not changed)
        :type status: one of backend.OK, backend.NEW, backend.CHANGED,
                      backend.DELETED


        """"""
        if not isinstance(vevent, icalendar.cal.Event):
            ical = icalendar.Event.from_ical(vevent)
            for component in ical.walk():
                if component.name == 'VEVENT':
                    vevent = component
        all_day_event = False
        if href == '' or href is None:
            href = get_random_href()
        if 'VALUE' in vevent['DTSTART'].params:
            if vevent['DTSTART'].params['VALUE'] == 'DATE':
                all_day_event = True

        dtstart = vevent['DTSTART'].dt

        if 'RRULE' in vevent.keys():
            rrulestr = vevent['RRULE'].to_ical()
            rrule = dateutil.rrule.rrulestr(rrulestr, dtstart=dtstart)
            today = datetime.datetime.today()
            if hasattr(dtstart, 'tzinfo') and dtstart.tzinfo is not None:
                # would be better to check if self is all day event
                today = self.conf.default.default_timezone.localize(today)
            rrule._until = today + datetime.timedelta(days=15 * 365)
            logging.debug('calculating recurrence dates for {0}, '
                          'this might take some time.'.format(href))
            dtstartl = list(rrule)
            if len(dtstartl) == 0:
                raise UpdateFailed('Unsupported recursion rule for event '
                                   '{0}:\\n{1}'.format(href, vevent.to_ical()))

            if 'DURATION' in vevent.keys():
                duration = vevent['DURATION'].dt
            else:
                duration = vevent['DTEND'].dt - vevent['DTSTART'].dt
            dtstartend = [(start, start + duration) for start in dtstartl]
        else:
            if 'DTEND' in vevent.keys():
                dtend = vevent['DTEND'].dt
            else:
                dtend = vevent['DTSTART'].dt + vevent['DURATION'].dt
            dtstartend = [(dtstart, dtend)]

        for dbname in [account + '_d', account + '_dt']:
            sql_s = ('DELETE FROM {0} WHERE href == ?'.format(dbname))
            self.sql_ex(sql_s, (href, ), commit=False)

        for dtstart, dtend in dtstartend:
            if all_day_event:
                dbstart = dtstart.strftime('%Y%m%d')
                dbend = dtend.strftime('%Y%m%d')
                dbname = account + '_d'
            else:
                # TODO: extract strange (aka non Olson) TZs from params['TZID']
                # perhaps better done in model/vevent
                if dtstart.tzinfo is None:
                    dtstart = self.conf.default.default_timezone.localize(dtstart)
                if dtend.tzinfo is None:
                    dtend = self.conf.default.default_timezone.localize(dtend)

                dtstart_utc = dtstart.astimezone(pytz.UTC)
                dtend_utc = dtend.astimezone(pytz.UTC)
                dbstart = calendar.timegm(dtstart_utc.timetuple())
                dbend = calendar.timegm(dtend_utc.timetuple())
                dbname = account + '_dt'

            sql_s = ('INSERT INTO {0} '
                     '(dtstart, dtend, href) '
                     'VALUES (?, ?, ?);'.format(dbname))
            stuple = (dbstart,
                      dbend,
                      href)
            self.sql_ex(sql_s, stuple, commit=False)

        sql_s = ('INSERT OR REPLACE INTO {0} '
                 '(status, vevent, etag, href) '
                 'VALUES (?, ?, ?, '
                 'COALESCE((SELECT href FROM {0} WHERE href = ?), ?)'
                 ');'.format(account + '_m'))

        stuple = (status,
                  vevent.to_ical().decode('utf-8'),
                  etag,
                  href,
                  href)
        self.sql_ex(sql_s, stuple, commit=False)
        self.conn.commit()","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to avoid hardcoded values.
3. Sanitize user input to prevent cross-site scripting attacks."
"    def update_href(self, oldhref, newhref, account, etag='', status=OK):
        """"""updates old_href to new_href, can also alter etag and status,
        see update() for an explanation of these parameters""""""
        stuple = (newhref, etag, status, oldhref)
        sql_s = 'UPDATE {0} SET href = ?, etag = ?, status = ? \\
             WHERE href = ?;'.format(account + '_m')
        self.sql_ex(sql_s, stuple)
        for dbname in [account + '_d', account + '_dt']:
            sql_s = 'UPDATE {0} SET href = ? WHERE href = ?;'.format(dbname)
            self.sql_ex(sql_s, (newhref, oldhref))","1. Use prepared statements instead of building the SQL query string directly. This will prevent SQL injection attacks.
2. Use parameterized queries instead of concatenating strings. This will prevent SQL injection attacks.
3. Use the `self.cursor.execute()` method to execute queries instead of `self.cursor.executescript()`. This will prevent stored procedure attacks."
"    def href_exists(self, href, account):
        """"""returns True if href already exists in db

        :param href: href
        :type href: str()
        :returns: True or False
        """"""
        sql_s = 'SELECT href FROM {0} WHERE href = ?;'.format(account)
        if len(self.sql_ex(sql_s, (href, ))) == 0:
            return False
        else:
            return True","1. Use prepared statements to prevent SQL injection attacks.
2. Sanitize user input to prevent cross-site scripting (XSS) attacks.
3. Use strong encryption to protect sensitive data."
"    def get_etag(self, href, account):
        """"""get etag for href

        type href: str()
        return: etag
        rtype: str()
        """"""
        sql_s = 'SELECT etag FROM {0} WHERE href=(?);'.format(account + '_m')
        etag = self.sql_ex(sql_s, (href,))[0][0]
        return etag","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Use the `fetchone()` method to retrieve a single row from the database, rather than using `fetchall()` and then iterating over the results.
3. Sanitize user input before using it in SQL queries to prevent cross-site scripting attacks."
"    def delete(self, href, account):
        """"""
        removes the event from the db,
        returns nothing
        """"""
        logging.debug(""locally deleting "" + str(href))
        for dbname in [account + '_d', account + '_dt', account + '_m']:
            sql_s = 'DELETE FROM {0} WHERE href = ? ;'.format(dbname)
            self.sql_ex(sql_s, (href, ))","1. Use prepared statements instead of parameterized queries to prevent SQL injection attacks.
2. Sanitize user input to prevent cross-site scripting attacks.
3. Use strong passwords for the database account."
"    def get_all_href_from_db(self, accounts):
        """"""returns a list with all hrefs
        """"""
        result = list()
        for account in accounts:
            hrefs = self.sql_ex('SELECT href FROM {0}'.format(account + '_m'))
            result = result + [(href[0], account) for href in hrefs]
        return result","1. Use prepared statements instead of string concatenation to prevent SQL injection attacks.
2. Use a database authentication mechanism such as Kerberos or LDAP to prevent unauthorized access to the database.
3. Use access control lists to restrict which users have access to which data in the database."
"    def get_all_href_from_db_not_new(self, accounts):
        """"""returns list of all not new hrefs""""""
        result = list()
        for account in accounts:
            sql_s = 'SELECT href FROM {0} WHERE status != (?)'.format(account + '_m')
            stuple = (NEW,)
            hrefs = self.sql_ex(sql_s, stuple)
            result = result + [(href[0], account) for href in hrefs]
        return result","1. Use prepared statements instead of building SQL queries with string concatenation. This will prevent SQL injection attacks.
2. Use parameterized queries instead of passing values directly into SQL statements. This will prevent SQL injection attacks.
3. Use the `fetchall()` method to retrieve all rows from a database query, rather than using `fetchone()` to retrieve rows one at a time. This will prevent you from accidentally accessing more rows than you intended."
"    def get_time_range(self, start, end, account, color='', readonly=False,
                       unicode_symbols=True, show_deleted=True):
        """"""returns
        :type start: datetime.datetime
        :type end: datetime.datetime
        :param deleted: include deleted events in returned lsit
        """"""
        start = time.mktime(start.timetuple())
        end = time.mktime(end.timetuple())
        sql_s = ('SELECT href, dtstart, dtend FROM {0} WHERE '
                 'dtstart >= ? AND dtstart <= ? OR '
                 'dtend >= ? AND dtend <= ? OR '
                 'dtstart <= ? AND dtend >= ?').format(account + '_dt')
        stuple = (start, end, start, end, start, end)
        result = self.sql_ex(sql_s, stuple)
        event_list = list()
        for href, start, end in result:
            start = pytz.UTC.localize(datetime.datetime.utcfromtimestamp(start))
            end = pytz.UTC.localize(datetime.datetime.utcfromtimestamp(end))
            event = self.get_vevent_from_db(href, account,
                                            start=start, end=end,
                                            color=color,
                                            readonly=readonly,
                                            unicode_symbols=unicode_symbols)
            if show_deleted or event.status not in [DELETED, CALCHANGED, NEWDELETE]:
                event_list.append(event)

        return event_list","1. Use prepared statements instead of concatenation to prevent SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use strong passwords for the database and other services."
"    def get_allday_range(self, start, end=None, account=None,
                         color='', readonly=False, unicode_symbols=True, show_deleted=True):
        if account is None:
            raise Exception('need to specify an account')
        strstart = start.strftime('%Y%m%d')
        if end is None:
            end = start + datetime.timedelta(days=1)
        strend = end.strftime('%Y%m%d')
        sql_s = ('SELECT href, dtstart, dtend FROM {0} WHERE '
                 'dtstart >= ? AND dtstart < ? OR '
                 'dtend > ? AND dtend <= ? OR '
                 'dtstart <= ? AND dtend > ? ').format(account + '_d')
        stuple = (strstart, strend, strstart, strend, strstart, strend)
        result = self.sql_ex(sql_s, stuple)
        event_list = list()
        for href, start, end in result:
            start = time.strptime(str(start), '%Y%m%d')
            end = time.strptime(str(end), '%Y%m%d')
            start = datetime.date(start.tm_year, start.tm_mon, start.tm_mday)
            end = datetime.date(end.tm_year, end.tm_mon, end.tm_mday)
            vevent = self.get_vevent_from_db(href, account,
                                             start=start, end=end,
                                             color=color,
                                             readonly=readonly,
                                             unicode_symbols=unicode_symbols)
            if show_deleted or vevent.status not in [DELETED, CALCHANGED, NEWDELETE]:
                event_list.append(vevent)
        return event_list","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Sanitize user input to prevent cross-site scripting attacks.
3. Use strong passwords for the database account."
"    def hrefs_by_time_range_datetime(self, start, end, account, color=''):
        """"""returns
        :type start: datetime.datetime
        :type end: datetime.datetime
        """"""
        start = time.mktime(start.timetuple())
        end = time.mktime(end.timetuple())
        sql_s = ('SELECT href FROM {0} WHERE '
                 'dtstart >= ? AND dtstart <= ? OR '
                 'dtend >= ? AND dtend <= ? OR '
                 'dtstart <= ? AND dtend >= ?').format(account + '_dt')
        stuple = (start, end, start, end, start, end)
        result = self.sql_ex(sql_s, stuple)
        return [one[0] for one in result]","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Use a more secure timestamp format, such as YYYY-MM-DD HH:MM:SS.SSS.
3. Sanitize user input before using it in SQL queries."
"    def hrefs_by_time_range_date(self, start, end=None, account=None):
        if account is None:
            raise Exception('need to specify an account')
        strstart = start.strftime('%Y%m%d')
        if end is None:
            end = start + datetime.timedelta(days=1)
        strend = end.strftime('%Y%m%d')
        sql_s = ('SELECT href FROM {0} WHERE '
                 'dtstart >= ? AND dtstart < ? OR '
                 'dtend > ? AND dtend <= ? OR '
                 'dtstart <= ? AND dtend > ? ').format(account + '_d')
        stuple = (strstart, strend, strstart, strend, strstart, strend)
        result = self.sql_ex(sql_s, stuple)
        return [one[0] for one in result]","1. Use prepared statements instead of building SQL strings in Python.
2. Sanitize user input to prevent SQL injection attacks.
3. Use a database that supports secure access control."
"    def get_vevent_from_db(self, href, account, start=None, end=None,
                           readonly=False, color=lambda x: x,
                           unicode_symbols=True):
        """"""returns the Event matching href, if start and end are given, a
        specific Event from a Recursion set is returned, the Event as saved in
        the db

        All other parameters given to this function are handed over to the
        Event.
        """"""
        sql_s = 'SELECT vevent, status, etag FROM {0} WHERE href=(?)'.format(account + '_m')
        result = self.sql_ex(sql_s, (href, ))
        return Event(result[0][0],
                     local_tz=self.conf.default.local_timezone,
                     default_tz=self.conf.default.default_timezone,
                     start=start,
                     end=end,
                     color=color,
                     href=href,
                     account=account,
                     status=result[0][1],
                     readonly=readonly,
                     etag=result[0][2],
                     unicode_symbols=unicode_symbols)","1. Use prepared statements to prevent SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use strong passwords for database accounts."
"    def get_changed(self, account):
        """"""returns list of hrefs of locally edited vevents
        """"""
        sql_s = 'SELECT href FROM {0} WHERE status == (?)'.format(account + '_m')
        result = self.sql_ex(sql_s, (CHANGED, ))
        return [row[0] for row in result]","1. Use prepared statements instead of building SQL queries manually. This will prevent SQL injection attacks.
2. Use a database with proper access control to prevent unauthorized users from accessing data.
3. Use strong passwords for all database accounts."
"    def get_new(self, account):
        """"""returns list of hrefs of locally added vcards
        """"""
        sql_s = 'SELECT href FROM {0} WHERE status == (?)'.format(account + '_m')
        result = self.sql_ex(sql_s, (NEW, ))
        return [row[0] for row in result]","1. Use prepared statements instead of building SQL strings in code. This will prevent SQL injection attacks.
2. Use the `fetchall()` method to retrieve all rows from a query, rather than using `fetchone()` repeatedly. This will prevent you from accidentally skipping rows.
3. Use the `rowcount()` method to check the number of rows returned by a query, rather than relying on the length of the result list. This will prevent you from accidentally accessing rows that don't exist."
"    def get_marked_delete(self, account):
        """"""returns list of tuples (hrefs, etags) of locally deleted vcards
        """"""
        sql_s = ('SELECT href, etag FROM {0} WHERE status == '
                 '(?)'.format(account + '_m'))
        result = self.sql_ex(sql_s, (DELETED, ))
        return result","1. Use prepared statements instead of parameterized queries to avoid SQL injection.
2. Use a more secure hashing algorithm than MD5.
3. Sanitize user input to prevent cross-site scripting attacks."
"    def mark_delete(self, href, account):
        """"""marks the entry as to be deleted on server on next sync
        """"""
        sql_s = 'UPDATE {0} SET STATUS = ? WHERE href = ?'.format(account + '_m')
        self.sql_ex(sql_s, (DELETED, href, ))","1. Use prepared statements instead of parameterized queries to prevent SQL injection attacks.
2. Sanitize user input to prevent cross-site scripting attacks.
3. Use strong encryption to protect sensitive data."
"    def set_status(self, href, status, account):
        """"""sets the status of vcard
        """"""
        sql_s = 'UPDATE {0} SET STATUS = ? WHERE href = ?'.format(account + '_m')
        self.sql_ex(sql_s, (status, href, ))","1. Use prepared statements instead of building queries with string concatenation. This will prevent SQL injection attacks.
2. Sanitize user input before using it in SQL queries. This will prevent against attacks such as cross-site scripting (XSS).
3. Use strong passwords for the database account. This will help protect the database from being compromised."
"    def reset_flag(self, href, account):
        """"""
        resets the status for a given href to 0 (=not edited locally)
        """"""
        sql_s = 'UPDATE {0} SET status = ? WHERE href = ?'.format(account + '_m')
        self.sql_ex(sql_s, (OK, href, ))","1. Use prepared statements to prevent SQL injection.
2. Use `LIKE` instead of `=` to avoid triggering an UPDATE when the `href` is not found in the database.
3. Use `mysqli_real_escape_string()` to escape special characters in the `href` parameter."
"    def get_status(self, href, account):
        """"""
        gets the status of the event associated with href in `account`
        """"""
        sql_s = 'SELECT status FROM {0} WHERE href = (?)'.format(account + '_m')
        return self.sql_ex(sql_s, (href, ))[0][0]","1. Use prepared statements to prevent SQL injection.
2. Sanitize user input to prevent cross-site scripting (XSS).
3. Use strong hashing algorithms and salt values to protect passwords."
"    def _sync_caldav(self):
        syncer = caldav.Syncer(self._resource,
                               user=self._username,
                               password=self._password,
                               verify=self._ssl_verify,
                               auth=self._auth)
        self._dbtool.check_account_table(self.name)
        logging.debug('syncing events in the next 365 days')
        start = datetime.datetime.utcnow() - datetime.timedelta(days=30)
        start_utc = self._local_timezone.localize(start).astimezone(pytz.UTC)
        end_utc = start_utc + datetime.timedelta(days=365)
        href_etag_list = syncer.get_hel(start=start_utc, end=end_utc)
        need_update = self._dbtool.needs_update(self.name, href_etag_list)
        logging.debug('{number} event(s) need(s) an '
                      'update'.format(number=len(need_update)))
        vhe_list = syncer.get_vevents(need_update)
        for vevent, href, etag in vhe_list:
            try:
                self._dbtool.update(vevent,
                                    self.name,
                                    href=href,
                                    etag=etag)
            except backend.UpdateFailed as error:
                logging.error(error)
        # syncing local new events
        hrefs = self._dbtool.get_new(self.name)

        logging.debug('{number} new events need to be '
                      'uploaded'.format(number=len(hrefs)))
        try:
            for href in hrefs:
                event = self._dbtool.get_vevent_from_db(href, self.name)
                (href_new, etag_new) = syncer.upload(event.vevent,
                                                     self._default_timezone)
                self._dbtool.update_href(href,
                                         href_new,
                                         self.name,
                                         status=OK)
        except caldav.NoWriteSupport:
            logging.info('failed to upload a new event, '
                         'you need to enable write support to use this feature'
                         ', see the documentation.')

        # syncing locally modified events
        hrefs = self._dbtool.get_changed(self.name)
        for href in hrefs:
            event = self._dbtool.get_vevent_from_db(href, self.name)
            etag = syncer.update(event.vevent, event.href, event.etag)

        # looking for events deleted on the server but still in the local db
        locale_hrefs = self._dbtool.hrefs_by_time_range(start_utc,
                                                        end_utc,
                                                        self.name)
        remote_hrefs = [href for href, _ in href_etag_list]
        may_be_deleted = list(set(locale_hrefs) - set(remote_hrefs))
        if may_be_deleted != list():
            for href in may_be_deleted:
                if syncer.test_deleted(href) and self._dbtool.get_status(href, self.name) != NEW:
                    logging.debug('removing remotely deleted event {0} from '
                                  'the local db'.format(href))
                    self._dbtool.delete(href, self.name)","1. Use HTTPS instead of HTTP to protect the data from being intercepted.
2. Use strong passwords and avoid reusing passwords across multiple services.
3. Enable two-factor authentication for added security."
"    def _sync_http(self):
        """"""
        simple syncer to import events from .ics files
        """"""
        import icalendar
        self.syncer = caldav.HTTPSyncer(self._resource,
                                        user=self._username,
                                        password=self._password,
                                        verify=self._ssl_verify,
                                        auth=self._auth)
        self._dbtool.check_account_table(self.name)
        ics = self.syncer.get_ics()
        cal = icalendar.Calendar.from_ical(ics)
        remote_uids = list()
        for component in cal.walk():
            if component.name in ['VEVENT']:
                remote_uids.append(str(component['UID']))
                try:
                    self._dbtool.update(component,
                                        self.name,
                                        href=str(component['UID']),
                                        etag='',
                                        status=OK)
                except backend.UpdateFailed as error:
                    logging.error(error)
        # events from an icalendar retrieved over stupid http have no href
        # themselves, so their uid is safed in the `href` column
        locale_uids = [uid for uid, account in self._dbtool.get_all_href_from_db([self.name])]
        remote_deleted = list(set(locale_uids) - set(remote_uids))
        if remote_deleted != list():
            for uid in remote_deleted:
                logging.debug('removing remotely deleted event {0} from '
                              'the local db'.format(uid))
                self._dbtool.delete(uid, self.name)","1. Use HTTPS instead of HTTP to protect the traffic from eavesdropping.
2. Use strong passwords and enable two-factor authentication for the caldav account.
3. Sanitize user input to prevent cross-site scripting attacks."
"    def __init__(
        self,
        url: str = ""sqlite://"",
        index: str = ""document"",
        label_index: str = ""label"",
        update_existing_documents: bool = False,
    ):
        """"""
        An SQL backed DocumentStore. Currently supports SQLite, PostgreSQL and MySQL backends.

        :param url: URL for SQL database as expected by SQLAlchemy. More info here: https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
        :param index: The documents are scoped to an index attribute that can be used when writing, querying, or deleting documents. 
                      This parameter sets the default value for document index.
        :param label_index: The default value of index attribute for the labels.
        :param update_existing_documents: Whether to update any existing documents with the same ID when adding
                                          documents. When set as True, any document with an existing ID gets updated.
                                          If set to False, an error is raised if the document ID of the document being
                                          added already exists. Using this parameter could cause performance degradation
                                          for document insertion.
        """"""
        engine = create_engine(url)
        ORMBase.metadata.create_all(engine)
        Session = sessionmaker(bind=engine)
        self.session = Session()
        self.index = index
        self.label_index = label_index
        self.update_existing_documents = update_existing_documents
        if getattr(self, ""similarity"", None) is None:
            self.similarity = None","1. Use prepared statements instead of string concatenation to prevent SQL injection attacks.
2. Use a salt when hashing passwords to make them more difficult to crack.
3. Use HTTPS to protect data in transit."
"    def get_all_documents_generator(
        self,
        index: Optional[str] = None,
        filters: Optional[Dict[str, List[str]]] = None,
        return_embedding: Optional[bool] = None,
        batch_size: int = 10_000,
    ) -> Generator[Document, None, None]:
        """"""
        Get documents from the document store. Under-the-hood, documents are fetched in batches from the
        document store and yielded as individual documents. This method can be used to iteratively process
        a large number of documents without having to load all documents in memory.

        :param index: Name of the index to get the documents from. If None, the
                      DocumentStore's default index (self.index) will be used.
        :param filters: Optional filters to narrow down the documents to return.
                        Example: {""name"": [""some"", ""more""], ""category"": [""only_one""]}
        :param return_embedding: Whether to return the document embeddings.
        """"""

        index = index or self.index
        # Generally ORM objects kept in memory cause performance issue
        # Hence using directly column name improve memory and performance.
        # Refer https://stackoverflow.com/questions/23185319/why-is-loading-sqlalchemy-objects-via-the-orm-5-8x-slower-than-rows-via-a-raw-my
        documents_query = self.session.query(
            DocumentORM.id,
            DocumentORM.text,
            DocumentORM.vector_id
        ).filter_by(index=index)

        if filters:
            documents_query = documents_query.join(MetaORM)
            for key, values in filters.items():
                documents_query = documents_query.filter(
                    MetaORM.name == key,
                    MetaORM.value.in_(values),
                    DocumentORM.id == MetaORM.document_id
                )

        documents_map = {}
        for i, row in enumerate(self._windowed_query(documents_query, DocumentORM.id, batch_size), start=1):
            documents_map[row.id] = Document(
                id=row.id,
                text=row.text,
                meta=None if row.vector_id is None else {""vector_id"": row.vector_id}  # type: ignore
            )
            if i % batch_size == 0:
                documents_map = self._get_documents_meta(documents_map)
                yield from documents_map.values()
                documents_map = {}
        if documents_map:
            documents_map = self._get_documents_meta(documents_map)
            yield from documents_map.values()","1. Use prepared statements instead of building queries dynamically.
2. Use parameterized queries instead of passing values directly to SQL statements.
3. Use bind variables instead of concatenating strings with user input."
"    def __init__(
            self,
            model_name_or_path: str = ""facebook/rag-token-nq"",
            retriever: Optional[DensePassageRetriever] = None,
            generator_type: RAGeneratorType = RAGeneratorType.TOKEN,
            top_k_answers: int = 2,
            max_length: int = 200,
            min_length: int = 2,
            num_beams: int = 2,
            embed_title: bool = True,
            prefix: Optional[str] = None,
            use_gpu: bool = True,
    ):
        """"""
        Load a RAG model from Transformers along with passage_embedding_model.
        See https://huggingface.co/transformers/model_doc/rag.html for more details

        :param model_name_or_path: Directory of a saved model or the name of a public model e.g.
                                   'facebook/rag-token-nq', 'facebook/rag-sequence-nq'.
                                   See https://huggingface.co/models for full list of available models.
        :param retriever: `DensePassageRetriever` used to embedded passage
        :param generator_type: Which RAG generator implementation to use? RAG-TOKEN or RAG-SEQUENCE
        :param top_k_answers: Number of independently generated text to return
        :param max_length: Maximum length of generated text
        :param min_length: Minimum length of generated text
        :param num_beams: Number of beams for beam search. 1 means no beam search.
        :param embed_title: Embedded the title of passage while generating embedding
        :param prefix: The prefix used by the generator's tokenizer.
        :param use_gpu: Whether to use GPU (if available)
        """"""

        self.model_name_or_path = model_name_or_path
        self.top_k_answers = top_k_answers
        self.max_length = max_length
        self.min_length = min_length
        self.generator_type = generator_type
        self.num_beams = num_beams
        self.embed_title = embed_title
        self.prefix = prefix
        self.retriever = retriever

        if use_gpu and torch.cuda.is_available():
            self.device = torch.device(""cuda"")
        else:
            self.device = torch.device(""cpu"")

        self.tokenizer = RagTokenizer.from_pretrained(model_name_or_path)

        if self.generator_type == RAGeneratorType.SEQUENCE:
            raise NotImplementedError(""RagSequenceForGeneration is not implemented yet"")
            # TODO: Enable when transformers have it. Refer https://github.com/huggingface/transformers/issues/7905
            # Also refer refer https://github.com/huggingface/transformers/issues/7829
            # self.model = RagSequenceForGeneration.from_pretrained(model_name_or_path)
        else:
            self.model = RagTokenForGeneration.from_pretrained(model_name_or_path)","1. Use `torch.load` instead of `from_pretrained` to load the model, to avoid loading the model weights from the internet.
2. Use `torch.nn.DataParallel` to parallelize the model on multiple GPUs, if available.
3. Use `torch.jit.script` to JIT-compile the model, to improve performance."
"    def predict(self, question: str, documents: List[Document], top_k: Optional[int] = None) -> Dict:
        """"""
        Generate the answer to the input question. The generation will be conditioned on the supplied documents.
        These document can for example be retrieved via the Retriever.

        :param question: Question
        :param documents: Related documents (e.g. coming from a retriever) that the answer shall be conditioned on.
        :param top_k: Number of returned answers
        :return: Generated answers plus additional infos in a dict like this:

        ```python
        > {'question': 'who got the first nobel prize in physics',
        >    'answers':
        >        [{'question': 'who got the first nobel prize in physics',
        >          'answer': ' albert einstein',
        >          'meta': { 'doc_ids': [...],
        >                    'doc_scores': [80.42758 ...],
        >                    'doc_probabilities': [40.71379089355469, ...
        >                    'texts': ['Albert Einstein was a ...]
        >                    'titles': ['""Albert Einstein""', ...]
        >    }}]}
        ```
        """"""
        if len(documents) == 0:
            raise AttributeError(""generator need documents to predict the answer"")

        top_k_answers = top_k if top_k is not None else self.top_k_answers

        # Flatten the documents so easy to reference
        flat_docs_dict: Dict[str, Any] = {}
        for document in documents:
            for k, v in document.__dict__.items():
                if k not in flat_docs_dict:
                    flat_docs_dict[k] = []
                flat_docs_dict[k].append(v)

        # Extract title
        titles = [d.meta[""name""] if d.meta and ""name"" in d.meta else """" for d in documents]

        # Raw document embedding and set device of question_embedding
        passage_embeddings = self._prepare_passage_embeddings(docs=documents, embeddings=flat_docs_dict[""embedding""])

        # Question tokenization
        input_dict = self.tokenizer.prepare_seq2seq_batch(
            src_texts=[question],
            return_tensors=""pt""
        )

        # Question embedding
        question_embedding = self.model.question_encoder(input_dict[""input_ids""])[0]

        # Prepare contextualized input_ids of documents
        # (will be transformed into contextualized inputs inside generator)
        context_input_ids, context_attention_mask = self._get_contextualized_inputs(
            texts=flat_docs_dict[""text""],
            titles=titles,
            question=question
        )

        # Compute doc scores from docs_embedding
        doc_scores = torch.bmm(question_embedding.unsqueeze(1),
                               passage_embeddings.unsqueeze(0).transpose(1, 2)).squeeze(1)

        # TODO Need transformers 3.4.0
        # Refer https://github.com/huggingface/transformers/issues/7874
        # Pass it as parameter to generate function as follows -
        # n_docs=len(flat_docs_dict[""text""])
        self.model.config.n_docs = len(flat_docs_dict[""text""])

        # Get generated ids from generator
        generator_ids = self.model.generate(
            # TODO: Need transformers 3.4.0
            # Refer https://github.com/huggingface/transformers/issues/7871
            # Remove input_ids parameter once upgraded to 3.4.0
            input_ids=input_dict[""input_ids""],
            context_input_ids=context_input_ids,
            context_attention_mask=context_attention_mask,
            doc_scores=doc_scores,
            num_return_sequences=top_k_answers,
            num_beams=self.num_beams,
            max_length=self.max_length,
            min_length=self.min_length,
        )

        generated_answers = self.tokenizer.batch_decode(generator_ids, skip_special_tokens=True)
        answers: List[Any] = []

        for generated_answer in generated_answers:
            cur_answer = {
                ""question"": question,
                ""answer"": generated_answer,
                ""meta"": {
                    ""doc_ids"": flat_docs_dict[""id""],
                    ""doc_scores"": flat_docs_dict[""score""],
                    ""doc_probabilities"": flat_docs_dict[""probability""],
                    ""texts"": flat_docs_dict[""text""],
                    ""titles"": titles,
                }
            }
            answers.append(cur_answer)

        result = {""question"": question, ""answers"": answers}

        return result","1. Use `torch.no_grad()` to disable gradient calculation when not needed.
2. Use `torch.cuda.empty_cache()` to free up GPU memory after inference.
3. Use `torch.jit.save()` to save the model in a format that is not directly executable, making it more difficult to reverse engineer."
"    def query_by_embedding(self,
                           query_emb: np.array,
                           filters: Optional[Dict[str, List[str]]] = None,
                           top_k: int = 10,
                           index: Optional[str] = None) -> List[Document]:
        if index is None:
            index = self.index

        if not self.embedding_field:
            raise RuntimeError(""Please specify arg `embedding_field` in ElasticsearchDocumentStore()"")
        else:
            # +1 in similarity to avoid negative numbers (for cosine sim)
            body= {
                ""size"": top_k,
                ""query"": {
                    ""script_score"": {
                        ""query"": {""match_all"": {}},
                        ""script"": {
                            ""source"": f""{self.similarity_fn_name}(params.query_vector,'{self.embedding_field}') + 1.0"",
                            ""params"": {
                                ""query_vector"": query_emb.tolist()
                            }
                        }
                    }
                }
            }  # type: Dict[str,Any]

            if filters:
                for key, values in filters.items():
                    if type(values) != list:
                        raise ValueError(f'Wrong filter format for key ""{key}"": Please provide a list of allowed values for each key. '
                                         'Example: {""name"": [""some"", ""more""], ""category"": [""only_one""]} ')
                body[""query""][""script_score""][""query""] = {
                    ""terms"": filters
                }

            if self.excluded_meta_data:
                body[""_source""] = {""excludes"": self.excluded_meta_data}

            logger.debug(f""Retriever query: {body}"")
            result = self.client.search(index=index, body=body, request_timeout=300)[""hits""][""hits""]

            documents = [self._convert_es_hit_to_document(hit, adapt_score_for_embedding=True) for hit in result]
            return documents","1. Use `params` instead of `query_vector` to avoid exposing the query vector to the user.
2. Use `request_timeout` to limit the amount of time the search can take.
3. Use `filters` to restrict the results to a specific set of documents."
"    def _convert_es_hit_to_document(self, hit: dict, adapt_score_for_embedding: bool = False) -> Document:
        # We put all additional data of the doc into meta_data and return it in the API
        meta_data = {k:v for k,v in hit[""_source""].items() if k not in (self.text_field, self.faq_question_field, self.embedding_field)}
        name = meta_data.pop(self.name_field, None)
        if name:
            meta_data[""name""] = name

        score = hit[""_score""] if hit[""_score""] else None
        if score:
            if adapt_score_for_embedding:
                score -= 1
                probability = (score + 1) / 2  # scaling probability from cosine similarity
            else:
                probability = float(expit(np.asarray(score / 8)))  # scaling probability from TFIDF/BM25
        else:
            probability = None
        document = Document(
            id=hit[""_id""],
            text=hit[""_source""].get(self.text_field),
            meta=meta_data,
            score=score,
            probability=probability,
            question=hit[""_source""].get(self.faq_question_field),
            embedding=hit[""_source""].get(self.embedding_field)
        )
        return document","1. Use `hit[""_source""]` instead of `hit` to access the document fields, as `hit` may contain sensitive information such as the document ID.
2. Use `expit()` to scale the probability from cosine similarity, as the raw cosine similarity score can be very large.
3. Use `np.asarray()` to convert the score to a NumPy array, as this will prevent the score from being interpreted as a string."
"def convert_files_to_dicts(dir_path: str, clean_func: Optional[Callable] = None, split_paragraphs: bool = False) -> List[dict]:
    """"""
    Convert all files(.txt, .pdf) in the sub-directories of the given path to Python dicts that can be written to a
    Document Store.

    :param dir_path: path for the documents to be written to the DocumentStore
    :param clean_func: a custom cleaning function that gets applied to each doc (input: str, output:str)
    :param split_paragraphs: split text in paragraphs.

    :return: None
    """"""

    file_paths = [p for p in Path(dir_path).glob(""**/*"")]
    if "".pdf"" in [p.suffix.lower() for p in file_paths]:
        pdf_converter = PDFToTextConverter()  # type: Optional[PDFToTextConverter]
    else:
        pdf_converter = None

    documents = []
    for path in file_paths:
        if path.suffix.lower() == "".txt"":
            with open(path) as doc:
                text = doc.read()
        elif path.suffix.lower() == "".pdf"" and pdf_converter:
            document = pdf_converter.convert(path)
            text = document[""text""]
        else:
            raise Exception(f""Indexing of {path.suffix} files is not currently supported."")

        if clean_func:
            text = clean_func(text)

        if split_paragraphs:
            for para in text.split(""\\n\\n""):
                if not para.strip():  # skip empty paragraphs
                    continue
                documents.append({""text"": para, ""meta"": {""name"": path.name}})
        else:
            documents.append({""text"": text, ""meta"": {""name"": path.name}})

    return documents","1. Use `Pathlib` instead of `os.path` to handle file paths.
2. Use `type hints` to annotate the function parameters and return type.
3. Use `try/except` blocks to handle errors and exceptions."
"def tika_convert_files_to_dicts(
        dir_path: str,
        clean_func: Optional[Callable] = None,
        split_paragraphs: bool = False,
        merge_short: bool = True,
        merge_lowercase: bool = True
) -> List[dict]:
    """"""
    Convert all files(.txt, .pdf) in the sub-directories of the given path to Python dicts that can be written to a
    Document Store.

    :param dir_path: path for the documents to be written to the DocumentStore
    :param clean_func: a custom cleaning function that gets applied to each doc (input: str, output:str)
    :param split_paragraphs: split text in paragraphs.

    :return: None
    """"""
    converter = TikaConverter(remove_header_footer=True)
    file_paths = [p for p in Path(dir_path).glob(""**/*"")]

    documents = []
    for path in file_paths:
        document = converter.convert(path)
        meta = document[""meta""] or {}
        meta[""name""] = path.name
        text = document[""text""]
        pages = text.split(""\\f"")

        if split_paragraphs:
            if pages:
                paras = pages[0].split(""\\n\\n"")
                # pop the last paragraph from the first page
                last_para = paras.pop(-1) if paras else ''
                for page in pages[1:]:
                    page_paras = page.split(""\\n\\n"")
                    # merge the last paragraph in previous page to the first paragraph in this page
                    if page_paras:
                        page_paras[0] = last_para + ' ' + page_paras[0]
                        last_para = page_paras.pop(-1)
                        paras += page_paras
                if last_para:
                    paras.append(last_para)
                if paras:
                    last_para = ''
                    for para in paras:
                        para = para.strip()
                        if not para: continue
                        # merge paragraphs to improve qa
                        # merge this paragraph if less than 10 characters or 2 words
                        # or this paragraph starts with a lower case and last paragraph does not end with a punctuation
                        if merge_short and len(para) < 10 or len(re.findall('\\s+', para)) < 2 \\
                            or merge_lowercase and para and para[0].islower() and last_para and last_para[-1] not in '.?!""\\'\\]\\)':
                            last_para += ' ' + para
                        else:
                            if last_para:
                                documents.append({""text"": last_para, ""meta"": meta})
                            last_para = para
                    # don't forget the last one
                    if last_para:
                        documents.append({""text"": last_para, ""meta"": meta})
        else:
            if clean_func:
                text = clean_func(text)
            documents.append({""text"": text, ""meta"": meta})

    return documents","1. Use `Path.iterdir()` instead of `Path.glob()` to avoid traversing directories recursively.
2. Use `Path.read_text()` instead of `Path.read_bytes()` to avoid decoding errors.
3. Use `Path.joinpath()` instead of concatenating strings to avoid directory traversal attacks."
"def fetch_archive_from_http(url: str, output_dir: str, proxies: Optional[dict] = None):
    """"""
    Fetch an archive (zip or tar.gz) from a url via http and extract content to an output directory.

    :param url: http address
    :type url: str
    :param output_dir: local path
    :type output_dir: str
    :param proxies: proxies details as required by requests library
    :type proxies: dict
    :return: bool if anything got fetched
    """"""
    # verify & prepare local directory
    path = Path(output_dir)
    if not path.exists():
        path.mkdir(parents=True)

    is_not_empty = len(list(Path(path).rglob(""*""))) > 0
    if is_not_empty:
        logger.info(
            f""Found data stored in `{output_dir}`. Delete this first if you really want to fetch new data.""
        )
        return False
    else:
        logger.info(f""Fetching from {url} to `{output_dir}`"")

        # download & extract
        with tempfile.NamedTemporaryFile() as temp_file:
            http_get(url, temp_file, proxies=proxies)
            temp_file.flush()
            temp_file.seek(0)  # making tempfile accessible
            # extract
            if url[-4:] == "".zip"":
                zip_archive = zipfile.ZipFile(temp_file.name)
                zip_archive.extractall(output_dir)
            elif url[-7:] == "".tar.gz"":
                tar_archive = tarfile.open(temp_file.name)
                tar_archive.extractall(output_dir)
            # temp_file gets deleted here
        return True","1. Use `verify=False` when calling `http_get()` to disable SSL certificate verification. This is necessary because the remote server may not have a valid SSL certificate.
2. Use `tempfile.mkdtemp()` to create a temporary directory instead of `Path().mkdir()`. This is more secure because it will delete the directory when the function exits, even if there is an error.
3. Use `tempfile.NamedTemporaryFile()` to create a temporary file instead of `open()`. This is more secure because it will delete the file when the function exits, even if there is an error."
"    def __init__(self, text: str,
                 id: Optional[Union[str, UUID]] = None,
                 query_score: Optional[float] = None,
                 question: Optional[str] = None,
                 meta: Dict[str, Any] = None,
                 tags: Optional[Dict[str, Any]] = None,
                 embedding: Optional[List[float]] = None):
        """"""
        Object used to represent documents / passages in a standardized way within Haystack.
        For example, this is what the retriever will return from the DocumentStore,
        regardless if it's ElasticsearchDocumentStore or InMemoryDocumentStore.

        Note that there can be multiple Documents originating from one file (e.g. PDF),
        if you split the text into smaller passages. We'll have one Document per passage in this case.

        :param id: ID used within the DocumentStore
        :param text: Text of the document
        :param query_score: Retriever's query score for a retrieved document
        :param question: Question text for FAQs.
        :param meta: Meta fields for a document like name, url, or author.
        :param tags: Tags that allow filtering of the data
        :param embedding: Vector encoding of the text
        """"""

        self.text = text
        # Create a unique ID (either new one, or one from user input)
        if id:
            if isinstance(id, str):
                self.id = UUID(hex=str(id), version=4)
            if isinstance(id, UUID):
                self.id = id
        else:
            self.id = uuid4()

        self.query_score = query_score
        self.question = question
        self.meta = meta
        self.tags = tags # deprecate?
        self.embedding = embedding","1. Use `uuid4()` to generate a unique ID for each document.
2. Use `typing` to annotate the types of arguments and return values.
3. Validate user input for the `id`, `text`, and `query_score` parameters."
"    def __init__(self, question: str,
                 answer: str,
                 is_correct_answer: bool,
                 is_correct_document: bool,
                 origin: str,
                 document_id: Optional[UUID] = None,
                 offset_start_in_doc: Optional[int] = None,
                 no_answer: Optional[bool] = None,
                 model_id: Optional[int] = None):
        """"""
        Object used to represent label/feedback in a standardized way within Haystack.
        This includes labels from dataset like SQuAD, annotations from labeling tools,
        or, user-feedback from the Haystack REST API.

        :param question: the question(or query) for finding answers.
        :param answer: teh answer string.
        :param is_correct_answer: whether the sample is positive or negative.
        :param is_correct_document: in case of negative sample(is_correct_answer is False), there could be two cases;
                                    incorrect answer but correct document & incorrect document. This flag denotes if
                                    the returned document was correct.
        :param origin: the source for the labels. It can be used to later for filtering.
        :param document_id: the document_store's ID for the returned answer document.
        :param offset_start_in_doc: the answer start offset in the document.
        :param no_answer: whether the question in unanswerable.
        :param model_id: model_id used for prediction(in-case of user feedback).
        """"""
        self.no_answer = no_answer
        self.origin = origin
        self.question = question
        self.is_correct_answer = is_correct_answer
        self.is_correct_document = is_correct_document
        if document_id:
            if isinstance(document_id, str):
                self.document_id: Optional[UUID] = UUID(hex=str(document_id), version=4)
            if isinstance(document_id, UUID):
                self.document_id = document_id
        else:
            self.document_id = document_id
        self.answer = answer
        self.offset_start_in_doc = offset_start_in_doc
        self.model_id = model_id","1. Use `isinstance()` to check the type of `document_id` before casting it to a `UUID`. This will prevent a `ValueError` if the `document_id` is not a valid UUID string.
2. Use `optional()` to make the `document_id` and `offset_start_in_doc` parameters optional. This will prevent errors if these parameters are not provided.
3. Use `pydantic` to validate the input parameters of the `Label` class. This will help to catch errors early and prevent invalid data from being passed to the class."
"    def get_document_by_id(self, id: UUID, index: Optional[str] = None) -> Optional[Document]:
        pass","1. Use `pydantic` to validate the input parameters.
2. Use `cryptography` to encrypt the document ID.
3. Use `access control` to restrict who can access the document."
"    def get_document_by_id(self, id: Union[UUID, str], index=None) -> Optional[Document]:
        if index is None:
            index = self.index
        query = {""query"": {""ids"": {""values"": [id]}}}
        result = self.client.search(index=index, body=query)[""hits""][""hits""]

        document = self._convert_es_hit_to_document(result[0]) if result else None
        return document","1. Use `get_document_by_id(id, verify_es_response=True)` to verify that the response from Elasticsearch is valid.
2. Use `get_document_by_id(id, raise_on_not_found=True)` to raise an exception if the document is not found.
3. Use `get_document_by_id(id, **kwargs)` to pass additional keyword arguments to the Elasticsearch search request."
"    def write_labels(self, labels: Union[List[dict], List[Label]], index: Optional[str] = None):
        index = index or self.label_index
        label_objects = [Label.from_dict(l) if isinstance(l, dict) else l for l in labels]

        for label in label_objects:
            label_id = uuid.uuid4()
            self.indexes[index][label_id] = label","1. Use `json.dumps` to serialize the data instead of `str`. This will prevent attackers from injecting malicious code into the data.
2. Use `uuid.uuid4` to generate unique IDs for labels instead of relying on the user to provide them. This will prevent attackers from creating duplicate labels or hijacking existing labels.
3. Sanitize user input before using it to create labels. This will prevent attackers from creating labels with malicious names or descriptions."
"    def get_document_by_id(self, id: Union[str, UUID], index: Optional[str] = None) -> Document:
        index = index or self.index
        return self.indexes[index][id]","1. Use `id` as the key for the `indexes` dictionary instead of `index`. This will prevent users from accessing documents from other indexes.
2. Use `UUID` objects for `id` instead of strings. This will make it more difficult for attackers to forge IDs.
3. Sanitize user input before using it to construct queries. This will prevent attackers from injecting malicious code into the database."
"    def get_document_by_id(self, id: UUID, index=None) -> Optional[Document]:
        index = index or self.index
        document_row = self.session.query(DocumentORM).filter_by(index=index, id=id).first()
        document = document_row or self._convert_sql_row_to_document(document_row)
        return document","1. Use prepared statements to prevent SQL injection attacks.
2. Sanitize user input to prevent cross-site scripting (XSS) attacks.
3. Use strong passwords for the database and application."
"    def write_labels(self, labels, index=None):

        labels = [Label.from_dict(l) if isinstance(l, dict) else l for l in labels]
        index = index or self.index
        for label in labels:
            label_orm = LabelORM(
                document_id=label.document_id,
                no_answer=label.no_answer,
                origin=label.origin,
                question=label.question,
                is_correct_answer=label.is_correct_answer,
                is_correct_document=label.is_correct_document,
                answer=label.answer,
                offset_start_in_doc=label.offset_start_in_doc,
                model_id=label.model_id,
                index=index,
            )
            self.session.add(label_orm)
        self.session.commit()","1. Sanitize user input to prevent SQL injection attacks.
2. Use prepared statements to prevent SQL injection attacks.
3. Use a database access layer to abstract the database from the application code."
"    def __init__(
        self,
        document_store: Type[BaseDocumentStore],
        embedding_model: str,
        gpu: bool = True,
        model_format: str = ""farm"",
        pooling_strategy: str = ""reduce_mean"",
        emb_extraction_layer: int = -1,
    ):
        """"""
        TODO
        :param document_store:
        :param embedding_model:
        :param gpu:
        :param model_format:
        """"""
        self.document_store = document_store
        self.model_format = model_format
        self.embedding_model = embedding_model
        self.pooling_strategy = pooling_strategy
        self.emb_extraction_layer = emb_extraction_layer

        logger.info(f""Init retriever using embeddings of model {embedding_model}"")
        if model_format == ""farm"" or model_format == ""transformers"":
            self.embedding_model = Inferencer.load(
                embedding_model, task_type=""embeddings"", extraction_strategy=self.pooling_strategy,
                extraction_layer=self.emb_extraction_layer, gpu=gpu, batch_size=4, max_seq_len=512, num_processes=0
            )

        elif model_format == ""sentence_transformers"":
            from sentence_transformers import SentenceTransformer

            # pretrained embedding models coming from: https://github.com/UKPLab/sentence-transformers#pretrained-models
            # e.g. 'roberta-base-nli-stsb-mean-tokens'
            self.embedding_model = SentenceTransformer(embedding_model)
        else:
            raise NotImplementedError","1. Use `torch.load()` instead of `Inferencer.load()` to load the embedding model. This will prevent an attacker from injecting malicious code into the model.
2. Set `gpu=False` to disable GPU support. This will make the code more secure against attacks that exploit GPU vulnerabilities.
3. Use a secure hashing algorithm, such as SHA-256, to generate the document embeddings. This will make it more difficult for an attacker to forge embeddings."
"    def create_embedding(self, texts: [str]):
        """"""
        Create embeddings for each text in a list of texts using the retrievers model (`self.embedding_model`)
        :param texts: texts to embed
        :return: list of embeddings (one per input text). Each embedding is a list of floats.
        """"""

        # for backward compatibility: cast pure str input
        if type(texts) == str:
            texts = [texts]
        assert type(texts) == list, ""Expecting a list of texts, i.e. create_embeddings(texts=['text1',...])""

        if self.model_format == ""farm"":
            res = self.embedding_model.inference_from_dicts(dicts=[{""text"": t} for t in texts])
            emb = [list(r[""vec""]) for r in res] #cast from numpy
        elif self.model_format == ""sentence_transformers"":
            # text is single string, sentence-transformers needs a list of strings
            res = self.embedding_model.encode(texts)  # get back list of numpy embedding vectors
            emb = [list(r) for r in res] #cast from numpy
        return emb","1. Use `type()` to check the type of `texts` argument and raise an error if it is not a list.
2. Use `assert()` to check that the type of `texts` is a list.
3. Use `list()` to cast `texts` to a list if it is a string."
"def install_environment(
        prefix: Prefix,
        version: str,
        additional_dependencies: Sequence[str],
) -> None:
    helpers.assert_version_default('golang', version)
    directory = prefix.path(
        helpers.environment_dir(ENVIRONMENT_DIR, C.DEFAULT),
    )

    with clean_path_on_failure(directory):
        remote = git.get_remote_url(prefix.prefix_dir)
        repo_src_dir = os.path.join(directory, 'src', guess_go_dir(remote))

        # Clone into the goenv we'll create
        helpers.run_setup_cmd(prefix, ('git', 'clone', '.', repo_src_dir))

        if sys.platform == 'cygwin':  # pragma: no cover
            _, gopath, _ = cmd_output('cygpath', '-w', directory)
            gopath = gopath.strip()
        else:
            gopath = directory
        env = dict(os.environ, GOPATH=gopath)
        env.pop('GOBIN', None)
        cmd_output_b('go', 'get', './...', cwd=repo_src_dir, env=env)
        for dependency in additional_dependencies:
            cmd_output_b('go', 'get', dependency, cwd=repo_src_dir, env=env)
        # Same some disk space, we don't need these after installation
        rmtree(prefix.path(directory, 'src'))
        pkgdir = prefix.path(directory, 'pkg')
        if os.path.exists(pkgdir):  # pragma: no cover (go<1.10)
            rmtree(pkgdir)","1. Use `go build` instead of `go get` to download dependencies.
2. Use `go mod tidy` to ensure that all dependencies are up-to-date.
3. Use `go vet` to check for potential security vulnerabilities."
"def get_root() -> str:
    # Git 2.25 introduced a change to ""rev-parse --show-toplevel"" that exposed
    # underlying volumes for Windows drives mapped with SUBST.  We use
    # ""rev-parse --show-cdup"" to get the appropriate path, but must perform
    # an extra check to see if we are in the .git directory.
    try:
        root = os.path.realpath(
            cmd_output('git', 'rev-parse', '--show-cdup')[1].strip(),
        )
        git_dir = os.path.realpath(get_git_dir())
    except CalledProcessError:
        raise FatalError(
            'git failed. Is it installed, and are you in a Git repository '
            'directory?',
        )
    if os.path.commonpath((root, git_dir)) == git_dir:
        raise FatalError(
            'git toplevel unexpectedly empty! make sure you are not '
            'inside the `.git` directory of your repository.',
        )
    return root","1. Use `os.path.realpath` to get the absolute path of the directory, instead of using `os.path.join`.
2. Use `os.path.normpath` to normalize the path, removing any unnecessary leading or trailing slashes.
3. Use `os.path.isdir` to check if the path is a directory before accessing it."
"def get_env_patch(
        venv: str,
        language_version: str,
) -> PatchesT:
    patches: PatchesT = (
        ('GEM_HOME', os.path.join(venv, 'gems')),
        ('GEM_PATH', UNSET),
        ('BUNDLE_IGNORE_CONFIG', '1'),
    )
    if language_version == 'system':
        patches += (
            (
                'PATH', (
                    os.path.join(venv, 'gems', 'bin'), os.pathsep,
                    Var('PATH'),
                ),
            ),
        )
    else:  # pragma: win32 no cover
        patches += (
            ('RBENV_ROOT', venv),
            ('RBENV_VERSION', language_version),
            (
                'PATH', (
                    os.path.join(venv, 'gems', 'bin'), os.pathsep,
                    os.path.join(venv, 'shims'), os.pathsep,
                    os.path.join(venv, 'bin'), os.pathsep, Var('PATH'),
                ),
            ),
        )
    return patches","1. Use `os.path.join()` to concatenate strings instead of using `+`.
2. Use `os.path.expanduser()` to expand the user's home directory.
3. Use `os.path.isdir()` to check if a path exists before trying to access it."
"def get_default_version() -> str:
    # nodeenv does not yet support `-n system` on windows
    if sys.platform == 'win32':
        return C.DEFAULT
    # if node is already installed, we can save a bunch of setup time by
    # using the installed version
    elif all(parse_shebang.find_executable(exe) for exe in ('node', 'npm')):
        return 'system'
    else:
        return C.DEFAULT","1. Use `subprocess.check_output()` instead of `os.system()` to execute external commands. This will prevent your code from being vulnerable to command injection attacks.
2. Use `shlex.quote()` to quote arguments to external commands. This will prevent your code from being vulnerable to shell injection attacks.
3. Use `os.path.expanduser()` to expand user-provided paths. This will prevent your code from being vulnerable to path traversal attacks."
"def get_default_version() -> str:
    if all(parse_shebang.find_executable(exe) for exe in ('ruby', 'gem')):
        return 'system'
    else:
        return C.DEFAULT","1. Use `os.path.isfile` to check if the executable exists before running it.
2. Use `subprocess.check_output` to run the executable and capture its output.
3. Sanitize the output to prevent arbitrary code execution."
"def _find_by_py_launcher(
        version: str,
) -> Optional[str]:  # pragma: no cover (windows only)
    if version.startswith('python'):
        num = version[len('python'):]
        try:
            cmd = ('py', f'-{num}', '-c', 'import sys; print(sys.executable)')
            return cmd_output(*cmd)[1].strip()
        except CalledProcessError:
            pass
    return None","1. Use `subprocess.check_output` instead of `subprocess.call` to avoid
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   "
"def migrate_config(config_file: str, quiet: bool = False) -> int:
    with open(config_file) as f:
        orig_contents = contents = f.read()

    contents = _migrate_map(contents)
    contents = _migrate_sha_to_rev(contents)

    if contents != orig_contents:
        with open(config_file, 'w') as f:
            f.write(contents)

        print('Configuration has been migrated.')
    elif not quiet:
        print('Configuration is already migrated.')
    return 0","1. **Use `f.read()` instead of `f.readlines()`.** This will prevent an attacker from reading the entire file into memory, which could be used to launch a denial-of-service attack.
2. **Check the return value of `f.read()`.** If the file does not exist or cannot be read, the function will return `None`. This should be handled appropriately, such as by exiting the program or printing an error message.
3. **Use `f.seek()` to move the file pointer to the beginning of the file before writing.** This will ensure that the new contents overwrite the old contents, and that no data is lost."
"def py_interface(
        _dir: str,
        _make_venv: Callable[[str, str], None],
) -> Tuple[
    Callable[[Prefix, str], ContextManager[None]],
    Callable[[Prefix, str], bool],
    Callable[[Hook, Sequence[str], bool], Tuple[int, bytes]],
    Callable[[Prefix, str, Sequence[str]], None],
]:
    @contextlib.contextmanager
    def in_env(
            prefix: Prefix,
            language_version: str,
    ) -> Generator[None, None, None]:
        envdir = prefix.path(helpers.environment_dir(_dir, language_version))
        with envcontext(get_env_patch(envdir)):
            yield

    def healthy(prefix: Prefix, language_version: str) -> bool:
        envdir = helpers.environment_dir(_dir, language_version)
        exe_name = 'python.exe' if sys.platform == 'win32' else 'python'
        py_exe = prefix.path(bin_dir(envdir), exe_name)
        with in_env(prefix, language_version):
            retcode, _, _ = cmd_output_b(
                py_exe, '-c', 'import ctypes, datetime, io, os, ssl, weakref',
                cwd='/',
                retcode=None,
            )
        return retcode == 0

    def run_hook(
            hook: Hook,
            file_args: Sequence[str],
            color: bool,
    ) -> Tuple[int, bytes]:
        with in_env(hook.prefix, hook.language_version):
            return helpers.run_xargs(hook, hook.cmd, file_args, color=color)

    def install_environment(
            prefix: Prefix,
            version: str,
            additional_dependencies: Sequence[str],
    ) -> None:
        additional_dependencies = tuple(additional_dependencies)
        directory = helpers.environment_dir(_dir, version)

        env_dir = prefix.path(directory)
        with clean_path_on_failure(env_dir):
            if version != C.DEFAULT:
                python = norm_version(version)
            else:
                python = os.path.realpath(sys.executable)
            _make_venv(env_dir, python)
            with in_env(prefix, version):
                helpers.run_setup_cmd(
                    prefix, ('pip', 'install', '.') + additional_dependencies,
                )

    return in_env, healthy, run_hook, install_environment","1. Use `venv` to create a virtual environment for each Python version.
2. Use `subprocess.run()` to execute commands in the virtual environment.
3. Use `shutil.which()` to get the path of the Python executable."
"    def install_environment(
            prefix: Prefix,
            version: str,
            additional_dependencies: Sequence[str],
    ) -> None:
        additional_dependencies = tuple(additional_dependencies)
        directory = helpers.environment_dir(_dir, version)

        env_dir = prefix.path(directory)
        with clean_path_on_failure(env_dir):
            if version != C.DEFAULT:
                python = norm_version(version)
            else:
                python = os.path.realpath(sys.executable)
            _make_venv(env_dir, python)
            with in_env(prefix, version):
                helpers.run_setup_cmd(
                    prefix, ('pip', 'install', '.') + additional_dependencies,
                )","1. Use `venv` to create a virtual environment for each Python version.
2. Use `pip` to install dependencies into the virtual environment.
3. Use `in_env` to activate the virtual environment before running commands."
"def _log_and_exit(msg: str, exc: BaseException, formatted: str) -> None:
    error_msg = f'{msg}: {type(exc).__name__}: '.encode()
    error_msg += _exception_to_bytes(exc)
    output.write_line_b(error_msg)
    log_path = os.path.join(Store().directory, 'pre-commit.log')
    output.write_line(f'Check the log at {log_path}')

    with open(log_path, 'wb') as log:
        _log_line = functools.partial(output.write_line, stream=log)
        _log_line_b = functools.partial(output.write_line_b, stream=log)

        _log_line('### version information')
        _log_line()
        _log_line('```')
        _log_line(f'pre-commit version: {C.VERSION}')
        _log_line('sys.version:')
        for line in sys.version.splitlines():
            _log_line(f'    {line}')
        _log_line(f'sys.executable: {sys.executable}')
        _log_line(f'os.name: {os.name}')
        _log_line(f'sys.platform: {sys.platform}')
        _log_line('```')
        _log_line()

        _log_line('### error information')
        _log_line()
        _log_line('```')
        _log_line_b(error_msg)
        _log_line('```')
        _log_line()
        _log_line('```')
        _log_line(formatted)
        _log_line('```')
    raise SystemExit(1)","1. Use `logging` instead of `print` to log messages.
2. Use `functools.partial` to avoid creating duplicate functions.
3. Use `sys.exc_info()` to get the exception information."
"def cmd_output_b(
        *cmd: str,
        retcode: Optional[int] = 0,
        **kwargs: Any,
) -> Tuple[int, bytes, Optional[bytes]]:
    _setdefault_kwargs(kwargs)

    try:
        cmd = parse_shebang.normalize_cmd(cmd)
    except parse_shebang.ExecutableNotFoundError as e:
        returncode, stdout_b, stderr_b = e.to_output()
    else:
        proc = subprocess.Popen(cmd, **kwargs)
        stdout_b, stderr_b = proc.communicate()
        returncode = proc.returncode

    if retcode is not None and retcode != returncode:
        raise CalledProcessError(returncode, cmd, retcode, stdout_b, stderr_b)

    return returncode, stdout_b, stderr_b","1. Use `subprocess.check_output` instead of `subprocess.Popen` to avoid having to check the returncode manually.
2. Use `subprocess.PIPE` as the stdout and stderr arguments to capture the output and error streams.
3. Use `subprocess.call` instead of `subprocess.Popen` if you don't need to capture the output or error streams."
"    def cmd_output_p(
            *cmd: str,
            retcode: Optional[int] = 0,
            **kwargs: Any,
    ) -> Tuple[int, bytes, Optional[bytes]]:
        assert retcode is None
        assert kwargs['stderr'] == subprocess.STDOUT, kwargs['stderr']
        _setdefault_kwargs(kwargs)

        try:
            cmd = parse_shebang.normalize_cmd(cmd)
        except parse_shebang.ExecutableNotFoundError as e:
            return e.to_output()

        with open(os.devnull) as devnull, Pty() as pty:
            assert pty.r is not None
            kwargs.update({'stdin': devnull, 'stdout': pty.w, 'stderr': pty.w})
            proc = subprocess.Popen(cmd, **kwargs)
            pty.close_w()

            buf = b''
            while True:
                try:
                    bts = os.read(pty.r, 4096)
                except OSError as e:
                    if e.errno == errno.EIO:
                        bts = b''
                    else:
                        raise
                else:
                    buf += bts
                if not bts:
                    break

        return proc.wait(), buf, None","1. Use `subprocess.check_output()` instead of `subprocess.Popen()` to avoid leaking the child process's stdin/stdout/stderr.
2. Use `subprocess.PIPE` instead of `os.devnull` to capture the child process's stderr output.
3. Close the child process's stdin/stdout/stderr pipes after the child process has finished running."
"def no_git_env(_env=None):
    # Too many bugs dealing with environment variables and GIT:
    # https://github.com/pre-commit/pre-commit/issues/300
    # In git 2.6.3 (maybe others), git exports GIT_WORK_TREE while running
    # pre-commit hooks
    # In git 1.9.1 (maybe others), git exports GIT_DIR and GIT_INDEX_FILE
    # while running pre-commit hooks in submodules.
    # GIT_DIR: Causes git clone to clone wrong thing
    # GIT_INDEX_FILE: Causes 'error invalid object ...' during commit
    _env = _env if _env is not None else os.environ
    return {
        k: v for k, v in _env.items()
        if not k.startswith('GIT_') or
        k in {'GIT_EXEC_PATH', 'GIT_SSH', 'GIT_SSH_COMMAND'}
    }","1. **Use a whitelist of allowed environment variables.** This will prevent malicious actors from injecting their own environment variables into the process.
2. **Sanitize all input before using it.** This will prevent malicious actors from exploiting vulnerabilities in the code.
3. **Use strong cryptography to protect sensitive data.** This will make it more difficult for malicious actors to access sensitive data."
"def rmtree(path):
    """"""On windows, rmtree fails for readonly dirs.""""""
    def handle_remove_readonly(func, path, exc):  # pragma: no cover (windows)
        excvalue = exc[1]
        if (
                func in (os.rmdir, os.remove, os.unlink) and
                excvalue.errno == errno.EACCES
        ):
            os.chmod(path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            func(path)
        else:
            raise
    shutil.rmtree(path, ignore_errors=False, onerror=handle_remove_readonly)","1. Use `shutil.rmtree` with `ignore_errors=True` to avoid permission errors.
2. Use `os.chmod` to change the permissions of the directory before deleting it.
3. Use `os.makedirs` to create the directory if it doesn't exist."
"    def handle_remove_readonly(func, path, exc):  # pragma: no cover (windows)
        excvalue = exc[1]
        if (
                func in (os.rmdir, os.remove, os.unlink) and
                excvalue.errno == errno.EACCES
        ):
            os.chmod(path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            func(path)
        else:
            raise","1. Use `os.chmod` to change the file mode to allow the file to be deleted.
2. Use `os.remove` to delete the file.
3. Catch the `OSError` exception and handle it gracefully."
"def py_interface(_dir, _make_venv):
    @contextlib.contextmanager
    def in_env(prefix, language_version):
        envdir = prefix.path(helpers.environment_dir(_dir, language_version))
        with envcontext(get_env_patch(envdir)):
            yield

    def healthy(prefix, language_version):
        with in_env(prefix, language_version):
            retcode, _, _ = cmd_output(
                'python', '-c',
                'import ctypes, datetime, io, os, ssl, weakref',
                retcode=None,
            )
        return retcode == 0

    def run_hook(hook, file_args):
        with in_env(hook.prefix, hook.language_version):
            return helpers.run_xargs(hook, helpers.to_cmd(hook), file_args)

    def install_environment(prefix, version, additional_dependencies):
        additional_dependencies = tuple(additional_dependencies)
        directory = helpers.environment_dir(_dir, version)

        env_dir = prefix.path(directory)
        with clean_path_on_failure(env_dir):
            if version != C.DEFAULT:
                python = norm_version(version)
            else:
                python = os.path.realpath(sys.executable)
            _make_venv(env_dir, python)
            with in_env(prefix, version):
                helpers.run_setup_cmd(
                    prefix, ('pip', 'install', '.') + additional_dependencies,
                )

    return in_env, healthy, run_hook, install_environment","1. Use `venv` to create a virtual environment for each Python version.
2. Use `pip` to install dependencies into the virtual environment.
3. Use `run_xargs` to run commands inside the virtual environment."
"    def healthy(prefix, language_version):
        with in_env(prefix, language_version):
            retcode, _, _ = cmd_output(
                'python', '-c',
                'import ctypes, datetime, io, os, ssl, weakref',
                retcode=None,
            )
        return retcode == 0","1. Use `subprocess.check_output()` instead of `cmd_output()` to get the output of a command. This will ensure that the command is executed in a secure way and that the output is not tainted by malicious code.
2. Use `subprocess.check_call()` instead of `cmd_output()` to execute a command. This will ensure that the command is executed successfully and that any errors are handled appropriately.
3. Use `subprocess.Popen()` to create a subprocess. This will give you more control over the subprocess and allow you to take steps to secure it further."
"def _repo_ref(tmpdir, repo, ref):
    # if `ref` is explicitly passed, use it
    if ref:
        return repo, ref

    ref = git.head_rev(repo)
    # if it exists on disk, we'll try and clone it with the local changes
    if os.path.exists(repo) and git.has_diff('HEAD', repo=repo):
        logger.warning('Creating temporary repo with uncommitted changes...')

        shadow = os.path.join(tmpdir, 'shadow-repo')
        cmd_output('git', 'clone', repo, shadow)
        cmd_output('git', 'checkout', ref, '-b', '_pc_tmp', cwd=shadow)
        idx = git.git_path('index', repo=shadow)
        objs = git.git_path('objects', repo=shadow)
        env = dict(os.environ, GIT_INDEX_FILE=idx, GIT_OBJECT_DIRECTORY=objs)
        cmd_output('git', 'add', '-u', cwd=repo, env=env)
        git.commit(repo=shadow)

        return shadow, git.head_rev(shadow)
    else:
        return repo, ref","1. Use `subprocess.check_output` instead of `cmd_output` to avoid injecting arbitrary code into the shell.
2. Use `tempfile.mkdtemp` to create a temporary directory instead of hard-coding the path.
3. Use `git.clone_from` to clone the repository instead of `git.clone`."
"def get_staged_files():
    return zsplit(cmd_output(
        'git', 'diff', '--staged', '--name-only', '--no-ext-diff', '-z',
        # Everything except for D
        '--diff-filter=ACMRTUXB',
    )[1])","1. Use `subprocess.check_output()` instead of `cmd_output()` to avoid shell injection.
2. Use `subprocess.DEVNULL` instead of `subprocess.PIPE` to prevent leaking sensitive information.
3. Use `os.path.expanduser()` to expand user-specific paths to avoid directory traversal attacks."
"def xargs(cmd, varargs, **kwargs):
    """"""A simplified implementation of xargs.

    negate: Make nonzero successful and zero a failure
    target_concurrency: Target number of partitions to run concurrently
    """"""
    negate = kwargs.pop('negate', False)
    target_concurrency = kwargs.pop('target_concurrency', 1)
    retcode = 0
    stdout = b''
    stderr = b''

    try:
        cmd = parse_shebang.normalize_cmd(cmd)
    except parse_shebang.ExecutableNotFoundError as e:
        return e.to_output()

    partitions = partition(cmd, varargs, target_concurrency, **kwargs)

    def run_cmd_partition(run_cmd):
        return cmd_output(*run_cmd, encoding=None, retcode=None)

    threads = min(len(partitions), target_concurrency)
    with _thread_mapper(threads) as thread_map:
        results = thread_map(run_cmd_partition, partitions)

        for proc_retcode, proc_out, proc_err in results:
            # This is *slightly* too clever so I'll explain it.
            # First the xor boolean table:
            #     T | F |
            #   +-------+
            # T | F | T |
            # --+-------+
            # F | T | F |
            # --+-------+
            # When negate is True, it has the effect of flipping the return
            # code. Otherwise, the returncode is unchanged.
            retcode |= bool(proc_retcode) ^ negate
            stdout += proc_out
            stderr += proc_err

    return retcode, stdout, stderr","1. Use `subprocess.run` instead of `os.system` to avoid injecting arbitrary code into the system.
2. Sanitize user input to prevent command injection attacks.
3. Use a secure shell (SSH) to connect to remote servers instead of using `xargs`."
"    def run_cmd_partition(run_cmd):
        return cmd_output(*run_cmd, encoding=None, retcode=None)","1. Use `subprocess.run` instead of `subprocess.Popen` to avoid creating a child process that inherits the parent's security context.
2. Use `subprocess.check_output` instead of `subprocess.communicate` to avoid having to manually handle the child process's output.
3. Use `subprocess.DEVNULL` as the `stdout` and `stderr` arguments to `subprocess.run` to prevent the child process's output from being printed to the console."
"def normexe(orig_exe):
    if os.sep not in orig_exe:
        exe = find_executable(orig_exe)
        if exe is None:
            raise ExecutableNotFoundError(
                'Executable `{}` not found'.format(orig_exe),
            )
        return exe
    else:
        return orig_exe","1. Use `os.path.abspath()` to get the absolute path of the executable, instead of relying on the user to provide it. This will prevent users from tricking the code into running arbitrary code.
2. Use `subprocess.check_call()` to run the executable, instead of `os.system()`. This will give you more control over the execution process and allow you to handle errors more gracefully.
3. Use `shlex.quote()` to quote the executable path, so that it is not interpreted by the shell. This will prevent users from injecting malicious code into the command line."
"    def make_local(self, deps):
        def make_local_strategy(directory):
            copy_tree_to_path(resource_filename('empty_template'), directory)
        return self._new_repo(
            'local:{}'.format(','.join(sorted(deps))), C.LOCAL_REPO_VERSION,
            make_local_strategy,
        )","1. Use `os.path.join` to concatenate paths instead of string concatenation.
2. Use `tempfile.mkdtemp` to create a temporary directory instead of using `os.mkdir`.
3. Use `shutil.rmtree` to delete the temporary directory when you are done with it."
"        def make_local_strategy(directory):
            copy_tree_to_path(resource_filename('empty_template'), directory)","1. Use `os.makedirs` instead of `os.mkdir` to create directories recursively.
2. Use `shutil.copytree` instead of `shutil.copytree` to copy directories recursively.
3. Use `subprocess.check_call` instead of `subprocess.call` to execute commands and check the return code."
"def staged_files_only(patch_dir):
    """"""Clear any unstaged changes from the git working directory inside this
    context.
    """"""
    # Determine if there are unstaged files
    tree = cmd_output('git', 'write-tree')[1].strip()
    retcode, diff_stdout_binary, _ = cmd_output(
        'git', 'diff-index', '--ignore-submodules', '--binary',
        '--exit-code', '--no-color', '--no-ext-diff', tree, '--',
        retcode=None,
        encoding=None,
    )
    if retcode and diff_stdout_binary.strip():
        patch_filename = 'patch{}'.format(int(time.time()))
        patch_filename = os.path.join(patch_dir, patch_filename)
        logger.warning('Unstaged files detected.')
        logger.info(
            'Stashing unstaged files to {}.'.format(patch_filename),
        )
        # Save the current unstaged changes as a patch
        with io.open(patch_filename, 'wb') as patch_file:
            patch_file.write(diff_stdout_binary)

        # Clear the working directory of unstaged changes
        cmd_output('git', 'checkout', '--', '.')
        try:
            yield
        finally:
            # Try to apply the patch we saved
            try:
                _git_apply(patch_filename)
            except CalledProcessError:
                logger.warning(
                    'Stashed changes conflicted with hook auto-fixes... '
                    'Rolling back fixes...',
                )
                # We failed to apply the patch, presumably due to fixes made
                # by hooks.
                # Roll back the changes made by hooks.
                cmd_output('git', 'checkout', '--', '.')
                _git_apply(patch_filename)
            logger.info('Restored changes from {}.'.format(patch_filename))
    else:
        # There weren't any staged files so we don't need to do anything
        # special
        yield","1. Use `subprocess.check_output()` instead of `cmd_output()` to avoid leaking sensitive information to the console.
2. Use `tempfile.mkstemp()` to create a temporary file instead of hard-coding the file name.
3. Use `os.chmod()` to set the permissions of the temporary file to `0600` to make it only readable by the owner."
"def staged_files_only(cmd_runner):
    """"""Clear any unstaged changes from the git working directory inside this
    context.

    Args:
        cmd_runner - PrefixedCommandRunner
    """"""
    # Determine if there are unstaged files
    tree = cmd_runner.run(('git', 'write-tree'))[1].strip()
    retcode, diff_stdout_binary, _ = cmd_runner.run(
        (
            'git', 'diff-index', '--ignore-submodules', '--binary',
            '--exit-code', '--no-color', '--no-ext-diff', tree, '--',
        ),
        retcode=None,
        encoding=None,
    )
    if retcode and diff_stdout_binary.strip():
        patch_filename = cmd_runner.path('patch{}'.format(int(time.time())))
        logger.warning('Unstaged files detected.')
        logger.info(
            'Stashing unstaged files to {}.'.format(patch_filename),
        )
        # Save the current unstaged changes as a patch
        with io.open(patch_filename, 'wb') as patch_file:
            patch_file.write(diff_stdout_binary)

        # Clear the working directory of unstaged changes
        cmd_runner.run(('git', 'checkout', '--', '.'))
        try:
            yield
        finally:
            # Try to apply the patch we saved
            try:
                cmd_runner.run(
                    ('git', 'apply', '--whitespace=nowarn', patch_filename),
                    encoding=None,
                )
            except CalledProcessError:
                logger.warning(
                    'Stashed changes conflicted with hook auto-fixes... '
                    'Rolling back fixes...',
                )
                # We failed to apply the patch, presumably due to fixes made
                # by hooks.
                # Roll back the changes made by hooks.
                cmd_runner.run(('git', 'checkout', '--', '.'))
                cmd_runner.run(
                    ('git', 'apply', patch_filename, '--whitespace=nowarn'),
                    encoding=None,
                )
            logger.info('Restored changes from {}.'.format(patch_filename))
    else:
        # There weren't any staged files so we don't need to do anything
        # special
        yield","1. Use `subprocess.check_output` instead of `subprocess.run` to avoid
    having to manually handle the return code.
2. Use `subprocess.PIPE` instead of `subprocess.STDOUT` to avoid
    having to manually read the output.
3. Use `subprocess.call` instead of `subprocess.run` to avoid
    having to manually handle the exceptions."
"def _run_single_hook(hook, repo, args, skips, cols):
    filenames = get_filenames(args, hook['files'], hook['exclude'])
    if hook['id'] in skips:
        output.write(get_hook_message(
            _hook_msg_start(hook, args.verbose),
            end_msg=SKIPPED,
            end_color=color.YELLOW,
            use_color=args.color,
            cols=cols,
        ))
        return 0
    elif not filenames and not hook['always_run']:
        output.write(get_hook_message(
            _hook_msg_start(hook, args.verbose),
            postfix=NO_FILES,
            end_msg=SKIPPED,
            end_color=color.TURQUOISE,
            use_color=args.color,
            cols=cols,
        ))
        return 0

    # Print the hook and the dots first in case the hook takes hella long to
    # run.
    output.write(get_hook_message(
        _hook_msg_start(hook, args.verbose), end_len=6, cols=cols,
    ))
    sys.stdout.flush()

    diff_before = cmd_output('git', 'diff', retcode=None, encoding=None)
    retcode, stdout, stderr = repo.run_hook(
        hook,
        tuple(filenames) if hook['pass_filenames'] else (),
    )
    diff_after = cmd_output('git', 'diff', retcode=None, encoding=None)

    file_modifications = diff_before != diff_after

    # If the hook makes changes, fail the commit
    if file_modifications:
        retcode = 1

    if retcode:
        retcode = 1
        print_color = color.RED
        pass_fail = 'Failed'
    else:
        retcode = 0
        print_color = color.GREEN
        pass_fail = 'Passed'

    output.write_line(color.format_color(pass_fail, print_color, args.color))

    if (stdout or stderr or file_modifications) and (retcode or args.verbose):
        output.write_line('hookid: {}\\n'.format(hook['id']))

        # Print a message if failing due to file modifications
        if file_modifications:
            output.write('Files were modified by this hook.')

            if stdout or stderr:
                output.write_line(' Additional output:')

            output.write_line()

        for out in (stdout, stderr):
            assert type(out) is bytes, type(out)
            if out.strip():
                output.write_line(out.strip(), logfile_name=hook['log_file'])
        output.write_line()

    return retcode","1. Use `subprocess.check_output()` instead of `cmd_output()` to avoid
                    spawning a new process for every command.
2. Use `subprocess.DEVNULL` instead of `os.devnull` to avoid creating a temporary file.
3. Use `shlex.quote()` to quote arguments to avoid shell injection."
"def _run_single_hook(hook, repo, args, skips, cols):
    filenames = get_filenames(args, hook.get('files', ''), hook['exclude'])
    if hook['id'] in skips:
        output.write(get_hook_message(
            _hook_msg_start(hook, args.verbose),
            end_msg=SKIPPED,
            end_color=color.YELLOW,
            use_color=args.color,
            cols=cols,
        ))
        return 0
    elif not filenames and not hook['always_run']:
        output.write(get_hook_message(
            _hook_msg_start(hook, args.verbose),
            postfix=NO_FILES,
            end_msg=SKIPPED,
            end_color=color.TURQUOISE,
            use_color=args.color,
            cols=cols,
        ))
        return 0

    # Print the hook and the dots first in case the hook takes hella long to
    # run.
    output.write(get_hook_message(
        _hook_msg_start(hook, args.verbose), end_len=6, cols=cols,
    ))
    sys.stdout.flush()

    diff_before = cmd_output('git', 'diff', retcode=None, encoding=None)
    retcode, stdout, stderr = repo.run_hook(
        hook,
        tuple(filenames) if hook['pass_filenames'] else (),
    )
    diff_after = cmd_output('git', 'diff', retcode=None, encoding=None)

    file_modifications = diff_before != diff_after

    # If the hook makes changes, fail the commit
    if file_modifications:
        retcode = 1

    if retcode:
        retcode = 1
        print_color = color.RED
        pass_fail = 'Failed'
    else:
        retcode = 0
        print_color = color.GREEN
        pass_fail = 'Passed'

    output.write_line(color.format_color(pass_fail, print_color, args.color))

    if (stdout or stderr or file_modifications) and (retcode or args.verbose):
        output.write_line('hookid: {}\\n'.format(hook['id']))

        # Print a message if failing due to file modifications
        if file_modifications:
            output.write('Files were modified by this hook.')

            if stdout or stderr:
                output.write_line(' Additional output:')

            output.write_line()

        for out in (stdout, stderr):
            assert type(out) is bytes, type(out)
            if out.strip():
                output.write_line(out.strip(), logfile_name=hook['log_file'])
        output.write_line()

    return retcode","1. Use `subprocess.check_output` instead of `subprocess.Popen` to avoid
    spawning a shell.
2. Use `subprocess.PIPE` instead of `subprocess.STDOUT` to avoid
    leaking sensitive information to the environment.
3. Use `subprocess.call` instead of `subprocess.check_output` to
    allow for errors to be raised."
"def install_environment(
        repo_cmd_runner,
        version='default',
        additional_dependencies=(),
):
    additional_dependencies = tuple(additional_dependencies)
    directory = helpers.environment_dir(ENVIRONMENT_DIR, version)

    # Install a virtualenv
    with clean_path_on_failure(repo_cmd_runner.path(directory)):
        venv_cmd = [
            sys.executable, '-m', 'virtualenv',
            '{{prefix}}{}'.format(directory)
        ]
        if version != 'default':
            venv_cmd.extend(['-p', norm_version(version)])
        repo_cmd_runner.run(venv_cmd)
        with in_env(repo_cmd_runner, version):
            helpers.run_setup_cmd(
                repo_cmd_runner,
                ('pip', 'install', '.') + additional_dependencies,
            )","1. Use `venv` to create a separate environment for each version of Python.
2. Use `pip` to install dependencies into the virtual environment.
3. Use `run_setup_cmd` to run setup commands in the virtual environment."
"    def run(self, cmd, **kwargs):
        self._create_path_if_not_exists()
        replaced_cmd = _replace_cmd(cmd, prefix=self.prefix_dir)
        return cmd_output(*replaced_cmd, __popen=self.__popen, **kwargs)","1. Use `subprocess.run()` instead of `os.system()` to avoid injecting arbitrary code into the system.
2. Use `shlex.quote()` to escape any special characters in the command string.
3. Use `subprocess.check_output()` to check the return code of the command and handle errors appropriately."
"def _run_single_hook(hook, repo, args, write, skips=frozenset()):
    filenames = get_filenames(args, hook['files'], hook['exclude'])
    if hook['id'] in skips:
        _print_user_skipped(hook, write, args)
        return 0
    elif not filenames:
        _print_no_files_skipped(hook, write, args)
        return 0

    # Print the hook and the dots first in case the hook takes hella long to
    # run.
    write(get_hook_message(_hook_msg_start(hook, args.verbose), end_len=6))
    sys.stdout.flush()

    retcode, stdout, stderr = repo.run_hook(hook, filenames)

    if retcode != hook['expected_return_value']:
        retcode = 1
        print_color = color.RED
        pass_fail = 'Failed'
    else:
        retcode = 0
        print_color = color.GREEN
        pass_fail = 'Passed'

    write(color.format_color(pass_fail, print_color, args.color) + '\\n')

    if (stdout or stderr) and (retcode or args.verbose):
        write('hookid: {0}\\n'.format(hook['id']))
        write('\\n')
        for output in (stdout, stderr):
            if output.strip():
                write(output.strip() + '\\n')
        write('\\n')

    return retcode","1. Use `get_filenames` to get the list of files to be processed by the hook. This will prevent the hook from being run on files that are not intended to be processed.
2. Use `run_hook` to run the hook and capture the return code, stdout, and stderr. This will allow you to check the hook's return code and print any output that the hook generates.
3. Use `color.format_color` to format the output of the hook so that it is easy to read."
"def run_hook(env, hook, file_args):
    quoted_args = [pipes.quote(arg) for arg in hook['args']]
    return env.run(
        # Use -s 4000 (slightly less than posix mandated minimum)
        # This is to prevent ""xargs: ... Bad file number"" on windows
        ' '.join(['xargs', '-0', '-s4000', hook['entry']] + quoted_args),
        stdin=file_args_to_stdin(file_args),
        retcode=None,
    )","1. Use `subprocess.run` instead of `env.run` to get more control over the spawned process.
2. Use `subprocess.PIPE` instead of `stdin=file_args_to_stdin(file_args)` to avoid passing untrusted data to the spawned process.
3. Use `subprocess.check_output` instead of `retcode=None` to check the return code of the spawned process and handle errors appropriately."
"def run_hook(repo_cmd_runner, hook, file_args):
    # For PCRE the entry is the regular expression to match
    return repo_cmd_runner.run(
        [
            'xargs', '-0', 'sh', '-c',
            # Grep usually returns 0 for matches, and nonzero for non-matches
            # so we flip it here.
            '! grep -H -n -P {0} $@'.format(shell_escape(hook['entry'])),
            '--',
        ],
        stdin=file_args_to_stdin(file_args),
        retcode=None,
    )","1. Use `subprocess.check_output()` instead of `repo_cmd_runner.run()` to avoid leaking the password to the terminal.
2. Use `shlex.quote()` to quote the arguments to `grep` to prevent it from being tricked into executing arbitrary commands.
3. Use `os.fchmod()` to set the file mode of the temporary file to 0600 to prevent other users from reading its contents."
"def run_hook(repo_cmd_runner, hook, file_args):
    return repo_cmd_runner.run(
        ['xargs', '-0', '{{prefix}}{0}'.format(hook['entry'])] + hook['args'],
        # TODO: this is duplicated in pre_commit/languages/helpers.py
        stdin=file_args_to_stdin(file_args),
        retcode=None,
    )","1. Use `subprocess.run` instead of `repo_cmd_runner.run` to avoid code injection.
2. Use `shlex.quote` to escape arguments to prevent command injection.
3. Use `subprocess.check_output` to get the output of the command and check for errors."
"def run_hook(repo_cmd_runner, hook, file_args):
    return repo_cmd_runner.run(
        ['xargs', '-0'] + shlex.split(hook['entry']) + hook['args'],
        stdin=file_args_to_stdin(file_args),
        retcode=None,
    )","1. Sanitize user input to prevent command injection.
2. Use the `subprocess` module to run external commands instead of `os.system`.
3. Use `shlex.quote` to escape arguments to avoid shell expansion."
"def autoupdate(runner):
    """"""Auto-update the pre-commit config to the latest versions of repos.""""""
    retv = 0
    output_configs = []
    changed = False

    input_configs = load_config(
        runner.config_file_path,
        load_strategy=ordered_load,
    )

    for repo_config in input_configs:
        sys.stdout.write('Updating {0}...'.format(repo_config['repo']))
        sys.stdout.flush()
        try:
            new_repo_config = _update_repository(repo_config, runner)
        except RepositoryCannotBeUpdatedError as error:
            print(error.args[0])
            output_configs.append(repo_config)
            retv = 1
            continue

        if new_repo_config['sha'] != repo_config['sha']:
            changed = True
            print(
                'updating {0} -> {1}.'.format(
                    repo_config['sha'], new_repo_config['sha'],
                )
            )
            output_configs.append(new_repo_config)
        else:
            print('already up to date.')
            output_configs.append(repo_config)

    if changed:
        with open(runner.config_file_path, 'w') as config_file:
            config_file.write(
                ordered_dump(
                    remove_defaults(output_configs, CONFIG_JSON_SCHEMA),
                    **C.YAML_DUMP_KWARGS
                )
            )

    return retv","1. Use `verify_commit_sha` to verify that the commit SHA has not been tampered with.
2. Use `raise_if_not_latest_version` to raise an exception if the repository is not the latest version.
3. Use `write_config` to write the updated config file to disk."
"    def __init__(self, repo_config, repo_path_getter=None):
        repo_path_getter = None
        super(LocalRepository, self).__init__(repo_config, repo_path_getter)","1. Use `os.path.join` to concatenate strings instead of + operator to prevent directory traversal attacks.
2. Use `os.makedirs` with `exist_ok=True` to create directories if they don't exist, instead of `os.mkdir`.
3. Use `os.listdir` with `listdir_filter` to filter the list of files returned, instead of iterating over all files in the directory."
"def main(argv=None):
    argv = argv if argv is not None else sys.argv[1:]
    parser = argparse.ArgumentParser()

    # http://stackoverflow.com/a/8521644/812183
    parser.add_argument(
        '-V', '--version',
        action='version',
        version='%(prog)s {0}'.format(
            pkg_resources.get_distribution('pre-commit').version
        )
    )

    subparsers = parser.add_subparsers(dest='command')

    install_parser = subparsers.add_parser(
        'install', help='Install the pre-commit script.',
    )
    install_parser.add_argument(
        '-f', '--overwrite', action='store_true',
        help='Overwrite existing hooks / remove migration mode.',
    )
    install_parser.add_argument(
        '--install-hooks', action='store_true',
        help=(
            'Whether to install hook environments for all environments '
            'in the config file.'
        ),
    )
    install_parser.add_argument(
        '-t', '--hook-type', choices=('pre-commit', 'pre-push'),
        default='pre-commit',
    )

    uninstall_parser = subparsers.add_parser(
        'uninstall', help='Uninstall the pre-commit script.',
    )
    uninstall_parser.add_argument(
        '-t', '--hook-type', choices=('pre-commit', 'pre-push'),
        default='pre-commit',
    )

    subparsers.add_parser('clean', help='Clean out pre-commit files.')

    subparsers.add_parser(
        'autoupdate',
        help=""Auto-update pre-commit config to the latest repos' versions."",
    )

    run_parser = subparsers.add_parser('run', help='Run hooks.')
    run_parser.add_argument('hook', nargs='?', help='A single hook-id to run')
    run_parser.add_argument(
        '--color', default='auto', type=color.use_color,
        help='Whether to use color in output.  Defaults to `auto`',
    )
    run_parser.add_argument(
        '--no-stash', default=False, action='store_true',
        help='Use this option to prevent auto stashing of unstaged files.',
    )
    run_parser.add_argument(
        '--verbose', '-v', action='store_true', default=False,
    )

    run_parser.add_argument(
        '--origin', '-o',
        help='The origin branch""s commit_id when using `git push`',
    )
    run_parser.add_argument(
        '--source', '-s',
        help='The remote branch""s commit_id when using `git push`',
    )
    run_parser.add_argument(
        '--allow-unstaged-config', default=False, action='store_true',
        help='Allow an unstaged config to be present.  Note that this will'
        'be stashed before parsing unless --no-stash is specified'
    )
    run_mutex_group = run_parser.add_mutually_exclusive_group(required=False)
    run_mutex_group.add_argument(
        '--all-files', '-a', action='store_true', default=False,
        help='Run on all the files in the repo.  Implies --no-stash.',
    )
    run_mutex_group.add_argument(
        '--files', nargs='*', help='Specific filenames to run hooks on.',
    )

    help = subparsers.add_parser(
        'help', help='Show help for a specific command.'
    )
    help.add_argument('help_cmd', nargs='?', help='Command to show help for.')

    # Argparse doesn't really provide a way to use a `default` subparser
    if len(argv) == 0:
        argv = ['run']
    args = parser.parse_args(argv)

    if args.command == 'help':
        if args.help_cmd:
            parser.parse_args([args.help_cmd, '--help'])
        else:
            parser.parse_args(['--help'])

    with error_handler():
        runner = Runner.create()

        if args.command == 'install':
            return install(
                runner, overwrite=args.overwrite, hooks=args.install_hooks,
                hook_type=args.hook_type,
            )
        elif args.command == 'uninstall':
            return uninstall(runner, hook_type=args.hook_type)
        elif args.command == 'clean':
            return clean(runner)
        elif args.command == 'autoupdate':
            return autoupdate(runner)
        elif args.command == 'run':
            return run(runner, args)
        else:
            raise NotImplementedError(
                'Command {0} not implemented.'.format(args.command)
            )

        raise AssertionError(
            'Command {0} failed to exit with a returncode'.format(args.command)
        )","1. Use `argparse.ArgumentParser.add_argument_group()` to group related arguments.
2. Use `argparse.ArgumentParser.add_mutually_exclusive_group()` to group arguments that cannot be used together.
3. Use `argparse.ArgumentParser.add_argument_defaults()` to set default values for arguments."
"def sys_stdout_write_wrapper(s, stream=sys.stdout):
    """"""Python 2.6 chokes on unicode being passed to sys.stdout.write.

    This is an adapter because PY2 is ok with bytes and PY3 requires text.
    """"""
    assert type(s) is five.text
    if five.PY2:  # pragma: no cover (PY2)
        s = s.encode('UTF-8')
    stream.write(s)","1. Use `six.ensure_text` to check if the input is a text type.
2. Use `six.ensure_binary` to check if the input is a binary type.
3. Use `six.text_type` to convert the input to a text type if it is a binary type."
"def run(runner, args, write=sys.stdout.write, environ=os.environ):
    # Set up our logging handler
    logger.addHandler(LoggingHandler(args.color, write=write))
    logger.setLevel(logging.INFO)

    # Check if we have unresolved merge conflict files and fail fast.
    if _has_unmerged_paths(runner):
        logger.error('Unmerged files.  Resolve before committing.')
        return 1

    if args.no_stash or args.all_files:
        ctx = noop_context()
    else:
        ctx = staged_files_only(runner.cmd_runner)

    with ctx:
        if args.hook:
            return _run_hook(runner, args, write=write)
        else:
            return _run_hooks(runner, args, write=write, environ=environ)","1. Use `functools.lru_cache` to cache the result of `_has_unmerged_paths` to avoid unnecessary filesystem operations.
2. Use `os.fchmod` to set the file mode of the hook scripts to `0755` to prevent them from being overwritten by other users.
3. Use `subprocess.check_call` to execute the hook scripts to avoid leaking sensitive information to the environment."
"def staged_files_only(cmd_runner):
    """"""Clear any unstaged changes from the git working directory inside this
    context.

    Args:
        cmd_runner - PrefixedCommandRunner
    """"""
    # Determine if there are unstaged files
    retcode, diff_stdout, _ = cmd_runner.run(
        ['git', 'diff', '--ignore-submodules', '--binary', '--exit-code'],
        retcode=None,
    )
    if retcode and diff_stdout.strip():
        patch_filename = cmd_runner.path('patch{0}'.format(int(time.time())))
        logger.warning('Unstaged files detected.')
        logger.info(
            'Stashing unstaged files to {0}.'.format(patch_filename),
        )
        # Save the current unstaged changes as a patch
        with open(patch_filename, 'w') as patch_file:
            patch_file.write(diff_stdout)

        # Clear the working directory of unstaged changes
        cmd_runner.run(['git', 'checkout', '--', '.'])
        try:
            yield
        finally:
            # Try to apply the patch we saved
            try:
                cmd_runner.run(['git', 'apply', patch_filename])
            except CalledProcessError:
                logger.warning(
                    'Stashed changes conflicted with hook auto-fixes... '
                    'Rolling back fixes...'
                )
                # We failed to apply the patch, presumably due to fixes made
                # by hooks.
                # Roll back the changes made by hooks.
                cmd_runner.run(['git', 'checkout', '--', '.'])
                cmd_runner.run(['git', 'apply', patch_filename])
            logger.info('Restored changes from {0}.'.format(patch_filename))
    else:
        # There weren't any staged files so we don't need to do anything
        # special
        yield","1. Use `subprocess.check_output` instead of `subprocess.run` to avoid
    having to handle the return code.
2. Use `with open(patch_filename, 'w') as patch_file:` to ensure that the
    file is closed properly.
3. Use `cmd_runner.run(['git', 'checkout', '--', '.'])` to reset the working
    directory to the previous state instead of using `cmd_runner.run_and_reset()`."
"def run(runner, args, write=sys.stdout.write):
    # Set up our logging handler
    logger.addHandler(LoggingHandler(args.color, write=write))
    logger.setLevel(logging.INFO)

    if args.no_stash or args.all_files:
        ctx = noop_context()
    else:
        ctx = staged_files_only(runner.cmd_runner)

    with ctx:
        if args.hook:
            return _run_hook(runner, args.hook, args, write=write)
        else:
            return _run_hooks(runner, args, write=write)","1. Use `f.write()` instead of `sys.stdout.write()` to avoid shell injection.
2. Use `staged_files_only()` instead of `noop_context()` to only run hooks on files that are staged for commit.
3. Use `_run_hooks()` instead of `_run_hook()` to run all hooks for the given commit."
"def staged_files_only(cmd_runner):
    """"""Clear any unstaged changes from the git working directory inside this
    context.

    Args:
        cmd_runner - PrefixedCommandRunner
    """"""
    # Determine if there are unstaged files
    retcode, _, _ = cmd_runner.run(
        ['git', 'diff-files', '--quiet'],
        retcode=None,
    )
    if retcode:
        patch_filename = cmd_runner.path('patch{0}'.format(int(time.time())))
        logger.warning('Unstaged files detected.')
        logger.info(
            'Stashing unstaged files to {0}.'.format(patch_filename),
        )
        # Save the current unstaged changes as a patch
        with open(patch_filename, 'w') as patch_file:
            cmd_runner.run(['git', 'diff', '--binary'], stdout=patch_file)

        # Clear the working directory of unstaged changes
        cmd_runner.run(['git', 'checkout', '--', '.'])
        try:
            yield
        finally:
            # Try to apply the patch we saved
            try:
                cmd_runner.run(['git', 'apply', patch_filename])
            except CalledProcessError:
                logger.warning(
                    'Stashed changes conflicted with hook auto-fixes... '
                    'Rolling back fixes...'
                )
                # We failed to apply the patch, presumably due to fixes made
                # by hooks.
                # Roll back the changes made by hooks.
                cmd_runner.run(['git', 'checkout', '--', '.'])
                cmd_runner.run(['git', 'apply', patch_filename])
            logger.info('Restored changes from {0}.'.format(patch_filename))
    else:
        # There weren't any staged files so we don't need to do anything
        # special
        yield","1. Use `subprocess.check_output` instead of `subprocess.run` to avoid leaking sensitive information to the console.
2. Use `subprocess.DEVNULL` instead of `open(patch_filename, 'w')` to avoid creating a file with the patch contents.
3. Use `os.path.join(cmd_runner.working_dir, patch_filename)` instead of `cmd_runner.path(patch_filename)` to avoid leaking the full path to the patch file."
"    def thumbnail(self, size, resample=BICUBIC, reducing_gap=2.0):
        """"""
        Make this image into a thumbnail.  This method modifies the
        image to contain a thumbnail version of itself, no larger than
        the given size.  This method calculates an appropriate thumbnail
        size to preserve the aspect of the image, calls the
        :py:meth:`~PIL.Image.Image.draft` method to configure the file reader
        (where applicable), and finally resizes the image.

        Note that this function modifies the :py:class:`~PIL.Image.Image`
        object in place.  If you need to use the full resolution image as well,
        apply this method to a :py:meth:`~PIL.Image.Image.copy` of the original
        image.

        :param size: Requested size.
        :param resample: Optional resampling filter.  This can be one
           of :py:attr:`PIL.Image.NEAREST`, :py:attr:`PIL.Image.BILINEAR`,
           :py:attr:`PIL.Image.BICUBIC`, or :py:attr:`PIL.Image.LANCZOS`.
           If omitted, it defaults to :py:attr:`PIL.Image.BICUBIC`.
           (was :py:attr:`PIL.Image.NEAREST` prior to version 2.5.0).
           See: :ref:`concept-filters`.
        :param reducing_gap: Apply optimization by resizing the image
           in two steps. First, reducing the image by integer times
           using :py:meth:`~PIL.Image.Image.reduce` or
           :py:meth:`~PIL.Image.Image.draft` for JPEG images.
           Second, resizing using regular resampling. The last step
           changes size no less than by ``reducing_gap`` times.
           ``reducing_gap`` may be None (no first step is performed)
           or should be greater than 1.0. The bigger ``reducing_gap``,
           the closer the result to the fair resampling.
           The smaller ``reducing_gap``, the faster resizing.
           With ``reducing_gap`` greater or equal to 3.0, the result is
           indistinguishable from fair resampling in most cases.
           The default value is 2.0 (very close to fair resampling
           while still being faster in many cases).
        :returns: None
        """"""

        x, y = map(math.floor, size)
        if x >= self.width and y >= self.height:
            return

        def round_aspect(number, key):
            return max(min(math.floor(number), math.ceil(number), key=key), 1)

        # preserve aspect ratio
        aspect = self.width / self.height
        if x / y >= aspect:
            x = round_aspect(y * aspect, key=lambda n: abs(aspect - n / y))
        else:
            y = round_aspect(x / aspect, key=lambda n: abs(aspect - x / n))
        size = (x, y)

        box = None
        if reducing_gap is not None:
            res = self.draft(None, (size[0] * reducing_gap, size[1] * reducing_gap))
            if res is not None:
                box = res[1]

        if self.size != size:
            im = self.resize(size, resample, box=box, reducing_gap=reducing_gap)

            self.im = im.im
            self._size = size
            self.mode = self.im.mode

        self.readonly = 0
        self.pyaccess = None","1. Use `Image.draft` to reduce the image size before resizing it.
2. Use `Image.resize` with the `box` parameter to crop the image to the desired size.
3. Set the `readonly` attribute to 1 to prevent the image from being modified."
"    def _open(self):

        if self.fp.read(8) != _MAGIC:
            raise SyntaxError(""not a PNG file"")
        self.__fp = self.fp
        self.__frame = 0

        #
        # Parse headers up to the first IDAT or fDAT chunk

        self.png = PngStream(self.fp)

        while True:

            #
            # get next chunk

            cid, pos, length = self.png.read()

            try:
                s = self.png.call(cid, pos, length)
            except EOFError:
                break
            except AttributeError:
                logger.debug(""%r %s %s (unknown)"", cid, pos, length)
                s = ImageFile._safe_read(self.fp, length)

            self.png.crc(cid, s)

        #
        # Copy relevant attributes from the PngStream.  An alternative
        # would be to let the PngStream class modify these attributes
        # directly, but that introduces circular references which are
        # difficult to break if things go wrong in the decoder...
        # (believe me, I've tried ;-)

        self.mode = self.png.im_mode
        self._size = self.png.im_size
        self.info = self.png.im_info
        self._text = None
        self.tile = self.png.im_tile
        self.custom_mimetype = self.png.im_custom_mimetype
        self._n_frames = self.png.im_n_frames
        self.default_image = self.info.get(""default_image"", False)

        if self.png.im_palette:
            rawmode, data = self.png.im_palette
            self.palette = ImagePalette.raw(rawmode, data)

        if cid == b""fdAT"":
            self.__prepare_idat = length - 4
        else:
            self.__prepare_idat = length  # used by load_prepare()

        if self._n_frames is not None:
            self._close_exclusive_fp_after_loading = False
            self.png.save_rewind()
            self.__rewind_idat = self.__prepare_idat
            self.__rewind = self.__fp.tell()
            if self.default_image:
                # IDAT chunk contains default image and not first animation frame
                self._n_frames += 1
            self._seek(0)","1. Use a secure random number generator to generate the key.
2. Use a strong encryption algorithm, such as AES-256.
3. Use a salt to make the encryption more difficult to break."
"def fit(image, size, method=Image.NEAREST, bleed=0.0, centering=(0.5, 0.5)):
    """"""
    Returns a sized and cropped version of the image, cropped to the
    requested aspect ratio and size.

    This function was contributed by Kevin Cazabon.

    :param image: The image to size and crop.
    :param size: The requested output size in pixels, given as a
                 (width, height) tuple.
    :param method: What resampling method to use. Default is
                   :py:attr:`PIL.Image.NEAREST`.
    :param bleed: Remove a border around the outside of the image from all
                  four edges. The value is a decimal percentage (use 0.01 for
                  one percent). The default value is 0 (no border).
                  Cannot be greater than or equal to 0.5.
    :param centering: Control the cropping position.  Use (0.5, 0.5) for
                      center cropping (e.g. if cropping the width, take 50% off
                      of the left side, and therefore 50% off the right side).
                      (0.0, 0.0) will crop from the top left corner (i.e. if
                      cropping the width, take all of the crop off of the right
                      side, and if cropping the height, take all of it off the
                      bottom).  (1.0, 0.0) will crop from the bottom left
                      corner, etc. (i.e. if cropping the width, take all of the
                      crop off the left side, and if cropping the height take
                      none from the top, and therefore all off the bottom).
    :return: An image.
    """"""

    # by Kevin Cazabon, Feb 17/2000
    # kevin@cazabon.com
    # http://www.cazabon.com

    # ensure centering is mutable
    centering = list(centering)

    if not 0.0 <= centering[0] <= 1.0:
        centering[0] = 0.5
    if not 0.0 <= centering[1] <= 1.0:
        centering[1] = 0.5

    if not 0.0 <= bleed < 0.5:
        bleed = 0.0

    # calculate the area to use for resizing and cropping, subtracting
    # the 'bleed' around the edges

    # number of pixels to trim off on Top and Bottom, Left and Right
    bleed_pixels = (bleed * image.size[0], bleed * image.size[1])

    live_size = (
        image.size[0] - bleed_pixels[0] * 2,
        image.size[1] - bleed_pixels[1] * 2,
    )

    # calculate the aspect ratio of the live_size
    live_size_ratio = float(live_size[0]) / live_size[1]

    # calculate the aspect ratio of the output image
    output_ratio = float(size[0]) / size[1]

    # figure out if the sides or top/bottom will be cropped off
    if live_size_ratio >= output_ratio:
        # live_size is wider than what's needed, crop the sides
        crop_width = output_ratio * live_size[1]
        crop_height = live_size[1]
    else:
        # live_size is taller than what's needed, crop the top and bottom
        crop_width = live_size[0]
        crop_height = live_size[0] / output_ratio

    # make the crop
    crop_left = bleed_pixels[0] + (live_size[0] - crop_width) * centering[0]
    crop_top = bleed_pixels[1] + (live_size[1] - crop_height) * centering[1]

    crop = (crop_left, crop_top, crop_left + crop_width, crop_top + crop_height)

    # resize the image and return it
    return image.resize(size, method, box=crop)","1. Use `Image.LANCZOS` resampling method instead of `Image.NEAREST` to avoid aliasing artifacts.
2. Validate the `size` and `bleed` parameters to ensure that they are valid values.
3. Use `Image.ANTIALIAS` to improve the quality of the resized image."
"    def _load_libtiff(self):
        """""" Overload method triggered when we detect a compressed tiff
            Calls out to libtiff """"""

        pixel = Image.Image.load(self)

        if self.tile is None:
            raise IOError(""cannot load this image"")
        if not self.tile:
            return pixel

        self.load_prepare()

        if not len(self.tile) == 1:
            raise IOError(""Not exactly one tile"")

        # (self._compression, (extents tuple),
        #   0, (rawmode, self._compression, fp))
        extents = self.tile[0][1]
        args = list(self.tile[0][3]) + [self.tag_v2.offset]

        # To be nice on memory footprint, if there's a
        # file descriptor, use that instead of reading
        # into a string in python.
        # libtiff closes the file descriptor, so pass in a dup.
        try:
            fp = hasattr(self.fp, ""fileno"") and os.dup(self.fp.fileno())
            # flush the file descriptor, prevents error on pypy 2.4+
            # should also eliminate the need for fp.tell for py3
            # in _seek
            if hasattr(self.fp, ""flush""):
                self.fp.flush()
        except IOError:
            # io.BytesIO have a fileno, but returns an IOError if
            # it doesn't use a file descriptor.
            fp = False

        if fp:
            args[2] = fp

        decoder = Image._getdecoder(
            self.mode, ""libtiff"", tuple(args), self.decoderconfig
        )
        try:
            decoder.setimage(self.im, extents)
        except ValueError:
            raise IOError(""Couldn't set the image"")

        if hasattr(self.fp, ""getvalue""):
            # We've got a stringio like thing passed in. Yay for all in memory.
            # The decoder needs the entire file in one shot, so there's not
            # a lot we can do here other than give it the entire file.
            # unless we could do something like get the address of the
            # underlying string for stringio.
            #
            # Rearranging for supporting byteio items, since they have a fileno
            # that returns an IOError if there's no underlying fp. Easier to
            # deal with here by reordering.
            if DEBUG:
                print(""have getvalue. just sending in a string from getvalue"")
            n, err = decoder.decode(self.fp.getvalue())
        elif hasattr(self.fp, ""fileno""):
            # we've got a actual file on disk, pass in the fp.
            if DEBUG:
                print(""have fileno, calling fileno version of the decoder."")
            self.fp.seek(0)
            # 4 bytes, otherwise the trace might error out
            n, err = decoder.decode(b""fpfp"")
        else:
            # we have something else.
            if DEBUG:
                print(""don't have fileno or getvalue. just reading"")
            # UNDONE -- so much for that buffer size thing.
            n, err = decoder.decode(self.fp.read())

        self.tile = []
        self.readonly = 0
        # libtiff closed the fp in a, we need to close self.fp, if possible
        if self._exclusive_fp and not self._is_animated:
            self.fp.close()
            self.fp = None  # might be shared

        if err < 0:
            raise IOError(err)

        return Image.Image.load(self)","1. Use proper error handling to avoid potential errors.
2. Close the file descriptor after use to prevent resource leaks.
3. Sanitize user input to prevent potential attacks."
"    def _setup(self):
        """"""Setup this image object based on current tags""""""

        if 0xBC01 in self.tag_v2:
            raise IOError(""Windows Media Photo files not yet supported"")

        # extract relevant tags
        self._compression = COMPRESSION_INFO[self.tag_v2.get(COMPRESSION, 1)]
        self._planar_configuration = self.tag_v2.get(PLANAR_CONFIGURATION, 1)

        # photometric is a required tag, but not everyone is reading
        # the specification
        photo = self.tag_v2.get(PHOTOMETRIC_INTERPRETATION, 0)

        # old style jpeg compression images most certainly are YCbCr
        if self._compression == ""tiff_jpeg"":
            photo = 6

        fillorder = self.tag_v2.get(FILLORDER, 1)

        if DEBUG:
            print(""*** Summary ***"")
            print(""- compression:"", self._compression)
            print(""- photometric_interpretation:"", photo)
            print(""- planar_configuration:"", self._planar_configuration)
            print(""- fill_order:"", fillorder)
            print(""- YCbCr subsampling:"", self.tag.get(530))

        # size
        xsize = self.tag_v2.get(IMAGEWIDTH)
        ysize = self.tag_v2.get(IMAGELENGTH)
        self._size = xsize, ysize

        if DEBUG:
            print(""- size:"", self.size)

        sampleFormat = self.tag_v2.get(SAMPLEFORMAT, (1,))
        if len(sampleFormat) > 1 and max(sampleFormat) == min(sampleFormat) == 1:
            # SAMPLEFORMAT is properly per band, so an RGB image will
            # be (1,1,1).  But, we don't support per band pixel types,
            # and anything more than one band is a uint8. So, just
            # take the first element. Revisit this if adding support
            # for more exotic images.
            sampleFormat = (1,)

        bps_tuple = self.tag_v2.get(BITSPERSAMPLE, (1,))
        extra_tuple = self.tag_v2.get(EXTRASAMPLES, ())
        if photo in (2, 6, 8):  # RGB, YCbCr, LAB
            bps_count = 3
        elif photo == 5:  # CMYK
            bps_count = 4
        else:
            bps_count = 1
        bps_count += len(extra_tuple)
        # Some files have only one value in bps_tuple,
        # while should have more. Fix it
        if bps_count > len(bps_tuple) and len(bps_tuple) == 1:
            bps_tuple = bps_tuple * bps_count

        # mode: check photometric interpretation and bits per pixel
        key = (
            self.tag_v2.prefix,
            photo,
            sampleFormat,
            fillorder,
            bps_tuple,
            extra_tuple,
        )
        if DEBUG:
            print(""format key:"", key)
        try:
            self.mode, rawmode = OPEN_INFO[key]
        except KeyError:
            if DEBUG:
                print(""- unsupported format"")
            raise SyntaxError(""unknown pixel mode"")

        if DEBUG:
            print(""- raw mode:"", rawmode)
            print(""- pil mode:"", self.mode)

        self.info[""compression""] = self._compression

        xres = self.tag_v2.get(X_RESOLUTION, 1)
        yres = self.tag_v2.get(Y_RESOLUTION, 1)

        if xres and yres:
            resunit = self.tag_v2.get(RESOLUTION_UNIT)
            if resunit == 2:  # dots per inch
                self.info[""dpi""] = int(xres + 0.5), int(yres + 0.5)
            elif resunit == 3:  # dots per centimeter. convert to dpi
                self.info[""dpi""] = int(xres * 2.54 + 0.5), int(yres * 2.54 + 0.5)
            elif resunit is None:  # used to default to 1, but now 2)
                self.info[""dpi""] = int(xres + 0.5), int(yres + 0.5)
                # For backward compatibility,
                # we also preserve the old behavior
                self.info[""resolution""] = xres, yres
            else:  # No absolute unit of measurement
                self.info[""resolution""] = xres, yres

        # build tile descriptors
        x = y = layer = 0
        self.tile = []
        self.use_load_libtiff = READ_LIBTIFF or self._compression != ""raw""
        if self.use_load_libtiff:
            # Decoder expects entire file as one tile.
            # There's a buffer size limit in load (64k)
            # so large g4 images will fail if we use that
            # function.
            #
            # Setup the one tile for the whole image, then
            # use the _load_libtiff function.

            # libtiff handles the fillmode for us, so 1;IR should
            # actually be 1;I. Including the R double reverses the
            # bits, so stripes of the image are reversed.  See
            # https://github.com/python-pillow/Pillow/issues/279
            if fillorder == 2:
                # Replace fillorder with fillorder=1
                key = key[:3] + (1,) + key[4:]
                if DEBUG:
                    print(""format key:"", key)
                # this should always work, since all the
                # fillorder==2 modes have a corresponding
                # fillorder=1 mode
                self.mode, rawmode = OPEN_INFO[key]
            # libtiff always returns the bytes in native order.
            # we're expecting image byte order. So, if the rawmode
            # contains I;16, we need to convert from native to image
            # byte order.
            if rawmode == ""I;16"":
                rawmode = ""I;16N""
            if "";16B"" in rawmode:
                rawmode = rawmode.replace("";16B"", "";16N"")
            if "";16L"" in rawmode:
                rawmode = rawmode.replace("";16L"", "";16N"")

            # Offset in the tile tuple is 0, we go from 0,0 to
            # w,h, and we only do this once -- eds
            a = (rawmode, self._compression, False)
            self.tile.append((self._compression, (0, 0, xsize, ysize), 0, a))

        elif STRIPOFFSETS in self.tag_v2 or TILEOFFSETS in self.tag_v2:
            # striped image
            if STRIPOFFSETS in self.tag_v2:
                offsets = self.tag_v2[STRIPOFFSETS]
                h = self.tag_v2.get(ROWSPERSTRIP, ysize)
                w = self.size[0]
            else:
                # tiled image
                offsets = self.tag_v2[TILEOFFSETS]
                w = self.tag_v2.get(322)
                h = self.tag_v2.get(323)

            for offset in offsets:
                if x + w > xsize:
                    stride = w * sum(bps_tuple) / 8  # bytes per line
                else:
                    stride = 0

                tile_rawmode = rawmode
                if self._planar_configuration == 2:
                    # each band on it's own layer
                    tile_rawmode = rawmode[layer]
                    # adjust stride width accordingly
                    stride /= bps_count

                a = (tile_rawmode, int(stride), 1)
                self.tile.append(
                    (
                        self._compression,
                        (x, y, min(x + w, xsize), min(y + h, ysize)),
                        offset,
                        a,
                    )
                )
                x = x + w
                if x >= self.size[0]:
                    x, y = 0, y + h
                    if y >= self.size[1]:
                        x = y = 0
                        layer += 1
        else:
            if DEBUG:
                print(""- unsupported data organization"")
            raise SyntaxError(""unknown data organization"")

        # Fix up info.
        if ICCPROFILE in self.tag_v2:
            self.info[""icc_profile""] = self.tag_v2[ICCPROFILE]

        # fixup palette descriptor

        if self.mode in [""P"", ""PA""]:
            palette = [o8(b // 256) for b in self.tag_v2[COLORMAP]]
            self.palette = ImagePalette.raw(""RGB;L"", b"""".join(palette))","1. Use `Image.open` instead of `Image.open_tiff` to avoid using a deprecated function.
2. Check the `READ_LIBTIFF` flag before using the `_load_libtiff` function to avoid a buffer overflow.
3. Use `ImagePalette.raw` to create a palette instead of manually creating one to avoid errors."
"    def _seek(self, frame):

        if frame == 0:
            # rewind
            self.__offset = 0
            self.dispose = None
            self.dispose_extent = [0, 0, 0, 0]  # x0, y0, x1, y1
            self.__frame = -1
            self.__fp.seek(self.__rewind)
            self._prev_im = None
            self.disposal_method = 0
        else:
            # ensure that the previous frame was loaded
            if not self.im:
                self.load()

        if frame != self.__frame + 1:
            raise ValueError(""cannot seek to frame %d"" % frame)
        self.__frame = frame

        self.tile = []

        self.fp = self.__fp
        if self.__offset:
            # backup to last frame
            self.fp.seek(self.__offset)
            while self.data():
                pass
            self.__offset = 0

        if self.dispose:
            self.im.paste(self.dispose, self.dispose_extent)

        from copy import copy
        self.palette = copy(self.global_palette)

        info = {}
        while True:

            s = self.fp.read(1)
            if not s or s == b"";"":
                break

            elif s == b""!"":
                #
                # extensions
                #
                s = self.fp.read(1)
                block = self.data()
                if i8(s) == 249:
                    #
                    # graphic control extension
                    #
                    flags = i8(block[0])
                    if flags & 1:
                        info[""transparency""] = i8(block[3])
                    info[""duration""] = i16(block[1:3]) * 10

                    # disposal method - find the value of bits 4 - 6
                    dispose_bits = 0b00011100 & flags
                    dispose_bits = dispose_bits >> 2
                    if dispose_bits:
                        # only set the dispose if it is not
                        # unspecified. I'm not sure if this is
                        # correct, but it seems to prevent the last
                        # frame from looking odd for some animations
                        self.disposal_method = dispose_bits
                elif i8(s) == 254:
                    #
                    # comment extension
                    #
                    while block:
                        if ""comment"" in info:
                            info[""comment""] += block
                        else:
                            info[""comment""] = block
                        block = self.data()
                    continue
                elif i8(s) == 255:
                    #
                    # application extension
                    #
                    info[""extension""] = block, self.fp.tell()
                    if block[:11] == b""NETSCAPE2.0"":
                        block = self.data()
                        if len(block) >= 3 and i8(block[0]) == 1:
                            info[""loop""] = i16(block[1:3])
                while self.data():
                    pass

            elif s == b"","":
                #
                # local image
                #
                s = self.fp.read(9)

                # extent
                x0, y0 = i16(s[0:]), i16(s[2:])
                x1, y1 = x0 + i16(s[4:]), y0 + i16(s[6:])
                self.dispose_extent = x0, y0, x1, y1
                flags = i8(s[8])

                interlace = (flags & 64) != 0

                if flags & 128:
                    bits = (flags & 7) + 1
                    self.palette =\\
                        ImagePalette.raw(""RGB"", self.fp.read(3 << bits))

                # image data
                bits = i8(self.fp.read(1))
                self.__offset = self.fp.tell()
                self.tile = [(""gif"",
                             (x0, y0, x1, y1),
                             self.__offset,
                             (bits, interlace))]
                break

            else:
                pass
                # raise IOError, ""illegal GIF tag `%x`"" % i8(s)

        try:
            if self.disposal_method < 2:
                # do not dispose or none specified
                self.dispose = None
            elif self.disposal_method == 2:
                # replace with background colour
                self.dispose = Image.core.fill(""P"", self.size,
                                               self.info[""background""])
            else:
                # replace with previous contents
                if self.im:
                    self.dispose = self.im.copy()

            # only dispose the extent in this frame
            if self.dispose:
                self.dispose = self._crop(self.dispose, self.dispose_extent)
        except (AttributeError, KeyError):
            pass

        if not self.tile:
            # self.__fp = None
            raise EOFError

        for k in [""transparency"", ""duration"", ""comment"", ""extension"", ""loop""]:
            if k in info:
                self.info[k] = info[k]
            elif k in self.info:
                del self.info[k]

        self.mode = ""L""
        if self.palette:
            self.mode = ""P""",000_Didnt Work
"    def __new__(cls, text, lang, tkey):
        """"""
        :param cls: the class to use when creating the instance
        :param text: value for this key
        :param lang: language code
        :param tkey: UTF-8 version of the key name
        """"""

        self = str.__new__(cls, text)
        self.lang = lang
        self.tkey = tkey
        return self","1. Use `typing` to specify the types of arguments and return values.
2. Use `f-strings` to format strings instead of concatenation.
3. Use `black` to format the code to follow PEP8 style guidelines."
"    def load_end(self):
        ""internal: finished reading image data""
        while True:
            self.fp.read(4)  # CRC

            try:
                cid, pos, length = self.png.read()
            except (struct.error, SyntaxError):
                break

            if cid == b""IEND"":
                break

            try:
                self.png.call(cid, pos, length)
            except UnicodeDecodeError:
                break
        self._text = self.png.im_text
        self.png.close()
        self.png = None","1. Use `png.read_data()` instead of `png.read()` to avoid parsing errors.
2. Close the PNG file handle after reading the image data.
3. Handle UnicodeDecodeError exceptions more gracefully."
"    def convert(self, mode=None, matrix=None, dither=None,
                palette=WEB, colors=256):
        """"""
        Returns a converted copy of this image. For the ""P"" mode, this
        method translates pixels through the palette.  If mode is
        omitted, a mode is chosen so that all information in the image
        and the palette can be represented without a palette.

        The current version supports all possible conversions between
        ""L"", ""RGB"" and ""CMYK."" The **matrix** argument only supports ""L""
        and ""RGB"".

        When translating a color image to black and white (mode ""L""),
        the library uses the ITU-R 601-2 luma transform::

            L = R * 299/1000 + G * 587/1000 + B * 114/1000

        The default method of converting a greyscale (""L"") or ""RGB""
        image into a bilevel (mode ""1"") image uses Floyd-Steinberg
        dither to approximate the original image luminosity levels. If
        dither is NONE, all non-zero values are set to 255 (white). To
        use other thresholds, use the :py:meth:`~PIL.Image.Image.point`
        method.

        :param mode: The requested mode. See: :ref:`concept-modes`.
        :param matrix: An optional conversion matrix.  If given, this
           should be 4- or 12-tuple containing floating point values.
        :param dither: Dithering method, used when converting from
           mode ""RGB"" to ""P"" or from ""RGB"" or ""L"" to ""1"".
           Available methods are NONE or FLOYDSTEINBERG (default).
        :param palette: Palette to use when converting from mode ""RGB""
           to ""P"".  Available palettes are WEB or ADAPTIVE.
        :param colors: Number of colors to use for the ADAPTIVE palette.
           Defaults to 256.
        :rtype: :py:class:`~PIL.Image.Image`
        :returns: An :py:class:`~PIL.Image.Image` object.
        """"""

        self.load()

        if not mode and self.mode == ""P"":
            # determine default mode
            if self.palette:
                mode = self.palette.mode
            else:
                mode = ""RGB""
        if not mode or (mode == self.mode and not matrix):
            return self.copy()

        if matrix:
            # matrix conversion
            if mode not in (""L"", ""RGB""):
                raise ValueError(""illegal conversion"")
            im = self.im.convert_matrix(mode, matrix)
            return self._new(im)

        if mode == ""P"" and self.mode == ""RGBA"":
            return self.quantize(colors)

        trns = None
        delete_trns = False
        # transparency handling
        if ""transparency"" in self.info and \\
                self.info['transparency'] is not None:
            if self.mode in ('L', 'RGB') and mode == 'RGBA':
                # Use transparent conversion to promote from transparent
                # color to an alpha channel.
                new_im = self._new(self.im.convert_transparent(
                    mode, self.info['transparency']))
                del(new_im.info['transparency'])
                return new_im
            elif self.mode in ('L', 'RGB', 'P') and mode in ('L', 'RGB', 'P'):
                t = self.info['transparency']
                if isinstance(t, bytes):
                    # Dragons. This can't be represented by a single color
                    warnings.warn('Palette images with Transparency  ' +
                                  ' expressed in bytes should be converted ' +
                                  'to RGBA images')
                    delete_trns = True
                else:
                    # get the new transparency color.
                    # use existing conversions
                    trns_im = Image()._new(core.new(self.mode, (1, 1)))
                    if self.mode == 'P':
                        trns_im.putpalette(self.palette)
                        if isinstance(t, tuple):
                            try:
                                t = trns_im.palette.getcolor(t)
                            except:
                                raise ValueError(""Couldn't allocate a palette ""
                                                 ""color for transparency"")
                    trns_im.putpixel((0, 0), t)

                    if mode in ('L', 'RGB'):
                        trns_im = trns_im.convert(mode)
                    else:
                        # can't just retrieve the palette number, got to do it
                        # after quantization.
                        trns_im = trns_im.convert('RGB')
                    trns = trns_im.getpixel((0, 0))

            elif self.mode == 'P' and mode == 'RGBA':
                t = self.info['transparency']
                delete_trns = True

                if isinstance(t, bytes):
                    self.im.putpalettealphas(t)
                elif isinstance(t, int):
                    self.im.putpalettealpha(t, 0)
                else:
                    raise ValueError(""Transparency for P mode should"" +
                                     "" be bytes or int"")

        if mode == ""P"" and palette == ADAPTIVE:
            im = self.im.quantize(colors)
            new = self._new(im)
            from . import ImagePalette
            new.palette = ImagePalette.raw(""RGB"", new.im.getpalette(""RGB""))
            if delete_trns:
                # This could possibly happen if we requantize to fewer colors.
                # The transparency would be totally off in that case.
                del(new.info['transparency'])
            if trns is not None:
                try:
                    new.info['transparency'] = new.palette.getcolor(trns)
                except:
                    # if we can't make a transparent color, don't leave the old
                    # transparency hanging around to mess us up.
                    del(new.info['transparency'])
                    warnings.warn(""Couldn't allocate palette entry "" +
                                  ""for transparency"")
            return new

        # colorspace conversion
        if dither is None:
            dither = FLOYDSTEINBERG

        try:
            im = self.im.convert(mode, dither)
        except ValueError:
            try:
                # normalize source image and try again
                im = self.im.convert(getmodebase(self.mode))
                im = im.convert(mode, dither)
            except KeyError:
                raise ValueError(""illegal conversion"")

        new_im = self._new(im)
        if delete_trns:
            # crash fail if we leave a bytes transparency in an rgb/l mode.
            del(new_im.info['transparency'])
        if trns is not None:
            if new_im.mode == 'P':
                try:
                    new_im.info['transparency'] = new_im.palette.getcolor(trns)
                except:
                    del(new_im.info['transparency'])
                    warnings.warn(""Couldn't allocate palette entry "" +
                                  ""for transparency"")
            else:
                new_im.info['transparency'] = trns
        return new_im","1. Use `Image._new()` instead of `Image()` to create a new image object.
2. Use `Image.putpalette()` to set the palette for a P mode image.
3. Use `Image.getpalette()` to get the palette for a P mode image."
"    def multiline_text(self, xy, text, fill=None, font=None, anchor=None,
                       spacing=4, align=""left"", direction=None, features=None):
        widths = []
        max_width = 0
        lines = self._multiline_split(text)
        line_spacing = self.textsize('A', font=font)[1] + spacing
        for line in lines:
            line_width, line_height = self.textsize(line, font)
            widths.append(line_width)
            max_width = max(max_width, line_width)
        left, top = xy
        for idx, line in enumerate(lines):
            if align == ""left"":
                pass  # left = x
            elif align == ""center"":
                left += (max_width - widths[idx]) / 2.0
            elif align == ""right"":
                left += (max_width - widths[idx])
            else:
                assert False, 'align must be ""left"", ""center"" or ""right""'
            self.text((left, top), line, fill, font, anchor,
                      direction=direction, features=features)
            top += line_spacing
            left = xy[0]","1. Use a secure random number generator to generate the random number.
2. Use a constant for the maximum width instead of a variable.
3. Sanitize the input text to prevent cross-site scripting attacks."
"    def paste(self, im, box=None):
        """"""
        Paste a PIL image into the photo image.  Note that this can
        be very slow if the photo image is displayed.

        :param im: A PIL image. The size must match the target region.  If the
                   mode does not match, the image is converted to the mode of
                   the bitmap image.
        :param box: A 4-tuple defining the left, upper, right, and lower pixel
                    coordinate. See :ref:`coordinate-system`. If None is given
                    instead of a tuple, all of the image is assumed.
        """"""

        # convert to blittable
        im.load()
        image = im.im
        if image.isblock() and im.mode == self.__mode:
            block = image
        else:
            block = image.new_block(self.__mode, im.size)
            image.convert2(block, image)  # convert directly between buffers

        tk = self.__photo.tk

        try:
            tk.call(""PyImagingPhoto"", self.__photo, block.id)
        except tkinter.TclError:
            # activate Tkinter hook
            try:
                from . import _imagingtk
                try:
                    if hasattr(tk, 'interp'):
                        # Pypy is using a ffi cdata element
                        # (Pdb) self.tk.interp
                        #  <cdata 'Tcl_Interp *' 0x3061b50>
                        _imagingtk.tkinit(
                            int(ffi.cast(""uintptr_t"", tk.interp)), 1)
                    else:
                        _imagingtk.tkinit(tk.interpaddr(), 1)
                except AttributeError:
                    _imagingtk.tkinit(id(tk), 0)
                tk.call(""PyImagingPhoto"", self.__photo, block.id)
            except (ImportError, AttributeError, tkinter.TclError):
                raise  # configuration problem; cannot attach to Tkinter","1. Use `tkinter.Tk.call()` instead of `tk.call()` to avoid Tcl errors.
2. Use `_imagingtk.tkinit()` to initialize Tkinter hook.
3. Use `id()` to get the tkinter object id and pass it to `_imagingtk.tkinit()`."
"def APP(self, marker):
    #
    # Application marker.  Store these in the APP dictionary.
    # Also look for well-known application markers.

    n = i16(self.fp.read(2))-2
    s = ImageFile._safe_read(self.fp, n)

    app = ""APP%d"" % (marker & 15)

    self.app[app] = s  # compatibility
    self.applist.append((app, s))

    if marker == 0xFFE0 and s[:4] == b""JFIF"":
        # extract JFIF information
        self.info[""jfif""] = version = i16(s, 5)  # version
        self.info[""jfif_version""] = divmod(version, 256)
        # extract JFIF properties
        try:
            jfif_unit = i8(s[7])
            jfif_density = i16(s, 8), i16(s, 10)
        except:
            pass
        else:
            if jfif_unit == 1:
                self.info[""dpi""] = jfif_density
            self.info[""jfif_unit""] = jfif_unit
            self.info[""jfif_density""] = jfif_density
    elif marker == 0xFFE1 and s[:5] == b""Exif\\0"":
        # extract Exif information (incomplete)
        self.info[""exif""] = s  # FIXME: value will change
    elif marker == 0xFFE2 and s[:5] == b""FPXR\\0"":
        # extract FlashPix information (incomplete)
        self.info[""flashpix""] = s  # FIXME: value will change
    elif marker == 0xFFE2 and s[:12] == b""ICC_PROFILE\\0"":
        # Since an ICC profile can be larger than the maximum size of
        # a JPEG marker (64K), we need provisions to split it into
        # multiple markers. The format defined by the ICC specifies
        # one or more APP2 markers containing the following data:
        #   Identifying string      ASCII ""ICC_PROFILE\\0""  (12 bytes)
        #   Marker sequence number  1, 2, etc (1 byte)
        #   Number of markers       Total of APP2's used (1 byte)
        #   Profile data            (remainder of APP2 data)
        # Decoders should use the marker sequence numbers to
        # reassemble the profile, rather than assuming that the APP2
        # markers appear in the correct sequence.
        self.icclist.append(s)
    elif marker == 0xFFEE and s[:5] == b""Adobe"":
        self.info[""adobe""] = i16(s, 5)
        # extract Adobe custom properties
        try:
            adobe_transform = i8(s[1])
        except:
            pass
        else:
            self.info[""adobe_transform""] = adobe_transform
    elif marker == 0xFFE2 and s[:4] == b""MPF\\0"":
        # extract MPO information
        self.info[""mp""] = s[4:]
        # offset is current location minus buffer size
        # plus constant header size
        self.info[""mpoffset""] = self.fp.tell() - n + 4

    # If DPI isn't in JPEG header, fetch from EXIF
    if ""dpi"" not in self.info and ""exif"" in self.info:
        exif = self._getexif()
        try:
            resolution_unit = exif[0x0128]
            x_resolution = exif[0x011A]
            dpi = x_resolution[0] / x_resolution[1]
            if resolution_unit == 3: # cm
                # 1 dpcm = 2.54 dpi
                dpi *= 2.54
            self.info[""dpi""] = dpi, dpi
        except KeyError:
            self.info[""dpi""] = 72, 72","1. Use `ImageFile._safe_read` to read data from the file instead of `fp.read`.
2. Sanitize user input to prevent malicious code from being executed.
3. Validate the length of the data being read to prevent buffer overflow attacks."
"    def load(self):
        ""Load image data based on tile list""

        pixel = Image.Image.load(self)

        if self.tile is None:
            raise IOError(""cannot load this image"")
        if not self.tile:
            return pixel

        self.map = None
        use_mmap = self.filename and len(self.tile) == 1
        # As of pypy 2.1.0, memory mapping was failing here.
        use_mmap = use_mmap and not hasattr(sys, 'pypy_version_info')

        readonly = 0

        # look for read/seek overrides
        try:
            read = self.load_read
            # don't use mmap if there are custom read/seek functions
            use_mmap = False
        except AttributeError:
            read = self.fp.read

        try:
            seek = self.load_seek
            use_mmap = False
        except AttributeError:
            seek = self.fp.seek

        if use_mmap:
            # try memory mapping
            d, e, o, a = self.tile[0]
            if d == ""raw"" and a[0] == self.mode and a[0] in Image._MAPMODES:
                try:
                    if hasattr(Image.core, ""map""):
                        # use built-in mapper  WIN32 only
                        self.map = Image.core.map(self.filename)
                        self.map.seek(o)
                        self.im = self.map.readimage(
                            self.mode, self.size, a[1], a[2]
                            )
                    else:
                        # use mmap, if possible
                        import mmap
                        fp = open(self.filename, ""r"")
                        size = os.path.getsize(self.filename)
                        self.map = mmap.mmap(fp.fileno(), size, access=mmap.ACCESS_READ)
                        self.im = Image.core.map_buffer(
                            self.map, self.size, d, e, o, a
                            )
                    readonly = 1
                    # After trashing self.im, we might need to reload the palette data.
                    if self.palette:
                        self.palette.dirty = 1
                except (AttributeError, EnvironmentError, ImportError):
                    self.map = None

        self.load_prepare()

        if not self.map:
            # sort tiles in file order
            self.tile.sort(key=_tilesort)

            try:
                # FIXME: This is a hack to handle TIFF's JpegTables tag.
                prefix = self.tile_prefix
            except AttributeError:
                prefix = b""""

            for decoder_name, extents, offset, args in self.tile:
                decoder = Image._getdecoder(self.mode, decoder_name,
                                      args, self.decoderconfig)
                seek(offset)
                try:
                    decoder.setimage(self.im, extents)
                except ValueError:
                    continue
                if decoder.pulls_fd:
                    decoder.setfd(self.fp)
                    status, err_code = decoder.decode(b"""")
                else:
                    b = prefix
                    while True:
                        try:
                            s = read(self.decodermaxblock)
                        except (IndexError, struct.error):  # truncated png/gif
                            if LOAD_TRUNCATED_IMAGES:
                                break
                            else:
                                raise IOError(""image file is truncated"")

                        if not s and not decoder.handles_eof:  # truncated jpeg
                            self.tile = []

                            # JpegDecode needs to clean things up here either way
                            # If we don't destroy the decompressor,
                            # we have a memory leak.
                            decoder.cleanup()

                            if LOAD_TRUNCATED_IMAGES:
                                break
                            else:
                                raise IOError(""image file is truncated ""
                                              ""(%d bytes not processed)"" % len(b))

                        b = b + s
                        n, err_code = decoder.decode(b)
                        if n < 0:
                            break
                        b = b[n:]

                # Need to cleanup here to prevent leaks in PyPy
                decoder.cleanup()

        self.tile = []
        self.readonly = readonly

        self.fp = None  # might be shared

        if not self.map and not LOAD_TRUNCATED_IMAGES and err_code < 0:
            # still raised if decoder fails to return anything
            raise_ioerror(err_code)

        # post processing
        if hasattr(self, ""tile_post_rotate""):
            # FIXME: This is a hack to handle rotated PCD's
            self.im = self.im.rotate(self.tile_post_rotate)
            self.size = self.im.size

        self.load_end()

        return Image.Image.load(self)","1. Use `Image.core.map_buffer()` instead of `mmap.mmap()` to avoid memory leaks.
2. Check for truncated images and raise an exception if they are found.
3. Close the file handle after loading the image to prevent leaks."
"    def _open(self):

        # HEAD
        s = self.fp.read(32)
        if i32(s) != 0x59a66a95:
            raise SyntaxError(""not an SUN raster file"")

        offset = 32

        self.size = i32(s[4:8]), i32(s[8:12])

        depth = i32(s[12:16])
        if depth == 1:
            self.mode, rawmode = ""1"", ""1;I""
        elif depth == 8:
            self.mode = rawmode = ""L""
        elif depth == 24:
            self.mode, rawmode = ""RGB"", ""BGR""
        else:
            raise SyntaxError(""unsupported mode"")

        compression = i32(s[20:24])

        if i32(s[24:28]) != 0:
            length = i32(s[28:32])
            offset = offset + length
            self.palette = ImagePalette.raw(""RGB;L"", self.fp.read(length))
            if self.mode == ""L"":
                self.mode = rawmode = ""P""

        stride = (((self.size[0] * depth + 7) // 8) + 3) & (~3)

        if compression == 1:
            self.tile = [(""raw"", (0, 0)+self.size, offset, (rawmode, stride))]
        elif compression == 2:
            self.tile = [(""sun_rle"", (0, 0)+self.size, offset, rawmode)]","1. Use `Image.open()` instead of `Image.core.imread()` to open images.
2. Validate the image header before parsing it.
3. Use a secure random number generator to generate the image's pixel data."
"    def _bitmap(self, header=0, offset=0):
        """""" Read relevant info about the BMP """"""
        read, seek = self.fp.read, self.fp.seek
        if header:
            seek(header)
        file_info = dict()
        file_info['header_size'] = i32(read(4))  # read bmp header size @offset 14 (this is part of the header size)
        file_info['direction'] = -1
        # --------------------- If requested, read header at a specific position
        header_data = ImageFile._safe_read(self.fp, file_info['header_size'] - 4)  # read the rest of the bmp header, without its size
        # --------------------------------------------------- IBM OS/2 Bitmap v1
        # ------ This format has different offsets because of width/height types
        if file_info['header_size'] == 12:
            file_info['width'] = i16(header_data[0:2])
            file_info['height'] = i16(header_data[2:4])
            file_info['planes'] = i16(header_data[4:6])
            file_info['bits'] = i16(header_data[6:8])
            file_info['compression'] = self.RAW
            file_info['palette_padding'] = 3
        # ---------------------------------------------- Windows Bitmap v2 to v5
        elif file_info['header_size'] in (40, 64, 108, 124):  # v3, OS/2 v2, v4, v5
            if file_info['header_size'] >= 40:  # v3 and OS/2
                file_info['y_flip'] = i8(header_data[7]) == 0xff
                file_info['direction'] = 1 if file_info['y_flip'] else -1
                file_info['width'] = i32(header_data[0:4])
                file_info['height'] = i32(header_data[4:8]) if not file_info['y_flip'] else 2**32 - i32(header_data[4:8])
                file_info['planes'] = i16(header_data[8:10])
                file_info['bits'] = i16(header_data[10:12])
                file_info['compression'] = i32(header_data[12:16])
                file_info['data_size'] = i32(header_data[16:20])  # byte size of pixel data
                file_info['pixels_per_meter'] = (i32(header_data[20:24]), i32(header_data[24:28]))
                file_info['colors'] = i32(header_data[28:32])
                file_info['palette_padding'] = 4
                self.info[""dpi""] = tuple(
                    map(lambda x: int(math.ceil(x / 39.3701)),
                        file_info['pixels_per_meter']))
                if file_info['compression'] == self.BITFIELDS:
                    if len(header_data) >= 52:
                        for idx, mask in enumerate(['r_mask', 'g_mask', 'b_mask', 'a_mask']):
                            file_info[mask] = i32(header_data[36+idx*4:40+idx*4])
                    else:
                        for mask in ['r_mask', 'g_mask', 'b_mask', 'a_mask']:
                            file_info[mask] = i32(read(4))
                    file_info['rgb_mask'] = (file_info['r_mask'], file_info['g_mask'], file_info['b_mask'])
                    file_info['rgba_mask'] = (file_info['r_mask'], file_info['g_mask'], file_info['b_mask'], file_info['a_mask'])
        else:
            raise IOError(""Unsupported BMP header type (%d)"" % file_info['header_size'])
        # ------------------ Special case : header is reported 40, which
        # ---------------------- is shorter than real size for bpp >= 16
        self.size = file_info['width'], file_info['height']
        # -------- If color count was not found in the header, compute from bits
        file_info['colors'] = file_info['colors'] if file_info.get('colors', 0) else (1 << file_info['bits'])
        # -------------------------------- Check abnormal values for DOS attacks
        if file_info['width'] * file_info['height'] > 2**31:
            raise IOError(""Unsupported BMP Size: (%dx%d)"" % self.size)
        # ----------------------- Check bit depth for unusual unsupported values
        self.mode, raw_mode = BIT2MODE.get(file_info['bits'], (None, None))
        if self.mode is None:
            raise IOError(""Unsupported BMP pixel depth (%d)"" % file_info['bits'])
        # ----------------- Process BMP with Bitfields compression (not palette)
        if file_info['compression'] == self.BITFIELDS:
            SUPPORTED = {
                32: [(0xff0000, 0xff00, 0xff, 0x0), (0xff0000, 0xff00, 0xff, 0xff000000), (0x0, 0x0, 0x0, 0x0)],
                24: [(0xff0000, 0xff00, 0xff)],
                16: [(0xf800, 0x7e0, 0x1f), (0x7c00, 0x3e0, 0x1f)]
            }
            MASK_MODES = {
                (32, (0xff0000, 0xff00, 0xff, 0x0)): ""BGRX"",
                (32, (0xff0000, 0xff00, 0xff, 0xff000000)): ""BGRA"",
                (32, (0x0, 0x0, 0x0, 0x0)): ""BGRA"",
                (24, (0xff0000, 0xff00, 0xff)): ""BGR"",
                (16, (0xf800, 0x7e0, 0x1f)): ""BGR;16"",
                (16, (0x7c00, 0x3e0, 0x1f)): ""BGR;15""
            }
            if file_info['bits'] in SUPPORTED:
                if file_info['bits'] == 32 and file_info['rgba_mask'] in SUPPORTED[file_info['bits']]:
                    raw_mode = MASK_MODES[(file_info['bits'], file_info['rgba_mask'])]
                    self.mode = ""RGBA"" if raw_mode in (""BGRA"",) else self.mode
                elif file_info['bits'] in (24, 16) and file_info['rgb_mask'] in SUPPORTED[file_info['bits']]:
                    raw_mode = MASK_MODES[(file_info['bits'], file_info['rgb_mask'])]
                else:
                    raise IOError(""Unsupported BMP bitfields layout"")
            else:
                raise IOError(""Unsupported BMP bitfields layout"")
        elif file_info['compression'] == self.RAW:
            if file_info['bits'] == 32 and header == 22:  # 32-bit .cur offset
                raw_mode, self.mode = ""BGRA"", ""RGBA""
        else:
            raise IOError(""Unsupported BMP compression (%d)"" % file_info['compression'])
        # ---------------- Once the header is processed, process the palette/LUT
        if self.mode == ""P"":  # Paletted for 1, 4 and 8 bit images
            # ----------------------------------------------------- 1-bit images
            if not (0 < file_info['colors'] <= 65536):
                raise IOError(""Unsupported BMP Palette size (%d)"" % file_info['colors'])
            else:
                padding = file_info['palette_padding']
                palette = read(padding * file_info['colors'])
                greyscale = True
                indices = (0, 255) if file_info['colors'] == 2 else list(range(file_info['colors']))
                # ------------------ Check if greyscale and ignore palette if so
                for ind, val in enumerate(indices):
                    rgb = palette[ind*padding:ind*padding + 3]
                    if rgb != o8(val) * 3:
                        greyscale = False
                # -------- If all colors are grey, white or black, ditch palette
                if greyscale:
                    self.mode = ""1"" if file_info['colors'] == 2 else ""L""
                    raw_mode = self.mode
                else:
                    self.mode = ""P""
                    self.palette = ImagePalette.raw(""BGRX"" if padding == 4 else ""BGR"", palette)

        # ----------------------------- Finally set the tile data for the plugin
        self.info['compression'] = file_info['compression']
        self.tile = [('raw', (0, 0, file_info['width'], file_info['height']), offset or self.fp.tell(),
                      (raw_mode, ((file_info['width'] * file_info['bits'] + 31) >> 3) & (~3), file_info['direction'])
                      )]","1. Use `ImageFile._safe_read` to read data from the file instead of `read`. This will prevent malicious files from corrupting the interpreter's memory.
2. Check the header size before reading it. This will prevent malicious files from crashing the program.
3. Check the bit depth of the image before reading it. This will prevent malicious files from causing an overflow."
"    def _setitem(self, tag, value, legacy_api):
        basetypes = (Number, bytes, str)
        if bytes is str:
            basetypes += unicode,

        info = TiffTags.lookup(tag)
        values = [value] if isinstance(value, basetypes) else value

        if tag not in self.tagtype:
            if info.type:
                self.tagtype[tag] = info.type
            else:
                self.tagtype[tag] = 7
                if all(isinstance(v, IFDRational) for v in values):
                    self.tagtype[tag] = 5
                elif all(isinstance(v, int) for v in values):
                    if all(v < 2 ** 16 for v in values):
                        self.tagtype[tag] = 3
                    else:
                        self.tagtype[tag] = 4
                elif all(isinstance(v, float) for v in values):
                    self.tagtype[tag] = 12
                else:
                    if bytes is str:
                        # Never treat data as binary by default on Python 2.
                        self.tagtype[tag] = 2
                    else:
                        if all(isinstance(v, str) for v in values):
                            self.tagtype[tag] = 2

        if self.tagtype[tag] == 7 and bytes is not str:
            values = [value.encode(""ascii"", 'replace') if isinstance(value, str) else value
                      for value in values]

        values = tuple(info.cvt_enum(value) for value in values)

        dest = self._tags_v1 if legacy_api else self._tags_v2

        if info.length == 1:
            if legacy_api and self.tagtype[tag] in [5, 10]:
                values = values,
            dest[tag], = values
        else:
            dest[tag] = values","1. Use `validate_args` to check if the input arguments are valid.
2. Use `assert` to check if the internal state of the function is valid.
3. Use `sanitize_args` to sanitize the input arguments before using them."
"    def load_byte(self, data, legacy_api=True):
        return (data if legacy_api else
                tuple(map(ord, data) if bytes is str else data))","1. Use `bytes.fromhex()` to decode hexadecimal strings securely.
2. Use `bytes.decode()` to decode text strings securely.
3. Use `int.from_bytes()` to convert bytes to integers securely."
"def get_sampling(im):
    sampling = im.layer[0][1:3] + im.layer[1][1:3] + im.layer[2][1:3]
    return samplings.get(sampling, -1)","1. Use `im.copy()` instead of `im` to avoid modifying the original image.
2. Use `hashlib.sha256()` to generate a secure hash of the image data.
3. Check the hash against a list of known good hashes to verify the image integrity."
"def openid():

    oidc_configuration, jwt_key_set = get_oidc_configuration(current_app)
    token_endpoint = oidc_configuration['token_endpoint']
    userinfo_endpoint = oidc_configuration['userinfo_endpoint']

    data = {
        'grant_type': 'authorization_code',
        'code': request.json['code'],
        'redirect_uri': request.json['redirectUri'],
        'client_id': request.json['clientId'],
        'client_secret': current_app.config['OAUTH2_CLIENT_SECRET'],
    }
    r = requests.post(token_endpoint, data)
    token = r.json()

    if 'error' in token:
        error_text = token.get('error_description') or token['error']
        raise ApiError(error_text)

    try:
        if current_app.config['OIDC_VERIFY_TOKEN']:
            jwt_header = jwt.get_unverified_header(token['id_token'])
            public_key = jwt_key_set[jwt_header['kid']]

            id_token = jwt.decode(
                token['id_token'],
                key=public_key,
                algorithms=jwt_header['alg']
            )
        else:
            id_token = jwt.decode(
                token['id_token'],
                verify=False
            )
    except Exception:
        current_app.logger.warning('No ID token in OpenID Connect token response.')
        id_token = {}

    try:
        headers = {'Authorization': '{} {}'.format(token.get('token_type', 'Bearer'), token['access_token'])}
        r = requests.get(userinfo_endpoint, headers=headers)
        userinfo = r.json()
    except Exception:
        raise ApiError('No access token in OpenID Connect token response.')

    subject = userinfo['sub']
    name = userinfo.get('name') or id_token.get('name')
    username = userinfo.get('preferred_username') or id_token.get('preferred_username')
    nickname = userinfo.get('nickname') or id_token.get('nickname')
    email = userinfo.get('email') or id_token.get('email')
    email_verified = userinfo.get('email_verified', id_token.get('email_verified', bool(email)))
    email_verified = True if email_verified == 'true' else email_verified  # Cognito returns string boolean
    picture = userinfo.get('picture') or id_token.get('picture')

    role_claim = current_app.config['OIDC_ROLE_CLAIM']
    group_claim = current_app.config['OIDC_GROUP_CLAIM']
    custom_claims = {
        role_claim: userinfo.get(role_claim) or id_token.get(role_claim, []),
        group_claim: userinfo.get(group_claim) or id_token.get(group_claim, []),
    }

    login = username or nickname or email
    if not login:
        raise ApiError(""Must support one of the following OpenID claims: 'preferred_username', 'nickname' or 'email'"", 400)

    user = User.find_by_id(id=subject)
    if not user:
        user = User(id=subject, name=name, login=login, password='', email=email,
                    roles=current_app.config['USER_ROLES'], text='', email_verified=email_verified)
        user.create()
    else:
        user.update(login=login, email=email)

    roles = custom_claims[role_claim] + user.roles
    groups = custom_claims[group_claim]

    if user.status != 'active':
        raise ApiError('User {} is not active'.format(login), 403)

    if not_authorized('ALLOWED_OIDC_ROLES', roles) or not_authorized('ALLOWED_EMAIL_DOMAINS', groups=[user.domain]):
        raise ApiError('User {} is not authorized'.format(login), 403)
    user.update_last_login()

    scopes = Permission.lookup(login, roles=roles)
    customers = get_customers(login, groups=[user.domain] + groups)

    auth_audit_trail.send(current_app._get_current_object(), event='openid-login', message='user login via OpenID Connect',
                          user=login, customers=customers, scopes=scopes, **custom_claims,
                          resource_id=subject, type='user', request=request)

    token = create_token(user_id=subject, name=name, login=login, provider=current_app.config['AUTH_PROVIDER'],
                         customers=customers, scopes=scopes, **custom_claims,
                         email=email, email_verified=email_verified, picture=picture)
    return jsonify(token=token.tokenize)","1. Use a secure random number generator to generate the client_secret.
2. Use HTTPS for all communication between the client and the server.
3. Validate the JWT token before using it to access protected resources."
"    def __init__(self, iss: str, typ: str, sub: str, aud: str, exp: dt, nbf: dt, iat: dt, jti: str = None, **kwargs) -> None:

        self.issuer = iss
        self.type = typ
        self.subject = sub
        self.audience = aud
        self.expiration = exp
        self.not_before = nbf
        self.issued_at = iat
        self.jwt_id = jti

        self.name = kwargs.get('name')
        self.preferred_username = kwargs.get('preferred_username')
        self.email = kwargs.get('email')
        self.provider = kwargs.get('provider')
        self.orgs = kwargs.get('orgs', list())
        self.groups = kwargs.get('groups', list())
        self.roles = kwargs.get('roles', list())
        self.scopes = kwargs.get('scopes', list())
        self.email_verified = kwargs.get('email_verified')
        self.picture = kwargs.get('picture')
        self.customers = kwargs.get('customers')","1. Use a cryptographically secure random number generator to generate the JWT ID.
2. Use a strong algorithm for signing the JWT, such as RS256 or ES256.
3. Include the expiration time in the JWT, and make sure it is not set too far in the future."
"    def parse(cls, token: str, key: str = None, verify: bool = True, algorithm: str = 'HS256') -> 'Jwt':
        try:
            json = jwt.decode(
                token,
                key=key or current_app.config['SECRET_KEY'],
                verify=verify,
                algorithms=algorithm,
                audience=current_app.config['OAUTH2_CLIENT_ID'] or current_app.config['SAML2_ENTITY_ID'] or absolute_url()
            )
        except (DecodeError, ExpiredSignature, InvalidAudience):
            raise

        return Jwt(
            iss=json.get('iss', None),
            typ=json.get('typ', None),
            sub=json.get('sub', None),
            aud=json.get('aud', None),
            exp=json.get('exp', None),
            nbf=json.get('nbf', None),
            iat=json.get('iat', None),
            jti=json.get('jti', None),
            name=json.get('name', None),
            preferred_username=json.get('preferred_username', None),
            email=json.get('email', None),
            provider=json.get('provider', None),
            orgs=json.get('orgs', list()),
            groups=json.get('groups', list()),
            roles=json.get('roles', list()),
            scopes=json.get('scope', '').split(' '),  # eg. scope='read write' => scopes=['read', 'write']
            email_verified=json.get('email_verified', None),
            picture=json.get('picture', None),
            customers=[json['customer']] if 'customer' in json else json.get('customers', list())
        )","1. Use a secure algorithm for signing the token, such as RS256 or ES256.
2. Use a strong secret key.
3. Validate the audience of the token to ensure that it is intended for your application."
"    def serialize(self) -> Dict[str, Any]:
        data = {
            'iss': self.issuer,
            'typ': self.type,
            'sub': self.subject,
            'aud': self.audience,
            'exp': self.expiration,
            'nbf': self.not_before,
            'iat': self.issued_at,
            'jti': self.jwt_id
        }
        if self.name:
            data['name'] = self.name
        if self.preferred_username:
            data['preferred_username'] = self.preferred_username
        if self.email:
            data['email'] = self.email
        if self.provider:
            data['provider'] = self.provider
        if self.orgs:
            data['orgs'] = self.orgs
        if self.groups:
            data['groups'] = self.groups
        if self.roles:
            data['roles'] = self.roles
        if self.scopes:
            data['scope'] = ' '.join(self.scopes)

        if self.email_verified is not None:
            data['email_verified'] = self.email_verified
        if self.picture is not None:
            data['picture'] = self.picture
        if current_app.config['CUSTOMER_VIEWS']:
            data['customers'] = self.customers
        return data","1. Use a secure hashing algorithm for the jwt_id.
2. Use a secure signing algorithm for the JWT token.
3. Protect the private key used to sign the JWT token."
"def housekeeping():
    expired_threshold = request.args.get('expired', current_app.config['DEFAULT_EXPIRED_DELETE_HRS'], type='int')
    info_threshold = request.args.get('info', current_app.config['DEFAULT_INFO_DELETE_HRS'], type='int')

    has_expired, has_timedout = Alert.housekeeping(expired_threshold, info_threshold)

    errors = []
    for alert in has_expired:
        try:
            alert, _, text, timeout = process_action(alert, action='expired', text='', timeout=current_app.config['ALERT_TIMEOUT'])
            alert = alert.from_expired(text, timeout)
        except RejectException as e:
            write_audit_trail.send(current_app._get_current_object(), event='alert-expire-rejected', message=alert.text,
                                   user=g.login, customers=g.customers, scopes=g.scopes, resource_id=alert.id, type='alert',
                                   request=request)
            errors.append(str(e))
            continue
        except Exception as e:
            raise ApiError(str(e), 500)

        write_audit_trail.send(current_app._get_current_object(), event='alert-expired', message=text, user=g.login,
                               customers=g.customers, scopes=g.scopes, resource_id=alert.id, type='alert', request=request)

    for alert in has_timedout:
        try:
            alert, _, text, timeout = process_action(alert, action='timeout', text='', timeout=current_app.config['ALERT_TIMEOUT'])
            alert = alert.from_timeout(text, timeout)
        except RejectException as e:
            write_audit_trail.send(current_app._get_current_object(), event='alert-timeout-rejected', message=alert.text,
                                   user=g.login, customers=g.customers, scopes=g.scopes, resource_id=alert.id, type='alert',
                                   request=request)
            errors.append(str(e))
            continue
        except Exception as e:
            raise ApiError(str(e), 500)

        write_audit_trail.send(current_app._get_current_object(), event='alert-timeout', message=text, user=g.login,
                               customers=g.customers, scopes=g.scopes, resource_id=alert.id, type='alert', request=request)

    if errors:
        raise ApiError('housekeeping failed', 500, errors=errors)
    else:
        return jsonify(
            status='ok',
            expired=[a.id for a in has_expired],
            timedout=[a.id for a in has_timedout],
            count=len(has_expired) + len(has_timedout)
        )","1. Use `flask.current_app.config` instead of `current_app.config` to access the configuration. This will prevent accidental modification of the configuration by users.
2. Use `request.args.get()` to get the query parameters instead of directly accessing them. This will prevent users from injecting malicious code into the request.
3. Use `jsonify()` to return the response in JSON format. This will make it easier to parse the response and prevent users from injecting malicious code into the response."
"    def __init__(self, match: str, scopes: List[Scope], **kwargs) -> None:

        self.id = kwargs.get('id', str(uuid4()))
        self.match = match
        self.scopes = scopes or list()","1. Use a cryptographically secure random number generator to generate the `id` field.
2. Validate the `scopes` field to ensure that it only contains a list of valid scopes.
3. Sanitize the `match` field to prevent malicious users from injecting code or other attacks."
"    def __init__(self, resource: str, event: str, **kwargs) -> None:

        if not resource:
            raise ValueError('Missing mandatory value for ""resource""')
        if not event:
            raise ValueError('Missing mandatory value for ""event""')
        if any(['.' in key for key in kwargs.get('attributes', dict()).keys()]) \\
                or any(['$' in key for key in kwargs.get('attributes', dict()).keys()]):
            raise ValueError('Attribute keys must not contain ""."" or ""$""')
        if isinstance(kwargs.get('value', None), int):
            kwargs['value'] = str(kwargs['value'])
        for attr in ['create_time', 'receive_time', 'last_receive_time']:
            if not isinstance(kwargs.get(attr), (datetime, NoneType)):  # type: ignore
                raise ValueError(""Attribute '{}' must be datetime type"".format(attr))

        self.id = kwargs.get('id', None) or str(uuid4())
        self.resource = resource
        self.event = event
        self.environment = kwargs.get('environment', None) or ''
        self.severity = kwargs.get('severity', None) or alarm_model.DEFAULT_NORMAL_SEVERITY
        self.correlate = kwargs.get('correlate', None) or list()
        if self.correlate and event not in self.correlate:
            self.correlate.append(event)
        self.status = kwargs.get('status', None) or alarm_model.DEFAULT_STATUS
        self.service = kwargs.get('service', None) or list()
        self.group = kwargs.get('group', None) or 'Misc'
        self.value = kwargs.get('value', None)
        self.text = kwargs.get('text', None) or ''
        self.tags = kwargs.get('tags', None) or list()
        self.attributes = kwargs.get('attributes', None) or dict()
        self.origin = kwargs.get('origin', None) or '{}/{}'.format(os.path.basename(sys.argv[0]), platform.uname()[1])
        self.event_type = kwargs.get('event_type', kwargs.get('type', None)) or 'exceptionAlert'
        self.create_time = kwargs.get('create_time', None) or datetime.utcnow()
        timeout = kwargs.get('timeout')
        self.timeout = timeout if timeout is not None else current_app.config['ALERT_TIMEOUT']
        self.raw_data = kwargs.get('raw_data', None)
        self.customer = kwargs.get('customer', None)

        self.duplicate_count = kwargs.get('duplicate_count', None)
        self.repeat = kwargs.get('repeat', None)
        self.previous_severity = kwargs.get('previous_severity', None)
        self.trend_indication = kwargs.get('trend_indication', None)
        self.receive_time = kwargs.get('receive_time', None) or datetime.utcnow()
        self.last_receive_id = kwargs.get('last_receive_id', None)
        self.last_receive_time = kwargs.get('last_receive_time', None)
        self.update_time = kwargs.get('update_time', None)
        self.history = kwargs.get('history', None) or list()","1. Use `json.dumps` to serialize the attributes instead of `str()`.
2. Validate the input data before storing it in the model.
3. Use `uuid4()` to generate the id instead of a string."
"    def __init__(self, origin: str=None, tags: List[str]=None, create_time: datetime=None, timeout: int=None, customer: str=None, **kwargs) -> None:
        self.id = kwargs.get('id', str(uuid4()))
        self.origin = origin or '{}/{}'.format(os.path.basename(sys.argv[0]), platform.uname()[1])
        self.tags = tags or list()
        self.event_type = kwargs.get('event_type', kwargs.get('type', None)) or 'Heartbeat'
        self.create_time = create_time or datetime.utcnow()
        self.timeout = timeout if timeout is not None else current_app.config['HEARTBEAT_TIMEOUT']
        self.receive_time = kwargs.get('receive_time', None) or datetime.utcnow()
        self.customer = customer","1. Use `typing` to annotate the function parameters and return type.
2. Use `os.getenv()` to get the environment variable instead of hard-coding it.
3. Use `uuid.uuid4()` to generate a random ID instead of using a predictable string."
"def parse_grafana(alert: JSON, match: Dict[str, Any], args: ImmutableMultiDict) -> Alert:
    alerting_severity = args.get('severity', 'major')

    if alert['state'] == 'alerting':
        severity = alerting_severity
    elif alert['state'] == 'ok':
        severity = 'normal'
    else:
        severity = 'indeterminate'

    environment = args.get('environment', 'Production')  # TODO: verify at create?
    event_type = args.get('event_type', 'performanceAlert')
    group = args.get('group', 'Performance')
    origin = args.get('origin', 'Grafana')
    service = args.get('service', 'Grafana')
    timeout = args.get('timeout', current_app.config['ALERT_TIMEOUT'])

    attributes = match.get('tags', None) or dict()
    attributes = {k.replace('.', '_'): v for (k, v) in attributes.items()}

    attributes['ruleId'] = str(alert['ruleId'])
    if 'ruleUrl' in alert:
        attributes['ruleUrl'] = '<a href=""%s"" target=""_blank"">Rule</a>' % alert['ruleUrl']
    if 'imageUrl' in alert:
        attributes['imageUrl'] = '<a href=""%s"" target=""_blank"">Image</a>' % alert['imageUrl']

    return Alert(
        resource=match['metric'],
        event=alert['ruleName'],
        environment=environment,
        severity=severity,
        service=[service],
        group=group,
        value='%s' % match['value'],
        text=alert.get('message', None) or alert.get('title', alert['state']),
        tags=list(),
        attributes=attributes,
        origin=origin,
        event_type=event_type,
        timeout=timeout,
        raw_data=json.dumps(alert)
    )","1. Use `json.dumps()` to escape any special characters in the alert data.
2. Sanitize the user-provided arguments to prevent injection attacks.
3. Use `current_app.config` to access the application's configuration instead of hard-coding it in the code."
"    def setup_logging(app):
        del app.logger.handlers[:]

        # for key in logging.Logger.manager.loggerDict:
        #     print(key)

        loggers = [
            app.logger,
            logging.getLogger('alerta'),  # ??
            # logging.getLogger('flask'),  # ??
            logging.getLogger('flask_compress'),  # ??
            # logging.getLogger('flask_cors'),  # ??
            logging.getLogger('pymongo'),  # ??
            logging.getLogger('raven'),  # ??
            logging.getLogger('requests'),  # ??
            logging.getLogger('sentry'),  # ??
            logging.getLogger('urllib3'),  # ??
            logging.getLogger('werkzeug'),  # ??
        ]

        if app.debug:
            log_level = logging.DEBUG
        else:
            log_level = logging.INFO

        if app.config['LOG_FILE']:
            from logging.handlers import RotatingFileHandler
            handler = RotatingFileHandler(
                filename=app.config['LOG_FILE'],
                maxBytes=app.config['LOG_MAX_BYTES'],
                backupCount=app.config['LOG_BACKUP_COUNT']
            )
            handler.setLevel(log_level)
            handler.setFormatter(logging.Formatter(app.config['LOG_FORMAT']))
        else:
            handler = logging.StreamHandler()
            handler.setLevel(log_level)
            handler.setFormatter(logging.Formatter(app.config['LOG_FORMAT']))

        for logger in loggers:
            logger.addHandler(handler)
            logger.setLevel(log_level)
            logger.propagate = True","1. Use a secure logging format that does not include sensitive information.
2. Use a rotating file handler to limit the amount of log data that is stored.
3. Set the log level to INFO or higher to only log important messages."
"def parse_prometheus(alert, external_url):

    status = alert.get('status', 'firing')

    labels = copy(alert['labels'])
    annotations = copy(alert['annotations'])

    starts_at = parse_date(alert['startsAt'])
    if alert['endsAt'] == '0001-01-01T00:00:00Z':
        ends_at = None
    else:
        ends_at = parse_date(alert['endsAt'])

    if status == 'firing':
        severity = labels.pop('severity', 'warning')
        create_time = starts_at
    elif status == 'resolved':
        severity = 'normal'
        create_time = ends_at
    else:
        severity = 'unknown'
        create_time = ends_at or starts_at

    # get labels
    resource = labels.pop('exported_instance', None) or labels.pop('instance', 'n/a')
    event = labels.pop('alertname')
    environment = labels.pop('environment', 'Production')

    # get annotations
    correlate = annotations.pop('correlate').split(',') if 'correlate' in annotations else None
    service = annotations.pop('service', '').split(',')
    group = annotations.pop('job', 'Prometheus')
    value = annotations.pop('value', None)

    # build alert text
    summary = annotations.pop('summary', None)
    description = annotations.pop('description', None)
    text = description or summary or '%s: %s on %s' % (labels['job'], labels['alertname'], labels['instance'])

    try:
        timeout = int(labels.pop('timeout', 0)) or None
    except ValueError:
        timeout = None

    if external_url:
        annotations['externalUrl'] = external_url
    if 'generatorURL' in alert:
        annotations['moreInfo'] = '<a href=""%s"" target=""_blank"">Prometheus Graph</a>' % alert['generatorURL']

    return Alert(
        resource=resource,
        event=event,
        environment=environment,
        severity=severity,
        correlate=correlate,
        service=service,
        group=group,
        value=value,
        text=text,
        attributes=annotations,
        origin='prometheus/' + labels.pop('monitor', '-'),
        event_type='prometheusAlert',
        create_time=create_time.astimezone(tz=pytz.UTC).replace(tzinfo=None),
        timeout=timeout,
        raw_data=alert,
        tags=[""%s=%s"" % t for t in labels.items()]  # any labels left are used for tags
    )","1. Use `str.format()` instead of `+` to concatenate strings.
2. Use `json.dumps()` to serialize objects to JSON.
3. Use `urllib.parse.quote()` to escape special characters in URLs."
"def parse_notification(notification):

    notification = json.loads(notification)

    if notification['Type'] == 'SubscriptionConfirmation':

        return Alert(
            resource=notification['TopicArn'],
            event=notification['Type'],
            environment='Production',
            severity='informational',
            service=['Unknown'],
            group='AWS/CloudWatch',
            text='%s <a href=""%s"" target=""_blank"">SubscribeURL</a>' % (notification['Message'], notification['SubscribeURL']),
            origin=notification['TopicArn'],
            event_type='cloudwatchAlarm',
            create_time=datetime.strptime(notification['Timestamp'], '%Y-%m-%dT%H:%M:%S.%fZ'),
            raw_data=notification,
        )

    elif notification['Type'] == 'Notification':

        alarm = json.loads(notification['Message'])

        if 'Trigger' not in alarm:
            raise ValueError(""SNS message is not a Cloudwatch notification"")

        return Alert(
            resource='%s:%s' % (alarm['Trigger']['Dimensions'][0]['name'], alarm['Trigger']['Dimensions'][0]['value']),
            event=alarm['AlarmName'],
            environment='Production',
            severity=cw_state_to_severity(alarm['NewStateValue']),
            service=[alarm['AWSAccountId']],
            group=alarm['Trigger']['Namespace'],
            value=alarm['NewStateValue'],
            text=alarm['AlarmDescription'],
            tags=[alarm['Region']],
            attributes={
                'incidentKey': alarm['AlarmName'],
                'thresholdInfo': alarm['Trigger']
            },
            origin=notification['TopicArn'],
            event_type='cloudwatchAlarm',
            create_time=datetime.strptime(notification['Timestamp'], '%Y-%m-%dT%H:%M:%S.%fZ'),
            raw_data=alarm
        )","1. **Use a library to deserialize JSON data.** This will help to prevent against JSON injection attacks.
2. **Validate the structure of the incoming data.** This will help to ensure that the data is in the correct format and that it does not contain malicious code.
3. **Use proper error handling.** This will help to prevent against potential security vulnerabilities that could be introduced by errors in the code."
"def cloudwatch():

    try:
        incomingAlert = parse_notification(request.data)
    except ValueError as e:
        raise ApiError(str(e), 400)

    incomingAlert.customer = assign_customer(wanted=incomingAlert.customer)
    add_remote_ip(request, incomingAlert)

    try:
        alert = process_alert(incomingAlert)
    except RejectException as e:
        raise ApiError(str(e), 403)
    except Exception as e:
        raise ApiError(str(e), 500)

    if alert:
        return jsonify(status=""ok"", id=alert.id, alert=alert.serialize), 201
    else:
        raise ApiError(""insert or update of cloudwatch alarm failed"", 500)","1. Use proper validation to ensure that the request data is valid.
2. Sanitize the request data to prevent injection attacks.
3. Use proper authorization to ensure that only authorized users can access the API."
"def cloudwatch():

    try:
        incomingAlert = parse_notification(request.json)
    except ValueError as e:
        raise ApiError(str(e), 400)

    incomingAlert.customer = assign_customer(wanted=incomingAlert.customer)
    add_remote_ip(request, incomingAlert)

    try:
        alert = process_alert(incomingAlert)
    except RejectException as e:
        raise ApiError(str(e), 403)
    except Exception as e:
        raise ApiError(str(e), 500)

    if alert:
        return jsonify(status=""ok"", id=alert.id, alert=alert.serialize), 201
    else:
        raise ApiError(""insert or update of cloudwatch alarm failed"", 500)","1. Use `json.loads` instead of `request.json` to parse the incoming notification. This will protect against JSON injection attacks.
2. Use `request.remote_addr` to get the remote IP address of the client and add it to the alert. This will help to track down the source of any malicious activity.
3. Use `logging.exception` to log all exceptions, including those that are caught and handled. This will help to troubleshoot any problems that occur."
"    def housekeeping(self, expired_threshold, info_threshold):
        # delete 'closed' or 'expired' alerts older than ""expired_threshold"" hours
        # and 'informational' alerts older than ""info_threshold"" hours
        expired_hours_ago = datetime.utcnow() - timedelta(hours=expired_threshold)
        g.db.alerts.remove({""status"": {'$in': [""closed"", ""expired""]}, ""lastReceiveTime"": {'$lt': expired_hours_ago}})

        info_hours_ago = datetime.utcnow() - timedelta(hours=info_threshold)
        g.db.alerts.remove({""severity"": ""informational"", ""lastReceiveTime"": {'$lt': info_hours_ago}})

        # get list of alerts to be newly expired
        pipeline = [
            {'$project': {
                ""event"": 1, ""status"": 1, ""lastReceiveId"": 1, ""timeout"": 1,
                ""expireTime"": {'$add': [""$lastReceiveTime"", {'$multiply': [""$timeout"", 1000]}]}}
            },
            {'$match': {""status"": {'$nin': ['expired', 'shelved']}, ""expireTime"": {'$lt': datetime.utcnow()}, ""timeout"": {'$ne': 0}}}
        ]
        expired = [(r['_id'], r['event'], r['lastReceiveId']) for r in g.db.alerts.aggregate(pipeline)]

        # get list of alerts to be unshelved
        pipeline = [
            {'$project': {
                ""event"": 1, ""status"": 1, ""lastReceiveId"": 1, ""timeout"": 1,
                ""expireTime"": {'$add': [""$lastReceiveTime"", {'$multiply': [""$timeout"", 1000]}]}}
            },
            {'$match': {""status"": 'shelved', ""expireTime"": {'$lt': datetime.utcnow()}, ""timeout"": {'$ne': 0}}}
        ]
        unshelved = [(r['_id'], r['event'], r['lastReceiveId']) for r in g.db.alerts.aggregate(pipeline)]

        return (expired, unshelved)","1. Use prepared statements instead of raw SQL queries to prevent SQL injection attacks.
2. Use parameterized queries instead of bind variables to prevent SQL injection attacks.
3. Use the least privileges principle to grant only the necessary permissions to users."
"    def housekeeping(self, expired_threshold, info_threshold):
        # delete 'closed' or 'expired' alerts older than ""expired_threshold"" hours
        # and 'informational' alerts older than ""info_threshold"" hours
        delete = """"""
            DELETE FROM alerts
             WHERE (status IN ('closed', 'expired')
                    AND last_receive_time < (NOW() at time zone 'utc' - INTERVAL '%(expired_threshold)s hours'))
                OR (severity='informational'
                    AND last_receive_time < (NOW() at time zone 'utc' - INTERVAL '%(info_threshold)s hours'))
        """"""
        self._delete(delete, {""expired_threshold"": expired_threshold, ""info_threshold"": info_threshold})

        # get list of alerts to be newly expired
        update = """"""
            SELECT id, event, last_receive_id
              FROM alerts
             WHERE status NOT IN ('expired','shelved') AND timeout!=0
               AND (last_receive_time + INTERVAL '1 second' * timeout) < (NOW() at time zone 'utc')
        """"""
        expired = self._fetchall(update, {})

        # get list of alerts to be unshelved
        update = """"""
            SELECT id, event, last_receive_id
              FROM alerts
             WHERE status='shelved'
               AND (last_receive_time + INTERVAL '1 second' * timeout) < (NOW() at time zone 'utc')
        """"""
        unshelved = self._fetchall(update, {})

        return (expired, unshelved)","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to avoid hardcoded values.
3. Use proper escaping for strings."
"    def housekeeping(self, expired_threshold, info_threshold):
        # delete 'closed' or 'expired' alerts older than ""expired_threshold"" hours
        # and 'informational' alerts older than ""info_threshold"" hours
        expired_hours_ago = datetime.utcnow() - timedelta(hours=expired_threshold)
        g.db.alerts.remove({""status"": {'$in': [""closed"", ""expired""]}, ""lastReceiveTime"": {'$lt': expired_hours_ago}})

        info_hours_ago = datetime.utcnow() - timedelta(hours=info_threshold)
        g.db.alerts.remove({""severity"": ""informational"", ""lastReceiveTime"": {'$lt': info_hours_ago}})

        # get list of alerts to be newly expired
        pipeline = [
            {'$project': {
                ""event"": 1, ""status"": 1, ""lastReceiveId"": 1, ""timeout"": 1,
                ""expireTime"": {'$add': [""$lastReceiveTime"", {'$multiply': [""$timeout"", 1000]}]}}
            },
            {'$match': {""status"": {'$nin': ['expired', 'shelved']}, ""expireTime"": {'$lt': datetime.utcnow()}, ""timeout"": {'$ne': 0}}}
        ]
        expired = [(r['_id'], r['event'], 'expired', r['lastReceiveId']) for r in g.db.alerts.aggregate(pipeline)]

        # get list of alerts to be unshelved
        pipeline = [
            {'$project': {
                ""event"": 1, ""status"": 1, ""lastReceiveId"": 1, ""timeout"": 1,
                ""expireTime"": {'$add': [""$lastReceiveTime"", {'$multiply': [""$timeout"", 1000]}]}}
            },
            {'$match': {""status"": 'shelved', ""expireTime"": {'$lt': datetime.utcnow()}, ""timeout"": {'$ne': 0}}}
        ]
        unshelved = [(r['_id'], r['event'], 'open', r['lastReceiveId']) for r in g.db.alerts.aggregate(pipeline)]

        return expired + unshelved","1. Use prepared statements instead of building queries with string concatenation.
2. Use bind variables instead of passing parameters directly to the query.
3. Use the least privileges necessary for the code to function."
"    def housekeeping(self, expired_threshold, info_threshold):
        # delete 'closed' or 'expired' alerts older than ""expired_threshold"" hours
        # and 'informational' alerts older than ""info_threshold"" hours
        delete = """"""
            DELETE FROM alerts
             WHERE (status IN ('closed', 'expired')
                    AND last_receive_time < (NOW() at time zone 'utc' - INTERVAL '%(expired_threshold)s hours'))
                OR (severity='informational'
                    AND last_receive_time < (NOW() at time zone 'utc' - INTERVAL '%(info_threshold)s hours'))
        """"""
        self._delete(delete, {""expired_threshold"": expired_threshold, ""info_threshold"": info_threshold})

        # get list of alerts to be newly expired
        update = """"""
            SELECT id, event, 'expired', last_receive_id
              FROM alerts
             WHERE status NOT IN ('expired','shelved') AND timeout!=0
               AND (last_receive_time + INTERVAL '1 second' * timeout) < (NOW() at time zone 'utc')
        """"""
        expired = self._fetchall(update, {})

        # get list of alerts to be unshelved
        update = """"""
            SELECT id, event, 'open', last_receive_id
              FROM alerts
             WHERE status='shelved'
               AND (last_receive_time + INTERVAL '1 second' * timeout) < (NOW() at time zone 'utc')
        """"""
        unshelved = self._fetchall(update, {})

        return expired + unshelved","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to avoid hardcoded values.
3. Use bind variables to prevent XSS attacks."
"    def housekeeping(expired_threshold=2, info_threshold=12):
        for (id, event, status, last_receive_id) in db.housekeeping(expired_threshold, info_threshold):
            if status == 'open':
                text = 'unshelved after timeout'
            elif status == 'expired':
                text = 'expired after timeout'
            else:
                text = 'alert timeout status change'
            history = History(
                id=last_receive_id,
                event=event,
                status=status,
                text=text,
                change_type=""status"",
                update_time=datetime.utcnow()
            )
            db.set_status(id, status, timeout=current_app.config['ALERT_TIMEOUT'], history=history)","1. Use prepared statements to prevent SQL injection.
2. Use Fernet to encrypt sensitive data.
3. Use a secret key to sign the JWT token."
"    def is_flapping(self, alert, window=1800, count=2):
        """"""
        Return true if alert severity has changed more than X times in Y seconds
        """"""
        pipeline = [
            {'$match': {""environment"": alert.environment, ""resource"": alert.resource, ""event"": alert.event}},
            {'$unwind': '$history'},
            {'$match': {""history.updateTime"": {'$gt': datetime.utcnow() - timedelta(seconds=window)}},
             ""history.type"": ""severity""
            },
            {'$group': {""_id"": '$history.type', ""count"": {'$sum': 1}}}
        ]
        responses = g.db.alerts.aggregate(pipeline)
        for r in responses:
            if r['count'] > count:
                return True
        return False","1. Use prepared statements instead of building queries manually. This will help to prevent SQL injection attacks.
2. Use the `escape_string` function to escape any user-provided input before using it in a query. This will help to prevent cross-site scripting attacks.
3. Use the `sha256` function to hash passwords before storing them in the database. This will help to prevent unauthorized access to accounts."
"def main():
    app.run(host='0.0.0.0', port=8080, threaded=True)","1. Use `app.run()` with `host='localhost'` and `port=8080` to restrict access to the server.
2. Use `app.run()` with `threaded=False` to improve performance.
3. Use `app.run()` with `debug=False` to disable debug mode."
"    def set(self, value):

        db.metrics.update_one(
            {
                ""group"": self.group,
                ""name"": self.name
            },
            {
                ""group"": self.group,
                ""name"": self.name,
                ""title"": self.title,
                ""description"": self.description,
                ""value"": value,
                ""type"": ""gauge""
            },
            True
        )","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to prevent SQL injection.
3. Use the `db.collection.find_one_and_update()` method to atomically update a document."
"    def inc(self):

        db.metrics.update_one(
            {
                ""group"": self.group,
                ""name"": self.name
            },
            {
                '$set': {
                    ""group"": self.group,
                    ""name"": self.name,
                    ""title"": self.title,
                    ""description"": self.description,
                    ""type"": ""counter""
                },
                '$inc': {""count"": 1}
            },
            True
        )","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to prevent SQL injection.
3. Use strong hashing algorithms and salt values to protect passwords."
"    def stop_timer(self, start):

        db.metrics.update_one(
            {
                ""group"": self.group,
                ""name"": self.name
            },
            {
                '$set': {
                    ""group"": self.group,
                    ""name"": self.name,
                    ""title"": self.title,
                    ""description"": self.description,
                    ""type"": ""timer""
                },
                '$inc': {""count"": 1, ""totalTime"": self._time_in_millis() - start}
            },
            True
        )","1. Use prepared statements to avoid SQL injection attacks.
2. Sanitize user input to prevent cross-site scripting (XSS) attacks.
3. Use strong passwords for database accounts and other sensitive systems."
"def refine_from_db(path, video):
    if isinstance(video, Episode):
        db = sqlite3.connect(os.path.join(args.config_dir, 'db', 'bazarr.db'), timeout=30)
        c = db.cursor()
        data = c.execute(""SELECT table_shows.title, table_episodes.season, table_episodes.episode, table_episodes.title, table_shows.year, table_shows.tvdbId, table_shows.alternateTitles, table_episodes.format, table_episodes.resolution, table_episodes.video_codec, table_episodes.audio_codec FROM table_episodes INNER JOIN table_shows on table_shows.sonarrSeriesId = table_episodes.sonarrSeriesId WHERE table_episodes.path = ?"", (path_replace_reverse(path),)).fetchone()
        db.close()
        if data:
            video.series = re.sub(r'(\\(\\d\\d\\d\\d\\))' , '', data[0])
            video.season = int(data[1])
            video.episode = int(data[2])
            video.title = data[3]
            if int(data[4]) > 0: video.year = int(data[4])
            video.series_tvdb_id = int(data[5])
            video.alternative_series = ast.literal_eval(data[6])
            if not video.format:
                video.format = str(data[7])
            if not video.resolution:
                video.resolution = str(data[8])
            if not video.video_codec:
                if data[9]: video.video_codec = data[9]
            if not video.audio_codec:
                if data[10]: video.audio_codec = data[10]
    elif isinstance(video, Movie):
        db = sqlite3.connect(os.path.join(args.config_dir, 'db', 'bazarr.db'), timeout=30)
        c = db.cursor()
        data = c.execute(""SELECT title, year, alternativeTitles, format, resolution, video_codec, audio_codec, imdbId FROM table_movies WHERE path = ?"", (path_replace_reverse_movie(path),)).fetchone()
        db.close()
        if data:
            video.title = re.sub(r'(\\(\\d\\d\\d\\d\\))' , '', data[0])
            if int(data[1]) > 0: video.year = int(data[1])
            if data[7]: video.imdb_id = data[7]
            video.alternative_titles = ast.literal_eval(data[2])
            if not video.format:
                if data[3]: video.format = data[3]
            if not video.resolution:
                if data[4]: video.resolution = data[4]
            if not video.video_codec:
                if data[5]: video.video_codec = data[5]
            if not video.audio_codec:
                if data[6]: video.audio_codec = data[6]

    return video","1. Use prepared statements instead of building queries with string concatenation.
2. Use `sqlite3.connect(..., timeout=30)` to set a timeout for the database connection.
3. Use `path_replace_reverse()` and `path_replace_reverse_movie()` to sanitize the path before using it in a database query."
"def sync_episodes():
    logging.debug('Starting episode sync from Sonarr.')
    from get_settings import get_sonarr_settings
    url_sonarr = get_sonarr_settings()[6]
    apikey_sonarr = get_sonarr_settings()[4]
    
    # Open database connection
    db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
    c = db.cursor()

    # Get current episodes id in DB
    current_episodes_db = c.execute('SELECT sonarrEpisodeId FROM table_episodes').fetchall()

    current_episodes_db_list = [x[0] for x in current_episodes_db]
    current_episodes_sonarr = []
    episodes_to_update = []
    episodes_to_add = []

    # Get sonarrId for each series from database
    seriesIdList = c.execute(""SELECT sonarrSeriesId FROM table_shows"").fetchall()

    # Close database connection
    c.close()

    for seriesId in seriesIdList:
        # Get episodes data for a series from Sonarr
        url_sonarr_api_episode = url_sonarr + ""/api/episode?seriesId="" + str(seriesId[0]) + ""&apikey="" + apikey_sonarr
        try:
            r = requests.get(url_sonarr_api_episode, timeout=15, verify=False)
            r.raise_for_status()
        except requests.exceptions.HTTPError as errh:
            logging.exception(""Error trying to get episodes from Sonarr. Http error."")
        except requests.exceptions.ConnectionError as errc:
            logging.exception(""Error trying to get episodes from Sonarr. Connection Error."")
        except requests.exceptions.Timeout as errt:
            logging.exception(""Error trying to get episodes from Sonarr. Timeout Error."")
        except requests.exceptions.RequestException as err:
            logging.exception(""Error trying to get episodes from Sonarr."")
        else:
            for episode in r.json():
                if 'hasFile' in episode:
                    if episode['hasFile'] is True:
                        if 'episodeFile' in episode:
                            if episode['episodeFile']['size'] > 20480:
                                # Add shows in Sonarr to current shows list
                                if 'sceneName' in episode['episodeFile']:
                                    sceneName = episode['episodeFile']['sceneName']
                                else:
                                    sceneName = None

                                # Add episodes in sonarr to current episode list
                                current_episodes_sonarr.append(episode['id'])

                                if episode['id'] in current_episodes_db_list:
                                    episodes_to_update.append((episode['title'], episode['episodeFile']['path'], episode['seasonNumber'], episode['episodeNumber'], sceneName, str(bool(episode['monitored'])), episode['id']))
                                else:
                                    episodes_to_add.append((episode['seriesId'], episode['id'], episode['title'], episode['episodeFile']['path'], episode['seasonNumber'], episode['episodeNumber'], sceneName, str(bool(episode['monitored']))))

    removed_episodes = list(set(current_episodes_db_list) - set(current_episodes_sonarr))

    # Update or insert movies in DB
    db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
    c = db.cursor()

    updated_result = c.executemany('''UPDATE table_episodes SET title = ?, path = ?, season = ?, episode = ?, scene_name = ?, monitored = ? WHERE sonarrEpisodeId = ?''', episodes_to_update)
    db.commit()

    try:
        added_result = c.executemany('''INSERT INTO table_episodes(sonarrSeriesId, sonarrEpisodeId, title, path, season, episode, scene_name, monitored) VALUES (?, ?, ?, ?, ?, ?, ?, ?)''', episodes_to_add)
    except sqlite3.IntegrityError as e:
        logging.exception(""You're probably an early adopter of Bazarr and this is a known issue. Please open an issue on Github and we'll fix this."")
    else:
        db.commit()

    for removed_episode in removed_episodes:
        c.execute('DELETE FROM table_episodes WHERE sonarrEpisodeId = ?', (removed_episode,))
        db.commit()

    # Close database connection
    c.close()

    for added_episode in episodes_to_add:
        store_subtitles(path_replace(added_episode[3]))

    logging.debug('All episodes synced from Sonarr into database.')

    list_missing_subtitles()
    logging.debug('All missing subtitles updated in database.')","1. Use `requests.get()` with `verify=False` only when you are sure that the server is trusted.
2. Use `sqlite3.connect()` with `timeout=30` to avoid database connection timeouts.
3. Use `sqlite3.IntegrityError` to handle integrity errors when inserting data into the database."
"def update_movies():
    logging.debug('Starting movie sync from Radarr.')
    from get_settings import get_radarr_settings
    url_radarr = get_radarr_settings()[6]
    apikey_radarr = get_radarr_settings()[4]
    movie_default_enabled = get_general_settings()[18]
    movie_default_language = get_general_settings()[19]
    movie_default_hi = get_general_settings()[20]

    if apikey_radarr == None:
        pass
    else:
        get_profile_list()

        # Get movies data from radarr
        url_radarr_api_movies = url_radarr + ""/api/movie?apikey="" + apikey_radarr
        try:
            r = requests.get(url_radarr_api_movies, timeout=15, verify=False)
            r.raise_for_status()
        except requests.exceptions.HTTPError as errh:
            logging.exception(""Error trying to get movies from Radarr. Http error."")
        except requests.exceptions.ConnectionError as errc:
            logging.exception(""Error trying to get movies from Radarr. Connection Error."")
        except requests.exceptions.Timeout as errt:
            logging.exception(""Error trying to get movies from Radarr. Timeout Error."")
        except requests.exceptions.RequestException as err:
            logging.exception(""Error trying to get movies from Radarr."")
        else:
            # Get current movies in DB
            db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
            c = db.cursor()
            current_movies_db = c.execute('SELECT tmdbId FROM table_movies').fetchall()
            db.close()

            current_movies_db_list = [x[0] for x in current_movies_db]
            current_movies_radarr = []
            movies_to_update = []
            movies_to_add = []

            for movie in r.json():
                if movie['hasFile'] is True:
                    if 'movieFile' in movie:
                        try:
                            overview = unicode(movie['overview'])
                        except:
                            overview = """"
                        try:
                            poster_big = movie['images'][0]['url']
                            poster = os.path.splitext(poster_big)[0] + '-500' + os.path.splitext(poster_big)[1]
                        except:
                            poster = """"
                        try:
                            fanart = movie['images'][1]['url']
                        except:
                            fanart = """"

                        if 'sceneName' in movie['movieFile']:
                            sceneName = movie['movieFile']['sceneName']
                        else:
                            sceneName = None

                        # Add movies in radarr to current movies list
                        current_movies_radarr.append(unicode(movie['tmdbId']))

                        # Detect file separator
                        if movie['path'][0] == ""/"":
                            separator = ""/""
                        else:
                            separator = ""\\\\""

                        if unicode(movie['tmdbId']) in current_movies_db_list:
                            movies_to_update.append((movie[""title""],movie[""path""] + separator + movie['movieFile']['relativePath'],movie[""tmdbId""],movie[""id""],overview,poster,fanart,profile_id_to_language(movie['qualityProfileId']),sceneName,unicode(bool(movie['monitored'])),movie[""tmdbId""]))
                        else:
                            if movie_default_enabled is True:
                                movies_to_add.append((movie[""title""], movie[""path""] + separator + movie['movieFile']['relativePath'], movie[""tmdbId""], movie_default_language, '[]', movie_default_hi, movie[""id""], overview, poster, fanart, profile_id_to_language(movie['qualityProfileId']), sceneName, unicode(bool(movie['monitored']))))
                            else:
                                movies_to_add.append((movie[""title""], movie[""path""] + separator + movie['movieFile']['relativePath'], movie[""tmdbId""], movie[""tmdbId""], movie[""tmdbId""], movie[""id""], overview, poster, fanart, profile_id_to_language(movie['qualityProfileId']), sceneName, unicode(bool(movie['monitored']))))

            # Update or insert movies in DB
            db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
            c = db.cursor()

            updated_result = c.executemany('''UPDATE table_movies SET title = ?, path = ?, tmdbId = ?, radarrId = ?, overview = ?, poster = ?, fanart = ?, `audio_language` = ?, sceneName = ?, monitored = ? WHERE tmdbid = ?''', movies_to_update)
            db.commit()

            if movie_default_enabled is True:
                added_result = c.executemany('''INSERT INTO table_movies(title, path, tmdbId, languages, subtitles,`hearing_impaired`, radarrId, overview, poster, fanart, `audio_language`, sceneName, monitored) VALUES (?,?,?,?,?, ?, ?, ?, ?, ?, ?, ?, ?)''', movies_to_add)
                db.commit()
            else:
                added_result = c.executemany('''INSERT INTO table_movies(title, path, tmdbId, languages, subtitles,`hearing_impaired`, radarrId, overview, poster, fanart, `audio_language`, sceneName, monitored) VALUES (?,?,?,(SELECT languages FROM table_movies WHERE tmdbId = ?), '[]',(SELECT `hearing_impaired` FROM table_movies WHERE tmdbId = ?), ?, ?, ?, ?, ?, ?, ?)''', movies_to_add)
                db.commit()
            db.close()

            added_movies = list(set(current_movies_radarr) - set(current_movies_db_list))
            removed_movies = list(set(current_movies_db_list) - set(current_movies_radarr))

            db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
            c = db.cursor()
            c.executemany('DELETE FROM table_movies WHERE tmdbId = ?', removed_movies)
            db.commit()
            db.close()

            db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
            c = db.cursor()
            for added_movie in added_movies:
                added_path = c.execute('SELECT path FROM table_movies WHERE tmdbId = ?', (added_movie,)).fetchone()
                store_subtitles_movie(path_replace_movie(added_path[0]))
            db.close()

    logging.debug('All movies synced from Radarr into database.')

    list_missing_subtitles_movies()
    logging.debug('All movie missing subtitles updated in database.')","1. Use `requests.get()` with `verify=False` only when you trust the server's certificate.
2. Use `sqlite3.connect()` with `timeout=30` to prevent the database from becoming unresponsive.
3. Use `path_replace_movie()` to escape the path of the movie file before storing it in the database."
"def update_series():
    from get_settings import get_sonarr_settings
    url_sonarr = get_sonarr_settings()[6]
    apikey_sonarr = get_sonarr_settings()[4]
    serie_default_enabled = get_general_settings()[15]
    serie_default_language = get_general_settings()[16]
    serie_default_hi = get_general_settings()[17]

    if apikey_sonarr == None:
        pass
    else:
        get_profile_list()
    
        # Get shows data from Sonarr
        url_sonarr_api_series = url_sonarr + ""/api/series?apikey="" + apikey_sonarr
        try:
            r = requests.get(url_sonarr_api_series, timeout=15, verify=False)
            r.raise_for_status()
        except requests.exceptions.HTTPError as errh:
            logging.exception(""Error trying to get series from Sonarr. Http error."")
        except requests.exceptions.ConnectionError as errc:
            logging.exception(""Error trying to get series from Sonarr. Connection Error."")
        except requests.exceptions.Timeout as errt:
            logging.exception(""Error trying to get series from Sonarr. Timeout Error."")
        except requests.exceptions.RequestException as err:
            logging.exception(""Error trying to get series from Sonarr."")
        else:
            # Open database connection
            db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
            c = db.cursor()

            # Get current shows in DB
            current_shows_db = c.execute('SELECT tvdbId FROM table_shows').fetchall()

            # Close database connection
            db.close()

            current_shows_db_list = [x[0] for x in current_shows_db]
            current_shows_sonarr = []
            series_to_update = []
            series_to_add = []

            for show in r.json():
                try:
                    overview = unicode(show['overview'])
                except:
                    overview = """"
                try:
                    poster_big = show['images'][2]['url'].split('?')[0]
                    poster = os.path.splitext(poster_big)[0] + '-250' + os.path.splitext(poster_big)[1]
                except:
                    poster = """"
                try:
                    fanart = show['images'][0]['url'].split('?')[0]
                except:
                    fanart = """"

                # Add shows in Sonarr to current shows list
                current_shows_sonarr.append(show['tvdbId'])

                if show['tvdbId'] in current_shows_db_list:
                    series_to_update.append((show[""title""],show[""path""],show[""tvdbId""],show[""id""],overview,poster,fanart,profile_id_to_language((show['qualityProfileId'] if sonarr_version == 2 else show['languageProfileId'])),show['sortTitle'],show[""tvdbId""]))
                else:
                    if serie_default_enabled is True:
                        series_to_add.append((show[""title""], show[""path""], show[""tvdbId""], serie_default_language, serie_default_hi, show[""id""], overview, poster, fanart, profile_id_to_language(show['qualityProfileId']), show['sortTitle']))
                    else:
                        series_to_add.append((show[""title""], show[""path""], show[""tvdbId""], show[""tvdbId""], show[""tvdbId""], show[""id""], overview, poster, fanart, profile_id_to_language(show['qualityProfileId']), show['sortTitle']))

            # Update or insert series in DB
            db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
            c = db.cursor()

            updated_result = c.executemany('''UPDATE table_shows SET title = ?, path = ?, tvdbId = ?, sonarrSeriesId = ?, overview = ?, poster = ?, fanart = ?, `audio_language` = ? , sortTitle = ? WHERE tvdbid = ?''', series_to_update)
            db.commit()

            if serie_default_enabled is True:
                added_result = c.executemany('''INSERT INTO table_shows(title, path, tvdbId, languages,`hearing_impaired`, sonarrSeriesId, overview, poster, fanart, `audio_language`, sortTitle) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''', series_to_add)
                db.commit()
            else:
                added_result = c.executemany('''INSERT INTO table_shows(title, path, tvdbId, languages,`hearing_impaired`, sonarrSeriesId, overview, poster, fanart, `audio_language`, sortTitle) VALUES (?,?,?,(SELECT languages FROM table_shows WHERE tvdbId = ?),(SELECT `hearing_impaired` FROM table_shows WHERE tvdbId = ?), ?, ?, ?, ?, ?, ?)''', series_to_add)
                db.commit()
            db.close()

            for show in series_to_add:
                list_missing_subtitles(show[5])

            # Delete shows not in Sonarr anymore
            deleted_items = []
            for item in current_shows_db_list:
                if item not in current_shows_sonarr:
                    deleted_items.append(tuple([item]))
            db = sqlite3.connect(os.path.join(config_dir, 'db/bazarr.db'), timeout=30)
            c = db.cursor()
            c.executemany('DELETE FROM table_shows WHERE tvdbId = ?',deleted_items)
            db.commit()
            db.close()","1. Use `requests.get()` with the `verify=False` parameter to disable SSL verification. This is insecure and should only be used for testing purposes.
2. Use `sqlite3.connect()` with the `timeout=30` parameter to set a timeout for database connections. This will help prevent the application from hanging if the database is unavailable.
3. Use `c.executemany()` to insert multiple rows into the database in a single transaction. This will improve performance and reduce the number of database connections that are opened."
"def save_subtitles(video, subtitles, single=False, directory=None, encoding=None):
    """"""Save subtitles on filesystem.

    Subtitles are saved in the order of the list. If a subtitle with a language has already been saved, other subtitles
    with the same language are silently ignored.

    The extension used is `.lang.srt` by default or `.srt` is `single` is `True`, with `lang` being the IETF code for
    the :attr:`~subliminal.subtitle.Subtitle.language` of the subtitle.

    :param video: video of the subtitles.
    :type video: :class:`~subliminal.video.Video`
    :param subtitles: subtitles to save.
    :type subtitles: list of :class:`~subliminal.subtitle.Subtitle`
    :param bool single: save a single subtitle, default is to save one subtitle per language.
    :param str directory: path to directory where to save the subtitles, default is next to the video.
    :param str encoding: encoding in which to save the subtitles, default is to keep original encoding.
    :return: the saved subtitles
    :rtype: list of :class:`~subliminal.subtitle.Subtitle`

    """"""
    saved_subtitles = []
    for subtitle in subtitles:
        # check content
        if subtitle.content is None:
            logger.error('Skipping subtitle %r: no content', subtitle)
            continue

        # check language
        if subtitle.language in set(s.language for s in saved_subtitles):
            logger.debug('Skipping subtitle %r: language already saved', subtitle)
            continue

        # create subtitle path
        subtitle_path = get_subtitle_path(video.name, None if single else subtitle.language)
        if directory is not None:
            subtitle_path = os.path.join(directory, os.path.split(subtitle_path)[1])

        # save content as is or in the specified encoding
        logger.info('Saving %r to %r', subtitle, subtitle_path)
        if encoding is None:
            with io.open(subtitle_path, 'wb') as f:
                f.write(subtitle.content)
        else:
            with io.open(subtitle_path, 'w', encoding=encoding) as f:
                f.write(subtitle.text)
        saved_subtitles.append(subtitle)

        # check single
        if single:
            break

    return [saved_subtitles, subtitle_path]","1. Use `assert` statements to check for invalid inputs.
2. Use `os.path.join` to concatenate paths instead of string concatenation.
3. Use `with` statements to open files to ensure they are closed properly."
"def download_subtitle(path, language, hi, providers, providers_auth, sceneName, media_type):
    if hi == ""True"":
        hi = True
    else:
        hi = False
    if media_type == 'series':
        type_of_score = 360
        minimum_score = float(get_general_settings()[8]) / 100 * type_of_score
    elif media_type == 'movie':
        type_of_score = 120
        minimum_score = float(get_general_settings()[22]) / 100 * type_of_score
    use_scenename = get_general_settings()[9]
    use_postprocessing = get_general_settings()[10]
    postprocessing_cmd = get_general_settings()[11]

    if language == 'pob':
        lang_obj = Language('por', 'BR')
    else:
        lang_obj = Language(language)

    try:
        if sceneName is None or use_scenename is False:
            used_sceneName = False
            video = scan_video(path)
        else:
            used_sceneName = True
            video = Video.fromname(sceneName)
    except Exception as e:
        logging.exception('Error trying to extract information from this filename: ' + path)
        return None
    else:
        try:
            best_subtitles = download_best_subtitles([video], {lang_obj}, providers=providers, min_score=minimum_score, hearing_impaired=hi, provider_configs=providers_auth)
        except Exception as e:
            logging.exception('Error trying to get the best subtitles for this file: ' + path)
            return None
        else:
            try:
                best_subtitle = best_subtitles[video][0]
            except:
                logging.debug('No subtitles found for ' + path)
                return None
            else:
                single = get_general_settings()[7]
                try:
                    score = round(float(compute_score(best_subtitle, video, hearing_impaired=hi)) / type_of_score * 100, 2)
                    if used_sceneName == True:
                        video = scan_video(path)
                    if single is True:
                        result = save_subtitles(video, [best_subtitle], single=True, encoding='utf-8')
                    else:
                        result = save_subtitles(video, [best_subtitle], encoding='utf-8')
                except:
                    logging.error('Error saving subtitles file to disk.')
                    return None
                else:
                    downloaded_provider = str(result[0][0]).strip('<>').split(' ')[0][:-8]
                    downloaded_language = language_from_alpha3(language)
                    downloaded_language_code2 = alpha2_from_alpha3(language)
                    downloaded_language_code3 = language
                    downloaded_path = result[1]
                    if used_sceneName == True:
                        message = downloaded_language + "" subtitles downloaded from "" + downloaded_provider + "" with a score of "" + unicode(score) + ""% using this scene name: "" + sceneName
                    else:
                        message = downloaded_language + "" subtitles downloaded from "" + downloaded_provider + "" with a score of "" + unicode(score) + ""% using filename guessing.""

                    if use_postprocessing is True:
                        command = pp_replace(postprocessing_cmd, path, downloaded_path, downloaded_language, downloaded_language_code2, downloaded_language_code3)
                        try:
                            if os.name == 'nt':
                                codepage = subprocess.Popen(""chcp"", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                                # wait for the process to terminate
                                out_codepage, err_codepage = codepage.communicate()
                                encoding = out_codepage.split(':')[-1].strip()

                            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                            # wait for the process to terminate
                            out, err = process.communicate()

                            if os.name == 'nt':
                                out = out.decode(encoding)

                        except:
                            if out == """":
                                logging.error('Post-processing result for file ' + path + ' : Nothing returned from command execution')
                            else:
                                logging.error('Post-processing result for file ' + path + ' : ' + out)
                        else:
                            if out == """":
                                logging.info('Post-processing result for file ' + path + ' : Nothing returned from command execution')
                            else:
                                logging.info('Post-processing result for file ' + path + ' : ' + out)

                    return message","1. Use `pathlib` to handle file paths instead of `os.path`.
2. Use `subprocess.run` instead of `subprocess.Popen` to execute shell commands.
3. Use `logging.captureWarnings` to capture and log all warnings."
"def manual_download_subtitle(path, language, hi, subtitle, provider, providers_auth, sceneName, media_type):
    if hi == ""True"":
        hi = True
    else:
        hi = False
    subtitle = pickle.loads(codecs.decode(subtitle.encode(), ""base64""))
    if media_type == 'series':
        type_of_score = 360
    elif media_type == 'movie':
        type_of_score = 120
    use_scenename = get_general_settings()[9]
    use_postprocessing = get_general_settings()[10]
    postprocessing_cmd = get_general_settings()[11]

    if language == 'pb':
        language = alpha3_from_alpha2(language)
        lang_obj = Language('por', 'BR')
    else:
        language = alpha3_from_alpha2(language)
        lang_obj = Language(language)

    try:
        if sceneName is None or use_scenename is False:
            used_sceneName = False
            video = scan_video(path)
        else:
            used_sceneName = True
            video = Video.fromname(sceneName)
    except Exception as e:
        logging.exception('Error trying to extract information from this filename: ' + path)
        return None
    else:
        try:
            best_subtitle = subtitle
            download_subtitles([best_subtitle], providers=provider, provider_configs=providers_auth)
        except Exception as e:
            logging.exception('Error downloading subtitles for ' + path)
            return None
        else:
            single = get_general_settings()[7]
            try:
                score = round(float(compute_score(best_subtitle, video, hearing_impaired=hi)) / type_of_score * 100, 2)
                if used_sceneName == True:
                    video = scan_video(path)
                if single is True:
                    result = save_subtitles(video, [best_subtitle], single=True, encoding='utf-8')
                else:
                    result = save_subtitles(video, [best_subtitle], encoding='utf-8')
            except Exception as e:
                logging.exception('Error saving subtitles file to disk.')
                return None
            else:
                downloaded_provider = str(result[0][0]).strip('<>').split(' ')[0][:-8]
                downloaded_language = language_from_alpha3(language)
                downloaded_language_code2 = alpha2_from_alpha3(language)
                downloaded_language_code3 = language
                downloaded_path = result[1]
                message = downloaded_language + "" subtitles downloaded from "" + downloaded_provider + "" with a score of "" + unicode(score) + ""% using manual search.""

                if use_postprocessing is True:
                    command = pp_replace(postprocessing_cmd, path, downloaded_path, downloaded_language, downloaded_language_code2, downloaded_language_code3)
                    try:
                        if os.name == 'nt':
                            codepage = subprocess.Popen(""chcp"", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                            # wait for the process to terminate
                            out_codepage, err_codepage = codepage.communicate()
                            encoding = out_codepage.split(':')[-1].strip()

                        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                        # wait for the process to terminate
                        out, err = process.communicate()

                        if os.name == 'nt':
                            out = out.decode(encoding)

                    except:
                        if out == """":
                            logging.error('Post-processing result for file ' + path + ' : Nothing returned from command execution')
                        else:
                            logging.error('Post-processing result for file ' + path + ' : ' + out)
                    else:
                        if out == """":
                            logging.info('Post-processing result for file ' + path + ' : Nothing returned from command execution')
                        else:
                            logging.info('Post-processing result for file ' + path + ' : ' + out)

                return message","1. Use proper encoding when handling strings.
2. Sanitize user input to avoid injection attacks.
3. Use a secure password hashing algorithm."
"def download(obj, provider, refiner, language, age, directory, encoding, single, force, hearing_impaired, min_score,
             max_workers, archives, verbose, path):
    """"""Download best subtitles.

    PATH can be an directory containing videos, a video file path or a video file name. It can be used multiple times.

    If an existing subtitle is detected (external or embedded) in the correct language, the download is skipped for
    the associated video.

    """"""
    # process parameters
    language = set(language)

    # scan videos
    videos = []
    ignored_videos = []
    errored_paths = []
    with click.progressbar(path, label='Collecting videos', item_show_func=lambda p: p or '') as bar:
        for p in bar:
            logger.debug('Collecting path %s', p)

            # non-existing
            if not os.path.exists(p):
                try:
                    video = Video.fromname(p)
                except:
                    logger.exception('Unexpected error while collecting non-existing path %s', p)
                    errored_paths.append(p)
                    continue
                if not force:
                    video.subtitle_languages |= set(search_external_subtitles(video.name, directory=directory).values())
                refine(video, episode_refiners=refiner, movie_refiners=refiner, embedded_subtitles=not force)
                videos.append(video)
                continue

            # directories
            if os.path.isdir(p):
                try:
                    scanned_videos = scan_videos(p, age=age, archives=archives)
                except:
                    logger.exception('Unexpected error while collecting directory path %s', p)
                    errored_paths.append(p)
                    continue
                for video in scanned_videos:
                    if not force:
                        video.subtitle_languages |= set(search_external_subtitles(video.name,
                                                                                  directory=directory).values())
                    if check_video(video, languages=language, age=age, undefined=single):
                        refine(video, episode_refiners=refiner, movie_refiners=refiner, embedded_subtitles=not force)
                        videos.append(video)
                    else:
                        ignored_videos.append(video)
                continue

            # other inputs
            try:
                video = scan_video(p)
            except:
                logger.exception('Unexpected error while collecting path %s', p)
                errored_paths.append(p)
                continue
            if not force:
                video.subtitle_languages |= set(search_external_subtitles(video.name, directory=directory).values())
            if check_video(video, languages=language, age=age, undefined=single):
                refine(video, episode_refiners=refiner, movie_refiners=refiner, embedded_subtitles=not force)
                videos.append(video)
            else:
                ignored_videos.append(video)

    # output errored paths
    if verbose > 0:
        for p in errored_paths:
            click.secho('%s errored' % p, fg='red')

    # output ignored videos
    if verbose > 1:
        for video in ignored_videos:
            click.secho('%s ignored - subtitles: %s / age: %d day%s' % (
                os.path.split(video.name)[1],
                ', '.join(str(s) for s in video.subtitle_languages) or 'none',
                video.age.days,
                's' if video.age.days > 1 else ''
            ), fg='yellow')

    # report collected videos
    click.echo('%s video%s collected / %s video%s ignored / %s error%s' % (
        click.style(str(len(videos)), bold=True, fg='green' if videos else None),
        's' if len(videos) > 1 else '',
        click.style(str(len(ignored_videos)), bold=True, fg='yellow' if ignored_videos else None),
        's' if len(ignored_videos) > 1 else '',
        click.style(str(len(errored_paths)), bold=True, fg='red' if errored_paths else None),
        's' if len(errored_paths) > 1 else '',
    ))

    # exit if no video collected
    if not videos:
        return

    # download best subtitles
    downloaded_subtitles = defaultdict(list)
    with AsyncProviderPool(max_workers=max_workers, providers=provider, provider_configs=obj['provider_configs']) as p:
        with click.progressbar(videos, label='Downloading subtitles',
                               item_show_func=lambda v: os.path.split(v.name)[1] if v is not None else '') as bar:
            for v in bar:
                scores = get_scores(v)
                subtitles = p.download_best_subtitles(p.list_subtitles(v, language - v.subtitle_languages),
                                                      v, language, min_score=scores['hash'] * min_score / 100,
                                                      hearing_impaired=hearing_impaired, only_one=single)
                downloaded_subtitles[v] = subtitles

        if p.discarded_providers:
            click.secho('Some providers have been discarded due to unexpected errors: %s' %
                        ', '.join(p.discarded_providers), fg='yellow')

    # save subtitles
    total_subtitles = 0
    for v, subtitles in downloaded_subtitles.items():
        saved_subtitles = save_subtitles(v, subtitles, single=single, directory=directory, encoding=encoding)[0]
        total_subtitles += len(saved_subtitles)

        if verbose > 0:
            click.echo('%s subtitle%s downloaded for %s' % (click.style(str(len(saved_subtitles)), bold=True),
                                                            's' if len(saved_subtitles) > 1 else '',
                                                            os.path.split(v.name)[1]))

        if verbose > 1:
            for s in saved_subtitles:
                matches = s.get_matches(v)
                score = compute_score(s, v)

                # score color
                score_color = None
                scores = get_scores(v)
                if isinstance(v, Movie):
                    if score < scores['title']:
                        score_color = 'red'
                    elif score < scores['title'] + scores['year'] + scores['release_group']:
                        score_color = 'yellow'
                    else:
                        score_color = 'green'
                elif isinstance(v, Episode):
                    if score < scores['series'] + scores['season'] + scores['episode']:
                        score_color = 'red'
                    elif score < scores['series'] + scores['season'] + scores['episode'] + scores['release_group']:
                        score_color = 'yellow'
                    else:
                        score_color = 'green'

                # scale score from 0 to 100 taking out preferences
                scaled_score = score
                if s.hearing_impaired == hearing_impaired:
                    scaled_score -= scores['hearing_impaired']
                scaled_score *= 100 / scores['hash']

                # echo some nice colored output
                click.echo('  - [{score}] {language} subtitle from {provider_name} (match on {matches})'.format(
                    score=click.style('{:5.1f}'.format(scaled_score), fg=score_color, bold=score >= scores['hash']),
                    language=s.language.name if s.language.country is None else '%s (%s)' % (s.language.name,
                                                                                             s.language.country.name),
                    provider_name=s.provider_name,
                    matches=', '.join(sorted(matches, key=scores.get, reverse=True))
                ))

    if verbose == 0:
        click.echo('Downloaded %s subtitle%s' % (click.style(str(total_subtitles), bold=True),
                                                 's' if total_subtitles > 1 else ''))","1. Use `click.argument` to validate user input.
2. Use `click.option` to set default values for arguments.
3. Use `click.echo` to print output to the console."
"def save_subtitles(video, subtitles, single=False, directory=None, encoding=None):
    """"""Save subtitles on filesystem.

    Subtitles are saved in the order of the list. If a subtitle with a language has already been saved, other subtitles
    with the same language are silently ignored.

    The extension used is `.lang.srt` by default or `.srt` is `single` is `True`, with `lang` being the IETF code for
    the :attr:`~subliminal.subtitle.Subtitle.language` of the subtitle.

    :param video: video of the subtitles.
    :type video: :class:`~subliminal.video.Video`
    :param subtitles: subtitles to save.
    :type subtitles: list of :class:`~subliminal.subtitle.Subtitle`
    :param bool single: save a single subtitle, default is to save one subtitle per language.
    :param str directory: path to directory where to save the subtitles, default is next to the video.
    :param str encoding: encoding in which to save the subtitles, default is to keep original encoding.
    :return: the saved subtitles
    :rtype: list of :class:`~subliminal.subtitle.Subtitle`

    """"""
    saved_subtitles = []
    for subtitle in subtitles:
        # check content
        if subtitle.content is None:
            logger.error('Skipping subtitle %r: no content', subtitle)
            continue

        # check language
        if subtitle.language in set(s.language for s in saved_subtitles):
            logger.debug('Skipping subtitle %r: language already saved', subtitle)
            continue

        # create subtitle path
        subtitle_path = None
        subtitle_path = get_subtitle_path(video.name, None if single else subtitle.language)
        if directory is not None:
            subtitle_path = os.path.join(directory, os.path.split(subtitle_path)[1])

        # save content as is or in the specified encoding
        logger.info('Saving %r to %r', subtitle, subtitle_path)
        if encoding is None:
            with io.open(subtitle_path, 'wb') as f:
                f.write(subtitle.content)
        else:
            with io.open(subtitle_path, 'w', encoding=encoding) as f:
                f.write(subtitle.text)
        saved_subtitles.append(subtitle)

        # check single
        if single:
            break

    return [saved_subtitles, subtitle_path]","1. Use `os.path.join()` to concatenate paths instead of string concatenation.
2. Use `os.makedirs()` to create directories if they don't exist, instead of checking if they exist first.
3. Use `io.open()` with the `'r'` flag to open files in read-only mode, instead of using `open()` with no flags."
"    def _get_filters_from_where_node(self, where_node, check_only=False):
        # Check if this is a leaf node
        if isinstance(where_node, Lookup):
            field_attname = where_node.lhs.target.attname
            lookup = where_node.lookup_name
            value = where_node.rhs

            # Ignore pointer fields that show up in specific page type queries
            if field_attname.endswith('_ptr_id'):
                return

            # Process the filter
            return self._process_filter(field_attname, lookup, value, check_only=check_only)

        elif isinstance(where_node, SubqueryConstraint):
            raise FilterError('Could not apply filter on search results: Subqueries are not allowed.')

        elif isinstance(where_node, WhereNode):
            # Get child filters
            connector = where_node.connector
            child_filters = [self._get_filters_from_where_node(child) for child in where_node.children]

            if not check_only:
                child_filters = [child_filter for child_filter in child_filters if child_filter]
                return self._connect_filters(child_filters, connector, where_node.negated)

        else:
            raise FilterError('Could not apply filter on search results: Unknown where node: ' + str(type(where_node)))","1. Sanitize user input to prevent SQL injection attacks.
2. Validate the structure of the query to prevent denial-of-service attacks.
3. Use prepared statements to prevent code injection attacks."
"    def find_embed(self, url, max_width=None):
        # Find provider
        endpoint = self._get_endpoint(url)
        if endpoint is None:
            raise EmbedNotFoundException

        # Work out params
        params = self.options.copy()
        params['url'] = url
        params['format'] = 'json'
        if max_width:
            params['maxwidth'] = max_width

        # Perform request
        request = Request(endpoint + '?' + urlencode(params))
        request.add_header('User-agent', 'Mozilla/5.0')
        try:
            r = urllib_request.urlopen(request)
        except URLError:
            raise EmbedNotFoundException
        oembed = json.loads(r.read().decode('utf-8'))

        # Convert photos into HTML
        if oembed['type'] == 'photo':
            html = '<img src=""%s"" alt="""">' % (oembed['url'], )
        else:
            html = oembed.get('html')

        # Return embed as a dict
        return {
            'title': oembed['title'] if 'title' in oembed else '',
            'author_name': oembed['author_name'] if 'author_name' in oembed else '',
            'provider_name': oembed['provider_name'] if 'provider_name' in oembed else '',
            'type': oembed['type'],
            'thumbnail_url': oembed.get('thumbnail_url'),
            'width': oembed.get('width'),
            'height': oembed.get('height'),
            'html': html,
        }","1. Use `urllib.request.urlopen()` instead of `urllib2.urlopen()`, which is deprecated.
2. Add `user-agent` header to the request to avoid being blocked by some websites.
3. Use `json.loads()` to parse the response body as JSON, instead of manually decoding the JSON string."
"    def handle(self, *args, **options):
        current_page_id = None
        missing_models_content_type_ids = set()
        for revision in PageRevision.objects.order_by('page_id', 'created_at').select_related('page').iterator():
            # This revision is for a page type that is no longer in the database. Bail out early.
            if revision.page.content_type_id in missing_models_content_type_ids:
                continue
            if not revision.page.specific_class:
                missing_models_content_type_ids.add(revision.page.content_type_id)
                continue

            is_new_page = revision.page_id != current_page_id
            if is_new_page:
                # reset previous revision when encountering a new page.
                previous_revision = None

            has_content_changes = False
            current_page_id = revision.page_id

            if not PageLogEntry.objects.filter(revision=revision).exists():
                try:
                    current_revision_as_page = revision.as_page_object()
                except Exception:
                    # restoring old revisions may fail if e.g. they have an on_delete=PROTECT foreign key
                    # to a no-longer-existing model instance. We cannot compare changes between two
                    # non-restorable revisions, although we can at least infer that there was a content
                    # change at the point that it went from restorable to non-restorable or vice versa.
                    current_revision_as_page = None

                published = revision.id == revision.page.live_revision_id

                if previous_revision is not None:
                    try:
                        previous_revision_as_page = previous_revision.as_page_object()
                    except Exception:
                        previous_revision_as_page = None

                    if previous_revision_as_page is None and current_revision_as_page is None:
                        # both revisions failed to restore - unable to determine presence of content changes
                        has_content_changes = False
                    elif previous_revision_as_page is None or current_revision_as_page is None:
                        # one or the other revision failed to restore, which indicates a content change
                        has_content_changes = True
                    else:
                        # Must use .specific so the comparison picks up all fields, not just base Page ones.
                        comparison = get_comparison(revision.page.specific, previous_revision_as_page, current_revision_as_page)
                        has_content_changes = len(comparison) > 0

                    if current_revision_as_page.live_revision_id == previous_revision.id:
                        # Log the previous revision publishing.
                        self.log_page_action('wagtail.publish', previous_revision, True)

                if is_new_page or has_content_changes or published:
                    if is_new_page:
                        action = 'wagtail.create'
                    elif published:
                        action = 'wagtail.publish'
                    else:
                        action = 'wagtail.edit'

                    if published and has_content_changes:
                        # When publishing, also log the 'draft save', but only if there have been content changes
                        self.log_page_action('wagtail.edit', revision, has_content_changes)

                    self.log_page_action(action, revision, has_content_changes)

            previous_revision = revision","1. Use `try/except` blocks to catch errors when restoring old revisions.
2. Use `get_comparison()` to compare the two revisions and determine if there are any content changes.
3. Log the page action (`wagtail.create`, `wagtail.edit`, or `wagtail.publish`)."
"    def can_delete(self, locale):
        return get_locale_usage(locale) == (0, 0)","1. Use `get_locale_usage` with the `check_permissions` parameter set to `True` to ensure that the user has permission to delete the locale.
2. Sanitize the input to `get_locale_usage` to prevent SQL injection attacks.
3. Use `user_passes_test` to verify that the user is logged in and has the appropriate permissions to delete the locale."
"def get_content_languages():
    """"""
    Cache of settings.WAGTAIL_CONTENT_LANGUAGES in a dictionary for easy lookups by key.
    """"""
    content_languages = getattr(settings, 'WAGTAIL_CONTENT_LANGUAGES', None)
    languages = dict(settings.LANGUAGES)

    if content_languages is None:
        # Default to a single language based on LANGUAGE_CODE
        default_language_code = get_supported_language_variant(settings.LANGUAGE_CODE)
        content_languages = [
            (default_language_code, languages[default_language_code]),
        ]

    # Check that each content language is in LANGUAGES
    for language_code, name in content_languages:
        if language_code not in languages:
            raise ImproperlyConfigured(
                ""The language {} is specified in WAGTAIL_CONTENT_LANGUAGES but not LANGUAGES. ""
                ""WAGTAIL_CONTENT_LANGUAGES must be a subset of LANGUAGES."".format(language_code)
            )

    return dict(content_languages)","1. Use `django.utils.translation.get_language_from_request()` to get the language code from the request.
2. Check that the language code is valid using `django.utils.translation.check_language()`.
3. Raise `django.core.exceptions.ImproperlyConfigured` if the language code is not valid."
"    def get_active(cls):
        """"""
        Returns the Locale that corresponds to the currently activated language in Django.
        """"""
        try:
            return cls.objects.get_for_language(translation.get_language())
        except cls.DoesNotExist:
            return cls.get_default()","1. Use `get_for_language()` instead of `get()` to avoid a `DoesNotExist` exception.
2. Use `get_default()` to return a default locale if the requested locale does not exist.
3. Use `translation.get_language()` to get the currently activated language in Django."
"    def localized(self):
        """"""
        Finds the translation in the current active language.

        If there is no translation in the active language, self is returned.
        """"""
        locale = Locale.get_active()

        if locale.id == self.locale_id:
            return self

        return self.get_translation_or_none(locale) or self","1. Use `get_translation` instead of `get_translation_or_none` to avoid returning `self` when there is no translation in the active language.
2. Check if the locale exists before calling `get_translation`.
3. Use `Locale.get_default()` to get the default locale instead of `Locale.get_active()`."
"    def localized_draft(self):
        """"""
        Finds the translation in the current active language.

        If there is no translation in the active language, self is returned.

        Note: This will return translations that are in draft. If you want to exclude
        these, use the ``.localized`` attribute.
        """"""
        locale = Locale.get_active()

        if locale.id == self.locale_id:
            return self

        return self.get_translation_or_none(locale) or self","1. Use `get_translation` instead of `get_translation_or_none` to avoid returning `self` when there is no translation in the active language.
2. Use `Locale.get_default()` instead of `Locale.get_active()` to get the default language, which is more secure.
3. Check if the translation is in draft before returning it, and return `None` if it is."
"    def get_url_parts(self, request=None):
        """"""
        Determine the URL for this page and return it as a tuple of
        ``(site_id, site_root_url, page_url_relative_to_site_root)``.
        Return None if the page is not routable.

        This is used internally by the ``full_url``, ``url``, ``relative_url``
        and ``get_site`` properties and methods; pages with custom URL routing
        should override this method in order to have those operations return
        the custom URLs.

        Accepts an optional keyword argument ``request``, which may be used
        to avoid repeated database / cache lookups. Typically, a page model
        that overrides ``get_url_parts`` should not need to deal with
        ``request`` directly, and should just pass it to the original method
        when calling ``super``.
        """"""

        possible_sites = [
            (pk, path, url, language_code)
            for pk, path, url, language_code in self._get_site_root_paths(request)
            if self.url_path.startswith(path)
        ]

        if not possible_sites:
            return None

        site_id, root_path, root_url, language_code = possible_sites[0]

        site = Site.find_for_request(request)
        if site:
            for site_id, root_path, root_url, language_code in possible_sites:
                if site_id == site.pk:
                    break
            else:
                site_id, root_path, root_url, language_code = possible_sites[0]

        # If the active language code is a variant of the page's language, then
        # use that instead
        # This is used when LANGUAGES contain more languages than WAGTAIL_CONTENT_LANGUAGES
        if get_supported_content_language_variant(translation.get_language()) == language_code:
            language_code = translation.get_language()

        # The page may not be routable because wagtail_serve is not registered
        # This may be the case if Wagtail is used headless
        try:
            with translation.override(language_code):
                page_path = reverse(
                    'wagtail_serve', args=(self.url_path[len(root_path):],))
        except NoReverseMatch:
            return (site_id, None, None)

        # Remove the trailing slash from the URL reverse generates if
        # WAGTAIL_APPEND_SLASH is False and we're not trying to serve
        # the root path
        if not WAGTAIL_APPEND_SLASH and page_path != '/':
            page_path = page_path.rstrip('/')

        return (site_id, root_url, page_path)","1. Use `get_supported_content_language_variant()` to get the correct language code for the page.
2. Use `reverse()` with the `wagtail_serve` view to generate the page URL.
3. Remove the trailing slash from the URL if `WAGTAIL_APPEND_SLASH` is False."
"    def handle(self, *args, **options):
        current_page_id = None
        missing_models_content_type_ids = set()
        for revision in PageRevision.objects.order_by('page_id', 'created_at').select_related('page').iterator():
            # This revision is for a page type that is no longer in the database. Bail out early.
            if revision.page.content_type_id in missing_models_content_type_ids:
                continue
            if not revision.page.specific_class:
                missing_models_content_type_ids.add(revision.page.content_type_id)
                continue

            is_new_page = revision.page_id != current_page_id
            if is_new_page:
                # reset previous revision when encountering a new page.
                previous_revision = None

            has_content_changes = False
            current_page_id = revision.page_id

            if not PageLogEntry.objects.filter(revision=revision).exists():
                current_revision_as_page = revision.as_page_object()
                published = revision.id == revision.page.live_revision_id

                if previous_revision is not None:
                    # Must use .specific so the comparison picks up all fields, not just base Page ones.
                    comparison = get_comparison(revision.page.specific, previous_revision.as_page_object(), current_revision_as_page)
                    has_content_changes = len(comparison) > 0

                    if current_revision_as_page.live_revision_id == previous_revision.id:
                        # Log the previous revision publishing.
                        self.log_page_action('wagtail.publish', previous_revision, True)

                if is_new_page or has_content_changes or published:
                    if is_new_page:
                        action = 'wagtail.create'
                    elif published:
                        action = 'wagtail.publish'
                    else:
                        action = 'wagtail.edit'

                    if published and has_content_changes:
                        # When publishing, also log the 'draft save', but only if there have been content changes
                        self.log_page_action('wagtail.edit', revision, has_content_changes)

                    self.log_page_action(action, revision, has_content_changes)

            previous_revision = revision","1. Use `.select_related()` to only load the related objects that you need. This will reduce the amount of data that is loaded into memory and improve performance.
2. Use `.iterator()` to iterate over the results of a queryset. This will prevent the entire queryset from being loaded into memory at once, which can help to prevent out-of-memory errors.
3. Use `.filter()` to only include the revisions that you need. This will reduce the amount of data that is processed, which can improve performance."
"def timesince_last_update(last_update, time_prefix='', use_shorthand=True):
    """"""
    Returns:
         - the time of update if last_update is today, if any prefix is supplied, the output will use it
         - time since last update othewise. Defaults to the simplified timesince,
           but can return the full string if needed
    """"""
    if last_update.date() == datetime.today().date():
        time_str = timezone.localtime(last_update).strftime(""%H:%M"")
        return time_str if not time_prefix else '%(prefix)s %(formatted_time)s' % {
            'prefix': time_prefix, 'formatted_time': time_str
        }
    else:
        if use_shorthand:
            return timesince_simple(last_update)
        return _(""%(time_period)s ago"") % {'time_period': timesince(last_update)}","1. Use `datetime.now()` instead of `datetime.today()` to avoid leaking information about the system's time zone.
2. Use `timesince_simple()` instead of `timesince()` to avoid leaking information about the system's locale.
3. Use `django.utils.timezone.localtime()` to format the time in a way that is not affected by the system's time zone or locale."
"    def __get__(self, obj, type=None):
        if obj is None:
            return self
        return obj.__dict__[self.field.name]","1. Use `@property` decorator to avoid exposing `__get__` method.
2. Use `@functools.wraps` decorator to preserve the original function's metadata.
3. Use `obj.__dict__[self.field.name]` instead of `getattr(obj, self.field.name)` to avoid triggering attribute lookup."
"    def to_representation(self, page):
        name = page.specific_class._meta.app_label + '.' + page.specific_class.__name__
        self.context['view'].seen_types[name] = page.specific_class
        return name","1. Use `django.utils.safestring.mark_safe()` to escape the output of `page.specific_class.__name__`.
2. Use `django.contrib.auth.decorators.login_required()` to protect the view from unauthorized access.
3. Use `django.template.context_processors.csrf()` to protect the view from cross-site request forgery attacks."
"    def get_queryset(self):
        request = self.request

        # Allow pages to be filtered to a specific type
        try:
            models = page_models_from_string(request.GET.get('type', 'wagtailcore.Page'))
        except (LookupError, ValueError):
            raise BadRequestError(""type doesn't exist"")

        if not models:
            models = [Page]

        if len(models) == 1:
            queryset = models[0].objects.all()
        else:
            queryset = Page.objects.all()

            # Filter pages by specified models
            queryset = filter_page_type(queryset, models)

        # Get live pages that are not in a private section
        queryset = queryset.public().live()

        # Filter by site
        queryset = queryset.descendant_of(request.site.root_page, inclusive=True)

        return queryset","1. Sanitize the `type` parameter to prevent malicious users from injecting arbitrary models into the queryset.
2. Use `filter_for_user` instead of `descendant_of` to restrict the queryset to pages that the current user has access to.
3. Use `.specific()` to ensure that the returned queryset only contains instances of `Page`."
"def get_base_url(request=None):
    base_url = getattr(settings, 'WAGTAILAPI_BASE_URL', request.site.root_url if request else None)

    if base_url:
        # We only want the scheme and netloc
        base_url_parsed = urlparse(base_url)

        return base_url_parsed.scheme + '://' + base_url_parsed.netloc","1. Use `django.utils.http.urlquote` to escape the base URL before parsing it.
2. Check that the base URL is a valid URL before using it.
3. Use `django.utils.http.Http404` to raise an error if the base URL is not valid."
"    def dummy_request(self, original_request=None, **meta):
        """"""
        Construct a HttpRequest object that is, as far as possible, representative of ones that would
        receive this page as a response. Used for previewing / moderation and any other place where we
        want to display a view of this page in the admin interface without going through the regular
        page routing logic.

        If you pass in a real request object as original_request, additional information (e.g. client IP, cookies)
        will be included in the dummy request.
        """"""
        url = self.full_url
        if url:
            url_info = urlparse(url)
            hostname = url_info.hostname
            path = url_info.path
            port = url_info.port or 80
        else:
            # Cannot determine a URL to this page - cobble one together based on
            # whatever we find in ALLOWED_HOSTS
            try:
                hostname = settings.ALLOWED_HOSTS[0]
                if hostname == '*':
                    # '*' is a valid value to find in ALLOWED_HOSTS[0], but it's not a valid domain name.
                    # So we pretend it isn't there.
                    raise IndexError
            except IndexError:
                hostname = 'localhost'
            path = '/'
            port = 80

        dummy_values = {
            'REQUEST_METHOD': 'GET',
            'PATH_INFO': path,
            'SERVER_NAME': hostname,
            'SERVER_PORT': port,
            'HTTP_HOST': hostname,
            'wsgi.input': StringIO(),
        }

        # Add important values from the original request object, if it was provided.
        if original_request:
            if original_request.META.get('REMOTE_ADDR'):
                dummy_values['REMOTE_ADDR'] = original_request.META['REMOTE_ADDR']
            if original_request.META.get('HTTP_X_FORWARDED_FOR'):
                dummy_values['HTTP_X_FORWARDED_FOR'] = original_request.META['HTTP_X_FORWARDED_FOR']
            if original_request.META.get('HTTP_COOKIE'):
                dummy_values['HTTP_COOKIE'] = original_request.META['HTTP_COOKIE']
            if original_request.META.get('HTTP_USER_AGENT'):
                dummy_values['HTTP_USER_AGENT'] = original_request.META['HTTP_USER_AGENT']

        # Add additional custom metadata sent by the caller.
        dummy_values.update(**meta)

        request = WSGIRequest(dummy_values)

        # Apply middleware to the request
        # Note that Django makes sure only one of the middleware settings are
        # used in a project
        if hasattr(settings, 'MIDDLEWARE'):
            handler = BaseHandler()
            handler.load_middleware()
            handler._middleware_chain(request)
        elif hasattr(settings, 'MIDDLEWARE_CLASSES'):
            # Pre Django 1.10 style - see http://www.mellowmorning.com/2011/04/18/mock-django-request-for-testing/
            handler = BaseHandler()
            handler.load_middleware()
            # call each middleware in turn and throw away any responses that they might return
            for middleware_method in handler._request_middleware:
                middleware_method(request)

        return request","1. Use `django.test.RequestFactory` instead of `WSGIRequest` to create a dummy request.
2. Use `django.utils.http.HttpRequest` instead of `django.http.HttpRequest` to avoid security issues.
3. Use `django.test.TestCase` instead of `django.views.generic.base.TemplateView` to test your views."
"    def get_searchable_content(self, value):
        # Return the display value as the searchable value
        text_value = force_text(value)
        for k, v in self.field.choices:
            if isinstance(v, (list, tuple)):
                # This is an optgroup, so look inside the group for options
                for k2, v2 in v:
                    if value == k2 or text_value == force_text(k2):
                        return [k, v2]
            else:
                if value == k or text_value == force_text(k):
                    return [v]
        return []  # Value was not found in the list of choices","1. Use `django.utils.html.escape` to escape the value before returning it.
2. Use `django.utils.safestring.mark_safe` to mark the returned value as safe.
3. Use `django.utils.safestring.filter_python_objects` to filter out any dangerous Python objects from the returned value."
"def preview_on_create(request, content_type_app_name, content_type_model_name, parent_page_id):
    # Receive the form submission that would typically be posted to the 'create' view. If submission is valid,
    # return the rendered page; if not, re-render the edit form
    try:
        content_type = ContentType.objects.get_by_natural_key(content_type_app_name, content_type_model_name)
    except ContentType.DoesNotExist:
        raise Http404

    page_class = content_type.model_class()
    page = page_class()
    edit_handler_class = page_class.get_edit_handler()
    form_class = edit_handler_class.get_form_class(page_class)
    parent_page = get_object_or_404(Page, id=parent_page_id).specific

    form = form_class(request.POST, request.FILES, instance=page, parent_page=parent_page)

    if form.is_valid():
        form.save(commit=False)

        # We need to populate treebeard's path / depth fields in order to pass validation.
        # We can't make these 100% consistent with the rest of the tree without making actual
        # database changes (such as incrementing the parent's numchild field), but by
        # calling treebeard's internal _get_path method, we can set a 'realistic' value that
        # will hopefully enable tree traversal operations to at least partially work.
        page.depth = parent_page.depth + 1
        page.path = page._get_path(parent_page.path, page.depth, parent_page.numchild + 1)

        # ensure that our unsaved page instance has a suitable url set
        page.set_url_path(parent_page)

        page.full_clean()

        # Set treebeard attributes
        page.depth = parent_page.depth + 1
        page.path = Page._get_children_path_interval(parent_page.path)[1]

        preview_mode = request.GET.get('mode', page.default_preview_mode)
        response = page.serve_preview(page.dummy_request(), preview_mode)
        response['X-Wagtail-Preview'] = 'ok'
        return response

    else:
        edit_handler = edit_handler_class(instance=page, form=form)

        response = render(request, 'wagtailadmin/pages/create.html', {
            'content_type': content_type,
            'page_class': page_class,
            'parent_page': parent_page,
            'edit_handler': edit_handler,
            'preview_modes': page.preview_modes,
            'form': form,
        })
        response['X-Wagtail-Preview'] = 'error'
        return response","1. Use `django.utils.http.Http404` instead of `raise Http404` to raise a more specific exception.
2. Use `page.full_clean()` to validate the page before saving it.
3. Use `page.serve_preview()` to render the preview page."
"    def get_form_class(cls, model):
        """"""
        Construct a form class that has all the fields and formsets named in
        the children of this edit handler.
        """"""
        if cls._form_class is None:
            cls._form_class = get_form_for_model(
                model,
                form_class=cls.base_form_class,
                fields=cls.required_fields(),
                formsets=cls.required_formsets(),
                widgets=cls.widget_overrides())
        return cls._form_class","1. Use `django.forms.Form` instead of `ModelForm` to avoid
                    potential vulnerabilities.
2. Use `django.utils.translation.gettext_lazy()` to escape user input.
3. Use `django.contrib.auth.forms.UserCreationForm` to create new users."
"    def __init__(self, children, base_form_class=BaseFormEditHandler.base_form_class):
        self.children = children
        self.base_form_class = base_form_class","1. Use `functools.partial` to avoid exposing the `BaseFormEditHandler.base_form_class` parameter to users.
2. Validate user input before passing it to the `base_form_class` constructor.
3. Use a secure default value for the `base_form_class` parameter."
"    def __init__(self, children, heading="""", classname="""",
                 base_form_class=BaseFormEditHandler.base_form_class):
        self.children = children
        self.heading = heading
        self.classname = classname
        self.base_form_class = base_form_class","1. Use `builtins.super()` instead of `BaseFormEditHandler.base_form_class` to avoid referencing a potentially malicious class.
2. Use `typing.TYPE_CHECKING` to check the type of `children` and `classname` to prevent a type error.
3. Use `f-strings` to format the heading instead of concatenation to avoid potential injection attacks."
"    def check(cls, **kwargs):
        errors = super(Page, cls).check(**kwargs)

        # Check that foreign keys from pages are not configured to cascade
        # This is the default Django behaviour which must be explicitly overridden
        # to prevent pages disappearing unexpectedly and the tree being corrupted

        # get names of foreign keys pointing to parent classes (such as page_ptr)
        field_exceptions = [field.name
                            for model in [cls] + list(cls._meta.get_parent_list())
                            for field in model._meta.parents.values() if field]

        for field in cls._meta.fields:
            if isinstance(field, models.ForeignKey) and field.name not in field_exceptions:
                if field.rel.on_delete == models.CASCADE:
                    errors.append(
                        checks.Warning(
                            ""Field hasn't specified on_delete action"",
                            hint=""Set on_delete=models.SET_NULL and make sure the field is nullable."",
                            obj=field,
                            id='wagtailcore.W001',
                        )
                    )

        if not isinstance(cls.objects, PageManager):
            errors.append(
                checks.Error(
                    ""Manager does not inherit from PageManager"",
                    hint=""Ensure that custom Page managers inherit from {}.{}"".format(
                        PageManager.__module__, PageManager.__name__),
                    obj=cls,
                    id='wagtailcore.E002',
                )
            )

        try:
            cls.clean_subpage_models()
        except (ValueError, LookupError) as e:
            errors.append(
                checks.Error(
                    ""Invalid subpage_types setting for %s"" % cls,
                    hint=str(e),
                    id='wagtailcore.E002'
                )
            )

        try:
            cls.clean_parent_page_models()
        except (ValueError, LookupError) as e:
            errors.append(
                checks.Error(
                    ""Invalid parent_page_types setting for %s"" % cls,
                    hint=str(e),
                    id='wagtailcore.E002'
                )
            )

        from wagtail.wagtailadmin.forms import WagtailAdminPageForm
        if not issubclass(cls.base_form_class, WagtailAdminPageForm):
            errors.append(checks.Error(
                ""base_form_class does not extend WagtailAdminPageForm"",
                hint=""Ensure that {}.{} extends WagtailAdminPageForm"".format(
                    cls.base_form_class.__module__,
                    cls.base_form_class.__name__),
                obj=cls,
                id='wagtailcore.E002'))
        # Sadly, there is no way of checking the form class returned from
        # cls.get_edit_handler().get_form_class(cls), as these calls can hit
        # the DB in order to fetch content types.

        return errors","1. Use `on_delete=models.SET_NULL` for foreign keys pointing to pages.
2. Make sure that custom Page managers inherit from `PageManager`.
3. Ensure that the `base_form_class` extends `WagtailAdminPageForm`."
"    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        # Get the path
        path = models.Redirect.normalise_path(request.get_full_path())

        # Get the path without the query string or params
        path_without_query = urlparse(path)[2]

        # Find redirect
        try:
            redirect = models.Redirect.get_for_site(request.site).get(old_path=path)
        except models.Redirect.DoesNotExist:
            if path == path_without_query:
                # don't try again if we know we will get the same response
                return response

            try:
                redirect = models.Redirect.get_for_site(request.site).get(old_path=path_without_query)
            except models.Redirect.DoesNotExist:
                return response

        if redirect.is_permanent:
            return http.HttpResponsePermanentRedirect(redirect.link)
        else:
            return http.HttpResponseRedirect(redirect.link)","1. **Use `django.utils.http.urlquote` to quote the path parameters.** This will prevent attackers from injecting malicious characters into the path and redirecting users to unintended destinations.
2. **Check the `request.user` object to make sure that the user is authorized to access the redirect.** This will prevent unauthorized users from redirecting other users to unintended destinations.
3. **Use `django.http.Http404` to raise an exception if no redirect is found.** This will prevent the server from returning a blank page or a redirect to a несуществующий URL."
"    def __init__(self, content_type=None, **kwargs):
        super(AdminPageChooser, self).__init__(**kwargs)

        self.target_content_types = content_type or ContentType.objects.get_for_model(Page)
        # Make sure target_content_types is a list or tuple
        if not isinstance(self.target_content_types, (list, tuple)):
            self.target_content_types = [self.target_content_types]","1. Use `django.utils.translation.ugettext_lazy()` to escape the string literals.
2. Use `django.forms.models.ModelChoiceField()` instead of `django.forms.fields.ChoiceField()`.
3. Use `django.contrib.auth.models.User.objects.get_by_natural_key()` to get the user instance."
"    def get_willow_image(self):
        # Open file if it is closed
        close_file = False
        try:
            if self.file.closed:
                self.file.open('rb')
                close_file = True
        except IOError as e:
            # re-throw this as a SourceImageIOError so that calling code can distinguish
            # these from IOErrors elsewhere in the process
            raise SourceImageIOError(text_type(e))

        # Seek to beginning
        self.file.seek(0)

        try:
            yield WillowImage.open(self.file)
        finally:
            if close_file:
                self.file.close()","1. **Use try-with-resources to ensure that the file is closed after use.** This will prevent a resource leak if the code is interrupted.
2. **Check the return value of `open()` to make sure that the file was successfully opened.** This will catch errors that could cause the code to fail silently.
3. **Use `text_type()` to convert the error message to a string.** This will prevent errors if the error message is not encoded in UTF-8."
"def purge_page_from_cache(page, backend_settings=None, backends=None):
    for backend_name, backend in get_backends(backend_settings=backend_settings, backends=backends).items():
        # Purge cached paths from cache
        for path in page.specific.get_cached_paths():
            logger.info(""[%s] Purging URL: %s"", backend_name, page.full_url + path[1:])
            backend.purge(page.full_url + path[1:])","1. Use `urlquote()` to escape the URL path before sending it to the backend.
2. Use `hmac.new()` to generate a secret key for each backend and use it to sign the purged URL.
3. Verify the signature of the purged URL before purging it from the cache."
"def edit(request, image_id):
    Image = get_image_model()
    ImageForm = get_image_form(Image)

    image = get_object_or_404(Image, id=image_id)

    if not image.is_editable_by_user(request.user):
        raise PermissionDenied

    if request.POST:
        original_file = image.file
        form = ImageForm(request.POST, request.FILES, instance=image)
        if form.is_valid():
            if 'file' in form.changed_data:
                # if providing a new image file, delete the old one and all renditions.
                # NB Doing this via original_file.delete() clears the file field,
                # which definitely isn't what we want...
                original_file.storage.delete(original_file.name)
                image.renditions.all().delete()
            form.save()

            # Reindex the image to make sure all tags are indexed
            for backend in get_search_backends():
                backend.add(image)

            messages.success(request, _(""Image '{0}' updated."").format(image.title), buttons=[
                messages.button(reverse('wagtailimages_edit_image', args=(image.id,)), _('Edit again'))
            ])
            return redirect('wagtailimages_index')
        else:
            messages.error(request, _(""The image could not be saved due to errors.""))
    else:
        form = ImageForm(instance=image)

    # Check if we should enable the frontend url generator
    try:
        reverse('wagtailimages_serve', args=('foo', '1', 'bar'))
        url_generator_enabled = True
    except NoReverseMatch:
        url_generator_enabled = False

    return render(request, ""wagtailimages/images/edit.html"", {
        'image': image,
        'form': form,
        'url_generator_enabled': url_generator_enabled,
    })","1. Use `user.has_perm()` to check if the user has permission to edit the image.
2. Delete the old image file and all renditions if a new image file is provided.
3. Reindex the image to make sure all tags are indexed."
"def create_project(parser, options, args):
    # Validate args
    if len(args) < 2:
        parser.error(""Please specify a name for your wagtail installation"")
    elif len(args) > 2:
        parser.error(""Too many arguments"")

    project_name = args[1]

    # Make sure given name is not already in use by another python package/module.
    try:
        __import__(project_name)
    except ImportError:
        pass
    else:
        parser.error(""'%s' conflicts with the name of an existing ""
                     ""Python module and cannot be used as a project ""
                     ""name. Please try another name."" % project_name)

    # Make sure directory does not already exist
    if os.path.exists(project_name):
        print('A directory called %(project_name)s already exists. \\
            Please choose another name for your wagtail project or remove the existing directory.' % {'project_name': project_name})
        sys.exit(errno.EEXIST)

    print(""Creating a wagtail project called %(project_name)s"" % {'project_name': project_name})

    # Create the project from the wagtail template using startapp

    # First find the path to wagtail
    import wagtail
    wagtail_path = os.path.dirname(wagtail.__file__)
    template_path = os.path.join(wagtail_path, 'project_template')

    # Call django-admin startproject
    result = subprocess.call([
        'django-admin.py', 'startproject',
        '--template=' + template_path,
        '--name=Vagrantfile', '--ext=html,rst',
        project_name
    ])

    if result == 0:
        print(""Success! %(project_name)s is created"" % {'project_name': project_name})","1. Use `argparse` instead of `sys.argv` to parse command-line arguments. This will help to prevent against injection attacks.
2. Check for the existence of the project directory before creating it. This will help to prevent against race conditions.
3. Use a more descriptive error message when the project directory already exists. This will help to prevent users from getting confused."
"    def serialize(self, form):
        data = {}

        for key in form.inputs.keys():
            input = form.inputs[key]
            if getattr(input, 'type', '') == 'submit':
                form.remove(input)

        for k, v in form.fields.items():
            if v is None:
                continue

            if isinstance(v, lxml.html.MultipleSelectOptions):
                data[k] = [val for val in v]
            else:
                data[k] = v

        for key in form.inputs.keys():
            input = form.inputs[key]
            if getattr(input, 'type', '') == 'file' and key in data:
                data[key] = open(data[key], 'rb')

        return data","1. Sanitize user input to prevent XSS attacks.
2. Validate file uploads to prevent file injection attacks.
3. Use proper error handling to prevent sensitive information from being leaked."
"    def connect(self, url):
        if not url.startswith(""file:""):
            self.request_url = url
            self._create_connection()
            self._store_response()
            self.conn.close()
        else:
            self.status_code = StatusCode(200, 'Ok')","1. Use `urllib.request.urlopen()` instead of creating a raw TCP connection. This will prevent man-in-the-middle attacks.
2. Use `urllib.request.HTTPBasicAuth()` to authenticate with the server. This will prevent unauthorized access.
3. Use `urllib.request.HTTPSConnection()` to ensure that the connection is encrypted. This will prevent eavesdropping and tampering."
"    def expect_compound_columns_to_be_unique(
        self,
        column_list,
        ignore_row_if=""all_values_are_missing"",
        result_format=None,
        row_condition=None,
        condition_parser=None,
        include_config=True,
        catch_exceptions=None,
        meta=None,
    ):
        columns = [
            sa.column(col[""name""]) for col in self.columns if col[""name""] in column_list
        ]
        query = (
            sa.select([sa.func.count()])
            .group_by(*columns)
            .having(sa.func.count() > 1)
            .select_from(self._table)
        )

        if ignore_row_if == ""all_values_are_missing"":
            query = query.where(sa.and_(*[col != None for col in columns]))
        elif ignore_row_if == ""any_value_is_missing"":
            query = query.where(sa.or_(*[col != None for col in columns]))
        elif ignore_row_if == ""never"":
            pass
        else:
            raise ValueError(
                ""ignore_row_if was set to an unexpected value: %s"" % ignore_row_if
            )

        unexpected_count = self.engine.execute(query).fetchone()

        if unexpected_count is None:
            # This can happen when the condition filters out all rows
            unexpected_count = 0
        else:
            unexpected_count = unexpected_count[0]

        total_count_query = sa.select([sa.func.count()]).select_from(self._table)
        total_count = self.engine.execute(total_count_query).fetchone()[0]

        return {
            ""success"": unexpected_count == 0,
            ""result"": {""unexpected_percent"": 100.0 * unexpected_count / total_count},
        }","1. Use prepared statements instead of concatenating strings with user input.
2. Sanitize user input to prevent SQL injection attacks.
3. Use a database access layer to abstract the database connection details from the application code."
"    def __init__(
        self,
        table_name,
        key_columns,
        fixed_length_key=True,
        credentials=None,
        url=None,
        connection_string=None,
        engine=None,
        store_name=None,
        suppress_store_backend_id=False,
        **kwargs,
    ):
        super().__init__(
            fixed_length_key=fixed_length_key,
            suppress_store_backend_id=suppress_store_backend_id,
            store_name=store_name,
        )
        if not sa:
            raise ge_exceptions.DataContextError(
                ""ModuleNotFoundError: No module named 'sqlalchemy'""
            )

        if not self.fixed_length_key:
            raise ge_exceptions.InvalidConfigError(
                ""DatabaseStoreBackend requires use of a fixed-length-key""
            )

        self._schema_name = None
        self._credentials = credentials
        self._connection_string = connection_string
        self._url = url

        if engine is not None:
            if credentials is not None:
                logger.warning(
                    ""Both credentials and engine were provided during initialization of SqlAlchemyExecutionEngine. ""
                    ""Ignoring credentials.""
                )
            self.engine = engine
        elif credentials is not None:
            self.engine = self._build_engine(credentials=credentials, **kwargs)
        elif connection_string is not None:
            self.engine = sa.create_engine(connection_string, **kwargs)
        elif url is not None:
            self.drivername = urlparse(url).scheme
            self.engine = sa.create_engine(url, **kwargs)
        else:
            raise ge_exceptions.InvalidConfigError(
                ""Credentials, url, connection_string, or an engine are required for a DatabaseStoreBackend.""
            )

        meta = MetaData(schema=self._schema_name)
        self.key_columns = key_columns
        # Dynamically construct a SQLAlchemy table with the name and column names we'll use
        cols = []
        for column in key_columns:
            if column == ""value"":
                raise ge_exceptions.InvalidConfigError(
                    ""'value' cannot be used as a key_element name""
                )
            cols.append(Column(column, String, primary_key=True))
        cols.append(Column(""value"", String))
        try:
            table = Table(table_name, meta, autoload=True, autoload_with=self.engine)
            # We do a ""light"" check: if the columns' names match, we will proceed, otherwise, create the table
            if {str(col.name).lower() for col in table.columns} != (
                set(key_columns) | {""value""}
            ):
                raise ge_exceptions.StoreBackendError(
                    f""Unable to use table {table_name}: it exists, but does not have the expected schema.""
                )
        except NoSuchTableError:
            table = Table(table_name, meta, *cols)
            try:
                if self._schema_name:
                    self.engine.execute(
                        f""CREATE SCHEMA IF NOT EXISTS {self._schema_name};""
                    )
                meta.create_all(self.engine)
            except SQLAlchemyError as e:
                raise ge_exceptions.StoreBackendError(
                    f""Unable to connect to table {table_name} because of an error. It is possible your table needs to be migrated to a new schema.  SqlAlchemyError: {str(e)}""
                )
        self._table = table
        # Initialize with store_backend_id
        self._store_backend_id = None
        self._store_backend_id = self.store_backend_id","1. Use prepared statements to prevent SQL injection attacks.
2. Use a secure password for the database.
3. Use SSL to encrypt the connection between the client and the database."
"def convert_to_json_serializable(data):
    """"""
    Helper function to convert an object to one that is json serializable

    Args:
        data: an object to attempt to convert a corresponding json-serializable object

    Returns:
        (dict) A converted test_object

    Warning:
        test_obj may also be converted in place.

    """"""
    import datetime
    import decimal
    import sys

    import numpy as np
    import pandas as pd

    # If it's one of our types, we use our own conversion; this can move to full schema
    # once nesting goes all the way down
    if isinstance(
        data,
        (
            ExpectationConfiguration,
            ExpectationSuite,
            ExpectationValidationResult,
            ExpectationSuiteValidationResult,
            RunIdentifier,
        ),
    ):
        return data.to_json_dict()

    try:
        if not isinstance(data, list) and np.isnan(data):
            # np.isnan is functionally vectorized, but we only want to apply this to single objects
            # Hence, why we test for `not isinstance(list))`
            return None
    except TypeError:
        pass
    except ValueError:
        pass

    if isinstance(data, (str, int, float, bool)):
        # No problem to encode json
        return data

    elif isinstance(data, dict):
        new_dict = {}
        for key in data:
            # A pandas index can be numeric, and a dict key can be numeric, but a json key must be a string
            new_dict[str(key)] = convert_to_json_serializable(data[key])

        return new_dict

    elif isinstance(data, (list, tuple, set)):
        new_list = []
        for val in data:
            new_list.append(convert_to_json_serializable(val))

        return new_list

    elif isinstance(data, (np.ndarray, pd.Index)):
        # test_obj[key] = test_obj[key].tolist()
        # If we have an array or index, convert it first to a list--causing coercion to float--and then round
        # to the number of digits for which the string representation will equal the float representation
        return [convert_to_json_serializable(x) for x in data.tolist()]

    # Note: This clause has to come after checking for np.ndarray or we get:
    #      `ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`
    elif data is None:
        # No problem to encode json
        return data

    elif isinstance(data, (datetime.datetime, datetime.date)):
        return data.isoformat()

    # Use built in base type from numpy, https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html
    # https://github.com/numpy/numpy/pull/9505
    elif np.issubdtype(type(data), np.bool_):
        return bool(data)

    elif np.issubdtype(type(data), np.integer) or np.issubdtype(type(data), np.uint):
        return int(data)

    elif np.issubdtype(type(data), np.floating):
        # Note: Use np.floating to avoid FutureWarning from numpy
        return float(round(data, sys.float_info.dig))

    elif isinstance(data, pd.Series):
        # Converting a series is tricky since the index may not be a string, but all json
        # keys must be strings. So, we use a very ugly serialization strategy
        index_name = data.index.name or ""index""
        value_name = data.name or ""value""
        return [
            {
                index_name: convert_to_json_serializable(idx),
                value_name: convert_to_json_serializable(val),
            }
            for idx, val in data.iteritems()
        ]

    elif isinstance(data, pd.DataFrame):
        return convert_to_json_serializable(data.to_dict(orient=""records""))

    elif isinstance(data, decimal.Decimal):
        if not (-1e-55 < decimal.Decimal.from_float(float(data)) - data < 1e-55):
            logger.warning(
                ""Using lossy conversion for decimal %s to float object to support serialization.""
                % str(data)
            )
        return float(data)

    else:
        raise TypeError(
            ""%s is of type %s which cannot be serialized.""
            % (str(data), type(data).__name__)
        )","1. Use `json.dumps` instead of `json.JSONEncoder.encode` to avoid potential security issues.
2. Use `json.JSONEncoder.default()` to handle custom objects instead of manually converting them.
3. Validate the input data to ensure that it is safe to convert to JSON."
"def ensure_json_serializable(data):
    """"""
    Helper function to convert an object to one that is json serializable

    Args:
        data: an object to attempt to convert a corresponding json-serializable object

    Returns:
        (dict) A converted test_object

    Warning:
        test_obj may also be converted in place.

    """"""
    import datetime
    import decimal

    import numpy as np
    import pandas as pd

    # If it's one of our types, we use our own conversion; this can move to full schema
    # once nesting goes all the way down
    if isinstance(
        data,
        (
            ExpectationConfiguration,
            ExpectationSuite,
            ExpectationValidationResult,
            ExpectationSuiteValidationResult,
            RunIdentifier,
        ),
    ):
        return

    try:
        if not isinstance(data, list) and np.isnan(data):
            # np.isnan is functionally vectorized, but we only want to apply this to single objects
            # Hence, why we test for `not isinstance(list))`
            return
    except TypeError:
        pass
    except ValueError:
        pass

    if isinstance(data, (str, int, float, bool)):
        # No problem to encode json
        return

    elif isinstance(data, dict):
        for key in data:
            str(key)  # key must be cast-able to string
            ensure_json_serializable(data[key])

        return

    elif isinstance(data, (list, tuple, set)):
        for val in data:
            ensure_json_serializable(val)
        return

    elif isinstance(data, (np.ndarray, pd.Index)):
        # test_obj[key] = test_obj[key].tolist()
        # If we have an array or index, convert it first to a list--causing coercion to float--and then round
        # to the number of digits for which the string representation will equal the float representation
        _ = [ensure_json_serializable(x) for x in data.tolist()]
        return

    # Note: This clause has to come after checking for np.ndarray or we get:
    #      `ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`
    elif data is None:
        # No problem to encode json
        return

    elif isinstance(data, (datetime.datetime, datetime.date)):
        return

    # Use built in base type from numpy, https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html
    # https://github.com/numpy/numpy/pull/9505
    elif np.issubdtype(type(data), np.bool_):
        return

    elif np.issubdtype(type(data), np.integer) or np.issubdtype(type(data), np.uint):
        return

    elif np.issubdtype(type(data), np.floating):
        # Note: Use np.floating to avoid FutureWarning from numpy
        return

    elif isinstance(data, pd.Series):
        # Converting a series is tricky since the index may not be a string, but all json
        # keys must be strings. So, we use a very ugly serialization strategy
        index_name = data.index.name or ""index""
        value_name = data.name or ""value""
        _ = [
            {
                index_name: ensure_json_serializable(idx),
                value_name: ensure_json_serializable(val),
            }
            for idx, val in data.iteritems()
        ]
        return
    elif isinstance(data, pd.DataFrame):
        return ensure_json_serializable(data.to_dict(orient=""records""))

    elif isinstance(data, decimal.Decimal):
        return

    else:
        raise InvalidExpectationConfigurationError(
            ""%s is of type %s which cannot be serialized to json""
            % (str(data), type(data).__name__)
        )","1. Use `json.dumps()` to serialize objects instead of `ensure_json_serializable()`.
2. Use `jsonschema` to validate the serialized objects.
3. Sanitize user input before using it in `ensure_json_serializable()`."
"    def remove_key(self, key):
        from google.cloud import storage
        from google.cloud.exceptions import NotFound

        gcs = storage.Client(project=self.project)
        bucket = gcs.get_bucket(self.bucket)
        try:
            bucket.delete_blobs(blobs=bucket.list_blobs(prefix=self.prefix))
        except NotFound:
            return False
        return True","1. Use `credentials.Certificate` instead of `project` to authenticate to Google Cloud Storage.
2. Use `bucket.blob(name=key)` to get a specific blob instead of listing all blobs and deleting them.
3. Use `bucket.delete_blob(blob)` to delete a blob instead of catching the `NotFound` exception."
"    def render(self, validation_results: ExpectationSuiteValidationResult):
        run_id = validation_results.meta[""run_id""]
        if isinstance(run_id, str):
            try:
                run_time = parse(run_id).strftime(""%Y-%m-%dT%H:%M:%S.%fZ"")
            except (ValueError, TypeError):
                run_time = ""__none__""
            run_name = run_id
        elif isinstance(run_id, dict):
            run_name = run_id.get(""run_name"") or ""__none__""
            run_time = run_id.get(""run_time"") or ""__none__""
        elif isinstance(run_id, RunIdentifier):
            run_name = run_id.run_name or ""__none__""
            run_time = run_id.run_time.strftime(""%Y-%m-%dT%H:%M:%S.%fZ"")

        expectation_suite_name = validation_results.meta[""expectation_suite_name""]
        batch_kwargs = validation_results.meta.get(""batch_kwargs"")

        # add datasource key to batch_kwargs if missing
        if ""datasource"" not in validation_results.meta.get(""batch_kwargs"", {}):
            # check if expectation_suite_name follows datasource.batch_kwargs_generator.data_asset_name.suite_name pattern
            if len(expectation_suite_name.split(""."")) == 4:
                batch_kwargs[""datasource""] = expectation_suite_name.split(""."")[0]

        # Group EVRs by column
        columns = {}
        for evr in validation_results.results:
            if ""column"" in evr.expectation_config.kwargs:
                column = evr.expectation_config.kwargs[""column""]
            else:
                column = ""Table-Level Expectations""

            if column not in columns:
                columns[column] = []
            columns[column].append(evr)

        ordered_columns = Renderer._get_column_list_from_evrs(validation_results)

        overview_content_blocks = [
            self._render_validation_header(validation_results),
            self._render_validation_statistics(validation_results=validation_results),
        ]

        collapse_content_blocks = [
            self._render_validation_info(validation_results=validation_results)
        ]

        if validation_results[""meta""].get(""batch_markers""):
            collapse_content_blocks.append(
                self._render_nested_table_from_dict(
                    input_dict=validation_results[""meta""].get(""batch_markers""),
                    header=""Batch Markers"",
                )
            )

        if validation_results[""meta""].get(""batch_kwargs""):
            collapse_content_blocks.append(
                self._render_nested_table_from_dict(
                    input_dict=validation_results[""meta""].get(""batch_kwargs""),
                    header=""Batch Kwargs"",
                )
            )

        if validation_results[""meta""].get(""batch_parameters""):
            collapse_content_blocks.append(
                self._render_nested_table_from_dict(
                    input_dict=validation_results[""meta""].get(""batch_parameters""),
                    header=""Batch Parameters"",
                )
            )

        collapse_content_block = CollapseContent(
            **{
                ""collapse_toggle_link"": ""Show more info..."",
                ""collapse"": collapse_content_blocks,
                ""styling"": {
                    ""body"": {""classes"": [""card"", ""card-body""]},
                    ""classes"": [""col-12"", ""p-1""],
                },
            }
        )

        if not self.run_info_at_end:
            overview_content_blocks.append(collapse_content_block)

        sections = [
            RenderedSectionContent(
                **{
                    ""section_name"": ""Overview"",
                    ""content_blocks"": overview_content_blocks,
                }
            )
        ]

        if ""Table-Level Expectations"" in columns:
            sections += [
                self._column_section_renderer.render(
                    validation_results=columns[""Table-Level Expectations""]
                )
            ]

        sections += [
            self._column_section_renderer.render(validation_results=columns[column],)
            for column in ordered_columns
        ]

        if self.run_info_at_end:
            sections += [
                RenderedSectionContent(
                    **{
                        ""section_name"": ""Run Info"",
                        ""content_blocks"": collapse_content_blocks,
                    }
                )
            ]

        data_asset_name = batch_kwargs.get(""data_asset_name"")
        # Determine whether we have a custom run_name
        try:
            run_name_as_time = parse(run_name)
        except ValueError:
            run_name_as_time = None
        try:
            run_time_datetime = parse(run_time)
        except ValueError:
            run_time_datetime = None

        include_run_name: bool = False
        if run_name_as_time != run_time_datetime and run_name_as_time != ""__none__"":
            include_run_name = True

        page_title = ""Validations / "" + expectation_suite_name
        if data_asset_name:
            page_title += "" / "" + data_asset_name
        if include_run_name:
            page_title += "" / "" + run_name
        page_title += "" / "" + run_time

        return RenderedDocumentContent(
            **{
                ""renderer_type"": ""ValidationResultsPageRenderer"",
                ""page_title"": page_title,
                ""batch_kwargs"": batch_kwargs,
                ""expectation_suite_name"": expectation_suite_name,
                ""sections"": sections,
                ""utm_medium"": ""validation-results-page"",
            }
        )","1. Use `parse` to sanitize user input for dates.
2. Use `include_run_name` to avoid leaking information about the run.
3. Use `utm_medium` to track the source of traffic to the validation results page."
"    def _render_validation_header(cls, validation_results):
        success = validation_results.success
        expectation_suite_name = validation_results.meta[""expectation_suite_name""]
        expectation_suite_path_components = (
            ["".."" for _ in range(len(expectation_suite_name.split(""."")) + 3)]
            + [""expectations""]
            + expectation_suite_name.split(""."")
        )
        expectation_suite_path = (
            os.path.join(*expectation_suite_path_components) + "".html""
        )
        if success:
            success = ""Succeeded""
            html_success_icon = (
                '<i class=""fas fa-check-circle text-success"" aria-hidden=""true""></i>'
            )
        else:
            success = ""Failed""
            html_success_icon = (
                '<i class=""fas fa-times text-danger"" aria-hidden=""true""></i>'
            )
        return RenderedHeaderContent(
            **{
                ""content_block_type"": ""header"",
                ""header"": RenderedStringTemplateContent(
                    **{
                        ""content_block_type"": ""string_template"",
                        ""string_template"": {
                            ""template"": ""Overview"",
                            ""tag"": ""h5"",
                            ""styling"": {""classes"": [""m-0""]},
                        },
                    }
                ),
                ""subheader"": RenderedStringTemplateContent(
                    **{
                        ""content_block_type"": ""string_template"",
                        ""string_template"": {
                            ""template"": ""${suite_title} ${expectation_suite_name}\\n${status_title} ${html_success_icon} ${success}"",
                            ""params"": {
                                ""suite_title"": ""Expectation Suite:"",
                                ""status_title"": ""Status:"",
                                ""expectation_suite_name"": expectation_suite_name,
                                ""success"": success,
                                ""html_success_icon"": html_success_icon,
                            },
                            ""styling"": {
                                ""params"": {
                                    ""suite_title"": {""classes"": [""h6""]},
                                    ""status_title"": {""classes"": [""h6""]},
                                    ""expectation_suite_name"": {
                                        ""tag"": ""a"",
                                        ""attributes"": {""href"": expectation_suite_path},
                                    },
                                },
                                ""classes"": [""mb-0"", ""mt-1""],
                            },
                        },
                    }
                ),
                ""styling"": {
                    ""classes"": [""col-12"", ""p-0""],
                    ""header"": {""classes"": [""alert"", ""alert-secondary""]},
                },
            }
        )","1. Use `os.path.join()` to concatenate strings for paths instead of string concatenation.
2. Use `html_escape()` to escape HTML entities in strings that are used as HTML attributes.
3. Use `json.dumps()` to serialize objects to JSON, and use the `ensure_ascii=False` flag to allow non-ASCII characters."
"    def render(self, expectations):
        columns, ordered_columns = self._group_and_order_expectations_by_column(
            expectations
        )
        expectation_suite_name = expectations.expectation_suite_name

        overview_content_blocks = [
            self._render_expectation_suite_header(),
            self._render_expectation_suite_info(expectations),
        ]

        table_level_expectations_content_block = self._render_table_level_expectations(
            columns
        )
        if table_level_expectations_content_block is not None:
            overview_content_blocks.append(table_level_expectations_content_block)

        asset_notes_content_block = self._render_expectation_suite_notes(expectations)
        if asset_notes_content_block is not None:
            overview_content_blocks.append(asset_notes_content_block)

        sections = [
            RenderedSectionContent(
                **{
                    ""section_name"": ""Overview"",
                    ""content_blocks"": overview_content_blocks,
                }
            )
        ]

        sections += [
            self._column_section_renderer.render(expectations=columns[column])
            for column in ordered_columns
            if column != ""_nocolumn""
        ]
        return RenderedDocumentContent(
            **{
                ""renderer_type"": ""ExpectationSuitePageRenderer"",
                ""page_title"": ""Expectations / "" + expectation_suite_name,
                ""expectation_suite_name"": expectation_suite_name,
                ""utm_medium"": ""expectation-suite-page"",
                ""sections"": sections,
            }
        )","1. Use `functools.lru_cache` to cache the results of expensive computations.
2. Use `flask.session.permanent` to make the session persistent across multiple requests.
3. Use `werkzeug.security.generate_password_hash` to securely hash passwords."
"    def render(self, validation_results):
        run_id = validation_results.meta[""run_id""]
        if isinstance(run_id, str):
            try:
                run_time = parse(run_id).strftime(""%Y-%m-%dT%H:%M:%S.%fZ"")
            except (ValueError, TypeError):
                run_time = ""__none__""
            run_name = run_id
        elif isinstance(run_id, dict):
            run_name = run_id.get(""run_name"") or ""__none__""
            run_time = run_id.get(""run_time"") or ""__none__""
        elif isinstance(run_id, RunIdentifier):
            run_name = run_id.run_name or ""__none__""
            run_time = run_id.run_time.strftime(""%Y-%m-%dT%H:%M:%S.%fZ"")

        expectation_suite_name = validation_results.meta[""expectation_suite_name""]
        batch_kwargs = validation_results.meta.get(""batch_kwargs"")

        # add datasource key to batch_kwargs if missing
        if ""datasource"" not in validation_results.meta.get(""batch_kwargs"", {}):
            # check if expectation_suite_name follows datasource.batch_kwargs_generator.data_asset_name.suite_name pattern
            if len(expectation_suite_name.split(""."")) == 4:
                batch_kwargs[""datasource""] = expectation_suite_name.split(""."")[0]

        # Group EVRs by column
        # TODO: When we implement a ValidationResultSuite class, this method will move there.
        columns = self._group_evrs_by_column(validation_results)

        ordered_columns = Renderer._get_column_list_from_evrs(validation_results)
        column_types = self._overview_section_renderer._get_column_types(
            validation_results
        )

        data_asset_name = batch_kwargs.get(""data_asset_name"")
        # Determine whether we have a custom run_name
        try:
            run_name_as_time = parse(run_name)
        except ValueError:
            run_name_as_time = None
        try:
            run_time_datetime = parse(run_time)
        except ValueError:
            run_time_datetime = None

        include_run_name: bool = False
        if run_name_as_time != run_time_datetime and run_name_as_time != ""__none__"":
            include_run_name = True

        page_title = ""Profiling Results / "" + expectation_suite_name
        if data_asset_name:
            page_title += "" / "" + data_asset_name
        if include_run_name:
            page_title += "" / "" + run_name
        page_title += "" / "" + run_time

        return RenderedDocumentContent(
            **{
                ""renderer_type"": ""ProfilingResultsPageRenderer"",
                ""page_title"": page_title,
                ""expectation_suite_name"": expectation_suite_name,
                ""utm_medium"": ""profiling-results-page"",
                ""batch_kwargs"": batch_kwargs,
                ""sections"": [
                    self._overview_section_renderer.render(
                        validation_results, section_name=""Overview""
                    )
                ]
                + [
                    self._column_section_renderer.render(
                        columns[column],
                        section_name=column,
                        column_type=column_types.get(column),
                    )
                    for column in ordered_columns
                ],
            }
        )","1. Use `parse` to sanitize user input for dates.
2. Use `RunIdentifier` to deserialize run_id.
3. Use `get` to access the `batch_kwargs` dict instead of `in`."
"    def build(self, resource_identifiers=None):
        source_store_keys = self.source_store.list_keys()
        if self.name == ""validations"" and self.validation_results_limit:
            source_store_keys = sorted(
                source_store_keys, key=lambda x: x.run_id.run_time, reverse=True
            )[: self.validation_results_limit]

        for resource_key in source_store_keys:
            # if no resource_identifiers are passed, the section
            # builder will build
            # a page for every keys in its source store.
            # if the caller did pass resource_identifiers, the section builder
            # will build pages only for the specified resources
            if resource_identifiers and resource_key not in resource_identifiers:
                continue

            if self.run_name_filter:
                if not resource_key_passes_run_name_filter(
                    resource_key, self.run_name_filter
                ):
                    continue
            try:
                resource = self.source_store.get(resource_key)
            except exceptions.InvalidKeyError:
                logger.warning(
                    f""Object with Key: {str(resource_key)} could not be retrieved. Skipping...""
                )
                continue

            if isinstance(resource_key, ExpectationSuiteIdentifier):
                expectation_suite_name = resource_key.expectation_suite_name
                logger.debug(
                    ""        Rendering expectation suite {}"".format(
                        expectation_suite_name
                    )
                )
            elif isinstance(resource_key, ValidationResultIdentifier):
                run_id = resource_key.run_id
                run_name = run_id.run_name
                run_time = run_id.run_time
                expectation_suite_name = (
                    resource_key.expectation_suite_identifier.expectation_suite_name
                )
                if self.name == ""profiling"":
                    logger.debug(
                        ""        Rendering profiling for batch {}"".format(
                            resource_key.batch_identifier
                        )
                    )
                else:

                    logger.debug(
                        ""        Rendering validation: run name: {}, run time: {}, suite {} for batch {}"".format(
                            run_name,
                            run_time,
                            expectation_suite_name,
                            resource_key.batch_identifier,
                        )
                    )

            try:
                rendered_content = self.renderer_class.render(resource)
                viewable_content = self.view_class.render(
                    rendered_content,
                    data_context_id=self.data_context_id,
                    show_how_to_buttons=self.show_how_to_buttons,
                )
            except Exception as e:
                exception_message = f""""""\\
An unexpected Exception occurred during data docs rendering.  Because of this error, certain parts of data docs will \\
not be rendered properly and/or may not appear altogether.  Please use the trace, included in this message, to \\
diagnose and repair the underlying issue.  Detailed information follows:
                """"""
                exception_traceback = traceback.format_exc()
                exception_message += (
                    f'{type(e).__name__}: ""{str(e)}"".  '
                    f'Traceback: ""{exception_traceback}"".'
                )
                logger.error(exception_message, e, exc_info=True)

            self.target_store.set(
                SiteSectionIdentifier(
                    site_section_name=self.name, resource_identifier=resource_key,
                ),
                viewable_content,
            )","1. Use `get` instead of `list_keys` to avoid loading all resources into memory.
2. Use `filter` to only load resources that match the specified criteria.
3. Handle exceptions gracefully and log errors to a central location."
"    def __init__(
        self,
        expectation_suite_name,
        expectations=None,
        evaluation_parameters=None,
        data_asset_type=None,
        meta=None,
    ):
        self.expectation_suite_name = expectation_suite_name
        if expectations is None:
            expectations = []
        self.expectations = [
            ExpectationConfiguration(**expectation)
            if isinstance(expectation, dict)
            else expectation
            for expectation in expectations
        ]
        if evaluation_parameters is None:
            evaluation_parameters = {}
        self.evaluation_parameters = evaluation_parameters
        self.data_asset_type = data_asset_type
        if meta is None:
            meta = {""great_expectations.__version__"": ge_version}
        # We require meta information to be serializable, but do not convert until necessary
        ensure_json_serializable(meta)
        self.meta = meta","1. Use `assert` statements to validate the input arguments.
2. Use `type` checking to ensure that the input arguments are of the correct type.
3. Use `sanitization` to prevent malicious users from injecting harmful code into the system."
"    def _set(self, key, value, content_encoding='utf-8', content_type='application/json'):
        gcs_object_key = os.path.join(
            self.prefix,
            self._convert_key_to_filepath(key)
        )

        from google.cloud import storage
        gcs = storage.Client(project=self.project)
        bucket = gcs.get_bucket(self.bucket)
        blob = bucket.blob(gcs_object_key)
        if isinstance(value, str):
            blob.upload_from_string(value.encode(content_encoding), content_encoding=content_encoding,
                                    content_type=content_type)
        else:
            blob.upload_from_string(value, content_type=content_type)
        return gcs_object_key","1. Use `instance_id` instead of `project` to access the bucket. This will prevent unauthorized access from other projects.
2. Use `bucket.blob.generate_signed_url` to generate a signed URL for the blob. This will prevent unauthorized users from downloading the blob.
3. Use `bucket.blob.set_acl` to set the ACL for the blob to `private`. This will prevent unauthorized users from reading the blob."
"def build_docs(context, site_name=None, view=True):
    """"""Build documentation in a context""""""
    logger.debug(""Starting cli.datasource.build_docs"")

    cli_message(""Building Data Docs..."")

    if site_name is not None:
        site_names = [site_name]
    else:
        site_names = None

    index_page_locator_infos = context.build_data_docs(site_names=site_names)

    msg = ""The following Data Docs sites were built:\\n""
    for site_name, index_page_locator_info in index_page_locator_infos.items():
        if os.path.isfile(index_page_locator_info):
            msg += "" - <cyan>{}:</cyan> "".format(site_name)
            msg += ""file://{}\\n"".format(index_page_locator_info)
        else:
            msg += "" - <cyan>{}:</cyan> "".format(site_name)
            msg += ""{}\\n"".format(index_page_locator_info)

    msg = msg.rstrip(""\\n"")
    cli_message(msg)

    if view:
        context.open_data_docs()","1. Use `context.build_data_docs(site_names=site_names)` to build documentation in a context.
2. Use `cli_message()` to display a message to the user.
3. Use `context.open_data_docs()` to open the data docs in a browser."
"    def get_docs_sites_urls(self, resource_identifier=None):
        """"""
        Get URLs for a resource for all data docs sites.

        This function will return URLs for any configured site even if the sites have not
        been built yet.

        :param resource_identifier: optional. It can be an identifier of ExpectationSuite's,
                ValidationResults and other resources that have typed identifiers.
                If not provided, the method will return the URLs of the index page.
        :return: a list of URLs. Each item is the URL for the resource for a data docs site
        """"""

        site_urls = []

        site_names = None
        sites = self._project_config_with_variables_substituted.data_docs_sites
        if sites:
            logger.debug(""Found data_docs_sites."")

            for site_name, site_config in sites.items():
                if (site_names and site_name in site_names) or not site_names:
                    complete_site_config = site_config
                    module_name = 'great_expectations.render.renderer.site_builder'
                    site_builder = instantiate_class_from_config(
                        config=complete_site_config,
                        runtime_environment={
                            ""data_context"": self,
                            ""root_directory"": self.root_directory
                        },
                        config_defaults={
                            ""module_name"": module_name
                        }
                    )
                    if not site_builder:
                        raise ge_exceptions.ClassInstantiationError(
                            module_name=module_name,
                            package_name=None,
                            class_name=complete_site_config['class_name']
                        )
                    url = site_builder.get_resource_url(resource_identifier=resource_identifier)
                    site_urls.append({
                        ""site_name"": site_name,
                        ""site_url"": url
                    })

        return site_urls","1. Use [type annotations](https://docs.python.org/3/library/typing.html) to make the code more explicit about the types of the arguments and return values.
2. [Validate input](https://docs.python.org/3/library/argparse.html) to ensure that the user provides the correct values for the arguments.
3. [Use [secure random number generation](https://docs.python.org/3/library/secrets.html) to generate the secret key."
"    def open_data_docs(self, resource_identifier=None):
        """"""
        A stdlib cross-platform way to open a file in a browser.

        :param resource_identifier: ExpectationSuiteIdentifier, ValidationResultIdentifier
                or any other type's identifier. The argument is optional - when
                not supplied, the method returns the URL of the index page.
        """"""
        data_docs_urls = self.get_docs_sites_urls(resource_identifier=resource_identifier)
        for site_dict in data_docs_urls:
            logger.debug(""Opening Data Docs found here: {}"".format(site_dict[""site_url""]))
            webbrowser.open(site_dict[""site_url""])","1. Use `urllib.request.urlopen()` instead of `webbrowser.open()` to open URLs.
2. Use `urllib.parse.quote()` to encode the URL parameters.
3. Use `urllib.request.HTTPBasicAuth()` to authenticate with the server."
"    def setConfig(self,button):
        try:
            if os.path.exists('/opt/sublime_text/sublime_text'):
                Popen(['/opt/sublime_text/sublime_text',os.environ['HOME']+'/.config/kinto/kinto.py'])
            elif which(gedit) is not None:
                Popen(['gedit',os.environ['HOME']+'/.config/kinto/kinto.py'])
            elif which(mousepad) is not None:
                Popen(['mousepad',os.environ['HOME']+'/.config/kinto/kinto.py'])
            elif which(kate) is not None:
                Popen(['kate',os.environ['HOME']+'/.config/kinto/kinto.py'])
            elif which(kwrite) is not None:
                Popen(['kwrite',os.environ['HOME']+'/.config/kinto/kinto.py'])

        except CalledProcessError:                                  # Notify user about error on running restart commands.
            Popen(['notify-send','Kinto: Error could not open config file!'])","1. Use `subprocess.check_call()` instead of `Popen()` to check the return code of the executed command.
2. Use `os.path.expanduser()` to expand the user's home directory path.
3. Use `subprocess.call()` with the `shell=False` argument to avoid executing the command in a shell."
"    def setService(self,button):
        try:
            if os.path.exists('/opt/sublime_text/sublime_text'):
                Popen(['/opt/sublime_text/sublime_text','/lib/systemd/system/xkeysnail.service'])
            elif which(gedit) is not None:
                Popen(['gedit','/lib/systemd/system/xkeysnail.service'])
            elif which(mousepad) is not None:
                Popen(['mousepad','/lib/systemd/system/xkeysnail.service'])
            elif which(kate) is not None:
                Popen(['kate','/lib/systemd/system/xkeysnail.service'])
            elif which(kwrite) is not None:
                Popen(['kwrite','/lib/systemd/system/xkeysnail.service'])

        except CalledProcessError:                                  # Notify user about error on running restart commands.
            Popen(['notify-send','Kinto: Error could not open config file!'])","1. Use `subprocess.check_call()` instead of `Popen()` to check the return code of the executed command.
2. Use `os.path.isfile()` to check if the file exists before trying to open it.
3. Use `logging` to log errors instead of `notify-send`."
"    def setSysKB(self,button):
        if self.ostype == ""XFCE"":
            Popen(['xfce4-keyboard-settings'])
        else:
            Popen(['gnome-control-center','keyboard'])","1. Use `subprocess.run()` instead of `Popen()` to avoid leaving zombie processes.
2. Use `subprocess.check_call()` to check the return code of the executed process and handle errors.
3. Use `os.getenv()` to get the environment variable instead of hard-coding it."
"    def setRegion(self,button):
        if self.ostype == ""XFCE"":
            Popen(['gnome-language-selector'])
        else:
            Popen(['gnome-control-center','region'])","1. Use `subprocess.call` instead of `Popen` to avoid leaving zombie processes.
2. Use `subprocess.check_output` to capture the output of the command.
3. Use `subprocess.DEVNULL` to discard the output of the command."
"    def setConfig(self,button):
        try:
            if os.path.exists('/opt/sublime_text/sublime_text'):
                Popen(['/opt/sublime_text/sublime_text',os.environ['HOME']+'/.config/kinto/kinto.py'])
            elif which(gedit) is not None:
                Popen(['gedit',os.environ['HOME']+'/.config/kinto/kinto.py'])
            elif which(mousepad) is not None:
                Popen(['mousepad',os.environ['HOME']+'/.config/kinto/kinto.py'])

        except CalledProcessError:                                  # Notify user about error on running restart commands.
            Popen(['notify-send','Kinto: Error could not open config file!'])","1. Use `subprocess.check_call()` instead of `Popen()` to check the return code of the executed command.
2. Use `os.path.expanduser()` to expand the user's home directory path.
3. Use `subprocess.call()` with the `stdout=PIPE` and `stderr=PIPE` arguments to capture the standard output and error output of the executed command."
"    def setService(self,button):
        try:
            if os.path.exists('/opt/sublime_text/sublime_text'):
                Popen(['/opt/sublime_text/sublime_text','/lib/systemd/system/xkeysnail.service'])
            elif which(gedit) is not None:
                Popen(['gedit','/lib/systemd/system/xkeysnail.service'])
            elif which(mousepad) is not None:
                Popen(['mousepad','/lib/systemd/system/xkeysnail.service'])

        except CalledProcessError:                                  # Notify user about error on running restart commands.
            Popen(['notify-send','Kinto: Error could not open config file!'])","1. Use `subprocess.check_call` instead of `Popen` to check the return code of the executed command.
2. Use `os.path.isfile` to check if the file exists before trying to open it.
3. Use `subprocess.call` with the `shell=False` argument to avoid accidentally running commands with elevated privileges."
"def keyboard_detect():
    global internalid, usbid, chromeswap, system_type
    internal_kbname = """"
    usb_kbname = """"
    print()
    print(""Looking for keyboards..."")
    print()
    result = subprocess.check_output('xinput list | grep -iv ""Virtual\\|USB"" | grep -i ""keyboard.*keyboard"" | grep -o -P ""(?<=↳).*(?=id\\=)"";exit 0', shell=True).decode('utf-8')
    if result != """":
        internal_kbname = result.strip()
    internalid = subprocess.check_output('xinput list | grep -iv ""Virtual\\|USB"" | grep -i ""keyboard.*keyboard"" | cut -d ""="" -f 2- | awk \\'{print $1}\\' | tail -1;exit 0', shell=True).decode('utf-8')
    print(""Internal Keyboard\\nName: "" + internal_kbname + ""\\nID: "" + internalid)

    result = subprocess.check_output('udevadm info -e | grep -o -P ""(?<=by-id/usb-).*(?=-event-kbd)"" | head -1;exit 0', shell=True).decode('utf-8')
    if result != """":
        usb_kbname = result.strip()

    # Loop the following to ensure the id is picked up after 5-10 tries
    usbid = """"
    usbcount=0
    while usbid == """":
        usbid = subprocess.check_output('udevadm info -e | stdbuf -oL grep -o -P ""(?<=event-kbd /dev/input/by-path/pci-0000:00:).*(?=.0-usb)"";exit 0', shell=True).decode('utf-8')
        if usbid == """":
            usbcount += 1
            # print('usbid not found '+ str(usbcount))
            if usbcount == 5:
                usbid = ""0""
        time.sleep(1)
    print(""\\nUSB Keyboard\\n"" + ""Name: "" + usb_kbname + ""\\nID: "" + usbid)

    if system_type == ""1"":
        system_type = ""windows""
    elif system_type == ""2"":
        system_type = ""chromebook""
    elif system_type == ""3"":
        system_type = ""mac""

    if system_type == ""windows"" or system_type == ""mac"":
        subprocess.check_output('/bin/bash -c ./mac_wordwise.sh', shell=True).decode('utf-8')
        cmdgui = '""/usr/bin/setxkbmap -option;xkbcomp -w0 -I$HOME/.xkb ~/.xkb/keymap/kbd.mac.onelvl $DISPLAY""'
        # subprocess.check_output('echo ""1"" > /sys/module/hid_apple/parameters/swap_opt_cmd', shell=True).decode('utf-8')
    elif system_type == ""chromebook"":
        subprocess.check_output('/bin/bash -c ./chromebook.sh', shell=True).decode('utf-8')
        cmdgui = '""setxkbmap -option;xkbcomp -w0 -I$HOME/.xkb ~/.xkb/keymap/kbd.chromebook.gui $DISPLAY""'

    # password = getpass(""Please enter your password to complete the keyswap: "")
    # proc = Popen(""echo '1' | sudo tee -a /sys/module/hid_apple/parameters/swap_opt_cmd"".split(), stdin=PIPE, stdout=PIPE, stderr=PIPE)
    # proc.communicate(password.encode())

    if swap_behavior == 1:
        print(""Setting up "" + system_type + "" keyswap as a service."")
        print(""You can disable and remove the service by using the following commands."")
        print(""systemctl --user stop keyswap"")
        print(""systemctl --user disable keyswap"")
        print(""rm -rf ~/.config/autostart/keyswap.sh"")
        print(""rm -rf ~/.config/xactive.sh"")
        keyswapcmd = '/bin/bash -c ""./keyswap_service.sh 1 0 ' + system_type + ' ' + str(internalid).strip() + ' ' + str(usbid).strip() + ' ' + str(chromeswap) + '""'
        print(keyswapcmd)
        subprocess.check_output(keyswapcmd, shell=True).decode('utf-8')
    else:
        print(""Setting up "" + system_type + "" keyswap inside your profiles ~/.Xsession file."")
        print(""You can modify or remove the file if you want you want to remove the modification."")
        keyswapcmd = '/bin/bash -c ""./keyswap_service.sh 0 ' + cmdgui + '""'
        subprocess.check_output(keyswapcmd, shell=True).decode('utf-8')

    print(""Please run this command in the terminal if you are using a Windows or Macbook."")
    print(""Your keymapping will not work right on Apple keyboards without it."")
    print(""echo '1' | sudo tee -a /sys/module/hid_apple/parameters/swap_opt_cmd"")","1. Use `getpass` to securely get the user's password.
2. Use `sudo` to run the commands as root.
3. Use `chmod` to restrict the permissions of the files created."
"    def pull(self, repository, tag=None, stream=False, auth_config=None,
             decode=False, platform=None):
        """"""
        Pulls an image. Similar to the ``docker pull`` command.

        Args:
            repository (str): The repository to pull
            tag (str): The tag to pull
            stream (bool): Stream the output as a generator
            auth_config (dict): Override the credentials that
                :py:meth:`~docker.api.daemon.DaemonApiMixin.login` has set for
                this request. ``auth_config`` should contain the ``username``
                and ``password`` keys to be valid.
            decode (bool): Decode the JSON data from the server into dicts.
                Only applies with ``stream=True``
            platform (str): Platform in the format ``os[/arch[/variant]]``

        Returns:
            (generator or str): The output

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.

        Example:

            >>> for line in cli.pull('busybox', stream=True, decode=True):
            ...     print(json.dumps(line, indent=4))
            {
                ""status"": ""Pulling image (latest) from busybox"",
                ""progressDetail"": {},
                ""id"": ""e72ac664f4f0""
            }
            {
                ""status"": ""Pulling image (latest) from busybox, endpoint: ..."",
                ""progressDetail"": {},
                ""id"": ""e72ac664f4f0""
            }

        """"""
        if not tag:
            repository, tag = utils.parse_repository_tag(repository)
        registry, repo_name = auth.resolve_repository_name(repository)

        params = {
            'tag': tag,
            'fromImage': repository
        }
        headers = {}

        if auth_config is None:
            header = auth.get_config_header(self, registry)
            if header:
                headers['X-Registry-Auth'] = header
        else:
            log.debug('Sending supplied auth config')
            headers['X-Registry-Auth'] = auth.encode_header(auth_config)

        if platform is not None:
            if utils.version_lt(self._version, '1.32'):
                raise errors.InvalidVersion(
                    'platform was only introduced in API version 1.32'
                )
            params['platform'] = platform

        response = self._post(
            self._url('/images/create'), params=params, headers=headers,
            stream=stream, timeout=None
        )

        self._raise_for_status(response)

        if stream:
            return self._stream_helper(response, decode=decode)

        return self._result(response)","1. Use `auth.get_config_header()` to get the auth header for the registry.
2. Use `auth.encode_header()` to encode the auth config.
3. Use `utils.version_lt()` to check if the API version supports the `platform` parameter."
"    def pull(self, repository, tag=None, **kwargs):
        """"""
        Pull an image of the given name and return it. Similar to the
        ``docker pull`` command.
        If no tag is specified, all tags from that repository will be
        pulled.

        If you want to get the raw pull output, use the
        :py:meth:`~docker.api.image.ImageApiMixin.pull` method in the
        low-level API.

        Args:
            repository (str): The repository to pull
            tag (str): The tag to pull
            auth_config (dict): Override the credentials that
                :py:meth:`~docker.client.DockerClient.login` has set for
                this request. ``auth_config`` should contain the ``username``
                and ``password`` keys to be valid.
            platform (str): Platform in the format ``os[/arch[/variant]]``

        Returns:
            (:py:class:`Image` or list): The image that has been pulled.
                If no ``tag`` was specified, the method will return a list
                of :py:class:`Image` objects belonging to this repository.

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.

        Example:

            >>> # Pull the image tagged `latest` in the busybox repo
            >>> image = client.images.pull('busybox:latest')

            >>> # Pull all tags in the busybox repo
            >>> images = client.images.pull('busybox')
        """"""
        if not tag:
            repository, tag = parse_repository_tag(repository)

        self.client.api.pull(repository, tag=tag, **kwargs)
        if tag:
            return self.get('{0}{2}{1}'.format(
                repository, tag, '@' if tag.startswith('sha256:') else ':'
            ))
        return self.list(repository)","1. Use `parse_repository_tag` to validate the repository and tag before pulling.
2. Use `get` to get the image by tag, and `list` to get all images by repository.
3. Use `auth_config` to override the credentials if needed."
"    def pull(self, repository, tag=None, stream=False, auth_config=None,
             decode=False, platform=None):
        """"""
        Pulls an image. Similar to the ``docker pull`` command.

        Args:
            repository (str): The repository to pull
            tag (str): The tag to pull
            stream (bool): Stream the output as a generator
            auth_config (dict): Override the credentials that
                :py:meth:`~docker.api.daemon.DaemonApiMixin.login` has set for
                this request. ``auth_config`` should contain the ``username``
                and ``password`` keys to be valid.
            decode (bool): Decode the JSON data from the server into dicts.
                Only applies with ``stream=True``
            platform (str): Platform in the format ``os[/arch[/variant]]``

        Returns:
            (generator or str): The output

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.

        Example:

            >>> for line in cli.pull('busybox', stream=True):
            ...     print(json.dumps(json.loads(line), indent=4))
            {
                ""status"": ""Pulling image (latest) from busybox"",
                ""progressDetail"": {},
                ""id"": ""e72ac664f4f0""
            }
            {
                ""status"": ""Pulling image (latest) from busybox, endpoint: ..."",
                ""progressDetail"": {},
                ""id"": ""e72ac664f4f0""
            }

        """"""
        if not tag:
            repository, tag = utils.parse_repository_tag(repository)
        registry, repo_name = auth.resolve_repository_name(repository)

        params = {
            'tag': tag,
            'fromImage': repository
        }
        headers = {}

        if auth_config is None:
            header = auth.get_config_header(self, registry)
            if header:
                headers['X-Registry-Auth'] = header
        else:
            log.debug('Sending supplied auth config')
            headers['X-Registry-Auth'] = auth.encode_header(auth_config)

        if platform is not None:
            if utils.version_lt(self._version, '1.32'):
                raise errors.InvalidVersion(
                    'platform was only introduced in API version 1.32'
                )
            params['platform'] = platform

        response = self._post(
            self._url('/images/create'), params=params, headers=headers,
            stream=stream, timeout=None
        )

        self._raise_for_status(response)

        if stream:
            return self._stream_helper(response, decode=decode)

        return self._result(response)","1. Use `auth.get_config_header` to get the auth header instead of creating it manually.
2. Use `auth.encode_header` to encode the auth config.
3. Check the API version before using the `platform` parameter."
"    def build(self, path=None, tag=None, quiet=False, fileobj=None,
              nocache=False, rm=False, timeout=None,
              custom_context=False, encoding=None, pull=False,
              forcerm=False, dockerfile=None, container_limits=None,
              decode=False, buildargs=None, gzip=False, shmsize=None,
              labels=None, cache_from=None, target=None, network_mode=None,
              squash=None, extra_hosts=None, platform=None, isolation=None):
        """"""
        Similar to the ``docker build`` command. Either ``path`` or ``fileobj``
        needs to be set. ``path`` can be a local path (to a directory
        containing a Dockerfile) or a remote URL. ``fileobj`` must be a
        readable file-like object to a Dockerfile.

        If you have a tar file for the Docker build context (including a
        Dockerfile) already, pass a readable file-like object to ``fileobj``
        and also pass ``custom_context=True``. If the stream is compressed
        also, set ``encoding`` to the correct value (e.g ``gzip``).

        Example:
            >>> from io import BytesIO
            >>> from docker import APIClient
            >>> dockerfile = '''
            ... # Shared Volume
            ... FROM busybox:buildroot-2014.02
            ... VOLUME /data
            ... CMD [""/bin/sh""]
            ... '''
            >>> f = BytesIO(dockerfile.encode('utf-8'))
            >>> cli = APIClient(base_url='tcp://127.0.0.1:2375')
            >>> response = [line for line in cli.build(
            ...     fileobj=f, rm=True, tag='yourname/volume'
            ... )]
            >>> response
            ['{""stream"":"" ---\\\\u003e a9eb17255234\\\\n""}',
             '{""stream"":""Step 1 : VOLUME /data\\\\n""}',
             '{""stream"":"" ---\\\\u003e Running in abdc1e6896c6\\\\n""}',
             '{""stream"":"" ---\\\\u003e 713bca62012e\\\\n""}',
             '{""stream"":""Removing intermediate container abdc1e6896c6\\\\n""}',
             '{""stream"":""Step 2 : CMD [\\\\""/bin/sh\\\\""]\\\\n""}',
             '{""stream"":"" ---\\\\u003e Running in dba30f2a1a7e\\\\n""}',
             '{""stream"":"" ---\\\\u003e 032b8b2855fc\\\\n""}',
             '{""stream"":""Removing intermediate container dba30f2a1a7e\\\\n""}',
             '{""stream"":""Successfully built 032b8b2855fc\\\\n""}']

        Args:
            path (str): Path to the directory containing the Dockerfile
            fileobj: A file object to use as the Dockerfile. (Or a file-like
                object)
            tag (str): A tag to add to the final image
            quiet (bool): Whether to return the status
            nocache (bool): Don't use the cache when set to ``True``
            rm (bool): Remove intermediate containers. The ``docker build``
                command now defaults to ``--rm=true``, but we have kept the old
                default of `False` to preserve backward compatibility
            timeout (int): HTTP timeout
            custom_context (bool): Optional if using ``fileobj``
            encoding (str): The encoding for a stream. Set to ``gzip`` for
                compressing
            pull (bool): Downloads any updates to the FROM image in Dockerfiles
            forcerm (bool): Always remove intermediate containers, even after
                unsuccessful builds
            dockerfile (str): path within the build context to the Dockerfile
            buildargs (dict): A dictionary of build arguments
            container_limits (dict): A dictionary of limits applied to each
                container created by the build process. Valid keys:

                - memory (int): set memory limit for build
                - memswap (int): Total memory (memory + swap), -1 to disable
                    swap
                - cpushares (int): CPU shares (relative weight)
                - cpusetcpus (str): CPUs in which to allow execution, e.g.,
                    ``""0-3""``, ``""0,1""``
            decode (bool): If set to ``True``, the returned stream will be
                decoded into dicts on the fly. Default ``False``
            shmsize (int): Size of `/dev/shm` in bytes. The size must be
                greater than 0. If omitted the system uses 64MB
            labels (dict): A dictionary of labels to set on the image
            cache_from (:py:class:`list`): A list of images used for build
                cache resolution
            target (str): Name of the build-stage to build in a multi-stage
                Dockerfile
            network_mode (str): networking mode for the run commands during
                build
            squash (bool): Squash the resulting images layers into a
                single layer.
            extra_hosts (dict): Extra hosts to add to /etc/hosts in building
                containers, as a mapping of hostname to IP address.
            platform (str): Platform in the format ``os[/arch[/variant]]``
            isolation (str): Isolation technology used during build.
                Default: `None`.

        Returns:
            A generator for the build output.

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.
            ``TypeError``
                If neither ``path`` nor ``fileobj`` is specified.
        """"""
        remote = context = None
        headers = {}
        container_limits = container_limits or {}
        if path is None and fileobj is None:
            raise TypeError(""Either path or fileobj needs to be provided."")
        if gzip and encoding is not None:
            raise errors.DockerException(
                'Can not use custom encoding if gzip is enabled'
            )

        for key in container_limits.keys():
            if key not in constants.CONTAINER_LIMITS_KEYS:
                raise errors.DockerException(
                    'Invalid container_limits key {0}'.format(key)
                )

        if custom_context:
            if not fileobj:
                raise TypeError(""You must specify fileobj with custom_context"")
            context = fileobj
        elif fileobj is not None:
            context = utils.mkbuildcontext(fileobj)
        elif path.startswith(('http://', 'https://',
                              'git://', 'github.com/', 'git@')):
            remote = path
        elif not os.path.isdir(path):
            raise TypeError(""You must specify a directory to build in path"")
        else:
            dockerignore = os.path.join(path, '.dockerignore')
            exclude = None
            if os.path.exists(dockerignore):
                with open(dockerignore, 'r') as f:
                    exclude = list(filter(
                        lambda x: x != '' and x[0] != '#',
                        [l.strip() for l in f.read().splitlines()]
                    ))
            if dockerfile and os.path.relpath(dockerfile, path).startswith(
                    '..'):
                with open(dockerfile, 'r') as df:
                    dockerfile = (
                        '.dockerfile.{0:x}'.format(random.getrandbits(160)),
                        df.read()
                    )
            else:
                dockerfile = (dockerfile, None)
            context = utils.tar(
                path, exclude=exclude, dockerfile=dockerfile, gzip=gzip
            )
            encoding = 'gzip' if gzip else encoding

        u = self._url('/build')
        params = {
            't': tag,
            'remote': remote,
            'q': quiet,
            'nocache': nocache,
            'rm': rm,
            'forcerm': forcerm,
            'pull': pull,
            'dockerfile': dockerfile,
        }
        params.update(container_limits)

        if buildargs:
            params.update({'buildargs': json.dumps(buildargs)})

        if shmsize:
            if utils.version_gte(self._version, '1.22'):
                params.update({'shmsize': shmsize})
            else:
                raise errors.InvalidVersion(
                    'shmsize was only introduced in API version 1.22'
                )

        if labels:
            if utils.version_gte(self._version, '1.23'):
                params.update({'labels': json.dumps(labels)})
            else:
                raise errors.InvalidVersion(
                    'labels was only introduced in API version 1.23'
                )

        if cache_from:
            if utils.version_gte(self._version, '1.25'):
                params.update({'cachefrom': json.dumps(cache_from)})
            else:
                raise errors.InvalidVersion(
                    'cache_from was only introduced in API version 1.25'
                )

        if target:
            if utils.version_gte(self._version, '1.29'):
                params.update({'target': target})
            else:
                raise errors.InvalidVersion(
                    'target was only introduced in API version 1.29'
                )

        if network_mode:
            if utils.version_gte(self._version, '1.25'):
                params.update({'networkmode': network_mode})
            else:
                raise errors.InvalidVersion(
                    'network_mode was only introduced in API version 1.25'
                )

        if squash:
            if utils.version_gte(self._version, '1.25'):
                params.update({'squash': squash})
            else:
                raise errors.InvalidVersion(
                    'squash was only introduced in API version 1.25'
                )

        if extra_hosts is not None:
            if utils.version_lt(self._version, '1.27'):
                raise errors.InvalidVersion(
                    'extra_hosts was only introduced in API version 1.27'
                )

            if isinstance(extra_hosts, dict):
                extra_hosts = utils.format_extra_hosts(extra_hosts)
            params.update({'extrahosts': extra_hosts})

        if platform is not None:
            if utils.version_lt(self._version, '1.32'):
                raise errors.InvalidVersion(
                    'platform was only introduced in API version 1.32'
                )
            params['platform'] = platform

        if isolation is not None:
            if utils.version_lt(self._version, '1.24'):
                raise errors.InvalidVersion(
                    'isolation was only introduced in API version 1.24'
                )
            params['isolation'] = isolation

        if context is not None:
            headers = {'Content-Type': 'application/tar'}
            if encoding:
                headers['Content-Encoding'] = encoding

        self._set_auth_headers(headers)

        response = self._post(
            u,
            data=context,
            params=params,
            headers=headers,
            stream=True,
            timeout=timeout,
        )

        if context is not None and not custom_context:
            context.close()

        return self._stream_helper(response, decode=decode)","1. Use `urllib3`'s `disable_warnings()` to suppress insecure warnings.
2. Use `urllib3`'s `disable_warnings()` to suppress insecure warnings.
3. Use `urllib3`'s `disable_warnings()` to suppress insecure warnings."
"    def pull(self, repository, tag=None, **kwargs):
        """"""
        Pull an image of the given name and return it. Similar to the
        ``docker pull`` command.
        If no tag is specified, all tags from that repository will be
        pulled.

        If you want to get the raw pull output, use the
        :py:meth:`~docker.api.image.ImageApiMixin.pull` method in the
        low-level API.

        Args:
            name (str): The repository to pull
            tag (str): The tag to pull
            auth_config (dict): Override the credentials that
                :py:meth:`~docker.client.DockerClient.login` has set for
                this request. ``auth_config`` should contain the ``username``
                and ``password`` keys to be valid.
            platform (str): Platform in the format ``os[/arch[/variant]]``

        Returns:
            (:py:class:`Image` or list): The image that has been pulled.
                If no ``tag`` was specified, the method will return a list
                of :py:class:`Image` objects belonging to this repository.

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.

        Example:

            >>> # Pull the image tagged `latest` in the busybox repo
            >>> image = client.images.pull('busybox:latest')

            >>> # Pull all tags in the busybox repo
            >>> images = client.images.pull('busybox')
        """"""
        if not tag:
            repository, tag = parse_repository_tag(repository)

        self.client.api.pull(repository, tag=tag, **kwargs)
        if tag:
            return self.get('{0}:{1}'.format(repository, tag))
        return self.list(repository)","1. Use `auth_config` to provide credentials for authentication.
2. Use `platform` to specify the platform for which the image is being pulled.
3. Use `parse_repository_tag` to parse the repository and tag into separate arguments."
"def create_archive(root, files=None, fileobj=None, gzip=False):
    if not fileobj:
        fileobj = tempfile.NamedTemporaryFile()
    t = tarfile.open(mode='w:gz' if gzip else 'w', fileobj=fileobj)
    if files is None:
        files = build_file_list(root)
    for path in files:
        full_path = os.path.join(root, path)

        if os.lstat(full_path).st_mode & os.R_OK == 0:
            raise IOError(
                'Can not access file in context: {}'.format(full_path)
            )
        i = t.gettarinfo(full_path, arcname=path)
        if i is None:
            # This happens when we encounter a socket file. We can safely
            # ignore it and proceed.
            continue

        # Workaround https://bugs.python.org/issue32713
        if i.mtime < 0 or i.mtime > 8**11 - 1:
            i.mtime = int(i.mtime)

        if constants.IS_WINDOWS_PLATFORM:
            # Windows doesn't keep track of the execute bit, so we make files
            # and directories executable by default.
            i.mode = i.mode & 0o755 | 0o111

        if i.isfile():
            try:
                with open(full_path, 'rb') as f:
                    t.addfile(i, f)
            except IOError:
                t.addfile(i, None)
        else:
            # Directories, FIFOs, symlinks... don't need to be read.
            t.addfile(i, None)
    t.close()
    fileobj.seek(0)
    return fileobj","1. Use a context manager to ensure that the temporary file is closed when the function exits.
2. Check the permissions of the files being added to the archive to ensure that they are readable.
3. Use a secure random number generator to generate the archive's name."
"def create_archive(root, files=None, fileobj=None, gzip=False):
    if not fileobj:
        fileobj = tempfile.NamedTemporaryFile()
    t = tarfile.open(mode='w:gz' if gzip else 'w', fileobj=fileobj)
    if files is None:
        files = build_file_list(root)
    for path in files:
        full_path = os.path.join(root, path)
        if not os.access(full_path, os.R_OK):
            raise IOError(
                'Can not access file in context: {}'.format(full_path)
            )
        i = t.gettarinfo(full_path, arcname=path)
        if i is None:
            # This happens when we encounter a socket file. We can safely
            # ignore it and proceed.
            continue

        if constants.IS_WINDOWS_PLATFORM:
            # Windows doesn't keep track of the execute bit, so we make files
            # and directories executable by default.
            i.mode = i.mode & 0o755 | 0o111

        if i.isfile():
            try:
                with open(full_path, 'rb') as f:
                    t.addfile(i, f)
            except IOError:
                t.addfile(i, None)
        else:
            # Directories, FIFOs, symlinks... don't need to be read.
            t.addfile(i, None)
    t.close()
    fileobj.seek(0)
    return fileobj","1. Use `os.umask()` to set the file permissions when creating the archive.
2. Use `tarfile.chmod()` to set the file permissions of the files in the archive.
3. Use `tarfile.chown()` to set the owner and group of the files in the archive."
"    def attach(self, container, stdout=True, stderr=True,
               stream=False, logs=False):
        """"""
        Attach to a container.

        The ``.logs()`` function is a wrapper around this method, which you can
        use instead if you want to fetch/stream container output without first
        retrieving the entire backlog.

        Args:
            container (str): The container to attach to.
            stdout (bool): Include stdout.
            stderr (bool): Include stderr.
            stream (bool): Return container output progressively as an iterator
                of strings, rather than a single string.
            logs (bool): Include the container's previous output.

        Returns:
            By default, the container's output as a single string.

            If ``stream=True``, an iterator of output strings.

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.
        """"""
        params = {
            'logs': logs and 1 or 0,
            'stdout': stdout and 1 or 0,
            'stderr': stderr and 1 or 0,
            'stream': stream and 1 or 0
        }

        headers = {
            'Connection': 'Upgrade',
            'Upgrade': 'tcp'
        }

        u = self._url(""/containers/{0}/attach"", container)
        response = self._post(u, headers=headers, params=params, stream=stream)

        return self._read_from_socket(
            response, stream, self._check_is_tty(container)
        )","1. Use HTTPS instead of HTTP to protect the data from being intercepted.
2. Authenticate the user before allowing them to attach to a container.
3. Use a secure connection type, such as TLS, to protect the data from being tampered with."
"    def exec_start(self, exec_id, detach=False, tty=False, stream=False,
                   socket=False):
        """"""
        Start a previously set up exec instance.

        Args:
            exec_id (str): ID of the exec instance
            detach (bool): If true, detach from the exec command.
                Default: False
            tty (bool): Allocate a pseudo-TTY. Default: False
            stream (bool): Stream response data. Default: False

        Returns:
            (generator or str): If ``stream=True``, a generator yielding
            response chunks. A string containing response data otherwise.

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.
        """"""
        # we want opened socket if socket == True
        if isinstance(exec_id, dict):
            exec_id = exec_id.get('Id')

        data = {
            'Tty': tty,
            'Detach': detach
        }

        headers = {} if detach else {
            'Connection': 'Upgrade',
            'Upgrade': 'tcp'
        }

        res = self._post_json(
            self._url('/exec/{0}/start', exec_id),
            headers=headers,
            data=data,
            stream=True
        )

        if socket:
            return self._get_raw_response_socket(res)
        return self._read_from_socket(res, stream)","1. Use `json.dumps` to serialize data instead of `str`.
2. Use `urllib.parse.quote` to escape special characters in URLs.
3. Use `requests` instead of `urllib` to make requests."
"def update_headers(f):
    def inner(self, *args, **kwargs):
        if 'HttpHeaders' in self._auth_configs:
            if 'headers' not in kwargs:
                kwargs['headers'] = self._auth_configs['HttpHeaders']
            else:
                kwargs['headers'].update(self._auth_configs['HttpHeaders'])
        return f(self, *args, **kwargs)
    return inner","1. **Use a library to handle authentication.** This code directly accesses the `_auth_configs` dictionary, which is not secure. Using a library to handle authentication will help to protect against unauthorized access.
2. **Sanitize user input.** The code does not sanitize user input, which could allow an attacker to inject malicious code into the request headers. Sanitizing user input will help to prevent this attack.
3. **Use strong cryptography.** The code does not use strong cryptography, which could allow an attacker to decrypt the request headers. Using strong cryptography will help to protect against this attack."
"    def inner(self, *args, **kwargs):
        if 'HttpHeaders' in self._auth_configs:
            if 'headers' not in kwargs:
                kwargs['headers'] = self._auth_configs['HttpHeaders']
            else:
                kwargs['headers'].update(self._auth_configs['HttpHeaders'])
        return f(self, *args, **kwargs)","1. Use `os.getenv()` to get the auth configurations instead of hardcoding them.
2. Use `json.dumps()` to serialize the auth configurations instead of passing them as a dictionary.
3. Use `urllib.parse.urlencode()` to encode the auth configurations into the request headers."
"    def _stream_helper(self, response):
        """"""Generator for data coming from a chunked-encoded HTTP response.""""""
        reader = response.raw
        assert reader._fp.chunked
        while not reader.closed:
            # this read call will block until we get a chunk
            data = reader.read(1)
            if not data:
                break
            if reader._fp.chunk_left:
                data += reader.read(reader._fp.chunk_left)
            yield data","1. **Use a secure protocol**. The code currently uses HTTP, which is not secure. It should be upgraded to HTTPS.
2. **Check the validity of the certificate**. When using HTTPS, the code should check the validity of the certificate to ensure that it is from a trusted source.
3. **Use strong encryption**. The code should use strong encryption to protect the data being transmitted."
"    def _stream_helper(self, response):
        """"""Generator for data coming from a chunked-encoded HTTP response.""""""
        socket_fp = self._get_raw_response_socket(response)
        socket_fp.setblocking(1)
        socket = socket_fp.makefile()
        while True:
            # Because Docker introduced newlines at the end of chunks in v0.9,
            # and only on some API endpoints, we have to cater for both cases.
            size_line = socket.readline()
            if size_line == '\\r\\n':
                size_line = socket.readline()

            size = int(size_line, 16)
            if size <= 0:
                break
            data = socket.readline()
            if not data:
                break
            yield data","1. Use `socket.settimeout()` to set a timeout for the socket connection.
2. Use `socket.makefile()` to create a file object for the socket connection.
3. Use `socket.readline()` to read data from the socket connection."
"    def _stream_helper(self, response):
        """"""Generator for data coming from a chunked-encoded HTTP response.""""""
        socket_fp = self._get_raw_response_socket(response)
        socket_fp.setblocking(1)
        socket = socket_fp.makefile()
        while True:
            size = int(socket.readline(), 16)
            if size <= 0:
                break
            data = socket.readline()
            if not data:
                break
            yield data","1. Use a context manager to ensure that the socket is closed after use.
2. Validate the size of the chunk before reading it.
3. Sanitize the data before yielding it to the caller."
"    def build(self, path=None, tag=None, quiet=False, fileobj=None,
              nocache=False, rm=False, stream=False, timeout=None):
        remote = context = headers = None
        if path is None and fileobj is None:
            raise Exception(""Either path or fileobj needs to be provided."")

        if fileobj is not None:
            context = utils.mkbuildcontext(fileobj)
        elif path.startswith(('http://', 'https://', 'git://', 'github.com/')):
            remote = path
        else:
            context = utils.tar(path)

        u = self._url('/build')
        params = {
            't': tag,
            'remote': remote,
            'q': quiet,
            'nocache': nocache,
            'rm': rm
        }
        if context is not None:
            headers = {'Content-Type': 'application/tar'}

        response = self._post(
            u,
            data=context,
            params=params,
            headers=headers,
            stream=stream,
            timeout=timeout,
        )

        if context is not None:
            context.close()

        if stream or utils.compare_version('1.8', self._version) >= 0:
            return self._stream_helper(response)
        else:
            output = self._result(response)
            srch = r'Successfully built ([0-9a-f]+)'
            match = re.search(srch, output)
            if not match:
                return None, output
            return match.group(1), output","1. Use `requests` library instead of `urllib` to handle requests.
2. Use `json` library to parse JSON response.
3. Sanitize user input before using it in the code."
"    def generate_evaluation_code(self, code):
        code.mark_pos(self.pos)
        self.allocate_temp_result(code)

        self.function.generate_evaluation_code(code)
        assert self.arg_tuple.mult_factor is None
        args = self.arg_tuple.args
        for arg in args:
            arg.generate_evaluation_code(code)

        # make sure function is in temp so that we can replace the reference below if it's a method
        reuse_function_temp = self.function.is_temp
        if reuse_function_temp:
            function = self.function.result()
        else:
            function = code.funcstate.allocate_temp(py_object_type, manage_ref=True)
            self.function.make_owned_reference(code)
            code.put(""%s = %s; "" % (function, self.function.py_result()))
            self.function.generate_disposal_code(code)
            self.function.free_temps(code)

        self_arg = code.funcstate.allocate_temp(py_object_type, manage_ref=True)
        code.putln(""%s = NULL;"" % self_arg)
        arg_offset_cname = code.funcstate.allocate_temp(PyrexTypes.c_int_type, manage_ref=False)
        code.putln(""%s = 0;"" % arg_offset_cname)

        def attribute_is_likely_method(attr):
            obj = attr.obj
            if obj.is_name and obj.entry.is_pyglobal:
                return False  # more likely to be a function
            return True

        if self.function.is_attribute:
            likely_method = 'likely' if attribute_is_likely_method(self.function) else 'unlikely'
        elif self.function.is_name and self.function.cf_state:
            # not an attribute itself, but might have been assigned from one (e.g. bound method)
            for assignment in self.function.cf_state:
                value = assignment.rhs
                if value and value.is_attribute and value.obj.type.is_pyobject:
                    if attribute_is_likely_method(value):
                        likely_method = 'likely'
                        break
            else:
                likely_method = 'unlikely'
        else:
            likely_method = 'unlikely'

        code.putln(""if (CYTHON_UNPACK_METHODS && %s(PyMethod_Check(%s))) {"" % (likely_method, function))
        code.putln(""%s = PyMethod_GET_SELF(%s);"" % (self_arg, function))
        # the following is always true in Py3 (kept only for safety),
        # but is false for unbound methods in Py2
        code.putln(""if (likely(%s)) {"" % self_arg)
        code.putln(""PyObject* function = PyMethod_GET_FUNCTION(%s);"" % function)
        code.put_incref(self_arg, py_object_type)
        code.put_incref(""function"", py_object_type)
        # free method object as early to possible to enable reuse from CPython's freelist
        code.put_decref_set(function, py_object_type, ""function"")
        code.putln(""%s = 1;"" % arg_offset_cname)
        code.putln(""}"")
        code.putln(""}"")

        # actually call the function
        code.globalstate.use_utility_code(
            UtilityCode.load_cached(""PyObjectFastCall"", ""ObjectHandling.c""))

        code.putln(""{"")
        code.putln(""PyObject *__pyx_callargs[%d] = {%s, %s};"" % (
            len(args)+1,
            self_arg,
            ', '.join(arg.py_result() for arg in args)))
        code.putln(""%s = __Pyx_PyObject_FastCall(%s, __pyx_callargs+1-%s, %d+%s);"" % (
            self.result(),
            function,
            arg_offset_cname,
            len(args),
            arg_offset_cname))

        code.put_xdecref_clear(self_arg, py_object_type)
        code.funcstate.release_temp(self_arg)
        code.funcstate.release_temp(arg_offset_cname)
        for arg in args:
            arg.generate_disposal_code(code)
            arg.free_temps(code)
        code.putln(code.error_goto_if_null(self.result(), self.pos))
        self.generate_gotref(code)

        if reuse_function_temp:
            self.function.generate_disposal_code(code)
            self.function.free_temps(code)
        else:
            code.put_decref_clear(function, py_object_type)
            code.funcstate.release_temp(function)
        code.putln(""}"")","1. Use `PyMethod_Check` to check if the function is a method.
2. Use `PyMethod_GET_SELF` to get the self argument of the method.
3. Use `PyMethod_GET_FUNCTION` to get the function object of the method."
"    def infer_type(self, env):
        # FIXME: this is way too redundant with analyse_types()
        node = self.analyse_as_cimported_attribute_node(env, target=False)
        if node is not None:
            return node.entry.type
        node = self.analyse_as_type_attribute(env)
        if node is not None:
            return node.entry.type
        obj_type = self.obj.infer_type(env)
        self.analyse_attribute(env, obj_type=obj_type)
        if obj_type.is_builtin_type and self.type.is_cfunction:
            # special case: C-API replacements for C methods of
            # builtin types cannot be inferred as C functions as
            # that would prevent their use as bound methods
            return py_object_type
        elif self.entry and self.entry.is_cmethod:
            # special case: bound methods should not be inferred
            # as their unbound method types
            return py_object_type
        return self.type","1. Use `py_object_type` instead of `self.type` to avoid inferring C functions as bound methods.
2. Use `self.analyse_as_cimported_attribute_node()` to check if the attribute is a C imported attribute.
3. Use `self.analyse_as_type_attribute()` to check if the attribute is a type attribute."
"    def generate_evaluation_code(self, code):
        code.mark_pos(self.pos)
        self.allocate_temp_result(code)

        self.function.generate_evaluation_code(code)
        assert self.arg_tuple.mult_factor is None
        args = self.arg_tuple.args
        for arg in args:
            arg.generate_evaluation_code(code)

        # make sure function is in temp so that we can replace the reference below if it's a method
        reuse_function_temp = self.function.is_temp
        if reuse_function_temp:
            function = self.function.result()
        else:
            function = code.funcstate.allocate_temp(py_object_type, manage_ref=True)
            self.function.make_owned_reference(code)
            code.put(""%s = %s; "" % (function, self.function.py_result()))
            self.function.generate_disposal_code(code)
            self.function.free_temps(code)

        self_arg = code.funcstate.allocate_temp(py_object_type, manage_ref=True)
        code.putln(""%s = NULL;"" % self_arg)
        arg_offset_cname = None
        if len(args) > 1:
            arg_offset_cname = code.funcstate.allocate_temp(PyrexTypes.c_int_type, manage_ref=False)
            code.putln(""%s = 0;"" % arg_offset_cname)

        def attribute_is_likely_method(attr):
            obj = attr.obj
            if obj.is_name and obj.entry.is_pyglobal:
                return False  # more likely to be a function
            return True

        if self.function.is_attribute:
            likely_method = 'likely' if attribute_is_likely_method(self.function) else 'unlikely'
        elif self.function.is_name and self.function.cf_state:
            # not an attribute itself, but might have been assigned from one (e.g. bound method)
            for assignment in self.function.cf_state:
                value = assignment.rhs
                if value and value.is_attribute and value.obj.type.is_pyobject:
                    if attribute_is_likely_method(value):
                        likely_method = 'likely'
                        break
            else:
                likely_method = 'unlikely'
        else:
            likely_method = 'unlikely'

        code.putln(""if (CYTHON_UNPACK_METHODS && %s(PyMethod_Check(%s))) {"" % (likely_method, function))
        code.putln(""%s = PyMethod_GET_SELF(%s);"" % (self_arg, function))
        # the following is always true in Py3 (kept only for safety),
        # but is false for unbound methods in Py2
        code.putln(""if (likely(%s)) {"" % self_arg)
        code.putln(""PyObject* function = PyMethod_GET_FUNCTION(%s);"" % function)
        code.put_incref(self_arg, py_object_type)
        code.put_incref(""function"", py_object_type)
        # free method object as early to possible to enable reuse from CPython's freelist
        code.put_decref_set(function, ""function"")
        if len(args) > 1:
            code.putln(""%s = 1;"" % arg_offset_cname)
        code.putln(""}"")
        code.putln(""}"")

        if not args:
            # fastest special case: try to avoid tuple creation
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""PyObjectCallNoArg"", ""ObjectHandling.c""))
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""PyObjectCallOneArg"", ""ObjectHandling.c""))
            code.putln(
                ""%s = (%s) ? __Pyx_PyObject_CallOneArg(%s, %s) : __Pyx_PyObject_CallNoArg(%s);"" % (
                    self.result(), self_arg,
                    function, self_arg,
                    function))
            code.put_xdecref_clear(self_arg, py_object_type)
            code.funcstate.release_temp(self_arg)
            code.putln(code.error_goto_if_null(self.result(), self.pos))
            code.put_gotref(self.py_result())
        elif len(args) == 1:
            # fastest special case: try to avoid tuple creation
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""PyObjectCall2Args"", ""ObjectHandling.c""))
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""PyObjectCallOneArg"", ""ObjectHandling.c""))
            arg = args[0]
            code.putln(
                ""%s = (%s) ? __Pyx_PyObject_Call2Args(%s, %s, %s) : __Pyx_PyObject_CallOneArg(%s, %s);"" % (
                    self.result(), self_arg,
                    function, self_arg, arg.py_result(),
                    function, arg.py_result()))
            code.put_xdecref_clear(self_arg, py_object_type)
            code.funcstate.release_temp(self_arg)
            arg.generate_disposal_code(code)
            arg.free_temps(code)
            code.putln(code.error_goto_if_null(self.result(), self.pos))
            code.put_gotref(self.py_result())
        else:
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""PyFunctionFastCall"", ""ObjectHandling.c""))
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""PyCFunctionFastCall"", ""ObjectHandling.c""))
            for test_func, call_prefix in [('PyFunction_Check', 'Py'), ('__Pyx_PyFastCFunction_Check', 'PyC')]:
                code.putln(""#if CYTHON_FAST_%sCALL"" % call_prefix.upper())
                code.putln(""if (%s(%s)) {"" % (test_func, function))
                code.putln(""PyObject *%s[%d] = {%s, %s};"" % (
                    Naming.quick_temp_cname,
                    len(args)+1,
                    self_arg,
                    ', '.join(arg.py_result() for arg in args)))
                code.putln(""%s = __Pyx_%sFunction_FastCall(%s, %s+1-%s, %d+%s); %s"" % (
                    self.result(),
                    call_prefix,
                    function,
                    Naming.quick_temp_cname,
                    arg_offset_cname,
                    len(args),
                    arg_offset_cname,
                    code.error_goto_if_null(self.result(), self.pos)))
                code.put_xdecref_clear(self_arg, py_object_type)
                code.put_gotref(self.py_result())
                for arg in args:
                    arg.generate_disposal_code(code)
                code.putln(""} else"")
                code.putln(""#endif"")

            code.putln(""{"")
            args_tuple = code.funcstate.allocate_temp(py_object_type, manage_ref=True)
            code.putln(""%s = PyTuple_New(%d+%s); %s"" % (
                args_tuple, len(args), arg_offset_cname,
                code.error_goto_if_null(args_tuple, self.pos)))
            code.put_gotref(args_tuple)

            if len(args) > 1:
                code.putln(""if (%s) {"" % self_arg)
            code.putln(""__Pyx_GIVEREF(%s); PyTuple_SET_ITEM(%s, 0, %s); %s = NULL;"" % (
                self_arg, args_tuple, self_arg, self_arg))  # stealing owned ref in this case
            code.funcstate.release_temp(self_arg)
            if len(args) > 1:
                code.putln(""}"")

            for i, arg in enumerate(args):
                arg.make_owned_reference(code)
                code.put_giveref(arg.py_result())
                code.putln(""PyTuple_SET_ITEM(%s, %d+%s, %s);"" % (
                    args_tuple, i, arg_offset_cname, arg.py_result()))
            if len(args) > 1:
                code.funcstate.release_temp(arg_offset_cname)

            for arg in args:
                arg.generate_post_assignment_code(code)
                arg.free_temps(code)

            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""PyObjectCall"", ""ObjectHandling.c""))
            code.putln(
                ""%s = __Pyx_PyObject_Call(%s, %s, NULL); %s"" % (
                    self.result(),
                    function, args_tuple,
                    code.error_goto_if_null(self.result(), self.pos)))
            code.put_gotref(self.py_result())

            code.put_decref_clear(args_tuple, py_object_type)
            code.funcstate.release_temp(args_tuple)

            if len(args) == 1:
                code.putln(""}"")
            code.putln(""}"")  # !CYTHON_FAST_PYCALL

        if reuse_function_temp:
            self.function.generate_disposal_code(code)
            self.function.free_temps(code)
        else:
            code.put_decref_clear(function, py_object_type)
            code.funcstate.release_temp(function)","1. Use `PyMethod_Check` to check if the function is a method, and if so, unpack it into a separate `self` argument.
2. Use `PyTuple_New` to create a tuple of arguments, and use `PyTuple_SET_ITEM` to set the `self` argument at index 0.
3. Use `__Pyx_PyObject_Call` to call the function with the tuple of arguments."
"def get_type_information_cname(code, dtype, maxdepth=None):
    """"""
    Output the run-time type information (__Pyx_TypeInfo) for given dtype,
    and return the name of the type info struct.

    Structs with two floats of the same size are encoded as complex numbers.
    One can separate between complex numbers declared as struct or with native
    encoding by inspecting to see if the fields field of the type is
    filled in.
    """"""
    namesuffix = mangle_dtype_name(dtype)
    name = ""__Pyx_TypeInfo_%s"" % namesuffix
    structinfo_name = ""__Pyx_StructFields_%s"" % namesuffix

    if dtype.is_error: return ""<error>""

    # It's critical that walking the type info doesn't use more stack
    # depth than dtype.struct_nesting_depth() returns, so use an assertion for this
    if maxdepth is None: maxdepth = dtype.struct_nesting_depth()
    if maxdepth <= 0:
        assert False

    if name not in code.globalstate.utility_codes:
        code.globalstate.utility_codes.add(name)
        typecode = code.globalstate['typeinfo']

        arraysizes = []
        if dtype.is_array:
            while dtype.is_array:
                arraysizes.append(dtype.size)
                dtype = dtype.base_type

        complex_possible = dtype.is_struct_or_union and dtype.can_be_complex()

        declcode = dtype.empty_declaration_code()
        if dtype.is_simple_buffer_dtype():
            structinfo_name = ""NULL""
        elif dtype.is_struct:
            fields = dtype.scope.var_entries
            # Must pre-call all used types in order not to recurse utility code
            # writing.
            assert len(fields) > 0
            types = [get_type_information_cname(code, f.type, maxdepth - 1)
                     for f in fields]
            typecode.putln(""static __Pyx_StructField %s[] = {"" % structinfo_name, safe=True)
            for f, typeinfo in zip(fields, types):
                typecode.putln('  {&%s, ""%s"", offsetof(%s, %s)},' %
                           (typeinfo, f.name, dtype.empty_declaration_code(), f.cname), safe=True)
            typecode.putln('  {NULL, NULL, 0}', safe=True)
            typecode.putln(""};"", safe=True)
        else:
            assert False

        rep = str(dtype)

        flags = ""0""
        is_unsigned = ""0""
        if dtype is PyrexTypes.c_char_type:
            is_unsigned = ""IS_UNSIGNED(%s)"" % declcode
            typegroup = ""'H'""
        elif dtype.is_int:
            is_unsigned = ""IS_UNSIGNED(%s)"" % declcode
            typegroup = ""%s ? 'U' : 'I'"" % is_unsigned
        elif complex_possible or dtype.is_complex:
            typegroup = ""'C'""
        elif dtype.is_float:
            typegroup = ""'R'""
        elif dtype.is_struct:
            typegroup = ""'S'""
            if dtype.packed:
                flags = ""__PYX_BUF_FLAGS_PACKED_STRUCT""
        elif dtype.is_pyobject:
            typegroup = ""'O'""
        else:
            assert False, dtype

        typeinfo = ('static __Pyx_TypeInfo %s = '
                        '{ ""%s"", %s, sizeof(%s), { %s }, %s, %s, %s, %s };')
        tup = (name, rep, structinfo_name, declcode,
               ', '.join([str(x) for x in arraysizes]) or '0', len(arraysizes),
               typegroup, is_unsigned, flags)
        typecode.putln(typeinfo % tup, safe=True)

    return name","1. Use `assert` statements to check for invalid inputs.
2. Sanitize user input before using it in your code.
3. Use secure coding practices, such as avoiding using `unsafe` functions."
"def p_typecast(s):
    # s.sy == ""<""
    pos = s.position()
    s.next()
    base_type = p_c_base_type(s)
    is_memslice = isinstance(base_type, Nodes.MemoryViewSliceTypeNode)
    is_template = isinstance(base_type, Nodes.TemplatedTypeNode)
    is_const_volatile = isinstance(base_type, Nodes.CConstOrVolatileTypeNode)
    if not is_memslice and not is_template and not is_const_volatile and base_type.name is None:
        s.error(""Unknown type"")
    declarator = p_c_declarator(s, empty = 1)
    if s.sy == '?':
        s.next()
        typecheck = 1
    else:
        typecheck = 0
    s.expect("">"")
    operand = p_factor(s)
    if is_memslice:
        return ExprNodes.CythonArrayNode(pos, base_type_node=base_type, operand=operand)

    return ExprNodes.TypecastNode(pos,
        base_type = base_type,
        declarator = declarator,
        operand = operand,
        typecheck = typecheck)","1. Use `type()` to check the type of a variable before casting it.
2. Use `isinstance()` to check if a variable is of a certain type.
3. Use `assert()` to verify that a variable is of a certain type."
"    def generate_evaluation_code(self, code):
        function = self.function
        if function.is_name or function.is_attribute:
            code.globalstate.use_entry_utility_code(function.entry)

        if not function.type.is_pyobject or len(self.arg_tuple.args) > 1 or (
                self.arg_tuple.args and self.arg_tuple.is_literal):
            super(SimpleCallNode, self).generate_evaluation_code(code)
            return

        # Special case 0-args and try to avoid explicit tuple creation for Python calls with 1 arg.
        arg = self.arg_tuple.args[0] if self.arg_tuple.args else None
        subexprs = (self.self, self.coerced_self, function, arg)
        for subexpr in subexprs:
            if subexpr is not None:
                subexpr.generate_evaluation_code(code)

        code.mark_pos(self.pos)
        assert self.is_temp
        self.allocate_temp_result(code)

        if arg is None:
            code.globalstate.use_utility_code(UtilityCode.load_cached(
                ""PyObjectCallNoArg"", ""ObjectHandling.c""))
            code.putln(
                ""%s = __Pyx_PyObject_CallNoArg(%s); %s"" % (
                    self.result(),
                    function.py_result(),
                    code.error_goto_if_null(self.result(), self.pos)))
        else:
            code.globalstate.use_utility_code(UtilityCode.load_cached(
                ""PyObjectCallOneArg"", ""ObjectHandling.c""))
            code.putln(
                ""%s = __Pyx_PyObject_CallOneArg(%s, %s); %s"" % (
                    self.result(),
                    function.py_result(),
                    arg.py_result(),
                    code.error_goto_if_null(self.result(), self.pos)))

        self.generate_gotref(code)

        for subexpr in subexprs:
            if subexpr is not None:
                subexpr.generate_disposal_code(code)
                subexpr.free_temps(code)","1. Use `Py_INCREF()` and `Py_DECREF()` to manage the reference counts of objects.
2. Check for errors returned by the Python API functions.
3. Sanitize user input to prevent code injection attacks."
"    def generate_result_code(self, code):
        func_type = self.function_type()
        if func_type.is_pyobject:
            arg_code = self.arg_tuple.py_result()
            code.globalstate.use_utility_code(UtilityCode.load_cached(
                ""PyObjectCall"", ""ObjectHandling.c""))
            code.putln(
                ""%s = __Pyx_PyObject_Call(%s, %s, NULL); %s"" % (
                    self.result(),
                    self.function.py_result(),
                    arg_code,
                    code.error_goto_if_null(self.result(), self.pos)))
            self.generate_gotref(code)
        elif func_type.is_cfunction:
            if self.has_optional_args:
                actual_nargs = len(self.args)
                expected_nargs = len(func_type.args) - func_type.optional_arg_count
                self.opt_arg_struct = code.funcstate.allocate_temp(
                    func_type.op_arg_struct.base_type, manage_ref=True)
                code.putln(""%s.%s = %s;"" % (
                        self.opt_arg_struct,
                        Naming.pyrex_prefix + ""n"",
                        len(self.args) - expected_nargs))
                args = list(zip(func_type.args, self.args))
                for formal_arg, actual_arg in args[expected_nargs:actual_nargs]:
                    code.putln(""%s.%s = %s;"" % (
                            self.opt_arg_struct,
                            func_type.opt_arg_cname(formal_arg.name),
                            actual_arg.result_as(formal_arg.type)))
            exc_checks = []
            if self.type.is_pyobject and self.is_temp:
                exc_checks.append(""!%s"" % self.result())
            elif self.type.is_memoryviewslice:
                assert self.is_temp
                exc_checks.append(self.type.error_condition(self.result()))
            elif func_type.exception_check != '+':
                exc_val = func_type.exception_value
                exc_check = func_type.exception_check
                if exc_val is not None:
                    exc_checks.append(""%s == %s"" % (self.result(), func_type.return_type.cast_code(exc_val)))
                if exc_check:
                    if self.nogil:
                        exc_checks.append(""__Pyx_ErrOccurredWithGIL()"")
                    else:
                        exc_checks.append(""PyErr_Occurred()"")
            if self.is_temp or exc_checks:
                rhs = self.c_call_code()
                if self.result():
                    lhs = ""%s = "" % self.result()
                    if self.is_temp and self.type.is_pyobject:
                        #return_type = self.type # func_type.return_type
                        #print ""SimpleCallNode.generate_result_code: casting"", rhs, \\
                        #    ""from"", return_type, ""to pyobject"" ###
                        rhs = typecast(py_object_type, self.type, rhs)
                else:
                    lhs = """"
                if func_type.exception_check == '+':
                    translate_cpp_exception(code, self.pos, '%s%s;' % (lhs, rhs),
                                            self.result() if self.type.is_pyobject else None,
                                            func_type.exception_value, self.nogil)
                else:
                    if (self.overflowcheck
                        and self.type.is_int
                        and self.type.signed
                        and self.function.result() in ('abs', 'labs', '__Pyx_abs_longlong')):
                        goto_error = 'if (unlikely(%s < 0)) { PyErr_SetString(PyExc_OverflowError, ""value too large""); %s; }' % (
                            self.result(), code.error_goto(self.pos))
                    elif exc_checks:
                        goto_error = code.error_goto_if("" && "".join(exc_checks), self.pos)
                    else:
                        goto_error = """"
                    code.putln(""%s%s; %s"" % (lhs, rhs, goto_error))
                if self.type.is_pyobject and self.result():
                    self.generate_gotref(code)
            if self.has_optional_args:
                code.funcstate.release_temp(self.opt_arg_struct)","1. Use `PyErr_SetString` to set a custom error message when an exception occurs.
2. Check for overflow errors when calling functions like `abs` or `labs`.
3. Use `goto_error` to jump to an error handler if any of the above checks fail."
"    def generate_evaluation_code(self, code):
        function = self.function
        if function.is_name or function.is_attribute:
            code.globalstate.use_entry_utility_code(function.entry)

        if not function.type.is_pyobject or len(self.arg_tuple.args) > 1 or (
                self.arg_tuple.args and self.arg_tuple.is_literal):
            super(SimpleCallNode, self).generate_evaluation_code(code)
            return

        # Special case 0-args and try to avoid explicit tuple creation for Python calls with 1 arg.
        arg = self.arg_tuple.args[0] if self.arg_tuple.args else None
        subexprs = (self.self, self.coerced_self, function, arg)
        for subexpr in subexprs:
            if subexpr is not None:
                subexpr.generate_evaluation_code(code)

        code.mark_pos(self.pos)
        assert self.is_temp
        self.allocate_temp_result(code)

        if arg is None:
            code.globalstate.use_utility_code(UtilityCode.load_cached(
                ""PyObjectCallNoArg"", ""ObjectHandling.c""))
            code.putln(
                ""%s = __Pyx_PyObject_CallNoArg(%s); %s"" % (
                    self.result(),
                    function.py_result(),
                    code.error_goto_if_null(self.result(), self.pos)))
        else:
            code.globalstate.use_utility_code(UtilityCode.load_cached(
                ""PyObjectCallOneArg"", ""ObjectHandling.c""))
            code.putln(
                ""%s = __Pyx_PyObject_CallOneArg(%s, %s); %s"" % (
                    self.result(),
                    function.py_result(),
                    arg.py_result(),
                    code.error_goto_if_null(self.result(), self.pos)))

        code.put_gotref(self.py_result())

        for subexpr in subexprs:
            if subexpr is not None:
                subexpr.generate_disposal_code(code)
                subexpr.free_temps(code)","1. Use `PyArg_ParseTupleAndKeywords` instead of `__Pyx_PyObject_CallNoArg` and `__Pyx_PyObject_CallOneArg` to validate arguments.
2. Sanitize user input to prevent injection attacks.
3. Use `Py_DECREF` to release references to objects when you are done with them."
"    def generate_result_code(self, code):
        func_type = self.function_type()
        if func_type.is_pyobject:
            arg_code = self.arg_tuple.py_result()
            code.globalstate.use_utility_code(UtilityCode.load_cached(
                ""PyObjectCall"", ""ObjectHandling.c""))
            code.putln(
                ""%s = __Pyx_PyObject_Call(%s, %s, NULL); %s"" % (
                    self.result(),
                    self.function.py_result(),
                    arg_code,
                    code.error_goto_if_null(self.result(), self.pos)))
            code.put_gotref(self.py_result())
        elif func_type.is_cfunction:
            if self.has_optional_args:
                actual_nargs = len(self.args)
                expected_nargs = len(func_type.args) - func_type.optional_arg_count
                self.opt_arg_struct = code.funcstate.allocate_temp(
                    func_type.op_arg_struct.base_type, manage_ref=True)
                code.putln(""%s.%s = %s;"" % (
                        self.opt_arg_struct,
                        Naming.pyrex_prefix + ""n"",
                        len(self.args) - expected_nargs))
                args = list(zip(func_type.args, self.args))
                for formal_arg, actual_arg in args[expected_nargs:actual_nargs]:
                    code.putln(""%s.%s = %s;"" % (
                            self.opt_arg_struct,
                            func_type.opt_arg_cname(formal_arg.name),
                            actual_arg.result_as(formal_arg.type)))
            exc_checks = []
            if self.type.is_pyobject and self.is_temp:
                exc_checks.append(""!%s"" % self.result())
            elif self.type.is_memoryviewslice:
                assert self.is_temp
                exc_checks.append(self.type.error_condition(self.result()))
            elif func_type.exception_check != '+':
                exc_val = func_type.exception_value
                exc_check = func_type.exception_check
                if exc_val is not None:
                    exc_checks.append(""%s == %s"" % (self.result(), func_type.return_type.cast_code(exc_val)))
                if exc_check:
                    if self.nogil:
                        exc_checks.append(""__Pyx_ErrOccurredWithGIL()"")
                    else:
                        exc_checks.append(""PyErr_Occurred()"")
            if self.is_temp or exc_checks:
                rhs = self.c_call_code()
                if self.result():
                    lhs = ""%s = "" % self.result()
                    if self.is_temp and self.type.is_pyobject:
                        #return_type = self.type # func_type.return_type
                        #print ""SimpleCallNode.generate_result_code: casting"", rhs, \\
                        #    ""from"", return_type, ""to pyobject"" ###
                        rhs = typecast(py_object_type, self.type, rhs)
                else:
                    lhs = """"
                if func_type.exception_check == '+':
                    translate_cpp_exception(code, self.pos, '%s%s;' % (lhs, rhs),
                                            self.result() if self.type.is_pyobject else None,
                                            func_type.exception_value, self.nogil)
                else:
                    if (self.overflowcheck
                        and self.type.is_int
                        and self.type.signed
                        and self.function.result() in ('abs', 'labs', '__Pyx_abs_longlong')):
                        goto_error = 'if (unlikely(%s < 0)) { PyErr_SetString(PyExc_OverflowError, ""value too large""); %s; }' % (
                            self.result(), code.error_goto(self.pos))
                    elif exc_checks:
                        goto_error = code.error_goto_if("" && "".join(exc_checks), self.pos)
                    else:
                        goto_error = """"
                    code.putln(""%s%s; %s"" % (lhs, rhs, goto_error))
                if self.type.is_pyobject and self.result():
                    code.put_gotref(self.py_result())
            if self.has_optional_args:
                code.funcstate.release_temp(self.opt_arg_struct)","1. Use `PyErr_SetString` to set a custom error message when an exception occurs.
2. Use `Py_DECREF` to release references to objects that are no longer needed.
3. Use `Py_INCREF` to increase the reference count of objects that are being borrowed."
"    def generate_stararg_copy_code(self, code):
        if not self.star_arg:
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""RaiseArgTupleInvalid"", ""FunctionArguments.c""))
            code.putln(""if (unlikely(%s > 0)) {"" % Naming.nargs_cname)
            code.put('__Pyx_RaiseArgtupleInvalid(""%s"", 1, 0, 0, %s); return %s;' % (
                self.name, Naming.nargs_cname, self.error_value()))
            code.putln(""}"")

        if self.starstar_arg:
            if self.star_arg or not self.starstar_arg.entry.cf_used:
                kwarg_check = ""unlikely(%s)"" % Naming.kwds_cname
            else:
                kwarg_check = ""%s"" % Naming.kwds_cname
        else:
            kwarg_check = ""unlikely(%s) && __Pyx_NumKwargs_%s(%s)"" % (
                Naming.kwds_cname, self.signature.fastvar, Naming.kwds_cname)
        code.globalstate.use_utility_code(
            UtilityCode.load_cached(""KeywordStringCheck"", ""FunctionArguments.c""))
        code.putln(
            ""if (%s && unlikely(!__Pyx_CheckKeywordStrings(%s, \\""%s\\"", %d))) return %s;"" % (
                kwarg_check, Naming.kwds_cname, self.name,
                bool(self.starstar_arg), self.error_value()))

        if self.starstar_arg and self.starstar_arg.entry.cf_used:
            code.putln(""if (%s) {"" % kwarg_check)
            code.putln(""%s = __Pyx_KwargsAsDict_%s(%s, %s);"" % (
                self.starstar_arg.entry.cname,
                self.signature.fastvar,
                Naming.kwds_cname,
                Naming.kwvalues_cname))
            code.putln(""if (unlikely(!%s)) return %s;"" % (
                self.starstar_arg.entry.cname, self.error_value()))
            code.put_gotref(self.starstar_arg.entry.cname, py_object_type)
            code.putln(""} else {"")
            allow_null = all(ref.node.allow_null for ref in self.starstar_arg.entry.cf_references)
            if allow_null:
                code.putln(""%s = NULL;"" % (self.starstar_arg.entry.cname,))
            else:
                code.putln(""%s = PyDict_New();"" % (self.starstar_arg.entry.cname,))
                code.putln(""if (unlikely(!%s)) return %s;"" % (
                    self.starstar_arg.entry.cname, self.error_value()))
                code.put_var_gotref(self.starstar_arg.entry)
            self.starstar_arg.entry.xdecref_cleanup = allow_null
            code.putln(""}"")

        if self.self_in_stararg and not self.target.is_staticmethod:
            assert not self.signature.use_fastcall
            # need to create a new tuple with 'self' inserted as first item
            code.put(""%s = PyTuple_New(%s + 1); if (unlikely(!%s)) "" % (
                self.star_arg.entry.cname,
                Naming.nargs_cname,
                self.star_arg.entry.cname))
            if self.starstar_arg and self.starstar_arg.entry.cf_used:
                code.putln(""{"")
                code.put_var_xdecref_clear(self.starstar_arg.entry)
                code.putln(""return %s;"" % self.error_value())
                code.putln(""}"")
            else:
                code.putln(""return %s;"" % self.error_value())
            code.put_var_gotref(self.star_arg.entry)
            code.put_incref(Naming.self_cname, py_object_type)
            code.put_giveref(Naming.self_cname, py_object_type)
            code.putln(""PyTuple_SET_ITEM(%s, 0, %s);"" % (
                self.star_arg.entry.cname, Naming.self_cname))
            temp = code.funcstate.allocate_temp(PyrexTypes.c_py_ssize_t_type, manage_ref=False)
            code.putln(""for (%s=0; %s < %s; %s++) {"" % (
                temp, temp, Naming.nargs_cname, temp))
            code.putln(""PyObject* item = PyTuple_GET_ITEM(%s, %s);"" % (
                Naming.args_cname, temp))
            code.put_incref(""item"", py_object_type)
            code.put_giveref(""item"", py_object_type)
            code.putln(""PyTuple_SET_ITEM(%s, %s+1, item);"" % (
                self.star_arg.entry.cname, temp))
            code.putln(""}"")
            code.funcstate.release_temp(temp)
            self.star_arg.entry.xdecref_cleanup = 0
        elif self.star_arg:
            assert not self.signature.use_fastcall
            code.put_incref(Naming.args_cname, py_object_type)
            code.putln(""%s = %s;"" % (
                self.star_arg.entry.cname,
                Naming.args_cname))
            self.star_arg.entry.xdecref_cleanup = 0","1. Use `Py_INCREF` and `Py_DECREF` to manage references to objects.
2. Check for errors and return early if an error is found.
3. Use `PyTuple_New` to create a new tuple instead of modifying an existing tuple."
"    def _build_fstring(self, pos, ustring, format_args):
        # Issues formatting warnings instead of errors since we really only catch a few errors by accident.
        args = iter(format_args)
        substrings = []
        can_be_optimised = True
        for s in re.split(self._parse_string_format_regex, ustring):
            if not s:
                continue
            if s == u'%%':
                substrings.append(ExprNodes.UnicodeNode(pos, value=EncodedString(u'%'), constant_result=u'%'))
                continue
            if s[0] != u'%':
                if s[-1] == u'%':
                    warning(pos, ""Incomplete format: '...%s'"" % s[-3:], level=1)
                    can_be_optimised = False
                substrings.append(ExprNodes.UnicodeNode(pos, value=EncodedString(s), constant_result=s))
                continue
            format_type = s[-1]
            try:
                arg = next(args)
            except StopIteration:
                warning(pos, ""Too few arguments for format placeholders"", level=1)
                can_be_optimised = False
                break
            if arg.is_starred:
                can_be_optimised = False
                break
            if format_type in u'asrfdoxX':
                format_spec = s[1:]
                conversion_char = None
                if format_type in u'doxX' and u'.' in format_spec:
                    # Precision is not allowed for integers in format(), but ok in %-formatting.
                    can_be_optimised = False
                elif format_type in u'ars':
                    format_spec = format_spec[:-1]
                    conversion_char = format_type
                elif format_type == u'd':
                    # '%d' formatting supports float, but '{obj:d}' does not => convert to int first.
                    conversion_char = 'd'
                substrings.append(ExprNodes.FormattedValueNode(
                    arg.pos, value=arg,
                    conversion_char=conversion_char,
                    format_spec=ExprNodes.UnicodeNode(
                        pos, value=EncodedString(format_spec), constant_result=format_spec)
                        if format_spec else None,
                ))
            else:
                # keep it simple for now ...
                can_be_optimised = False
                break

        if not can_be_optimised:
            # Print all warnings we can find before finally giving up here.
            return None

        try:
            next(args)
        except StopIteration: pass
        else:
            warning(pos, ""Too many arguments for format placeholders"", level=1)
            return None

        node = ExprNodes.JoinedStrNode(pos, values=substrings)
        return self.visit_JoinedStrNode(node)","1. Use `format_spec` to specify the format of the argument.
2. Do not use `.format()` with a `starred` argument.
3. Use `int()` to convert float to int before formatting."
"    def _build_fstring(self, pos, ustring, format_args):
        # Issues formatting warnings instead of errors since we really only catch a few errors by accident.
        args = iter(format_args)
        substrings = []
        can_be_optimised = True
        for s in re.split(self._parse_string_format_regex, ustring):
            if not s:
                continue
            if s == u'%%':
                substrings.append(ExprNodes.UnicodeNode(pos, value=EncodedString(u'%'), constant_result=u'%'))
                continue
            if s[0] != u'%':
                if s[-1] == u'%':
                    warning(pos, ""Incomplete format: '...%s'"" % s[-3:], level=1)
                    can_be_optimised = False
                substrings.append(ExprNodes.UnicodeNode(pos, value=EncodedString(s), constant_result=s))
                continue
            format_type = s[-1]
            try:
                arg = next(args)
            except StopIteration:
                warning(pos, ""Too few arguments for format placeholders"", level=1)
                can_be_optimised = False
                break
            if arg.is_starred:
                can_be_optimised = False
                break
            if format_type in u'asrfdoxX':
                format_spec = s[1:]
                if format_type in u'doxX' and u'.' in format_spec:
                    # Precision is not allowed for integers in format(), but ok in %-formatting.
                    can_be_optimised = False
                elif format_type in u'ars':
                    format_spec = format_spec[:-1]
                substrings.append(ExprNodes.FormattedValueNode(
                    arg.pos, value=arg,
                    conversion_char=format_type if format_type in u'ars' else None,
                    format_spec=ExprNodes.UnicodeNode(
                        pos, value=EncodedString(format_spec), constant_result=format_spec)
                        if format_spec else None,
                ))
            else:
                # keep it simple for now ...
                can_be_optimised = False
                break

        if not can_be_optimised:
            # Print all warnings we can find before finally giving up here.
            return None

        try:
            next(args)
        except StopIteration: pass
        else:
            warning(pos, ""Too many arguments for format placeholders"", level=1)
            return None

        node = ExprNodes.JoinedStrNode(pos, values=substrings)
        return self.visit_JoinedStrNode(node)","1. Use `format_spec` to specify the format of the argument.
2. Do not use `.` in the format specifier for integers.
3. Check for `*` in the format specifier and handle it correctly."
"    def declare_builtin(self, name, pos):
        return self.outer_scope.declare_builtin(name, pos)","1. **Use `functools.wraps` to preserve the original function metadata.** This will ensure that the function signature and docstring are correctly propagated to the new function.
2. **Check the function arguments for validity.** This will help to prevent errors and protect against malicious input.
3. **Return a `None` value if the function fails.** This will help to prevent silent errors."
"    def lookup(self, name):
        # Look up name in this scope or an enclosing one.
        # Return None if not found.
        name = self.mangle_class_private_name(name)
        return (self.lookup_here(name)
            or (self.outer_scope and self.outer_scope.lookup(name))
            or None)","1. Use `getattr` instead of `__dict__` to access attributes.
2. Sanitize user input before using it to construct objects.
3. Use `isinstance` to check if an object is of a certain type before calling its methods."
"    def lookup_here(self, name):
        # Look up in this scope only, return None if not found.
        name = self.mangle_class_private_name(name)
        return self.entries.get(name, None)","1. Use `getattr` instead of `get` to avoid accidentally accessing attributes of the class object itself.
2. Use `__class__` instead of `self` to avoid accidentally accessing attributes of the current instance.
3. Use `isinstance` to check if an object is a class before calling `lookup_here`."
"    def lookup_target(self, name):
        # Look up name in this scope only. Declare as Python
        # variable if not found.
        entry = self.lookup_here(name)
        if not entry:
            entry = self.declare_var(name, py_object_type, None)
        return entry","1. Use `getattr` instead of `lookup_here` to lookup variables.
2. Use `__builtins__` instead of `py_object_type` to get the Python builtin type.
3. Check if the variable is declared before assigning a value to it."
"    def mangle_class_private_name(self, name):
        # a few utilitycode names need to specifically be ignored
        if name and name.lower().startswith(""__pyx_""):
            return name
        return self.mangle_special_name(name)","1. Use `functools.lru_cache` to cache the results of the mangled name function. This will improve performance and prevent the function from being called multiple times for the same name.
2. Use `six.ensure_str` to ensure that the name parameter is a string. This will prevent errors from being thrown if the parameter is not a valid string.
3. Use `warnings.warn` to warn users if they try to use a name that starts with `__pyx_`. This will help to prevent users from accidentally using names that are reserved for internal use."
"    def declare_var(self, name, type, pos,
                    cname = None, visibility = 'private',
                    api = 0, in_pxd = 0, is_cdef = 0):
        name = self.mangle_special_name(name)
        if type is unspecified_type:
            type = py_object_type
        # Add an entry for a class attribute.
        entry = Scope.declare_var(self, name, type, pos,
                                  cname=cname, visibility=visibility,
                                  api=api, in_pxd=in_pxd, is_cdef=is_cdef)
        entry.is_pyglobal = 1
        entry.is_pyclass_attr = 1
        return entry","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `inspect.iscoroutinefunction` to check if the function is a coroutine.
3. Use `asyncio.coroutine` to mark the function as a coroutine."
"    def declare_var(self, name, type, pos,
                    cname = None, visibility = 'private',
                    api = 0, in_pxd = 0, is_cdef = 0):
        name = self.mangle_special_name(name)
        if is_cdef:
            # Add an entry for an attribute.
            if self.defined:
                error(pos,
                    ""C attributes cannot be added in implementation part of""
                    "" extension type defined in a pxd"")
            if not self.is_closure_class_scope and get_special_method_signature(name):
                error(pos,
                    ""The name '%s' is reserved for a special method.""
                        % name)
            if not cname:
                cname = name
                if visibility == 'private':
                    cname = c_safe_identifier(cname)
                cname = punycodify_name(cname, Naming.unicode_structmember_prefix)
            if type.is_cpp_class and visibility != 'extern':
                type.check_nullary_constructor(pos)
                self.use_utility_code(Code.UtilityCode(""#include <new>""))
            entry = self.declare(name, cname, type, pos, visibility)
            entry.is_variable = 1
            self.var_entries.append(entry)
            if type.is_memoryviewslice:
                self.has_memoryview_attrs = True
            elif type.is_cpp_class:
                self.has_cpp_class_attrs = True
            elif type.is_pyobject and (self.is_closure_class_scope or name != '__weakref__'):
                self.has_pyobject_attrs = True
                if (not type.is_builtin_type
                        or not type.scope or type.scope.needs_gc()):
                    self.has_cyclic_pyobject_attrs = True
            if visibility not in ('private', 'public', 'readonly'):
                error(pos,
                    ""Attribute of extension type cannot be declared %s"" % visibility)
            if visibility in ('public', 'readonly'):
                # If the field is an external typedef, we cannot be sure about the type,
                # so do conversion ourself rather than rely on the CPython mechanism (through
                # a property; made in AnalyseDeclarationsTransform).
                entry.needs_property = True
                if not self.is_closure_class_scope and name == ""__weakref__"":
                    error(pos, ""Special attribute __weakref__ cannot be exposed to Python"")
                if not (type.is_pyobject or type.can_coerce_to_pyobject(self)):
                    # we're not testing for coercion *from* Python here - that would fail later
                    error(pos, ""C attribute of type '%s' cannot be accessed from Python"" % type)
            else:
                entry.needs_property = False
            return entry
        else:
            if type is unspecified_type:
                type = py_object_type
            # Add an entry for a class attribute.
            entry = Scope.declare_var(self, name, type, pos,
                                      cname=cname, visibility=visibility,
                                      api=api, in_pxd=in_pxd, is_cdef=is_cdef)
            entry.is_member = 1
            entry.is_pyglobal = 1 # xxx: is_pyglobal changes behaviour in so many places that
                                  # I keep it in for now. is_member should be enough
                                  # later on
            self.namespace_cname = ""(PyObject *)%s"" % self.parent_type.typeptr_cname

            return entry","1. Use `c_safe_identifier` to ensure that the name is safe to use in C code.
2. Check for reserved names and special methods.
3. Use `declare` to declare variables and attributes."
"    def declare_from_annotation(self, env, as_target=False):
        """"""Implements PEP 526 annotation typing in a fairly relaxed way.

        Annotations are ignored for global variables, Python class attributes and already declared variables.
        String literals are allowed and ignored.
        The ambiguous Python types 'int' and 'long' are ignored and the 'cython.int' form must be used instead.
        """"""
        if not env.directives['annotation_typing']:
            return
        if env.is_module_scope or env.is_py_class_scope:
            # annotations never create global cdef names and Python classes don't support them anyway
            return
        name = self.name
        if self.entry or env.lookup_here(name) is not None:
            # already declared => ignore annotation
            return

        annotation = self.annotation
        if annotation.expr.is_string_literal:
            # name: ""description"" => not a type, but still a declared variable or attribute
            atype = None
        else:
            _, atype = annotation.analyse_type_annotation(env)
        if atype is None:
            atype = unspecified_type if as_target and env.directives['infer_types'] != False else py_object_type
        self.entry = env.declare_var(name, atype, self.pos, is_cdef=not as_target)
        self.entry.annotation = annotation.expr","1. Use `annotation.analyse_type_annotation` to check if the annotation is valid.
2. Use `env.declare_var` to declare the variable with the correct type.
3. Use `annotation.expr` to get the type of the annotation."
"    def analyse_type_annotation(self, env, assigned_value=None):
        annotation = self.expr
        base_type = None
        is_ambiguous = False
        explicit_pytype = explicit_ctype = False
        if annotation.is_dict_literal:
            warning(annotation.pos,
                    ""Dicts should no longer be used as type annotations. Use 'cython.int' etc. directly."")
            for name, value in annotation.key_value_pairs:
                if not name.is_string_literal:
                    continue
                if name.value in ('type', b'type'):
                    explicit_pytype = True
                    if not explicit_ctype:
                        annotation = value
                elif name.value in ('ctype', b'ctype'):
                    explicit_ctype = True
                    annotation = value
            if explicit_pytype and explicit_ctype:
                warning(annotation.pos, ""Duplicate type declarations found in signature annotation"")
        arg_type = annotation.analyse_as_type(env)
        if annotation.is_name and not annotation.cython_attribute and annotation.name in ('int', 'long', 'float'):
            # Map builtin numeric Python types to C types in safe cases.
            if assigned_value is not None and arg_type is not None and not arg_type.is_pyobject:
                assigned_type = assigned_value.infer_type(env)
                if assigned_type and assigned_type.is_pyobject:
                    # C type seems unsafe, e.g. due to 'None' default value  => ignore annotation type
                    is_ambiguous = True
                    arg_type = None
            # ignore 'int' and require 'cython.int' to avoid unsafe integer declarations
            if arg_type in (PyrexTypes.c_long_type, PyrexTypes.c_int_type, PyrexTypes.c_float_type):
                arg_type = PyrexTypes.c_double_type if annotation.name == 'float' else py_object_type
        elif arg_type is not None and annotation.is_string_literal:
            warning(annotation.pos,
                    ""Strings should no longer be used for type declarations. Use 'cython.int' etc. directly."",
                    level=1)
        if arg_type is not None:
            if explicit_pytype and not explicit_ctype and not arg_type.is_pyobject:
                warning(annotation.pos,
                        ""Python type declaration in signature annotation does not refer to a Python type"")
            base_type = Nodes.CAnalysedBaseTypeNode(
                annotation.pos, type=arg_type, is_arg=True)
        elif is_ambiguous:
            warning(annotation.pos, ""Ambiguous types in annotation, ignoring"")
        else:
            warning(annotation.pos, ""Unknown type declaration in annotation, ignoring"")
        return base_type, arg_type","1. Use `cython.int` instead of `int` to avoid unsafe integer declarations.
2. Use `py_object_type` instead of `c_long_type`, `c_int_type`, or `c_float_type` to avoid unsafe type declarations.
3. Use `annotation.is_name` and `annotation.cython_attribute` to check if the annotation is safe."
"    def _specialize_function_args(self, args, fused_to_specific):
        for arg in args:
            if arg.type.is_fused:
                arg.type = arg.type.specialize(fused_to_specific)
                if arg.type.is_memoryviewslice:
                    arg.type.validate_memslice_dtype(arg.pos)","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Validate the input arguments of the function to prevent invalid inputs.
3. Sanitize the output of the function to prevent malicious code execution."
"    def align_argument_type(self, env, arg):
        # @cython.locals()
        directive_locals = self.directive_locals
        orig_type = arg.type
        if arg.name in directive_locals:
            type_node = directive_locals[arg.name]
            other_type = type_node.analyse_as_type(env)
        elif isinstance(arg, CArgDeclNode) and arg.annotation and env.directives['annotation_typing']:
            type_node = arg.annotation
            other_type = arg.inject_type_from_annotations(env)
            if other_type is None:
                return arg
        else:
            return arg
        if other_type is None:
            error(type_node.pos, ""Not a type"")
        elif other_type.is_fused and any(orig_type.same_as(t) for t in other_type.types):
            pass # use specialized rather than fused type
        elif orig_type is not py_object_type and not orig_type.same_as(other_type):
            error(arg.base_type.pos, ""Signature does not agree with previous declaration"")
            error(type_node.pos, ""Previous declaration here"")
        else:
            arg.type = other_type
        return arg","1. Use `type()` to check if the argument is a valid type.
2. Use `same_as()` to check if the argument is the same as the expected type.
3. Use `error()` to raise an error if the argument is not valid."
"    def visit_FuncDefNode(self, node):
        """"""
        Analyse a function and its body, as that hasn't happened yet.  Also
        analyse the directive_locals set by @cython.locals().

        Then, if we are a function with fused arguments, replace the function
        (after it has declared itself in the symbol table!) with a
        FusedCFuncDefNode, and analyse its children (which are in turn normal
        functions). If we're a normal function, just analyse the body of the
        function.
        """"""
        env = self.current_env()

        self.seen_vars_stack.append(set())
        lenv = node.local_scope
        node.declare_arguments(lenv)

        # @cython.locals(...)
        for var, type_node in node.directive_locals.items():
            if not lenv.lookup_here(var):   # don't redeclare args
                type = type_node.analyse_as_type(lenv)
                if type:
                    lenv.declare_var(var, type, type_node.pos)
                else:
                    error(type_node.pos, ""Not a type"")

        if self._handle_fused(node):
            node = self._create_fused_function(env, node)
        else:
            node.body.analyse_declarations(lenv)
            self._handle_nogil_cleanup(lenv, node)
            self._super_visit_FuncDefNode(node)

        self.seen_vars_stack.pop()
        return node","1. Use `@cython.locals()` to declare local variables. This will help to prevent name collisions and improve performance.
2. Use `lenv.lookup_here()` to check if a variable has already been declared in the local scope. This will help to prevent errors.
3. Use `error()` to report errors. This will help to catch problems early on."
"    def py_operation_function(self, code):
        is_unicode_concat = False
        if isinstance(self.operand1, FormattedValueNode) or isinstance(self.operand2, FormattedValueNode):
            is_unicode_concat = True
        else:
            type1, type2 = self.operand1.type, self.operand2.type
            if type1 is unicode_type or type2 is unicode_type:
                is_unicode_concat = type1.is_builtin_type and type2.is_builtin_type

        if is_unicode_concat:
            if self.operand1.may_be_none() or self.operand2.may_be_none():
                return '__Pyx_PyUnicode_ConcatSafe'
            else:
                return '__Pyx_PyUnicode_Concat'
        return super(AddNode, self).py_operation_function(code)","1. **Use `__Pyx_PyUnicode_ConcatSafe` instead of `__Pyx_PyUnicode_Concat` when either operand could be `None`.** This will prevent a `NULL pointer dereference` if either operand is `None`.
2. **Use `PyUnicode_Check` to check if an operand is a `unicode` object before calling `__Pyx_PyUnicode_ConcatSafe` or `__Pyx_PyUnicode_Concat`.** This will prevent a `type error` if an operand is not a `unicode` object.
3. **Use `PyUnicode_AsUTF8` to convert the result of `__Pyx_PyUnicode_ConcatSafe` or `__Pyx_PyUnicode_Concat` to a `bytes` object before returning it.** This will prevent a `UnicodeEncodeError` if the result is not a valid `UTF-8` string."
"def unpack_source_tree(tree_file, dir=None):
    if dir is None:
        dir = tempfile.mkdtemp()
    header = []
    cur_file = None
    f = open(tree_file)
    try:
        lines = f.readlines()
    finally:
        f.close()
    del f
    try:
        for line in lines:
            if line[:5] == '#####':
                filename = line.strip().strip('#').strip().replace('/', os.path.sep)
                path = os.path.join(dir, filename)
                if not os.path.exists(os.path.dirname(path)):
                    os.makedirs(os.path.dirname(path))
                if cur_file is not None:
                    f, cur_file = cur_file, None
                    f.close()
                cur_file = open(path, 'w')
            elif cur_file is not None:
                cur_file.write(line)
            elif line.strip() and not line.lstrip().startswith('#'):
                if line.strip() not in ('""""""', ""'''""):
                    header.append(line)
    finally:
        if cur_file is not None:
            cur_file.close()
    return dir, ''.join(header)","1. Use `tempfile.mkdtemp()` to create a temporary directory instead of relying on the user to provide one. This will prevent users from creating arbitrary files on the system.
2. Use `os.makedirs()` to create the parent directories of a file before creating the file itself. This will prevent a race condition where a file is created with a parent directory that does not exist.
3. Use `f.close()` to close a file after you are finished with it. This will free up system resources and prevent data from being overwritten."
"    def analyse_types(self, env):
        self.bases = self.bases.analyse_types(env)
        if self.doc:
            self.doc = self.doc.analyse_types(env)
            self.doc = self.doc.coerce_to_pyobject(env)
        env.use_utility_code(UtilityCode.load_cached(""CreateClass"", ""ObjectHandling.c""))
        return self","1. Use `functools.wraps` to preserve the metadata of the decorated function.
2. Use `inspect.isclass` to check if the argument is a class before calling `analyse_types`.
3. Use `inspect.isfunction` to check if the argument is a function before calling `coerce_to_pyobject`."
"    def generate_result_code(self, code):
        cname = code.intern_identifier(self.name)

        if self.doc:
            code.put_error_if_neg(self.pos,
                'PyDict_SetItem(%s, %s, %s)' % (
                    self.dict.py_result(),
                    code.intern_identifier(
                        StringEncoding.EncodedString(""__doc__"")),
                    self.doc.py_result()))
        py_mod_name = self.get_py_mod_name(code)
        qualname = self.get_py_qualified_name(code)
        code.putln(
            '%s = __Pyx_CreateClass(%s, %s, %s, %s, %s); %s' % (
                self.result(),
                self.bases.py_result(),
                self.dict.py_result(),
                cname,
                qualname,
                py_mod_name,
                code.error_goto_if_null(self.result(), self.pos)))
        code.put_gotref(self.py_result())","1. Use `PyTypeObject` instead of `PyClassObject` to create a new class.
2. Use `PyDict_SetItemString()` to set the `__doc__` attribute.
3. Use `Py_INCREF()` to increment the reference count of the new class."
"    def generate_result_code(self, code):
        code.globalstate.use_utility_code(UtilityCode.load_cached(""Py3ClassCreate"", ""ObjectHandling.c""))
        cname = code.intern_identifier(self.name)
        if self.mkw:
            mkw = self.mkw.py_result()
        else:
            mkw = 'NULL'
        if self.metaclass:
            metaclass = self.metaclass.py_result()
        else:
            metaclass = ""((PyObject*)&__Pyx_DefaultClassType)""
        code.putln(
            '%s = __Pyx_Py3ClassCreate(%s, %s, %s, %s, %s, %d, %d); %s' % (
                self.result(),
                metaclass,
                cname,
                self.bases.py_result(),
                self.dict.py_result(),
                mkw,
                self.calculate_metaclass,
                self.allow_py2_metaclass,
                code.error_goto_if_null(self.result(), self.pos)))
        code.put_gotref(self.py_result())","1. Use `Py_INCREF` and `Py_DECREF` to manage references to objects.
2. Sanitize user input to prevent code injection attacks.
3. Use `assert` statements to check for errors and ensure that the code is behaving as expected."
"    def generate_result_code(self, code):
        if self.mkw:
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""Py3MetaclassGet"", ""ObjectHandling.c""))
            call = ""__Pyx_Py3MetaclassGet(%s, %s)"" % (
                self.bases.result(),
                self.mkw.result())
        else:
            code.globalstate.use_utility_code(
                UtilityCode.load_cached(""CalculateMetaclass"", ""ObjectHandling.c""))
            call = ""__Pyx_CalculateMetaclass(NULL, %s)"" % (
                self.bases.result())
        code.putln(
            ""%s = %s; %s"" % (
                self.result(), call,
                code.error_goto_if_null(self.result(), self.pos)))
        code.put_gotref(self.py_result())","1. Use `PyTypeObject* PyType_FromSpecWithBases` to create a new metaclass instead of `__Pyx_Py3MetaclassGet` and `__Pyx_CalculateMetaclass`.
2. Use `Py_INCREF` to increase the reference count of the returned metaclass object.
3. Use `Py_DECREF` to decrease the reference count of the returned metaclass object when it is no longer needed."
"    def analyse_types(self, env):
        if self.doc:
            self.doc = self.doc.analyse_types(env)
            self.doc = self.doc.coerce_to_pyobject(env)
        self.type = py_object_type
        self.is_temp = 1
        return self","1. Use `ast.literal_eval` instead of `eval` to sanitize user input.
2. Use `inspect.isbuiltin` to check if a function is a built-in function before calling it.
3. Use `functools.wraps` to preserve the original function's metadata when creating a wrapper function."
"    def generate_result_code(self, code):
        cname = code.intern_identifier(self.name)
        py_mod_name = self.get_py_mod_name(code)
        qualname = self.get_py_qualified_name(code)
        if self.doc:
            doc_code = self.doc.result()
        else:
            doc_code = '(PyObject *) NULL'
        if self.mkw:
            mkw = self.mkw.py_result()
        else:
            mkw = '(PyObject *) NULL'
        if self.metaclass:
            metaclass = self.metaclass.py_result()
        else:
            metaclass = ""(PyObject *) NULL""
        code.putln(
            ""%s = __Pyx_Py3MetaclassPrepare(%s, %s, %s, %s, %s, %s, %s); %s"" % (
                self.result(),
                metaclass,
                self.bases.result(),
                cname,
                qualname,
                mkw,
                py_mod_name,
                doc_code,
                code.error_goto_if_null(self.result(), self.pos)))
        code.put_gotref(self.py_result())","1. Use `Py_INCREF` and `Py_DECREF` to manage references to objects.
2. Check for errors using `PyErr_Occurred` and `PyErr_Clear`.
3. Sanitize user input to prevent against injection attacks."
"    def __init__(self, pos, name, bases, doc, body, decorators=None,
                 keyword_args=None, force_py3_semantics=False):
        StatNode.__init__(self, pos)
        self.name = name
        self.doc = doc
        self.body = body
        self.decorators = decorators
        self.bases = bases
        from . import ExprNodes
        if self.doc and Options.docstrings:
            doc = embed_position(self.pos, self.doc)
            doc_node = ExprNodes.StringNode(pos, value=doc)
        else:
            doc_node = None

        allow_py2_metaclass = not force_py3_semantics
        if keyword_args:
            allow_py2_metaclass = False
            self.is_py3_style_class = True
            if keyword_args.is_dict_literal:
                if keyword_args.key_value_pairs:
                    for i, item in list(enumerate(keyword_args.key_value_pairs))[::-1]:
                        if item.key.value == 'metaclass':
                            if self.metaclass is not None:
                                error(item.pos, ""keyword argument 'metaclass' passed multiple times"")
                            # special case: we already know the metaclass,
                            # so we don't need to do the ""build kwargs,
                            # find metaclass"" dance at runtime
                            self.metaclass = item.value
                            del keyword_args.key_value_pairs[i]
                    self.mkw = keyword_args
                else:
                    assert self.metaclass is not None
            else:
                # MergedDictNode
                self.mkw = ExprNodes.ProxyNode(keyword_args)

        if force_py3_semantics or self.bases or self.mkw or self.metaclass:
            if self.metaclass is None:
                if keyword_args and not keyword_args.is_dict_literal:
                    # **kwargs may contain 'metaclass' arg
                    mkdict = self.mkw
                else:
                    mkdict = None
                if (not mkdict and
                        self.bases.is_sequence_constructor and
                        not self.bases.args):
                    pass  # no base classes => no inherited metaclass
                else:
                    self.metaclass = ExprNodes.PyClassMetaclassNode(
                        pos, mkw=mkdict, bases=self.bases)
                needs_metaclass_calculation = False
            else:
                needs_metaclass_calculation = True

            self.dict = ExprNodes.PyClassNamespaceNode(
                pos, name=name, doc=doc_node,
                metaclass=self.metaclass, bases=self.bases, mkw=self.mkw)
            self.classobj = ExprNodes.Py3ClassNode(
                pos, name=name,
                bases=self.bases, dict=self.dict, doc=doc_node,
                metaclass=self.metaclass, mkw=self.mkw,
                calculate_metaclass=needs_metaclass_calculation,
                allow_py2_metaclass=allow_py2_metaclass)
        else:
            # no bases, no metaclass => old style class creation
            self.dict = ExprNodes.DictNode(pos, key_value_pairs=[])
            self.classobj = ExprNodes.ClassNode(
                pos, name=name,
                bases=bases, dict=self.dict, doc=doc_node)

        self.target = ExprNodes.NameNode(pos, name=name)
        self.class_cell = ExprNodes.ClassCellInjectorNode(self.pos)","1. Use `ExprNodes.PyClassMetaclassNode` to create a metaclass for the class.
2. Use `ExprNodes.PyClassNamespaceNode` to create a namespace for the class.
3. Use `ExprNodes.Py3ClassNode` to create the class object."
"    def as_cclass(self):
        """"""
        Return this node as if it were declared as an extension class
        """"""
        if self.is_py3_style_class:
            error(self.classobj.pos, ""Python3 style class could not be represented as C class"")
            return

        from . import ExprNodes
        return CClassDefNode(self.pos,
                             visibility='private',
                             module_name=None,
                             class_name=self.name,
                             bases=self.classobj.bases or ExprNodes.TupleNode(self.pos, args=[]),
                             decorators=self.decorators,
                             body=self.body,
                             in_pxd=False,
                             doc=self.doc)","1. Use `self.is_py3_style_class` to check if the class is a Python3 style class.
2. If the class is a Python3 style class, use `error()` to raise an error.
3. Return `CClassDefNode` instead of the original node."
"    def analyse_declarations(self, env):
        class_result = self.classobj
        if self.decorators:
            from .ExprNodes import SimpleCallNode
            for decorator in self.decorators[::-1]:
                class_result = SimpleCallNode(
                    decorator.pos,
                    function=decorator.decorator,
                    args=[class_result])
            self.decorators = None
        self.class_result = class_result
        self.class_result.analyse_declarations(env)
        self.target.analyse_target_declaration(env)
        cenv = self.create_scope(env)
        cenv.directives = env.directives
        cenv.class_obj_cname = self.target.entry.cname
        self.body.analyse_declarations(cenv)","1. Use `@staticmethod` instead of `@classmethod` to prevent instantiation of the class.
2. Validate user input before using it to construct objects.
3. Use `f-strings` instead of string concatenation to avoid potential injection attacks."
"    def analyse_expressions(self, env):
        if self.bases:
            self.bases = self.bases.analyse_expressions(env)
        if self.metaclass:
            self.metaclass = self.metaclass.analyse_expressions(env)
        if self.mkw:
            self.mkw = self.mkw.analyse_expressions(env)
        self.dict = self.dict.analyse_expressions(env)
        self.class_result = self.class_result.analyse_expressions(env)
        cenv = self.scope
        self.body = self.body.analyse_expressions(cenv)
        self.target.analyse_target_expression(env, self.classobj)
        self.class_cell = self.class_cell.analyse_expressions(cenv)
        return self","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `inspect.iscoroutinefunction` to check if the function is a coroutine.
3. Use `asyncio.coroutine` to mark a function as a coroutine."
"    def file_reporter(self, filename):
        # TODO: let coverage.py handle .py files itself
        #ext = os.path.splitext(filename)[1].lower()
        #if ext == '.py':
        #    from coverage.python import PythonFileReporter
        #    return PythonFileReporter(filename)

        filename = canonical_filename(os.path.abspath(filename))
        if self._c_files_map and filename in self._c_files_map:
            c_file, rel_file_path, code = self._c_files_map[filename]
        else:
            c_file, _ = self._find_source_files(filename)
            if not c_file:
                return None  # unknown file
            rel_file_path, code = self._parse_lines(c_file, filename)
        return CythonModuleReporter(c_file, filename, rel_file_path, code)","1. Use `os.path.relpath()` to get the relative path of the file, instead of using `os.path.abspath()`. This will prevent the file from being read from a different location.
2. Use `canonical_filename()` to get the canonicalized path of the file, which will remove any symbolic links. This will prevent the file from being read from a different location.
3. Use `_find_source_files()` to find the source files for the file, instead of hardcoding the path. This will prevent the file from being read from a different location."
"    def generate_module_preamble(self, env, options, cimported_modules, metadata, code):
        code.put_generated_by()
        if metadata:
            code.putln(""/* BEGIN: Cython Metadata"")
            code.putln(json.dumps(metadata, indent=4, sort_keys=True))
            code.putln(""END: Cython Metadata */"")
            code.putln("""")
        code.putln(""#define PY_SSIZE_T_CLEAN"")

        for inc in sorted(env.c_includes.values(), key=IncludeCode.sortkey):
            if inc.location == inc.INITIAL:
                inc.write(code)
        code.putln(""#ifndef Py_PYTHON_H"")
        code.putln(""    #error Python headers needed to compile C extensions, ""
                   ""please install development version of Python."")
        code.putln(""#elif PY_VERSION_HEX < 0x02070000 || ""
                   ""(0x03000000 <= PY_VERSION_HEX && PY_VERSION_HEX < 0x03030000)"")
        code.putln(""    #error Cython requires Python 2.7+ or Python 3.3+."")
        code.putln(""#else"")
        code.globalstate[""end""].putln(""#endif /* Py_PYTHON_H */"")

        from .. import __version__
        code.putln('#define CYTHON_ABI ""%s""' % __version__.replace('.', '_'))
        code.putln('#define CYTHON_HEX_VERSION %s' % build_hex_version(__version__))
        code.putln(""#define CYTHON_FUTURE_DIVISION %d"" % (
            Future.division in env.context.future_directives))

        self._put_setup_code(code, ""CModulePreamble"")
        if env.context.options.cplus:
            self._put_setup_code(code, ""CppInitCode"")
        else:
            self._put_setup_code(code, ""CInitCode"")
        self._put_setup_code(code, ""PythonCompatibility"")
        self._put_setup_code(code, ""MathInitCode"")

        if options.c_line_in_traceback:
            cinfo = ""%s = %s; "" % (Naming.clineno_cname, Naming.line_c_macro)
        else:
            cinfo = """"
        code.put(""""""
#define __PYX_ERR(f_index, lineno, Ln_error) \\\\
{ \\\\
  %s = %s[f_index]; %s = lineno; %sgoto Ln_error; \\\\
}
"""""" % (Naming.filename_cname, Naming.filetable_cname, Naming.lineno_cname, cinfo))

        code.putln("""")
        self.generate_extern_c_macro_definition(code)
        code.putln("""")

        code.putln(""#define %s"" % Naming.h_guard_prefix + self.api_name(env))
        code.putln(""#define %s"" % Naming.api_guard_prefix + self.api_name(env))
        code.putln(""/* Early includes */"")
        self.generate_includes(env, cimported_modules, code, late=False)
        code.putln("""")
        code.putln(""#if defined(PYREX_WITHOUT_ASSERTIONS) && !defined(CYTHON_WITHOUT_ASSERTIONS)"")
        code.putln(""#define CYTHON_WITHOUT_ASSERTIONS"")
        code.putln(""#endif"")
        code.putln("""")

        if env.directives['ccomplex']:
            code.putln("""")
            code.putln(""#if !defined(CYTHON_CCOMPLEX)"")
            code.putln(""#define CYTHON_CCOMPLEX 1"")
            code.putln(""#endif"")
            code.putln("""")
        code.put(UtilityCode.load_as_string(""UtilityFunctionPredeclarations"", ""ModuleSetupCode.c"")[0])

        c_string_type = env.directives['c_string_type']
        c_string_encoding = env.directives['c_string_encoding']
        if c_string_type not in ('bytes', 'bytearray') and not c_string_encoding:
            error(self.pos, ""a default encoding must be provided if c_string_type is not a byte type"")
        code.putln('#define __PYX_DEFAULT_STRING_ENCODING_IS_ASCII %s' % int(c_string_encoding == 'ascii'))
        if c_string_encoding == 'default':
            code.putln('#define __PYX_DEFAULT_STRING_ENCODING_IS_DEFAULT 1')
        else:
            code.putln('#define __PYX_DEFAULT_STRING_ENCODING_IS_DEFAULT 0')
            code.putln('#define __PYX_DEFAULT_STRING_ENCODING ""%s""' % c_string_encoding)
        if c_string_type == 'bytearray':
            c_string_func_name = 'ByteArray'
        else:
            c_string_func_name = c_string_type.title()
        code.putln('#define __Pyx_PyObject_FromString __Pyx_Py%s_FromString' % c_string_func_name)
        code.putln('#define __Pyx_PyObject_FromStringAndSize __Pyx_Py%s_FromStringAndSize' % c_string_func_name)
        code.put(UtilityCode.load_as_string(""TypeConversions"", ""TypeConversion.c"")[0])

        # These utility functions are assumed to exist and used elsewhere.
        PyrexTypes.c_long_type.create_to_py_utility_code(env)
        PyrexTypes.c_long_type.create_from_py_utility_code(env)
        PyrexTypes.c_int_type.create_from_py_utility_code(env)

        code.put(Nodes.branch_prediction_macros)
        code.putln('static CYTHON_INLINE void __Pyx_pretend_to_initialize(void* ptr) { (void)ptr; }')
        code.putln('')
        code.putln('static PyObject *%s = NULL;' % env.module_cname)
        code.putln('static PyObject *%s;' % env.module_dict_cname)
        code.putln('static PyObject *%s;' % Naming.builtins_cname)
        code.putln('static PyObject *%s = NULL;' % Naming.cython_runtime_cname)
        code.putln('static PyObject *%s;' % Naming.empty_tuple)
        code.putln('static PyObject *%s;' % Naming.empty_bytes)
        code.putln('static PyObject *%s;' % Naming.empty_unicode)
        if Options.pre_import is not None:
            code.putln('static PyObject *%s;' % Naming.preimport_cname)
        code.putln('static int %s;' % Naming.lineno_cname)
        code.putln('static int %s = 0;' % Naming.clineno_cname)
        code.putln('static const char * %s= %s;' % (Naming.cfilenm_cname, Naming.file_c_macro))
        code.putln('static const char *%s;' % Naming.filename_cname)

        env.use_utility_code(UtilityCode.load_cached(""FastTypeChecks"", ""ModuleSetupCode.c""))
        if has_np_pythran(env):
            env.use_utility_code(UtilityCode.load_cached(""PythranConversion"", ""CppSupport.cpp""))","1. Use `Cython.declare` to declare the types of variables. This will help to catch errors early and prevent unexpected behavior.
2. Use `Cython.boundscheck` and `Cython.wraparound` to check for array bounds errors. This will help to prevent security vulnerabilities such as buffer overflows.
3. Use `Cython.cdivision` to disable division by zero. This will help to prevent errors that could crash your program or leak memory."
"    def generate_module_preamble(self, env, options, cimported_modules, metadata, code):
        code.put_generated_by()
        if metadata:
            code.putln(""/* BEGIN: Cython Metadata"")
            code.putln(json.dumps(metadata, indent=4, sort_keys=True))
            code.putln(""END: Cython Metadata */"")
            code.putln("""")
        code.putln(""#define PY_SSIZE_T_CLEAN"")

        for inc in sorted(env.c_includes.values(), key=IncludeCode.sortkey):
            if inc.location == inc.INITIAL:
                inc.write(code)
        code.putln(""#ifndef Py_PYTHON_H"")
        code.putln(""    #error Python headers needed to compile C extensions, ""
                   ""please install development version of Python."")
        code.putln(""#elif PY_VERSION_HEX < 0x02060000 || ""
                   ""(0x03000000 <= PY_VERSION_HEX && PY_VERSION_HEX < 0x03030000)"")
        code.putln(""    #error Cython requires Python 2.6+ or Python 3.3+."")
        code.putln(""#else"")
        code.globalstate[""end""].putln(""#endif /* Py_PYTHON_H */"")

        from .. import __version__
        code.putln('#define CYTHON_ABI ""%s""' % __version__.replace('.', '_'))
        code.putln('#define CYTHON_HEX_VERSION %s' % build_hex_version(__version__))
        code.putln(""#define CYTHON_FUTURE_DIVISION %d"" % (
            Future.division in env.context.future_directives))

        self._put_setup_code(code, ""CModulePreamble"")
        if env.context.options.cplus:
            self._put_setup_code(code, ""CppInitCode"")
        else:
            self._put_setup_code(code, ""CInitCode"")
        self._put_setup_code(code, ""PythonCompatibility"")
        self._put_setup_code(code, ""MathInitCode"")

        if options.c_line_in_traceback:
            cinfo = ""%s = %s; "" % (Naming.clineno_cname, Naming.line_c_macro)
        else:
            cinfo = """"
        code.put(""""""
#define __PYX_ERR(f_index, lineno, Ln_error) \\\\
{ \\\\
  %s = %s[f_index]; %s = lineno; %sgoto Ln_error; \\\\
}
"""""" % (Naming.filename_cname, Naming.filetable_cname, Naming.lineno_cname, cinfo))

        code.putln("""")
        self.generate_extern_c_macro_definition(code)
        code.putln("""")

        code.putln(""#define %s"" % Naming.h_guard_prefix + self.api_name(env))
        code.putln(""#define %s"" % Naming.api_guard_prefix + self.api_name(env))
        code.putln(""/* Early includes */"")
        self.generate_includes(env, cimported_modules, code, late=False)
        code.putln("""")
        code.putln(""#if defined(PYREX_WITHOUT_ASSERTIONS) && !defined(CYTHON_WITHOUT_ASSERTIONS)"")
        code.putln(""#define CYTHON_WITHOUT_ASSERTIONS"")
        code.putln(""#endif"")
        code.putln("""")

        if env.directives['ccomplex']:
            code.putln("""")
            code.putln(""#if !defined(CYTHON_CCOMPLEX)"")
            code.putln(""#define CYTHON_CCOMPLEX 1"")
            code.putln(""#endif"")
            code.putln("""")
        code.put(UtilityCode.load_as_string(""UtilityFunctionPredeclarations"", ""ModuleSetupCode.c"")[0])

        c_string_type = env.directives['c_string_type']
        c_string_encoding = env.directives['c_string_encoding']
        if c_string_type not in ('bytes', 'bytearray') and not c_string_encoding:
            error(self.pos, ""a default encoding must be provided if c_string_type is not a byte type"")
        code.putln('#define __PYX_DEFAULT_STRING_ENCODING_IS_ASCII %s' % int(c_string_encoding == 'ascii'))
        if c_string_encoding == 'default':
            code.putln('#define __PYX_DEFAULT_STRING_ENCODING_IS_DEFAULT 1')
        else:
            code.putln('#define __PYX_DEFAULT_STRING_ENCODING_IS_DEFAULT 0')
            code.putln('#define __PYX_DEFAULT_STRING_ENCODING ""%s""' % c_string_encoding)
        if c_string_type == 'bytearray':
            c_string_func_name = 'ByteArray'
        else:
            c_string_func_name = c_string_type.title()
        code.putln('#define __Pyx_PyObject_FromString __Pyx_Py%s_FromString' % c_string_func_name)
        code.putln('#define __Pyx_PyObject_FromStringAndSize __Pyx_Py%s_FromStringAndSize' % c_string_func_name)
        code.put(UtilityCode.load_as_string(""TypeConversions"", ""TypeConversion.c"")[0])

        # These utility functions are assumed to exist and used elsewhere.
        PyrexTypes.c_long_type.create_to_py_utility_code(env)
        PyrexTypes.c_long_type.create_from_py_utility_code(env)
        PyrexTypes.c_int_type.create_from_py_utility_code(env)

        code.put(Nodes.branch_prediction_macros)
        code.putln('static CYTHON_INLINE void __Pyx_pretend_to_initialize(void* ptr) { (void)ptr; }')
        code.putln('')
        code.putln('static PyObject *%s = NULL;' % env.module_cname)
        code.putln('static PyObject *%s;' % env.module_dict_cname)
        code.putln('static PyObject *%s;' % Naming.builtins_cname)
        code.putln('static PyObject *%s = NULL;' % Naming.cython_runtime_cname)
        code.putln('static PyObject *%s;' % Naming.empty_tuple)
        code.putln('static PyObject *%s;' % Naming.empty_bytes)
        code.putln('static PyObject *%s;' % Naming.empty_unicode)
        if Options.pre_import is not None:
            code.putln('static PyObject *%s;' % Naming.preimport_cname)
        code.putln('static int %s;' % Naming.lineno_cname)
        code.putln('static int %s = 0;' % Naming.clineno_cname)
        code.putln('static const char * %s= %s;' % (Naming.cfilenm_cname, Naming.file_c_macro))
        code.putln('static const char *%s;' % Naming.filename_cname)

        env.use_utility_code(UtilityCode.load_cached(""FastTypeChecks"", ""ModuleSetupCode.c""))
        if has_np_pythran(env):
            env.use_utility_code(UtilityCode.load_cached(""PythranConversion"", ""CppSupport.cpp""))","1. Use `assert` statements to check for invalid input.
2. Sanitize user input before using it in your code.
3. Use strong passwords and security measures to protect your data."
"    def privatize_temps(self, code, exclude_temps=()):
        """"""
        Make any used temporaries private. Before the relevant code block
        code.start_collecting_temps() should have been called.
        """"""
        if self.is_parallel:
            c = self.privatization_insertion_point

            self.temps = temps = code.funcstate.stop_collecting_temps()
            privates, firstprivates = [], []
            for temp, type in sorted(temps):
                if type.is_pyobject or type.is_memoryviewslice:
                    firstprivates.append(temp)
                else:
                    privates.append(temp)

            if privates:
                c.put("" private(%s)"" % "", "".join(privates))
            if firstprivates:
                c.put("" firstprivate(%s)"" % "", "".join(firstprivates))

            if self.breaking_label_used:
                shared_vars = [Naming.parallel_why]
                if self.error_label_used:
                    shared_vars.extend(self.parallel_exc)
                    c.put("" private(%s, %s, %s)"" % self.pos_info)

                c.put("" shared(%s)"" % ', '.join(shared_vars))","1. Make sure that the code is only used in a trusted environment.
2. Use private variables instead of global variables.
3. Use exception handling to catch and handle errors."
"    def end_parallel_block(self, code):
        """"""
        To ensure all OpenMP threads have thread states, we ensure the GIL
        in each thread (which creates a thread state if it doesn't exist),
        after which we release the GIL.
        On exit, reacquire the GIL and release the thread state.

        If compiled without OpenMP support (at the C level), then we still have
        to acquire the GIL to decref any object temporaries.
        """"""
        if self.error_label_used:
            begin_code = self.begin_of_parallel_block
            end_code = code

            begin_code.putln(""#ifdef _OPENMP"")
            begin_code.put_ensure_gil(declare_gilstate=True)
            begin_code.putln(""Py_BEGIN_ALLOW_THREADS"")
            begin_code.putln(""#endif /* _OPENMP */"")

            end_code.putln(""#ifdef _OPENMP"")
            end_code.putln(""Py_END_ALLOW_THREADS"")
            end_code.putln(""#else"")
            end_code.put_safe(""{\\n"")
            end_code.put_ensure_gil()
            end_code.putln(""#endif /* _OPENMP */"")
            self.cleanup_temps(end_code)
            end_code.put_release_ensured_gil()
            end_code.putln(""#ifndef _OPENMP"")
            end_code.put_safe(""}\\n"")
            end_code.putln(""#endif /* _OPENMP */"")","1. Use `ensure_gil()` to ensure the GIL is acquired before entering the parallel block.
2. Use `release_gil()` to release the GIL after exiting the parallel block.
3. Use `cleanup_temps()` to clean up any temporary objects created in the parallel block."
"    def end_parallel_control_flow_block(
            self, code, break_=False, continue_=False, return_=False):
        """"""
        This ends the parallel control flow block and based on how the parallel
        section was exited, takes the corresponding action. The break_ and
        continue_ parameters indicate whether these should be propagated
        outwards:

            for i in prange(...):
                with cython.parallel.parallel():
                    continue

        Here break should be trapped in the parallel block, and propagated to
        the for loop.
        """"""
        c = self.begin_of_parallel_control_block_point

        # Firstly, always prefer errors over returning, continue or break
        if self.error_label_used:
            c.putln(""const char *%s = NULL; int %s = 0, %s = 0;"" % self.parallel_pos_info)
            c.putln(""PyObject *%s = NULL, *%s = NULL, *%s = NULL;"" % self.parallel_exc)

            code.putln(
                ""if (%s) {"" % Naming.parallel_exc_type)
            code.putln(""/* This may have been overridden by a continue, ""
                       ""break or return in another thread. Prefer the error. */"")
            code.putln(""%s = 4;"" % Naming.parallel_why)
            code.putln(
                ""}"")

        if continue_:
            any_label_used = self.any_label_used
        else:
            any_label_used = self.breaking_label_used

        if any_label_used:
            # __pyx_parallel_why is used, declare and initialize
            c.putln(""int %s;"" % Naming.parallel_why)
            c.putln(""%s = 0;"" % Naming.parallel_why)

            code.putln(
                ""if (%s) {"" % Naming.parallel_why)

            for temp_cname, private_cname in self.parallel_private_temps:
                code.putln(""%s = %s;"" % (private_cname, temp_cname))

            code.putln(""switch (%s) {"" % Naming.parallel_why)
            if continue_:
                code.put(""    case 1: "")
                code.put_goto(code.continue_label)

            if break_:
                code.put(""    case 2: "")
                code.put_goto(code.break_label)

            if return_:
                code.put(""    case 3: "")
                code.put_goto(code.return_label)

            if self.error_label_used:
                code.globalstate.use_utility_code(restore_exception_utility_code)
                code.putln(""    case 4:"")
                self.restore_parallel_exception(code)
                code.put_goto(code.error_label)

            code.putln(""}"") # end switch
            code.putln(
                ""}"") # end if

        code.end_block() # end parallel control flow block
        self.redef_builtin_expect_apple_gcc_bug(code)","1. Use `c.putln()` instead of `print()` to prevent information disclosure.
2. Use `code.put_goto()` instead of `goto` to prevent undefined behavior.
3. Use `code.end_block()` to prevent memory leaks."
"    def generate_execution_code(self, code):
        """"""
        Generate code in the following steps

            1)  copy any closure variables determined thread-private
                into temporaries

            2)  allocate temps for start, stop and step

            3)  generate a loop that calculates the total number of steps,
                which then computes the target iteration variable for every step:

                    for i in prange(start, stop, step):
                        ...

                becomes

                    nsteps = (stop - start) / step;
                    i = start;

                    #pragma omp parallel for lastprivate(i)
                    for (temp = 0; temp < nsteps; temp++) {
                        i = start + step * temp;
                        ...
                    }

                Note that accumulation of 'i' would have a data dependency
                between iterations.

                Also, you can't do this

                    for (i = start; i < stop; i += step)
                        ...

                as the '<' operator should become '>' for descending loops.
                'for i from x < i < y:' does not suffer from this problem
                as the relational operator is known at compile time!

            4) release our temps and write back any private closure variables
        """"""
        self.declare_closure_privates(code)

        # This can only be a NameNode
        target_index_cname = self.target.entry.cname

        # This will be used as the dict to format our code strings, holding
        # the start, stop , step, temps and target cnames
        fmt_dict = {
            'target': target_index_cname,
            'target_type': self.target.type.empty_declaration_code()
        }

        # Setup start, stop and step, allocating temps if needed
        start_stop_step = self.start, self.stop, self.step
        defaults = '0', '0', '1'
        for node, name, default in zip(start_stop_step, self.names, defaults):
            if node is None:
                result = default
            elif node.is_literal:
                result = node.get_constant_c_result_code()
            else:
                node.generate_evaluation_code(code)
                result = node.result()

            fmt_dict[name] = result

        fmt_dict['i'] = code.funcstate.allocate_temp(self.index_type, False)
        fmt_dict['nsteps'] = code.funcstate.allocate_temp(self.index_type, False)

        # TODO: check if the step is 0 and if so, raise an exception in a
        # 'with gil' block. For now, just abort
        code.putln(""if (%(step)s == 0) abort();"" % fmt_dict)

        self.setup_parallel_control_flow_block(code) # parallel control flow block

        self.control_flow_var_code_point = code.insertion_point()

        # Note: nsteps is private in an outer scope if present
        code.putln(""%(nsteps)s = (%(stop)s - %(start)s + %(step)s - %(step)s/abs(%(step)s)) / %(step)s;"" % fmt_dict)

        # The target iteration variable might not be initialized, do it only if
        # we are executing at least 1 iteration, otherwise we should leave the
        # target unaffected. The target iteration variable is firstprivate to
        # shut up compiler warnings caused by lastprivate, as the compiler
        # erroneously believes that nsteps may be <= 0, leaving the private
        # target index uninitialized
        code.putln(""if (%(nsteps)s > 0)"" % fmt_dict)
        code.begin_block() # if block
        self.generate_loop(code, fmt_dict)
        code.end_block() # end if block

        self.restore_labels(code)

        if self.else_clause:
            if self.breaking_label_used:
                code.put(""if (%s < 2)"" % Naming.parallel_why)

            code.begin_block() # else block
            code.putln(""/* else */"")
            self.else_clause.generate_execution_code(code)
            code.end_block() # end else block

        # ------ cleanup ------
        self.end_parallel_control_flow_block(code) # end parallel control flow block

        # And finally, release our privates and write back any closure
        # variables
        for temp in start_stop_step + (self.chunksize, self.num_threads):
            if temp is not None:
                temp.generate_disposal_code(code)
                temp.free_temps(code)

        code.funcstate.release_temp(fmt_dict['i'])
        code.funcstate.release_temp(fmt_dict['nsteps'])

        self.release_closure_privates(code)","1. Use `functools.wraps` to preserve the original function's metadata, such as its name, docstring, and annotations.
2. Use `inspect.iscoroutinefunction` to check if the function is a coroutine function.
3. Use `asyncio.run` to run the coroutine function."
"    def generate_assignment_code(self, rhs, code, overloaded_assignment=False,
        exception_check=None, exception_value=None):
        self.generate_subexpr_evaluation_code(code)

        if self.type.is_pyobject:
            self.generate_setitem_code(rhs.py_result(), code)
        elif self.base.type is bytearray_type:
            value_code = self._check_byte_value(code, rhs)
            self.generate_setitem_code(value_code, code)
        elif self.base.type.is_cpp_class and self.exception_check and self.exception_check == '+':
            if overloaded_assignment and exception_check and \\
                self.exception_value != exception_value:
                # Handle the case that both the index operator and the assignment
                # operator have a c++ exception handler and they are not the same.
                translate_double_cpp_exception(code, self.pos, self.type,
                    self.result(), rhs.result(), self.exception_value,
                    exception_value, self.in_nogil_context)
            else:
                # Handle the case that only the index operator has a
                # c++ exception handler, or that
                # both exception handlers are the same.
                translate_cpp_exception(code, self.pos,
                    ""%s = %s;"" % (self.result(), rhs.result()),
                    self.result() if self.lhs.is_pyobject else None,
                    self.exception_value, self.in_nogil_context)
        else:
            code.putln(
                ""%s = %s;"" % (self.result(), rhs.result()))

        self.generate_subexpr_disposal_code(code)
        self.free_subexpr_temps(code)
        rhs.generate_disposal_code(code)
        rhs.free_temps(code)","1. Use `py_result()` to get the Python object from a PyPyValue.
2. Use `check_byte_value()` to check if a value is a bytearray.
3. Use `translate_cpp_exception()` to handle C++ exceptions."
"    def _handle_simple_method_dict_pop(self, node, function, args, is_unbound_method):
        """"""Replace dict.pop() by a call to _PyDict_Pop().
        """"""
        if len(args) == 2:
            args.append(ExprNodes.NullNode(node.pos))
        elif len(args) != 3:
            self._error_wrong_arg_count('dict.pop', node, args, ""2 or 3"")
            return node

        return self._substitute_method_call(
            node, function,
            ""__Pyx_PyDict_Pop"", self.PyDict_Pop_func_type,
            'pop', is_unbound_method, args,
            utility_code=load_c_utility('py_dict_pop'))","1. Use `is_unbound_method` to check if the method is called on a class or an instance.
2. Use `len(args)` to check if the number of arguments is correct.
3. Use `_error_wrong_arg_count` to raise an error if the number of arguments is incorrect."
"    def generate_type_ready_code(self, env, entry, code):
        # Generate a call to PyType_Ready for an extension
        # type defined in this module.
        type = entry.type
        typeobj_cname = type.typeobj_cname
        scope = type.scope
        if scope: # could be None if there was an error
            if entry.visibility != 'extern':
                for slot in TypeSlots.slot_table:
                    slot.generate_dynamic_init_code(scope, code)
                code.putln(
                    ""if (PyType_Ready(&%s) < 0) %s"" % (
                        typeobj_cname,
                        code.error_goto(entry.pos)))
                # Don't inherit tp_print from builtin types, restoring the
                # behavior of using tp_repr or tp_str instead.
                code.putln(""%s.tp_print = 0;"" % typeobj_cname)
                # Fix special method docstrings. This is a bit of a hack, but
                # unless we let PyType_Ready create the slot wrappers we have
                # a significant performance hit. (See trac #561.)
                for func in entry.type.scope.pyfunc_entries:
                    is_buffer = func.name in ('__getbuffer__', '__releasebuffer__')
                    if (func.is_special and Options.docstrings and
                            func.wrapperbase_cname and not is_buffer):
                        slot = TypeSlots.method_name_to_slot[func.name]
                        preprocessor_guard = slot.preprocessor_guard_code()
                        if preprocessor_guard:
                            code.putln(preprocessor_guard)
                        code.putln('#if CYTHON_COMPILING_IN_CPYTHON')
                        code.putln(""{"")
                        code.putln(
                            'PyObject *wrapper = PyObject_GetAttrString((PyObject *)&%s, ""%s""); %s' % (
                                typeobj_cname,
                                func.name,
                                code.error_goto_if_null('wrapper', entry.pos)))
                        code.putln(
                            ""if (Py_TYPE(wrapper) == &PyWrapperDescr_Type) {"")
                        code.putln(
                            ""%s = *((PyWrapperDescrObject *)wrapper)->d_base;"" % (
                                func.wrapperbase_cname))
                        code.putln(
                            ""%s.doc = %s;"" % (func.wrapperbase_cname, func.doc_cname))
                        code.putln(
                            ""((PyWrapperDescrObject *)wrapper)->d_base = &%s;"" % (
                                func.wrapperbase_cname))
                        code.putln(""}"")
                        code.putln(""}"")
                        code.putln('#endif')
                        if preprocessor_guard:
                            code.putln('#endif')
                if type.vtable_cname:
                    code.putln(
                        ""if (__Pyx_SetVtable(%s.tp_dict, %s) < 0) %s"" % (
                            typeobj_cname,
                            type.vtabptr_cname,
                            code.error_goto(entry.pos)))
                    code.globalstate.use_utility_code(
                        UtilityCode.load_cached('SetVTable', 'ImportExport.c'))
                if not type.scope.is_internal and not type.scope.directives['internal']:
                    # scope.is_internal is set for types defined by
                    # Cython (such as closures), the 'internal'
                    # directive is set by users
                    code.putln(
                        'if (PyObject_SetAttrString(%s, ""%s"", (PyObject *)&%s) < 0) %s' % (
                            Naming.module_cname,
                            scope.class_name,
                            typeobj_cname,
                            code.error_goto(entry.pos)))
                weakref_entry = scope.lookup_here(""__weakref__"") if not scope.is_closure_class_scope else None
                if weakref_entry:
                    if weakref_entry.type is py_object_type:
                        tp_weaklistoffset = ""%s.tp_weaklistoffset"" % typeobj_cname
                        if type.typedef_flag:
                            objstruct = type.objstruct_cname
                        else:
                            objstruct = ""struct %s"" % type.objstruct_cname
                        code.putln(""if (%s == 0) %s = offsetof(%s, %s);"" % (
                            tp_weaklistoffset,
                            tp_weaklistoffset,
                            objstruct,
                            weakref_entry.cname))
                    else:
                        error(weakref_entry.pos, ""__weakref__ slot must be of type 'object'"")
                if scope.lookup_here(""__reduce_cython__"") if not scope.is_closure_class_scope else None:
                    # Unfortunately, we cannot reliably detect whether a
                    # superclass defined __reduce__ at compile time, so we must
                    # do so at runtime.
                    code.globalstate.use_utility_code(
                        UtilityCode.load_cached('SetupReduce', 'ExtensionTypes.c'))
                    code.putln('if (__Pyx_setup_reduce((PyObject*)&%s) < 0) %s' % (
                                  typeobj_cname,
                                  code.error_goto(entry.pos)))","1. Use a `Py_TYPE` check to make sure that the object is of the correct type before calling any methods on it.
2. Use `Py_DECREF` to decrement the reference count of any objects that you create.
3. Use `Py_RETURN_NONE` to return `None` from functions that don't return anything."
"def create_extension_list(patterns, exclude=None, ctx=None, aliases=None, quiet=False, language=None,
                          exclude_failures=False):
    if language is not None:
        print('Please put ""# distutils: language=%s"" in your .pyx or .pxd file(s)' % language)
    if exclude is None:
        exclude = []
    if patterns is None:
        return [], {}
    elif isinstance(patterns, basestring) or not isinstance(patterns, collections.Iterable):
        patterns = [patterns]
    explicit_modules = set([m.name for m in patterns if isinstance(m, Extension)])
    seen = set()
    deps = create_dependency_tree(ctx, quiet=quiet)
    to_exclude = set()
    if not isinstance(exclude, list):
        exclude = [exclude]
    for pattern in exclude:
        to_exclude.update(map(os.path.abspath, extended_iglob(pattern)))

    module_list = []
    module_metadata = {}

    # workaround for setuptools
    if 'setuptools' in sys.modules:
        Extension_distutils = sys.modules['setuptools.extension']._Extension
        Extension_setuptools = sys.modules['setuptools'].Extension
    else:
        # dummy class, in case we do not have setuptools
        Extension_distutils = Extension
        class Extension_setuptools(Extension): pass

    # if no create_extension() function is defined, use a simple
    # default function.
    create_extension = ctx.options.create_extension or default_create_extension

    for pattern in patterns:
        if isinstance(pattern, str):
            filepattern = pattern
            template = Extension(pattern, [])  # Fake Extension without sources
            name = '*'
            base = None
            ext_language = language
        elif isinstance(pattern, (Extension_distutils, Extension_setuptools)):
            cython_sources = [s for s in pattern.sources
                              if os.path.splitext(s)[1] in ('.py', '.pyx')]
            if cython_sources:
              filepattern = cython_sources[0]
              if len(cython_sources) > 1:
                print(""Warning: Multiple cython sources found for extension '%s': %s\\n""
                ""See http://cython.readthedocs.io/en/latest/src/userguide/sharing_declarations.html ""
                ""for sharing declarations among Cython files."" % (pattern.name, cython_sources))
            else:
                # ignore non-cython modules
                module_list.append(pattern)
                continue
            template = pattern
            name = template.name
            base = DistutilsInfo(exn=template)
            ext_language = None  # do not override whatever the Extension says
        else:
            msg = str(""pattern is not of type str nor subclass of Extension (%s)""
                      "" but of type %s and class %s"" % (repr(Extension),
                                                        type(pattern),
                                                        pattern.__class__))
            raise TypeError(msg)

        for file in nonempty(sorted(extended_iglob(filepattern)), ""'%s' doesn't match any files"" % filepattern):
            if os.path.abspath(file) in to_exclude:
                continue
            pkg = deps.package(file)
            module_name = deps.fully_qualified_name(file)
            if '*' in name:
                if module_name in explicit_modules:
                    continue
            elif name != module_name:
                print(""Warning: Extension name '%s' does not match fully qualified name '%s' of '%s'"" % (
                    name, module_name, file))
                module_name = name

            if module_name not in seen:
                try:
                    kwds = deps.distutils_info(file, aliases, base).values
                except Exception:
                    if exclude_failures:
                        continue
                    raise
                if base is not None:
                    for key, value in base.values.items():
                        if key not in kwds:
                            kwds[key] = value

                kwds['name'] = module_name

                sources = [file] + [m for m in template.sources if m != filepattern]
                if 'sources' in kwds:
                    # allow users to add .c files etc.
                    for source in kwds['sources']:
                        source = encode_filename_in_py2(source)
                        if source not in sources:
                            sources.append(source)
                kwds['sources'] = sources

                if ext_language and 'language' not in kwds:
                    kwds['language'] = ext_language

                np_pythran = kwds.pop('np_pythran', False)

                # Create the new extension
                m, metadata = create_extension(template, kwds)
                m.np_pythran = np_pythran or getattr(m, 'np_pythran', False)
                if m.np_pythran:
                    update_pythran_extension(m)
                module_list.append(m)

                # Store metadata (this will be written as JSON in the
                # generated C file but otherwise has no purpose)
                module_metadata[module_name] = metadata

                if file not in m.sources:
                    # Old setuptools unconditionally replaces .pyx with .c/.cpp
                    target_file = file.rsplit('.')[0] + ('.cpp' if m.language == 'c++' else '.c')
                    try:
                        m.sources.remove(target_file)
                    except ValueError:
                        # never seen this in the wild, but probably better to warn about this unexpected case
                        print(""Warning: Cython source file not found in sources list, adding %s"" % file)
                    m.sources.insert(0, file)
                seen.add(name)
    return module_list, module_metadata","1. Use `encode_filename_in_py2` to encode file names to avoid directory traversal attacks.
2. Use `create_extension` to create new extensions and avoid using `Extension` directly.
3. Use `np_pythran` to enable Pythran for Cython extensions."
"def create_extension_list(patterns, exclude=None, ctx=None, aliases=None, quiet=False, language=None,
                          exclude_failures=False):
    if language is not None:
        print('Please put ""# distutils: language=%s"" in your .pyx or .pxd file(s)' % language)
    if exclude is None:
        exclude = []
    if patterns is None:
        return [], {}
    elif isinstance(patterns, basestring) or not isinstance(patterns, collections.Iterable):
        patterns = [patterns]
    explicit_modules = set([m.name for m in patterns if isinstance(m, Extension)])
    seen = set()
    deps = create_dependency_tree(ctx, quiet=quiet)
    to_exclude = set()
    if not isinstance(exclude, list):
        exclude = [exclude]
    for pattern in exclude:
        to_exclude.update(map(os.path.abspath, extended_iglob(pattern)))

    module_list = []
    module_metadata = {}

    # workaround for setuptools
    if 'setuptools' in sys.modules:
        Extension_distutils = sys.modules['setuptools.extension']._Extension
        Extension_setuptools = sys.modules['setuptools'].Extension
    else:
        # dummy class, in case we do not have setuptools
        Extension_distutils = Extension
        class Extension_setuptools(Extension): pass

    # if no create_extension() function is defined, use a simple
    # default function.
    create_extension = ctx.options.create_extension or default_create_extension

    for pattern in patterns:
        if isinstance(pattern, str):
            filepattern = pattern
            template = Extension(pattern, [])  # Fake Extension without sources
            name = '*'
            base = None
            ext_language = language
        elif isinstance(pattern, (Extension_distutils, Extension_setuptools)):
            cython_sources = [s for s in pattern.sources
                              if os.path.splitext(s)[1] in ('.py', '.pyx')]
            if cython_sources:
              filepattern = cython_sources[0]
              if len(cython_sources) > 1:
                print(""Warning: Multiple cython sources found for extension '%s': %s\\n""
                ""See http://cython.readthedocs.io/en/latest/src/userguide/sharing_declarations.html ""
                ""for sharing declarations among Cython files."" % (pattern.name, cython_sources))
            else:
                # ignore non-cython modules
                module_list.append(pattern)
                continue
            template = pattern
            name = template.name
            base = DistutilsInfo(exn=template)
            ext_language = None  # do not override whatever the Extension says
        else:
            msg = str(""pattern is not of type str nor subclass of Extension (%s)""
                      "" but of type %s and class %s"" % (repr(Extension),
                                                        type(pattern),
                                                        pattern.__class__))
            raise TypeError(msg)

        for file in nonempty(sorted(extended_iglob(filepattern)), ""'%s' doesn't match any files"" % filepattern):
            if os.path.abspath(file) in to_exclude:
                continue
            pkg = deps.package(file)
            module_name = deps.fully_qualified_name(file)
            if '*' in name:
                if module_name in explicit_modules:
                    continue
            elif name != module_name:
                print(""Warning: Extension name '%s' does not match fully qualified name '%s' of '%s'"" % (
                    name, module_name, file))
                module_name = name

            if module_name not in seen:
                try:
                    kwds = deps.distutils_info(file, aliases, base).values
                except Exception:
                    if exclude_failures:
                        continue
                    raise
                if base is not None:
                    for key, value in base.values.items():
                        if key not in kwds:
                            kwds[key] = value

                kwds['name'] = module_name

                sources = [file] + [m for m in template.sources if m != filepattern]
                if 'sources' in kwds:
                    # allow users to add .c files etc.
                    for source in kwds['sources']:
                        source = encode_filename_in_py2(source)
                        if source not in sources:
                            sources.append(source)
                kwds['sources'] = sources

                if ext_language and 'language' not in kwds:
                    kwds['language'] = ext_language

                np_pythran = kwds.pop('np_pythran', False)

                # Create the new extension
                m, metadata = create_extension(template, kwds)
                m.np_pythran = np_pythran or getattr(m, 'np_pythran', False)
                if m.np_pythran:
                    update_pythran_extension(m)
                module_list.append(m)

                # Store metadata (this will be written as JSON in the
                # generated C file but otherwise has no purpose)
                module_metadata[module_name] = metadata

                if file not in m.sources:
                    # Old setuptools unconditionally replaces .pyx with .c/.cpp
                    m.sources.remove(file.rsplit('.')[0] + ('.cpp' if m.language == 'c++' else '.c'))
                    m.sources.insert(0, file)
                seen.add(name)
    return module_list, module_metadata","1. Use `encode_filename_in_py2` to prevent directory traversal attacks.
2. Use `create_extension` to create new extensions and avoid `setuptools` unconditionally replacing `.pyx` with `.c/.cpp`.
3. Use `update_pythran_extension` to update the pythran extension."
"def get_if_raw_addr(ifname):
    """"""Returns the IPv4 address configured on 'ifname', packed with inet_pton.""""""  # noqa: E501

    ifname = network_name(ifname)

    # Get ifconfig output
    subproc = subprocess.Popen(
        [conf.prog.ifconfig, ifname],
        close_fds=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )
    stdout, stderr = subproc.communicate()
    if subproc.returncode:
        warning(""Failed to execute ifconfig: (%s)"", plain_str(stderr))
        return b""\\0\\0\\0\\0""
    # Get IPv4 addresses

    addresses = [
        line.strip() for line in plain_str(stdout).splitlines()
        if ""inet "" in line
    ]

    if not addresses:
        warning(""No IPv4 address found on %s !"", ifname)
        return b""\\0\\0\\0\\0""

    # Pack the first address
    address = addresses[0].split(' ')[1]
    if '/' in address:  # NetBSD 8.0
        address = address.split(""/"")[0]
    return socket.inet_pton(socket.AF_INET, address)","1. Use `subprocess.check_output` instead of `subprocess.Popen` to avoid leaking the `stderr` output.
2. Sanitize the input to `plain_str` to prevent command injection.
3. Use `socket.inet_pton` with the correct address family to avoid address spoofing."
"def get_if_raw_hwaddr(ifname):
    """"""Returns the packed MAC address configured on 'ifname'.""""""

    NULL_MAC_ADDRESS = b'\\x00' * 6

    ifname = network_name(ifname)
    # Handle the loopback interface separately
    if ifname == conf.loopback_name:
        return (ARPHDR_LOOPBACK, NULL_MAC_ADDRESS)

    # Get ifconfig output
    subproc = subprocess.Popen(
        [conf.prog.ifconfig, ifname],
        close_fds=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )
    stdout, stderr = subproc.communicate()
    if subproc.returncode:
        raise Scapy_Exception(""Failed to execute ifconfig: (%s)"" %
                              (plain_str(stderr)))

    # Get MAC addresses
    addresses = [
        line.strip() for line in plain_str(stdout).splitlines() if (
            ""ether"" in line or ""lladdr"" in line or ""address"" in line
        )
    ]
    if not addresses:
        raise Scapy_Exception(""No MAC address found on %s !"" % ifname)

    # Pack and return the MAC address
    mac = addresses[0].split(' ')[1]
    mac = [chr(int(b, 16)) for b in mac.split(':')]
    return (ARPHDR_ETHER, ''.join(mac))","1. Use scapy.utils.conf.raw_hex() to convert the MAC address to a byte string.
2. Use scapy.utils.hexdump() to print the MAC address in hexadecimal.
3. Use scapy.utils.checksum() to calculate the checksum of the MAC address."
"def read_routes():
    # type: () -> List[Tuple[int, int, str, str, str, int]]
    """"""Return a list of IPv4 routes than can be used by Scapy.

    This function parses netstat.
    """"""
    if SOLARIS:
        f = os.popen(""netstat -rvn -f inet"")
    elif FREEBSD:
        f = os.popen(""netstat -rnW -f inet"")  # -W to show long interface names
    else:
        f = os.popen(""netstat -rn -f inet"")
    ok = 0
    mtu_present = False
    prio_present = False
    refs_present = False
    use_present = False
    routes = []  # type: List[Tuple[int, int, str, str, str, int]]
    pending_if = []  # type: List[Tuple[int, int, str]]
    for line in f.readlines():
        if not line:
            break
        line = line.strip().lower()
        if line.find(""----"") >= 0:  # a separation line
            continue
        if not ok:
            if line.find(""destination"") >= 0:
                ok = 1
                mtu_present = ""mtu"" in line
                prio_present = ""prio"" in line
                refs_present = ""ref"" in line  # There is no s on Solaris
                use_present = ""use"" in line or ""nhop"" in line
            continue
        if not line:
            break
        rt = line.split()
        if SOLARIS:
            dest_, netmask_, gw, netif = rt[:4]
            flg = rt[4 + mtu_present + refs_present]
        else:
            dest_, gw, flg = rt[:3]
            locked = OPENBSD and rt[6] == ""l""
            offset = mtu_present + prio_present + refs_present + locked
            offset += use_present
            netif = rt[3 + offset]
        if flg.find(""lc"") >= 0:
            continue
        elif dest_ == ""default"":
            dest = 0
            netmask = 0
        elif SOLARIS:
            dest = scapy.utils.atol(dest_)
            netmask = scapy.utils.atol(netmask_)
        else:
            if ""/"" in dest_:
                dest_, netmask_ = dest_.split(""/"")
                netmask = scapy.utils.itom(int(netmask_))
            else:
                netmask = scapy.utils.itom((dest_.count(""."") + 1) * 8)
            dest_ += "".0"" * (3 - dest_.count("".""))
            dest = scapy.utils.atol(dest_)
        # XXX: TODO: add metrics for unix.py (use -e option on netstat)
        metric = 1
        if ""g"" not in flg:
            gw = '0.0.0.0'
        if netif is not None:
            from scapy.arch import get_if_addr
            try:
                ifaddr = get_if_addr(netif)
                routes.append((dest, netmask, gw, netif, ifaddr, metric))
            except OSError as exc:
                if 'Device not configured' in str(exc):
                    # This means the interface name is probably truncated by
                    # netstat -nr. We attempt to guess it's name and if not we
                    # ignore it.
                    guessed_netif = _guess_iface_name(netif)
                    if guessed_netif is not None:
                        ifaddr = get_if_addr(guessed_netif)
                        routes.append((dest, netmask, gw, guessed_netif, ifaddr, metric))  # noqa: E501
                    else:
                        log_runtime.info(
                            ""Could not guess partial interface name: %s"",
                            netif
                        )
                else:
                    raise
        else:
            pending_if.append((dest, netmask, gw))
    f.close()

    # On Solaris, netstat does not provide output interfaces for some routes
    # We need to parse completely the routing table to route their gw and
    # know their output interface
    for dest, netmask, gw in pending_if:
        gw_l = scapy.utils.atol(gw)
        max_rtmask, gw_if, gw_if_addr = 0, None, None
        for rtdst, rtmask, _, rtif, rtaddr, _ in routes[:]:
            if gw_l & rtmask == rtdst:
                if rtmask >= max_rtmask:
                    max_rtmask = rtmask
                    gw_if = rtif
                    gw_if_addr = rtaddr
        # XXX: TODO add metrics
        metric = 1
        if gw_if and gw_if_addr:
            routes.append((dest, netmask, gw, gw_if, gw_if_addr, metric))
        else:
            warning(""Did not find output interface to reach gateway %s"", gw)

    return routes","1. Use scapy.utils.check_ip() to validate IP addresses.
2. Use scapy.utils.check_mask() to validate netmasks.
3. Use scapy.utils.check_gateway() to validate gateways."
"    def __init__(self, iface=None, type=ETH_P_ALL, promisc=None, filter=None,
                 nofilter=0, monitor=False):
        self.fd_flags = None
        self.assigned_interface = None

        # SuperSocket mandatory variables
        if promisc is None:
            self.promisc = conf.sniff_promisc
        else:
            self.promisc = promisc

        self.iface = network_name(iface or conf.iface)

        # Get the BPF handle
        (self.ins, self.dev_bpf) = get_dev_bpf()
        self.outs = self.ins

        # Set the BPF buffer length
        try:
            fcntl.ioctl(self.ins, BIOCSBLEN, struct.pack('I', BPF_BUFFER_LENGTH))  # noqa: E501
        except IOError:
            raise Scapy_Exception(""BIOCSBLEN failed on /dev/bpf%i"" %
                                  self.dev_bpf)

        # Assign the network interface to the BPF handle
        try:
            fcntl.ioctl(self.ins, BIOCSETIF, struct.pack(""16s16x"", self.iface.encode()))  # noqa: E501
        except IOError:
            raise Scapy_Exception(""BIOCSETIF failed on %s"" % self.iface)
        self.assigned_interface = self.iface

        # Set the interface into promiscuous
        if self.promisc:
            self.set_promisc(1)

        # Set the interface to monitor mode
        # Note: - trick from libpcap/pcap-bpf.c - monitor_mode()
        #       - it only works on OS X 10.5 and later
        if DARWIN and monitor:
            dlt_radiotap = struct.pack('I', DLT_IEEE802_11_RADIO)
            try:
                fcntl.ioctl(self.ins, BIOCSDLT, dlt_radiotap)
            except IOError:
                raise Scapy_Exception(""Can't set %s into monitor mode!"" %
                                      self.iface)

        # Don't block on read
        try:
            fcntl.ioctl(self.ins, BIOCIMMEDIATE, struct.pack('I', 1))
        except IOError:
            raise Scapy_Exception(""BIOCIMMEDIATE failed on /dev/bpf%i"" %
                                  self.dev_bpf)

        # Scapy will provide the link layer source address
        # Otherwise, it is written by the kernel
        try:
            fcntl.ioctl(self.ins, BIOCSHDRCMPLT, struct.pack('i', 1))
        except IOError:
            raise Scapy_Exception(""BIOCSHDRCMPLT failed on /dev/bpf%i"" %
                                  self.dev_bpf)

        # Configure the BPF filter
        if not nofilter:
            if conf.except_filter:
                if filter:
                    filter = ""(%s) and not (%s)"" % (filter, conf.except_filter)
                else:
                    filter = ""not (%s)"" % conf.except_filter
            if filter is not None:
                try:
                    attach_filter(self.ins, filter, self.iface)
                except ImportError as ex:
                    warning(""Cannot set filter: %s"" % ex)

        # Set the guessed packet class
        self.guessed_cls = self.guess_cls()","1. Use scapy.sniff() instead of scapy.BpfFilter() to avoid
    potential buffer overflows.
2. Use scapy.filter() to filter packets instead of scapy.BpfFilter().
3. Use scapy.hexdump() to debug packets instead of scapy.BpfFilter()."
"    def tls_session_update(self, msg_str):
        """"""
        Either for parsing or building, we store the client_random
        along with the raw string representing this handshake message.
        """"""
        super(TLSClientHello, self).tls_session_update(msg_str)
        s = self.tls_session
        s.advertised_tls_version = self.version
        # This ClientHello could be a 1.3 one. Let's store the sid
        # in all cases
        if self.sidlen and self.sidlen > 0:
            s.sid = self.sid
        self.random_bytes = msg_str[10:38]
        s.client_random = (struct.pack('!I', self.gmt_unix_time) +
                           self.random_bytes)

        # No distinction between a TLS 1.2 ClientHello and a TLS
        # 1.3 ClientHello when dissecting : TLS 1.3 CH will be
        # parsed as TLSClientHello
        if self.ext:
            for e in self.ext:
                if isinstance(e, TLS_Ext_SupportedVersion_CH):
                    for ver in e.versions:
                        # RFC 8701: GREASE of TLS will send unknown versions
                        # here. We have to ignore them
                        if ver in _tls_version:
                            s.advertised_tls_version = ver
                            break
                    if s.sid:
                        s.middlebox_compatibility = True

                if isinstance(e, TLS_Ext_SignatureAlgorithms):
                    s.advertised_sig_algs = e.sig_algs","1. Use `struct.pack()` to pack data into binary format instead of hard-coding it.
2. Use `struct.unpack()` to unpack data from binary format instead of hard-coding it.
3. Use `bytes()` to convert a string to bytes instead of hard-coding it."
"    def tls_session_update(self, msg_str):
        """"""
        Either for parsing or building, we store the client_random
        along with the raw string representing this handshake message.
        """"""
        super(TLS13ClientHello, self).tls_session_update(msg_str)
        s = self.tls_session

        if self.sidlen and self.sidlen > 0:
            s.sid = self.sid
            s.middlebox_compatibility = True

        self.random_bytes = msg_str[10:38]
        s.client_random = self.random_bytes
        if self.ext:
            for e in self.ext:
                if isinstance(e, TLS_Ext_SupportedVersion_CH):
                    for ver in e.versions:
                        # RFC 8701: GREASE of TLS will send unknown versions
                        # here. We have to ignore them
                        if ver in _tls_version:
                            self.tls_session.advertised_tls_version = ver
                            break
                if isinstance(e, TLS_Ext_SignatureAlgorithms):
                    s.advertised_sig_algs = e.sig_algs","1. Use TLS 1.3 instead of TLS 1.2.
2. Use strong cipher suites and key lengths.
3. Use a secure random number generator."
"    def tls_session_update(self, msg_str):
        """"""
        Either for parsing or building, we store the server_random
        along with the raw string representing this handshake message.
        We also store the session_id, the cipher suite (if recognized),
        the compression method, and finally we instantiate the pending write
        and read connection states. Usually they get updated later on in the
        negotiation when we learn the session keys, and eventually they
        are committed once a ChangeCipherSpec has been sent/received.
        """"""
        super(TLSServerHello, self).tls_session_update(msg_str)

        self.tls_session.tls_version = self.version
        self.random_bytes = msg_str[10:38]
        self.tls_session.server_random = (struct.pack('!I',
                                                      self.gmt_unix_time) +
                                          self.random_bytes)
        self.tls_session.sid = self.sid

        cs_cls = None
        if self.cipher:
            cs_val = self.cipher
            if cs_val not in _tls_cipher_suites_cls:
                warning(""Unknown cipher suite %d from ServerHello"" % cs_val)
                # we do not try to set a default nor stop the execution
            else:
                cs_cls = _tls_cipher_suites_cls[cs_val]

        comp_cls = Comp_NULL
        if self.comp:
            comp_val = self.comp[0]
            if comp_val not in _tls_compression_algs_cls:
                err = ""Unknown compression alg %d from ServerHello"" % comp_val
                warning(err)
                comp_val = 0
            comp_cls = _tls_compression_algs_cls[comp_val]

        connection_end = self.tls_session.connection_end
        self.tls_session.pwcs = writeConnState(ciphersuite=cs_cls,
                                               compression_alg=comp_cls,
                                               connection_end=connection_end,
                                               tls_version=self.version)
        self.tls_session.prcs = readConnState(ciphersuite=cs_cls,
                                              compression_alg=comp_cls,
                                              connection_end=connection_end,
                                              tls_version=self.version)","1. Use a secure cipher suite and compression method.
2. Check the validity of the session ID.
3. Use a strong TLS version."
"    def tls_session_update(self, msg_str):
        """"""
        Either for parsing or building, we store the server_random along with
        the raw string representing this handshake message. We also store the
        cipher suite (if recognized), and finally we instantiate the write and
        read connection states.
        """"""
        super(TLS13ServerHello, self).tls_session_update(msg_str)

        s = self.tls_session
        if self.ext:
            for e in self.ext:
                if isinstance(e, TLS_Ext_SupportedVersion_SH):
                    s.tls_version = e.version
                    break
        s.server_random = self.random_bytes
        s.ciphersuite = self.cipher

        cs_cls = None
        if self.cipher:
            cs_val = self.cipher
            if cs_val not in _tls_cipher_suites_cls:
                warning(""Unknown cipher suite %d from ServerHello"" % cs_val)
                # we do not try to set a default nor stop the execution
            else:
                cs_cls = _tls_cipher_suites_cls[cs_val]

        connection_end = s.connection_end
        if connection_end == ""server"":
            s.pwcs = writeConnState(ciphersuite=cs_cls,
                                    connection_end=connection_end,
                                    tls_version=s.tls_version)

            if not s.middlebox_compatibility:
                s.triggered_pwcs_commit = True
        elif connection_end == ""client"":

            s.prcs = readConnState(ciphersuite=cs_cls,
                                   connection_end=connection_end,
                                   tls_version=s.tls_version)
            if not s.middlebox_compatibility:
                s.triggered_prcs_commit = True

        if s.tls13_early_secret is None:
            # In case the connState was not pre-initialized, we could not
            # compute the early secrets at the ClientHello, so we do it here.
            s.compute_tls13_early_secrets()
        s.compute_tls13_handshake_secrets()
        if connection_end == ""server"":
            shts = s.tls13_derived_secrets[""server_handshake_traffic_secret""]
            s.pwcs.tls13_derive_keys(shts)
        elif connection_end == ""client"":
            shts = s.tls13_derived_secrets[""server_handshake_traffic_secret""]
            s.prcs.tls13_derive_keys(shts)","1. Use a modern cipher suite.
2. Use TLS 1.3 or later.
3. Authenticate the server using a certificate signed by a trusted CA."
"    def m2i(self, pkt, m):
        """"""
        Try to parse one of the TLS subprotocols (ccs, alert, handshake or
        application_data). This is used inside a loop managed by .getfield().
        """"""
        cls = Raw
        if pkt.type == 22:
            if len(m) >= 1:
                msgtype = orb(m[0])
                if ((pkt.tls_session.advertised_tls_version == 0x0304) or
                        (pkt.tls_session.tls_version and
                         pkt.tls_session.tls_version == 0x0304)):
                    cls = _tls13_handshake_cls.get(msgtype, Raw)
                else:
                    cls = _tls_handshake_cls.get(msgtype, Raw)

        elif pkt.type == 20:
            cls = TLSChangeCipherSpec
        elif pkt.type == 21:
            cls = TLSAlert
        elif pkt.type == 23:
            cls = TLSApplicationData

        if cls is Raw:
            return Raw(m)
        else:
            try:
                return cls(m, tls_session=pkt.tls_session)
            except Exception:
                if conf.debug_dissector:
                    raise
                return Raw(m)","1. Use `if ... else ...` instead of `or` to avoid potential bugs.
2. Use `try ... except ...` to catch exceptions and prevent the program from crashing.
3. Use `conf.debug_dissector` to enable debugging when needed."
"    def addfield(self, pkt, s, i):
        """"""
        There is a hack with the _ExtensionsField.i2len. It works only because
        we expect _ExtensionsField.i2m to return a string of the same size (if
        not of the same value) upon successive calls (e.g. through i2len here,
        then i2m when directly building the _ExtensionsField).

        XXX A proper way to do this would be to keep the extensions built from
        the i2len call here, instead of rebuilding them later on.
        """"""
        if i is None:
            if self.length_of is not None:
                fld, fval = pkt.getfield_and_val(self.length_of)

                tmp = pkt.tls_session.frozen
                pkt.tls_session.frozen = True
                f = fld.i2len(pkt, fval)
                pkt.tls_session.frozen = tmp

                i = self.adjust(pkt, f)
                if i == 0:  # for correct build if no ext and not explicitly 0
                    v = pkt.tls_session.tls_version
                    # Xith TLS 1.3, zero lengths are always explicit.
                    if v is None or v < 0x0304:
                        return s
                    else:
                        return s + struct.pack(self.fmt, i)
        return s + struct.pack(self.fmt, i)","1. Use a constant instead of a magic number in the `if` statement.
2. Use `struct.pack_into()` instead of `struct.pack()` to avoid building the extensions twice.
3. Use `pkt.tls_session.frozen = False` to avoid the `frozen` attribute from being set to `True`."
"    def getfield(self, pkt, s):
        tmp_len = self.length_from(pkt)
        if tmp_len is None:
            return s, []
        return s[tmp_len:], self.m2i(pkt, s[:tmp_len])","1. Use a secure random number generator to generate the length of the field.
2. Sanitize the input data to prevent buffer overflows.
3. Validate the length of the field to prevent denial of service attacks."
"    def build(self, *args, **kargs):
        r""""""
        We overload build() method in order to provide a valid default value
        for params based on TLS session if not provided. This cannot be done by
        overriding i2m() because the method is called on a copy of the packet.

        The 'params' field is built according to key_exchange.server_kx_msg_cls
        which should have been set after receiving a cipher suite in a
        previous ServerHello. Usual cases are:

        - None: for RSA encryption or fixed FF/ECDH. This should never happen,
          as no ServerKeyExchange should be generated in the first place.
        - ServerDHParams: for ephemeral FFDH. In that case, the parameter to
          server_kx_msg_cls does not matter.
        - ServerECDH\\*Params: for ephemeral ECDH. There are actually three
          classes, which are dispatched by _tls_server_ecdh_cls_guess on
          the first byte retrieved. The default here is b""\\03"", which
          corresponds to ServerECDHNamedCurveParams (implicit curves).

        When the Server\\*DHParams are built via .fill_missing(), the session
        server_kx_privkey will be updated accordingly.
        """"""
        fval = self.getfieldval(""params"")
        if fval is None:
            s = self.tls_session
            if s.pwcs:
                if s.pwcs.key_exchange.export:
                    cls = ServerRSAParams(tls_session=s)
                else:
                    cls = s.pwcs.key_exchange.server_kx_msg_cls(b""\\x03"")
                    cls = cls(tls_session=s)
                try:
                    cls.fill_missing()
                except Exception:
                    if conf.debug_dissector:
                        raise
            else:
                cls = Raw()
            self.params = cls

        fval = self.getfieldval(""sig"")
        if fval is None:
            s = self.tls_session
            if s.pwcs:
                if not s.pwcs.key_exchange.anonymous:
                    p = self.params
                    if p is None:
                        p = b""""
                    m = s.client_random + s.server_random + raw(p)
                    cls = _TLSSignature(tls_session=s)
                    cls._update_sig(m, s.server_key)
                else:
                    cls = Raw()
            else:
                cls = Raw()
            self.sig = cls

        return _TLSHandshake.build(self, *args, **kargs)","1. Use a secure random number generator to generate the session keys.
2. Use strong encryption algorithms and ciphers.
3. Implement proper authentication and authorization mechanisms."
"    def m2i(self, pkt, m):
        s = pkt.tls_session
        tmp_len = self.length_from(pkt)
        if s.prcs:
            cls = s.prcs.key_exchange.server_kx_msg_cls(m)
            if cls is None:
                return None, Raw(m[:tmp_len]) / Padding(m[tmp_len:])
            return cls(m, tls_session=s)
        else:
            try:
                p = ServerDHParams(m, tls_session=s)
                if pkcs_os2ip(p.load[:2]) not in _tls_hash_sig:
                    raise Exception
                return p
            except Exception:
                cls = _tls_server_ecdh_cls_guess(m)
                p = cls(m, tls_session=s)
                if pkcs_os2ip(p.load[:2]) not in _tls_hash_sig:
                    return None, Raw(m[:tmp_len]) / Padding(m[tmp_len:])
                return p","1. Use a more secure hash function than PKCS#1 v1.5.
2. Validate the length of the message before parsing it.
3. Check the hash of the message to ensure that it is valid."
"    def _process_packet(self, pkt):
        """"""Process each packet: matches the TCP seq/ack numbers
        to follow the TCP streams, and orders the fragments.
        """"""
        if self.app:
            # Special mode: Application layer. Use on top of TCP
            pay_class = pkt.__class__
            if not hasattr(pay_class, ""tcp_reassemble""):
                # Cannot tcp-reassemble
                return pkt
            self.data += bytes(pkt)
            pkt = pay_class.tcp_reassemble(self.data, self.metadata)
            if pkt:
                self.data = b""""
                self.metadata = {}
                return pkt
            return

        from scapy.layers.inet import IP, TCP
        if not pkt or TCP not in pkt:
            return pkt
        pay = pkt[TCP].payload
        if isinstance(pay, (NoPayload, conf.padding_layer)):
            return pkt
        new_data = pay.original
        # Match packets by a uniqute TCP identifier
        seq = pkt[TCP].seq
        ident = pkt.sprintf(self.fmt)
        data, metadata = self.tcp_frags[ident]
        # Let's guess which class is going to be used
        if ""pay_class"" not in metadata:
            pay_class = pay.__class__
            if not hasattr(pay_class, ""tcp_reassemble""):
                # Cannot tcp-reassemble
                return pkt
            metadata[""pay_class""] = pay_class
        else:
            pay_class = metadata[""pay_class""]
        # Get a relative sequence number for a storage purpose
        relative_seq = metadata.get(""relative_seq"", None)
        if relative_seq is None:
            relative_seq = metadata[""relative_seq""] = seq - 1
        seq = seq - relative_seq
        # Add the data to the buffer
        # Note that this take care of retransmission packets.
        data.append(new_data, seq)
        # Check TCP FIN or TCP RESET
        if pkt[TCP].flags.F or pkt[TCP].flags.R:
            metadata[""tcp_end""] = True

        # In case any app layer protocol requires it,
        # allow the parser to inspect TCP PSH flag
        if pkt[TCP].flags.P:
            metadata[""tcp_psh""] = True
        # XXX TODO: check that no empty space is missing in the buffer.
        # XXX Currently, if a TCP fragment was missing, we won't notice it.
        packet = None
        if data.full():
            # Reassemble using all previous packets
            packet = pay_class.tcp_reassemble(bytes(data), metadata)
        # Stack the result on top of the previous frames
        if packet:
            data.clear()
            del self.tcp_frags[ident]
            pay.underlayer.remove_payload()
            if IP in pkt:
                pkt[IP].len = None
                pkt[IP].chksum = None
            return pkt / packet","1. Use scapy.layers.inet.TCP instead of raw TCP to avoid vulnerabilities.
2. Use scapy.utils.conf.padding_layer to avoid injecting padding.
3. Use scapy.layers.inet.IP instead of raw IP to avoid vulnerabilities."
"    def __init__(self, filename, fdesc, magic):
        RawPcapReader.__init__(self, filename, fdesc, magic)
        try:
            self.LLcls = conf.l2types[self.linktype]
        except KeyError:
            warning(""PcapReader: unknown LL type [%i]/[%#x]. Using Raw packets"" % (self.linktype, self.linktype))  # noqa: E501
            self.LLcls = conf.raw_layer","1. Use `try-except` to handle `KeyError`.
2. Use `conf.raw_layer` as the default value for `self.LLcls`.
3. Use `noqa` to suppress the warning."
"    def read_packet(self, size=MTU):
        rp = super(PcapReader, self).read_packet(size=size)
        if rp is None:
            raise EOFError
        s, pkt_info = rp

        try:
            p = self.LLcls(s)
        except KeyboardInterrupt:
            raise
        except Exception:
            if conf.debug_dissector:
                from scapy.sendrecv import debug
                debug.crashed_on = (self.LLcls, s)
                raise
            p = conf.raw_layer(s)
        power = Decimal(10) ** Decimal(-9 if self.nano else -6)
        p.time = EDecimal(pkt_info.sec + power * pkt_info.usec)
        p.wirelen = pkt_info.wirelen
        return p","1. **Use scapy.layers.l2** instead of raw sockets to avoid potential buffer overflows.
2. **Sanitize user input** to prevent malicious packets from being injected into the network.
3. **Use scapy's built-in validation functions** to verify that packets are well-formed before processing them."
"    def read_packet(self, size=MTU):
        rp = super(PcapNgReader, self).read_packet(size=size)
        if rp is None:
            raise EOFError
        s, (linktype, tsresol, tshigh, tslow, wirelen) = rp
        try:
            p = conf.l2types[linktype](s)
        except KeyboardInterrupt:
            raise
        except Exception:
            if conf.debug_dissector:
                raise
            p = conf.raw_layer(s)
        if tshigh is not None:
            p.time = EDecimal((tshigh << 32) + tslow) / tsresol
        p.wirelen = wirelen
        return p","1. Use `TYPE_CHECKING` to check the type of arguments passed to functions.
2. Use `functools.wraps` to preserve the metadata of wrapped functions.
3. Use `logging.info` to log important information."
"    def m2i(self, pkt, val):
        ret = []
        for v in val:
            byte = orb(v)
            left = byte >> 4
            right = byte & 0xf
            if left == 0xf:
                ret.append(TBCD_TO_ASCII[right:right + 1])
            else:
                ret += [TBCD_TO_ASCII[right:right + 1], TBCD_TO_ASCII[left:left + 1]]  # noqa: E501
        return b"""".join(ret)","1. Use `ord()` instead of `orb()` to avoid signed integer overflow.
2. Use `bytes()` to create a byte string instead of `b""""`.
3. Use `chr()` to convert a character code to a string instead of `TBCD_TO_ASCII`."
"    def i2m(self, pkt, val):
        val = str(val)
        ret_string = """"
        for i in range(0, len(val), 2):
            tmp = val[i:i + 2]
            if len(tmp) == 2:
                ret_string += chr(int(tmp[1] + tmp[0], 16))
            else:
                ret_string += chr(int(""F"" + tmp[0], 16))
        return ret_string","1. Use `ord()` to convert string to integer instead of `int()`.
2. Use `chr()` to convert integer to string instead of `str()`.
3. Use `format()` to format the string instead of concatenation."
"    def i2m(self, pkt, s):
        s = b"""".join(chb(len(x)) + x for x in s.split("".""))
        return s","1. Use a cryptographically secure random number generator to generate the length prefix.
2. Use a salt to make the encoded string unique.
3. Use a secure hash function to hash the encoded string."
"    def __add__(self, other, **kwargs):
        return EDecimal(Decimal.__add__(self, other, **kwargs))","1. **Use `functools.wraps` to preserve the metadata of the original function.** This will ensure that the return type and documentation of the new function are correct.
2. **Check the types of the arguments to the function.** This will help to prevent errors and vulnerabilities.
3. **Use `assert` statements to verify the assumptions made by the function.** This will help to catch errors early and prevent them from causing problems."
"    def __sub__(self, other, **kwargs):
        return EDecimal(Decimal.__sub__(self, other, **kwargs))","1. Use `Decimal.from_float()` to sanitize user input before passing it to `Decimal.__sub__()`.
2. Use `Decimal.setcontext()` to set the rounding mode and precision.
3. Use `Decimal.quantize()` to round the result of the subtraction to the desired precision."
"    def __mul__(self, other, **kwargs):
        return EDecimal(Decimal.__mul__(self, other, **kwargs))","1. Use `Decimal.from_float` instead of `float()` to prevent precision loss.
2. Use `Decimal.setcontext` to set the rounding mode and precision.
3. Use `Decimal.quantize` to round the result to the desired precision."
"    def __truediv__(self, other, **kwargs):
        return EDecimal(Decimal.__truediv__(self, other, **kwargs))","1. Use `Decimal.from_float()` to sanitize user input before using it in a `Decimal` operation.
2. Use `Decimal.quantize()` to round the result of a `Decimal` operation to a specified precision.
3. Use `Decimal.compare()` to compare two `Decimal` values instead of using the ``==` operator."
"    def __floordiv__(self, other, **kwargs):
        return EDecimal(Decimal.__floordiv__(self, other, **kwargs))","1. Use `Decimal.from_float` to sanitize user input before passing it to `Decimal.__floordiv__`.
2. Check that the `other` argument is a `Decimal` instance before calling `Decimal.__floordiv__`.
3. Use `Decimal.setcontext` to set the rounding mode before calling `Decimal.__floordiv__`."
"    def __div__(self, other, **kwargs):
        return EDecimal(Decimal.__div__(self, other, **kwargs))","1. Use `Decimal.from_float` instead of `float()` to prevent overflow.
2. Use `Decimal.is_finite` to check if the result is finite.
3. Use `Decimal.compare` to compare decimals instead of `>`, `<`, `==`, `!=`."
"    def __mod__(self, other, **kwargs):
        return EDecimal(Decimal.__mod__(self, other, **kwargs))","1. Use `decimal.Decimal` instead of `float` to avoid rounding errors.
2. Use `decimal.getcontext().traps[decimal.DivisionByZero]` to raise an exception when dividing by zero.
3. Use `decimal.getcontext().traps[decimal.InvalidOperation]` to raise an exception when performing invalid operations."
"    def __divmod__(self, other, **kwargs):
        return EDecimal(Decimal.__divmod__(self, other, **kwargs))","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `inspect.getfullargspec` to get the full argument list of the wrapped function.
3. Use `inspect.ismethod` to check if the wrapped function is a method."
"    def __pow__(self, other, **kwargs):
        return EDecimal(Decimal.__pow__(self, other, **kwargs))","1. Use `Decimal.from_float()` instead of `Decimal(float)` to prevent integer overflow.
2. Use `Decimal.ln()` instead of `math.log()` to prevent floating-point errors.
3. Use `Decimal.sqrt()` instead of `math.sqrt()` to prevent floating-point errors."
"    def _check_len(self, pkt):
        """"""Check for odd packet length and pad according to Cisco spec.
        This padding is only used for checksum computation.  The original
        packet should not be altered.""""""
        if len(pkt) % 2:
            last_chr = pkt[-1]
            if last_chr <= b'\\x80':
                return pkt[:-1] + b'\\x00' + last_chr
            else:
                return pkt[:-1] + b'\\xff' + chb(orb(last_chr) - 1)
        else:
            return pkt","1. Use cryptographically secure padding instead of relying on the underlying protocol's padding rules.
2. Sanitize user input to prevent buffer overflow attacks.
3. Validate the checksum before using it to verify the integrity of the packet."
"    def post_build(self, p, pay):
        vlannamelen = 4 * ((len(self.vlanname) + 3) // 4)

        if self.len is None:
            tmp_len = vlannamelen + 12
            p = chr(tmp_len & 0xff) + p[1:]

        # Pad vlan name with zeros if vlannamelen > len(vlanname)
        tmp_len = vlannamelen - len(self.vlanname)
        if tmp_len != 0:
            p += b""\\x00"" * tmp_len

        p += pay

        return p","1. Use a cryptographically secure random number generator to generate the length field.
2. Sanitize the vlanname parameter to prevent injection attacks.
3. Validate the length of the pay parameter to prevent buffer overflow attacks."
"    def post_build(self, p, pay):
        if self.domnamelen is None:
            domnamelen = len(self.domname.strip(b""\\x00""))
            p = p[:3] + chr(domnamelen & 0xff) + p[4:]

        p += pay

        return p","1. Sanitize user input to prevent injection attacks.
2. Use a secure random number generator to generate the domain name length.
3. Use a cryptographically secure hash function to generate the payload."
"def compile_filter(filter_exp, iface=None, linktype=None,
                   promisc=False):
    """"""Asks libpcap to parse the filter, then build the matching
    BPF bytecode.

    :param iface: if provided, use the interface to compile
    :param linktype: if provided, use the linktype to compile
    """"""
    try:
        from scapy.libs.winpcapy import (
            PCAP_ERRBUF_SIZE,
            pcap_open_live,
            pcap_compile,
            pcap_compile_nopcap,
            pcap_close
        )
        from scapy.libs.structures import bpf_program
    except ImportError:
        raise Scapy_Exception(
            ""libpcap is not available. Cannot compile filter !""
        )
    root = WINDOWS or (os.geteuid() == 0)
    from ctypes import create_string_buffer
    bpf = bpf_program()
    bpf_filter = create_string_buffer(filter_exp.encode(""utf8""))
    if not linktype:
        # Try to guess linktype to avoid root
        if not iface:
            if not conf.iface:
                raise Scapy_Exception(
                    ""Please provide an interface or linktype!""
                )
            if WINDOWS:
                iface = conf.iface.pcap_name
            else:
                iface = conf.iface
        # Try to guess linktype to avoid requiring root
        try:
            arphd = get_if_raw_hwaddr(iface)[0]
            linktype = ARPHRD_TO_DLT.get(arphd)
        except Exception:
            # Failed to use linktype: use the interface
            if not root:
                raise Scapy_Exception(
                    ""Please provide a valid interface or linktype!""
                )
    if linktype is not None:
        ret = pcap_compile_nopcap(
            MTU, linktype, ctypes.byref(bpf), bpf_filter, 0, -1
        )
    elif iface:
        if not root:
            raise OSError(
                ""Compiling using an interface requires root.""
            )
        err = create_string_buffer(PCAP_ERRBUF_SIZE)
        iface = create_string_buffer(iface.encode(""utf8""))
        pcap = pcap_open_live(
            iface, MTU, promisc, 0, err
        )
        ret = pcap_compile(
            pcap, ctypes.byref(bpf), bpf_filter, 0, -1
        )
        pcap_close(pcap)
    if ret == -1:
        raise Scapy_Exception(
            ""Failed to compile filter expression %s (%s)"" % (filter_exp, ret)
        )
    if conf.use_pypy:
        # XXX PyPy has a broken behavior.
        # https://bitbucket.org/pypy/pypy/issues/3114
        return struct.pack(
            'HL',
            bpf.bf_len, ctypes.addressof(bpf.bf_insns.contents)
        )
    return bpf","1. Use scapy.compile instead of compile_filter to avoid potential security vulnerabilities.
2. Use scapy.Raw instead of create_string_buffer to avoid potential buffer overflows.
3. Use scapy.get_if_raw_hwaddr instead of get_if_hwaddr to avoid potential information disclosure."
"    def __eq__(self, other):
        if hasattr(other, ""parsed""):
            p2 = other.parsed
        else:
            p2, nm2 = self._parse_net(other)
        return self.parsed == p2","1. Use `getattr` instead of `hasattr` to avoid triggering a potential `setattr` call on `other`.
2. Use `type(other) is MyClass` instead of `hasattr(other, ""parsed"")` to check if `other` is an instance of `MyClass`.
3. Use `self.parsed == p2` instead of `self.parsed == other.parsed` to avoid triggering a potential `setattr` call on `other`."
"def explore(layer=None):
    """"""Function used to discover the Scapy layers and protocols.
    It helps to see which packets exists in contrib or layer files.

    params:
     - layer: If specified, the function will explore the layer. If not,
              the GUI mode will be activated, to browse the available layers

    examples:
      >>> explore()  # Launches the GUI
      >>> explore(""dns"")  # Explore scapy.layers.dns
      >>> explore(""http2"")  # Explore scapy.contrib.http2
      >>> explore(scapy.layers.bluetooth4LE)

    Note: to search a packet by name, use ls(""name"") rather than explore.
    """"""
    if layer is None:  # GUI MODE
        if not conf.interactive:
            raise Scapy_Exception(""explore() GUI-mode cannot be run in ""
                                  ""interactive mode. Please provide a ""
                                  ""'layer' parameter !"")
        # 0 - Imports
        try:
            import prompt_toolkit
        except ImportError:
            raise ImportError(""prompt_toolkit is not installed ! ""
                              ""You may install IPython, which contains it, via""
                              "" `pip install ipython`"")
        if not _version_checker(prompt_toolkit, (2, 0)):
            raise ImportError(""prompt_toolkit >= 2.0.0 is required !"")
        # Only available with prompt_toolkit > 2.0, not released on PyPi yet
        from prompt_toolkit.shortcuts.dialogs import radiolist_dialog, \\
            button_dialog
        from prompt_toolkit.formatted_text import HTML
        # Check for prompt_toolkit >= 3.0.0
        if _version_checker(prompt_toolkit, (3, 0)):
            call_ptk = lambda x: x.run()
        else:
            call_ptk = lambda x: x
        # 1 - Ask for layer or contrib
        btn_diag = button_dialog(
            title=""Scapy v%s"" % conf.version,
            text=HTML(
                six.text_type(
                    '<style bg=""white"" fg=""red"">Chose the type of packets'
                    ' you want to explore:</style>'
                )
            ),
            buttons=[
                (six.text_type(""Layers""), ""layers""),
                (six.text_type(""Contribs""), ""contribs""),
                (six.text_type(""Cancel""), ""cancel"")
            ])
        action = call_ptk(btn_diag)
        # 2 - Retrieve list of Packets
        if action == ""layers"":
            # Get all loaded layers
            _radio_values = conf.layers.layers()
            # Restrict to layers-only (not contribs) + packet.py and asn1*.py
            _radio_values = [x for x in _radio_values if (""layers"" in x[0] or
                                                          ""packet"" in x[0] or
                                                          ""asn1"" in x[0])]
        elif action == ""contribs"":
            # Get all existing contribs
            from scapy.main import list_contrib
            _radio_values = list_contrib(ret=True)
            _radio_values = [(x['name'], x['description'])
                             for x in _radio_values]
            # Remove very specific modules
            _radio_values = [x for x in _radio_values if not (""can"" in x[0])]
        else:
            # Escape/Cancel was pressed
            return
        # Python 2 compat
        if six.PY2:
            _radio_values = [(six.text_type(x), six.text_type(y))
                             for x, y in _radio_values]
        # 3 - Ask for the layer/contrib module to explore
        rd_diag = radiolist_dialog(
            values=_radio_values,
            title=""Scapy v%s"" % conf.version,
            text=HTML(
                six.text_type(
                    '<style bg=""white"" fg=""red"">Please select a layer among'
                    ' the following, to see all packets contained in'
                    ' it:</style>'
                )
            ))
        result = call_ptk(rd_diag)
        if result is None:
            return  # User pressed ""Cancel""
        # 4 - (Contrib only): load contrib
        if action == ""contribs"":
            from scapy.main import load_contrib
            load_contrib(result)
            result = ""scapy.contrib."" + result
    else:  # NON-GUI MODE
        # We handle layer as a short layer name, full layer name
        # or the module itself
        if isinstance(layer, types.ModuleType):
            layer = layer.__name__
        if isinstance(layer, str):
            if layer.startswith(""scapy.layers.""):
                result = layer
            else:
                if layer.startswith(""scapy.contrib.""):
                    layer = layer.replace(""scapy.contrib."", """")
                from scapy.main import load_contrib
                load_contrib(layer)
                result_layer, result_contrib = ((""scapy.layers.%s"" % layer),
                                                (""scapy.contrib.%s"" % layer))
                if result_layer in conf.layers.ldict:
                    result = result_layer
                elif result_contrib in conf.layers.ldict:
                    result = result_contrib
                else:
                    raise Scapy_Exception(""Unknown scapy module '%s'"" % layer)
        else:
            warning(""Wrong usage ! Check out help(explore)"")
            return

    # COMMON PART
    # Get the list of all Packets contained in that module
    try:
        all_layers = conf.layers.ldict[result]
    except KeyError:
        raise Scapy_Exception(""Unknown scapy module '%s'"" % layer)
    # Print
    print(conf.color_theme.layer_name(""Packets contained in %s:"" % result))
    rtlst = [(lay.__name__ or """", lay._name or """") for lay in all_layers]
    print(pretty_list(rtlst, [(""Class"", ""Name"")], borders=True))","1. Use `prompt_toolkit >= 3.0.0` to avoid a potential security vulnerability.
2. Use `conf.layers.layers()` to get the list of all packets, instead of hardcoding it.
3. Use `six.text_type()` to convert strings to unicode, to avoid errors when running on Python 2."
"    def post_build(self, p, pay):
        p += pay
        dataofs = self.dataofs
        if dataofs is None:
            dataofs = 5 + ((len(self.get_field(""options"").i2m(self, self.options)) + 3) // 4)  # noqa: E501
            p = p[:12] + chb((dataofs << 4) | orb(p[12]) & 0x0f) + p[13:]
        if self.chksum is None:
            if isinstance(self.underlayer, IP):
                ck = in4_chksum(socket.IPPROTO_TCP, self.underlayer, p)
                p = p[:16] + struct.pack(""!H"", ck) + p[18:]
            elif conf.ipv6_enabled and isinstance(self.underlayer, scapy.layers.inet6.IPv6) or isinstance(self.underlayer, scapy.layers.inet6._IPv6ExtHdr):  # noqa: E501
                ck = scapy.layers.inet6.in6_chksum(socket.IPPROTO_TCP, self.underlayer, p)  # noqa: E501
                p = p[:16] + struct.pack(""!H"", ck) + p[18:]
            else:
                warning(""No IP underlayer to compute checksum. Leaving null."")
        return p","1. Use `ord()` instead of `orb()` to avoid signed integer overflow.
2. Use `struct.pack_into()` instead of `struct.pack()` to avoid buffer overflow.
3. Use `warnings.warn()` instead of `print()` to avoid leaking information to attackers."
"    def send(self, x):
        iff, a, gw = x.route()
        if iff is None:
            iff = conf.iface
        sdto = (iff, self.type)
        self.outs.bind(sdto)
        sn = self.outs.getsockname()
        ll = lambda x: x
        if type(x) in conf.l3types:
            sdto = (iff, conf.l3types[type(x)])
        if sn[3] in conf.l2types:
            ll = lambda x: conf.l2types[sn[3]]() / x
        sx = raw(ll(x))
        try:
            self.outs.sendto(sx, sdto)
        except socket.error as msg:
            if msg.errno == 22 and len(sx) < conf.min_pkt_size:
                self.outs.send(sx + b""\\x00"" * (conf.min_pkt_size - len(sx)))
            elif conf.auto_fragment and msg.errno == 90:
                for p in x.fragment():
                    self.outs.sendto(raw(ll(p)), sdto)
            else:
                raise
        x.sent_time = time.time()","1. Use proper error handling to avoid crashing the program.
2. Sanitize user input to prevent injection attacks.
3. Use strong encryption to protect sensitive data."
"    def _find_fld(self):
        """"""Returns the Field subclass to be used, depending on the Packet
instance, or the default subclass.

DEV: since the Packet instance is not provided, we have to use a hack
to guess it. It should only be used if you cannot provide the current
Packet instance (for example, because of the current Scapy API).

If you have the current Packet instance, use ._find_fld_pkt_val() (if
the value to set is also known) of ._find_fld_pkt() instead.

        """"""
        # Hack to preserve current Scapy API
        # See https://stackoverflow.com/a/7272464/3223422
        frame = inspect.currentframe().f_back.f_back
        while frame is not None:
            try:
                pkt = frame.f_locals['self']
            except KeyError:
                pass
            else:
                if not pkt.default_fields:
                    # Packet not initialized
                    return self.dflt
                if isinstance(pkt, tuple(self.dflt.owners)):
                    return self._find_fld_pkt(pkt)
            frame = frame.f_back
        return self.dflt","1. Use `inspect.getframeinfo` instead of `inspect.currentframe` to get the frame information.
2. Use `isinstance` to check if the object is an instance of the specified class.
3. Use `return self.dflt` instead of `return None` to return the default value."
"    def addfield(self, pkt, s, val):
        len_pkt = self.length_from(pkt)
        return s + struct.pack(""%is"" % len_pkt, self.i2m(pkt, val))","1. Use a constant-time comparison to prevent timing attacks.
2. Sanitize user input to prevent buffer overflows.
3. Use strong cryptography to protect sensitive data."
"    def route(self):
        fld, dst = self.getfield_and_val(""pdst"")
        fld, dst = fld._find_fld_pkt_val(self, dst)
        if isinstance(dst, Gen):
            dst = next(iter(dst))
        if isinstance(fld, IP6Field):
            return conf.route6.route(dst)
        elif isinstance(fld, IPField):
            return conf.route.route(dst)
        else:
            return None","1. Use `ipaddress` module to validate IP addresses.
2. Sanitize user input to prevent injection attacks.
3. Use proper error handling to prevent leaking sensitive information."
"def fuzz(p, _inplace=0):
    """"""Transform a layer into a fuzzy layer by replacing some default values by random objects""""""  # noqa: E501
    if not _inplace:
        p = p.copy()
    q = p
    while not isinstance(q, NoPayload):
        for f in q.fields_desc:
            if isinstance(f, PacketListField):
                for r in getattr(q, f.name):
                    print(""fuzzing"", repr(r))
                    fuzz(r, _inplace=1)
            elif f.default is not None:
                if not isinstance(f, ConditionalField) or f._evalcond(q):
                    rnd = f.randval()
                    if rnd is not None:
                        q.default_fields[f.name] = rnd
        q = q.payload
    return p","1. Use `copy()` instead of assignment to avoid modifying the original packet.
2. Use `isinstance()` to check if a field is a `PacketListField` before iterating over it.
3. Use `ConditionalField._evalcond()` to check if a field is conditionally set before setting its default value."
"def read_routes():
    if SOLARIS:
        f = os.popen(""netstat -rvn"")  # -f inet
    elif FREEBSD:
        f = os.popen(""netstat -rnW"")  # -W to handle long interface names
    else:
        f = os.popen(""netstat -rn"")  # -f inet
    ok = 0
    mtu_present = False
    prio_present = False
    routes = []
    pending_if = []
    for line in f.readlines():
        if not line:
            break
        line = line.strip()
        if line.find(""----"") >= 0:  # a separation line
            continue
        if not ok:
            if line.find(""Destination"") >= 0:
                ok = 1
                mtu_present = ""Mtu"" in line
                prio_present = ""Prio"" in line
                refs_present = ""Refs"" in line
            continue
        if not line:
            break
        if SOLARIS:
            lspl = line.split()
            if len(lspl) == 10:
                dest, mask, gw, netif, mxfrg, rtt, ref, flg = lspl[:8]
            else:  # missing interface
                dest, mask, gw, mxfrg, rtt, ref, flg = lspl[:7]
                netif = None
        else:
            rt = line.split()
            dest, gw, flg = rt[:3]
            locked = OPENBSD and rt[6] == ""L""
            netif = rt[4 + mtu_present + prio_present + refs_present + locked]
        if flg.find(""Lc"") >= 0:
            continue
        if dest == ""default"":
            dest = 0
            netmask = 0
        else:
            if SOLARIS:
                netmask = scapy.utils.atol(mask)
            elif ""/"" in dest:
                dest, netmask = dest.split(""/"")
                netmask = scapy.utils.itom(int(netmask))
            else:
                netmask = scapy.utils.itom((dest.count(""."") + 1) * 8)
            dest += "".0"" * (3 - dest.count("".""))
            dest = scapy.utils.atol(dest)
        # XXX: TODO: add metrics for unix.py (use -e option on netstat)
        metric = 1
        if ""G"" not in flg:
            gw = '0.0.0.0'
        if netif is not None:
            try:
                ifaddr = get_if_addr(netif)
                routes.append((dest, netmask, gw, netif, ifaddr, metric))
            except OSError as exc:
                if exc.message == 'Device not configured':
                    # This means the interface name is probably truncated by
                    # netstat -nr. We attempt to guess it's name and if not we
                    # ignore it.
                    guessed_netif = _guess_iface_name(netif)
                    if guessed_netif is not None:
                        ifaddr = get_if_addr(guessed_netif)
                        routes.append((dest, netmask, gw, guessed_netif, ifaddr, metric))  # noqa: E501
                    else:
                        warning(""Could not guess partial interface name: %s"", netif)  # noqa: E501
                else:
                    raise
        else:
            pending_if.append((dest, netmask, gw))
    f.close()

    # On Solaris, netstat does not provide output interfaces for some routes
    # We need to parse completely the routing table to route their gw and
    # know their output interface
    for dest, netmask, gw in pending_if:
        gw_l = scapy.utils.atol(gw)
        max_rtmask, gw_if, gw_if_addr, = 0, None, None
        for rtdst, rtmask, _, rtif, rtaddr in routes[:]:
            if gw_l & rtmask == rtdst:
                if rtmask >= max_rtmask:
                    max_rtmask = rtmask
                    gw_if = rtif
                    gw_if_addr = rtaddr
        # XXX: TODO add metrics
        metric = 1
        if gw_if:
            routes.append((dest, netmask, gw, gw_if, gw_if_addr, metric))
        else:
            warning(""Did not find output interface to reach gateway %s"", gw)

    return routes","1. Use scapy.utils.ensure_str() to avoid potential injection attacks.
2. Use scapy.utils.hexdump() to print out hexadecimal values.
3. Use scapy.utils.convert_mac() to convert MAC addresses to hexadecimal values."
"def sniff(count=0, store=True, offline=None, prn=None, lfilter=None,
          L2socket=None, timeout=None, opened_socket=None,
          stop_filter=None, iface=None, started_callback=None,
          session=None, *arg, **karg):
    """"""Sniff packets and return a list of packets.

    Args:
        count: number of packets to capture. 0 means infinity.
        store: whether to store sniffed packets or discard them
        prn: function to apply to each packet. If something is returned, it
             is displayed.
             --Ex: prn = lambda x: x.summary()
        session: a session = a flow decoder used to handle stream of packets.
                 e.g: IPSession (to defragment on-the-flow) or NetflowSession
        filter: BPF filter to apply.
        lfilter: Python function applied to each packet to determine if
                 further action may be done.
                 --Ex: lfilter = lambda x: x.haslayer(Padding)
        offline: PCAP file (or list of PCAP files) to read packets from,
                 instead of sniffing them
        timeout: stop sniffing after a given time (default: None).
        L2socket: use the provided L2socket (default: use conf.L2listen).
        opened_socket: provide an object (or a list of objects) ready to use
                      .recv() on.
        stop_filter: Python function applied to each packet to determine if
                     we have to stop the capture after this packet.
                     --Ex: stop_filter = lambda x: x.haslayer(TCP)
        iface: interface or list of interfaces (default: None for sniffing
               on all interfaces).
        monitor: use monitor mode. May not be available on all OS
        started_callback: called as soon as the sniffer starts sniffing
                          (default: None).

    The iface, offline and opened_socket parameters can be either an
    element, a list of elements, or a dict object mapping an element to a
    label (see examples below).

    Examples:
      >>> sniff(filter=""arp"")
      >>> sniff(filter=""tcp"",
      ...       session=IPSession,  # defragment on-the-flow
      ...       prn=lambda x: x.summary())
      >>> sniff(lfilter=lambda pkt: ARP in pkt)
      >>> sniff(iface=""eth0"", prn=Packet.summary)
      >>> sniff(iface=[""eth0"", ""mon0""],
      ...       prn=lambda pkt: ""%s: %s"" % (pkt.sniffed_on,
      ...                                   pkt.summary()))
      >>> sniff(iface={""eth0"": ""Ethernet"", ""mon0"": ""Wifi""},
      ...       prn=lambda pkt: ""%s: %s"" % (pkt.sniffed_on,
      ...                                   pkt.summary()))
    """"""
    c = 0
    session = session or DefaultSession
    session = session(prn, store)  # instantiate session
    sniff_sockets = {}  # socket: label dict
    if opened_socket is not None:
        if isinstance(opened_socket, list):
            sniff_sockets.update((s, ""socket%d"" % i)
                                 for i, s in enumerate(opened_socket))
        elif isinstance(opened_socket, dict):
            sniff_sockets.update((s, label)
                                 for s, label in six.iteritems(opened_socket))
        else:
            sniff_sockets[opened_socket] = ""socket0""
    if offline is not None:
        flt = karg.get('filter')

        if not TCPDUMP and flt is not None:
            message = ""tcpdump is not available. Cannot use filter!""
            raise Scapy_Exception(message)

        if isinstance(offline, list):
            sniff_sockets.update((PcapReader(
                fname if flt is None else
                tcpdump(fname, args=[""-w"", ""-"", flt], getfd=True)
            ), fname) for fname in offline)
        elif isinstance(offline, dict):
            sniff_sockets.update((PcapReader(
                fname if flt is None else
                tcpdump(fname, args=[""-w"", ""-"", flt], getfd=True)
            ), label) for fname, label in six.iteritems(offline))
        else:
            sniff_sockets[PcapReader(
                offline if flt is None else
                tcpdump(offline, args=[""-w"", ""-"", flt], getfd=True)
            )] = offline
    if not sniff_sockets or iface is not None:
        if L2socket is None:
            L2socket = conf.L2listen
        if isinstance(iface, list):
            sniff_sockets.update(
                (L2socket(type=ETH_P_ALL, iface=ifname, *arg, **karg), ifname)
                for ifname in iface
            )
        elif isinstance(iface, dict):
            sniff_sockets.update(
                (L2socket(type=ETH_P_ALL, iface=ifname, *arg, **karg), iflabel)
                for ifname, iflabel in six.iteritems(iface)
            )
        else:
            sniff_sockets[L2socket(type=ETH_P_ALL, iface=iface,
                                   *arg, **karg)] = iface
    if timeout is not None:
        stoptime = time.time() + timeout
    remain = None

    # Get select information from the sockets
    _main_socket = next(iter(sniff_sockets))
    read_allowed_exceptions = _main_socket.read_allowed_exceptions
    select_func = _main_socket.select
    # We check that all sockets use the same select(), or raise a warning
    if not all(select_func == sock.select for sock in sniff_sockets):
        warning(""Warning: inconsistent socket types ! The used select function""
                ""will be the one of the first socket"")
    # Now let's build the select function, used later on
    _select = lambda sockets, remain: select_func(sockets, remain)[0]

    try:
        if started_callback:
            started_callback()
        continue_sniff = True
        while sniff_sockets and continue_sniff:
            if timeout is not None:
                remain = stoptime - time.time()
                if remain <= 0:
                    break
            for s in _select(sniff_sockets, remain):
                try:
                    p = s.recv()
                except socket.error as ex:
                    warning(""Socket %s failed with '%s' and thus""
                            "" will be ignored"" % (s, ex))
                    del sniff_sockets[s]
                    continue
                except read_allowed_exceptions:
                    continue
                if p is None:
                    try:
                        if s.promisc:
                            continue
                    except AttributeError:
                        pass
                    del sniff_sockets[s]
                    break
                if lfilter and not lfilter(p):
                    continue
                p.sniffed_on = sniff_sockets[s]
                c += 1
                # on_packet_received handles the prn/storage
                session.on_packet_received(p)
                if stop_filter and stop_filter(p):
                    continue_sniff = False
                    break
                if 0 < count <= c:
                    continue_sniff = False
                    break
    except KeyboardInterrupt:
        pass
    if opened_socket is None:
        for s in sniff_sockets:
            s.close()
    return session.toPacketList()","1. Use scapy.conf.L2listen instead of creating your own L2socket.
2. Use a more specific iface argument instead of ETH_P_ALL.
3. Use a timeout argument to prevent sniffing indefinitely."
"def tcpdump(pktlist, dump=False, getfd=False, args=None,
            prog=None, getproc=False, quiet=False, use_tempfile=None,
            read_stdin_opts=None, linktype=None, wait=True):
    """"""Run tcpdump or tshark on a list of packets.

    When using ``tcpdump`` on OSX (``prog == conf.prog.tcpdump``), this uses a
    temporary file to store the packets. This works around a bug in Apple's
    version of ``tcpdump``: http://apple.stackexchange.com/questions/152682/

    Otherwise, the packets are passed in stdin.

    This function can be explicitly enabled or disabled with the
    ``use_tempfile`` parameter.

    When using ``wireshark``, it will be called with ``-ki -`` to start
    immediately capturing packets from stdin.

    Otherwise, the command will be run with ``-r -`` (which is correct for
    ``tcpdump`` and ``tshark``).

    This can be overridden with ``read_stdin_opts``. This has no effect when
    ``use_tempfile=True``, or otherwise reading packets from a regular file.

pktlist: a Packet instance, a PacketList instance or a list of Packet
         instances. Can also be a filename (as a string), an open
         file-like object that must be a file format readable by
         tshark (Pcap, PcapNg, etc.) or None (to sniff)

dump:    when set to True, returns a string instead of displaying it.
getfd:   when set to True, returns a file-like object to read data
         from tcpdump or tshark from.
getproc: when set to True, the subprocess.Popen object is returned
args:    arguments (as a list) to pass to tshark (example for tshark:
         args=[""-T"", ""json""]).
prog:    program to use (defaults to tcpdump, will work with tshark)
quiet:   when set to True, the process stderr is discarded
use_tempfile: When set to True, always use a temporary file to store packets.
              When set to False, pipe packets through stdin.
              When set to None (default), only use a temporary file with
              ``tcpdump`` on OSX.
read_stdin_opts: When set, a list of arguments needed to capture from stdin.
                 Otherwise, attempts to guess.
linktype: A custom DLT value or name, to overwrite the default values.
wait: If True (default), waits for the process to terminate before returning
      to Scapy. If False, the process will be detached to the background. If
      dump, getproc or getfd is True, these have the same effect as
      ``wait=False``.

Examples:

>>> tcpdump([IP()/TCP(), IP()/UDP()])
reading from file -, link-type RAW (Raw IP)
16:46:00.474515 IP 127.0.0.1.20 > 127.0.0.1.80: Flags [S], seq 0, win 8192, length 0  # noqa: E501
16:46:00.475019 IP 127.0.0.1.53 > 127.0.0.1.53: [|domain]

>>> tcpdump([IP()/TCP(), IP()/UDP()], prog=conf.prog.tshark)
  1   0.000000    127.0.0.1 -> 127.0.0.1    TCP 40 20->80 [SYN] Seq=0 Win=8192 Len=0  # noqa: E501
  2   0.000459    127.0.0.1 -> 127.0.0.1    UDP 28 53->53 Len=0

To get a JSON representation of a tshark-parsed PacketList(), one can:
>>> import json, pprint
>>> json_data = json.load(tcpdump(IP(src=""217.25.178.5"", dst=""45.33.32.156""),
...                               prog=conf.prog.tshark, args=[""-T"", ""json""],
...                               getfd=True))
>>> pprint.pprint(json_data)
[{u'_index': u'packets-2016-12-23',
  u'_score': None,
  u'_source': {u'layers': {u'frame': {u'frame.cap_len': u'20',
                                      u'frame.encap_type': u'7',
[...]
                                      u'frame.time_relative': u'0.000000000'},
                           u'ip': {u'ip.addr': u'45.33.32.156',
                                   u'ip.checksum': u'0x0000a20d',
[...]
                                   u'ip.ttl': u'64',
                                   u'ip.version': u'4'},
                           u'raw': u'Raw packet data'}},
  u'_type': u'pcap_file'}]
>>> json_data[0]['_source']['layers']['ip']['ip.ttl']
u'64'
    """"""
    getfd = getfd or getproc
    if prog is None:
        prog = [conf.prog.tcpdump]
        _prog_name = ""windump()"" if WINDOWS else ""tcpdump()""
    elif isinstance(prog, six.string_types):
        _prog_name = ""{}()"".format(prog)
        prog = [prog]
    else:
        raise ValueError(""prog must be a string"")
    if prog[0] == conf.prog.tcpdump and not TCPDUMP:
        message = ""tcpdump is not available. Cannot use tcpdump() !""
        raise Scapy_Exception(message)

    if linktype is not None:
        # Tcpdump does not support integers in -y (yet)
        # https://github.com/the-tcpdump-group/tcpdump/issues/758
        if isinstance(linktype, int):
            # Guess name from value
            try:
                linktype_name = _guess_linktype_name(linktype)
            except StopIteration:
                linktype = -1
        else:
            # Guess value from name
            if linktype.startswith(""DLT_""):
                linktype = linktype[4:]
            linktype_name = linktype
            try:
                linktype = _guess_linktype_value(linktype)
            except KeyError:
                linktype = -1
        if linktype == -1:
            raise ValueError(
                ""Unknown linktype. Try passing its datalink name instead""
            )
        prog += [""-y"", linktype_name]

    # Build Popen arguments
    if args is None:
        args = []
    else:
        # Make a copy of args
        args = list(args)

    stdout = subprocess.PIPE if dump or getfd else None
    stderr = open(os.devnull) if quiet else None

    if use_tempfile is None:
        # Apple's tcpdump cannot read from stdin, see:
        # http://apple.stackexchange.com/questions/152682/
        use_tempfile = DARWIN and prog[0] == conf.prog.tcpdump

    if read_stdin_opts is None:
        if prog[0] == conf.prog.wireshark:
            # Start capturing immediately (-k) from stdin (-i -)
            read_stdin_opts = [""-ki"", ""-""]
        else:
            read_stdin_opts = [""-r"", ""-""]
    else:
        # Make a copy of read_stdin_opts
        read_stdin_opts = list(read_stdin_opts)

    if pktlist is None:
        # sniff
        with ContextManagerSubprocess(_prog_name, prog[0]):
            proc = subprocess.Popen(
                prog + args,
                stdout=stdout,
                stderr=stderr,
            )
    elif isinstance(pktlist, six.string_types):
        # file
        with ContextManagerSubprocess(_prog_name, prog[0]):
            proc = subprocess.Popen(
                prog + [""-r"", pktlist] + args,
                stdout=stdout,
                stderr=stderr,
            )
    elif use_tempfile:
        tmpfile = get_temp_file(autoext="".pcap"", fd=True)
        try:
            tmpfile.writelines(iter(lambda: pktlist.read(1048576), b""""))
        except AttributeError:
            wrpcap(tmpfile, pktlist, linktype=linktype)
        else:
            tmpfile.close()
        with ContextManagerSubprocess(_prog_name, prog[0]):
            proc = subprocess.Popen(
                prog + [""-r"", tmpfile.name] + args,
                stdout=stdout,
                stderr=stderr,
            )
    else:
        # pass the packet stream
        with ContextManagerSubprocess(_prog_name, prog[0]):
            proc = subprocess.Popen(
                prog + read_stdin_opts + args,
                stdin=subprocess.PIPE,
                stdout=stdout,
                stderr=stderr,
            )
        try:
            proc.stdin.writelines(iter(lambda: pktlist.read(1048576), b""""))
        except AttributeError:
            wrpcap(proc.stdin, pktlist, linktype=linktype)
        except UnboundLocalError:
            raise IOError(""%s died unexpectedly !"" % prog)
        else:
            proc.stdin.close()
    if dump:
        return b"""".join(iter(lambda: proc.stdout.read(1048576), b""""))
    if getproc:
        return proc
    if getfd:
        return proc.stdout
    if wait:
        proc.wait()","1. Use `subprocess.check_output()` instead of `subprocess.Popen()` to avoid leaving zombie processes behind.
2. Use `subprocess.PIPE` instead of `subprocess.STDOUT` to avoid leaking sensitive information to the child process.
3. Use `subprocess.DEVNULL` instead of `open(os.devnull)` to avoid creating unnecessary files."
"    def network_stats(self):
        """"""Return a dictionary containing a summary of the Dot11
        elements fields
        """"""
        summary = {}
        crypto = set()
        p = self.payload
        while isinstance(p, Dot11Elt):
            if p.ID == 0:
                summary[""ssid""] = plain_str(p.info)
            elif p.ID == 3:
                summary[""channel""] = ord(p.info)
            elif isinstance(p, Dot11EltRates):
                summary[""rates""] = p.rates
            elif isinstance(p, Dot11EltRSN):
                crypto.add(""WPA2"")
            elif p.ID == 221:
                if isinstance(p, Dot11EltMicrosoftWPA) or \\
                        p.info.startswith('\\x00P\\xf2\\x01\\x01\\x00'):
                    crypto.add(""WPA"")
            p = p.payload
        if not crypto:
            if self.cap.privacy:
                crypto.add(""WEP"")
            else:
                crypto.add(""OPN"")
        summary[""crypto""] = crypto
        return summary","1. Use proper escaping for strings.
2. Sanitize user input.
3. Use a secure cryptographic library."
"def _set_conf_sockets():
    """"""Populate the conf.L2Socket and conf.L3Socket
    according to the various use_* parameters
    """"""
    if conf.use_bpf and not DARWIN:
        Interceptor.set_from_hook(conf, ""use_bpf"", False)
        raise ScapyInvalidPlatformException(""Darwin (OSX) only !"")
    if conf.use_winpcapy and not WINDOWS:
        Interceptor.set_from_hook(conf, ""use_winpcapy"", False)
        raise ScapyInvalidPlatformException(""Windows only !"")
    # we are already in an Interceptor hook, use Interceptor.set_from_hook
    if conf.use_pcap or conf.use_dnet or conf.use_winpcapy:
        try:
            from scapy.arch.pcapdnet import L2pcapListenSocket, L2pcapSocket, \\
                L3pcapSocket
        except ImportError:
            warning(""No pcap provider available ! pcap won't be used"")
            Interceptor.set_from_hook(conf, ""use_winpcapy"", False)
            Interceptor.set_from_hook(conf, ""use_pcap"", False)
        else:
            conf.L2listen = L2pcapListenSocket
            conf.L2socket = L2pcapSocket
            conf.L3socket = L3pcapSocket
            return
    if conf.use_bpf:
        from scapy.arch.bpf.supersocket import L2bpfListenSocket, \\
            L2bpfSocket, L3bpfSocket
        conf.L2listen = L2bpfListenSocket
        conf.L2socket = L2bpfSocket
        conf.L3socket = L3bpfSocket
        return
    if LINUX:
        from scapy.arch.linux import L3PacketSocket, L2Socket, L2ListenSocket
        conf.L3socket = L3PacketSocket
        conf.L2socket = L2Socket
        conf.L2listen = L2ListenSocket
        return
    if WINDOWS:  # Should have been conf.use_winpcapy
        from scapy.arch.windows import _NotAvailableSocket
        conf.L2socket = _NotAvailableSocket
        conf.L2listen = _NotAvailableSocket
        conf.L3socket = _NotAvailableSocket
        return
    from scapy.supersocket import L3RawSocket
    conf.L3socket = L3RawSocket","1. Use `conf.L2listen` and `conf.L2socket` instead of `L2pcapListenSocket` and `L2pcapSocket`.
2. Use `conf.L3socket` instead of `L3RawSocket`.
3. Use `conf.use_winpcapy` instead of `WINDOWS`."
"def get_ip_from_name(ifname, v6=False):
    """"""Backward compatibility: indirectly calls get_ips
    Deprecated.""""""
    return get_ips(v6=v6).get(ifname, """")[0]","1. Use `ipaddress` module instead of `socket` module to get the IP address.
2. Validate the input parameters to prevent injection attacks.
3. Use `ipaddr.ip_interface` to represent the IP address instead of a string."
"    def load_from_powershell(self):
        if not conf.prog.os_access:
            return
        ifaces_ips = None
        for i in get_windows_if_list():
            try:
                interface = NetworkInterface(i)
                self.data[interface.guid] = interface
                # If no IP address was detected using winpcap and if
                # the interface is not the loopback one, look for
                # internal windows interfaces
                if not interface.ip:
                    if not ifaces_ips:  # ifaces_ips is used as a cache
                        ifaces_ips = get_ips()
                    # If it exists, retrieve the interface's IP from the cache
                    interface.ip = ifaces_ips.get(interface.name, """")[0]
            except (KeyError, PcapNameNotFoundError):
                pass

        if not self.data and conf.use_winpcapy:
            _detect = pcap_service_status()

            def _ask_user():
                if not conf.interactive:
                    return False
                while True:
                    _confir = input(""Do you want to start it ? (yes/no) [y]: "").lower().strip()  # noqa: E501
                    if _confir in [""yes"", ""y"", """"]:
                        return True
                    elif _confir in [""no"", ""n""]:
                        return False
                return False
            _error_msg = ""No match between your pcap and windows network interfaces found. ""  # noqa: E501
            if _detect[0] and not _detect[2] and not (hasattr(self, ""restarted_adapter"") and self.restarted_adapter):  # noqa: E501
                warning(""Scapy has detected that your pcap service is not running !"")  # noqa: E501
                if not conf.interactive or _ask_user():
                    succeed = pcap_service_start(askadmin=conf.interactive)
                    self.restarted_adapter = True
                    if succeed:
                        log_loading.info(""Pcap service started !"")
                        self.load_from_powershell()
                        return
                _error_msg = ""Could not start the pcap service ! ""
            warning(_error_msg +
                    ""You probably won't be able to send packets. ""
                    ""Deactivating unneeded interfaces and restarting Scapy might help. ""  # noqa: E501
                    ""Check your winpcap and powershell installation, and access rights."")  # noqa: E501
        else:
            # Loading state: remove invalid interfaces
            self.remove_invalid_ifaces()
            # Replace LOOPBACK_INTERFACE
            try:
                scapy.consts.LOOPBACK_INTERFACE = self.dev_from_name(
                    scapy.consts.LOOPBACK_NAME,
                )
            except ValueError:
                pass","1. Use `conf.interactive` to check if the user is interactive before asking for input. This prevents automated scripts from starting the pcap service.
2. Use `pcap_service_status()` to check if the pcap service is running before trying to start it. This prevents the user from being prompted to start the service if it is already running.
3. Use `self.remove_invalid_ifaces()` to remove invalid interfaces from the list of interfaces before trying to send packets. This prevents the user from sending packets to interfaces that do not exist."
"def _exec_query_ps(cmd, fields):
    """"""Execute a PowerShell query, using the cmd command,
    and select and parse the provided fields.
    """"""
    if not conf.prog.powershell:
        raise OSError(""Scapy could not detect powershell !"")
    # Build query
    query_cmd = cmd + ['|', 'select %s' % ', '.join(fields),  # select fields
                       '|', 'fl',  # print as a list
                       '|', 'out-string', '-Width', '4096']  # do not crop
    lines = []
    # Ask the powershell manager to process the query
    stdout = POWERSHELL_PROCESS.query(query_cmd)
    # Process stdout
    for line in stdout:
        if not line.strip():  # skip empty lines
            continue
        sl = line.split(':', 1)
        if len(sl) == 1:
            lines[-1] += sl[0].strip()
            continue
        else:
            lines.append(sl[1].strip())
        if len(lines) == len(fields):
            yield lines
            lines = []","1. Use `subprocess` instead of `cmd` to execute PowerShell commands. This will prevent PowerShell from being used to execute arbitrary code.
2. Use `shell=False` when calling `subprocess.Popen` to prevent PowerShell from interpreting the command as a script.
3. Use `universal_newlines=True` when calling `subprocess.communicate` to ensure that the output from PowerShell is properly decoded."
"def get_ips(v6=False):
    """"""Returns all available IPs matching to interfaces, using the windows system.
    Should only be used as a WinPcapy fallback.""""""
    res = {}
    for descr, ipaddr in exec_query(['Get-WmiObject',
                                     'Win32_NetworkAdapterConfiguration'],
                                    ['Description', 'IPAddress']):
        if ipaddr.strip():
            res[descr] = ipaddr.split("","", 1)[v6].strip('{}').strip()
    return res","1. Use `getpass.getpass()` instead of `input()` to prompt for passwords.
2. Use `subprocess.check_output()` instead of `exec_query()` to execute commands.
3. Use `ipaddress.ip_address()` to parse IP addresses instead of splitting strings."
"def get_ip_from_name(ifname, v6=False):
    """"""Backward compatibility: indirectly calls get_ips
    Deprecated.""""""
    return get_ips(v6=v6).get(ifname, """")","1. Use `ipaddress` module to validate IP addresses.
2. Sanitize user input to prevent injection attacks.
3. Use `cryptography` module to securely generate random strings."
"    def load_from_powershell(self):
        if not conf.prog.os_access:
            return
        ifaces_ips = None
        for i in get_windows_if_list():
            try:
                interface = NetworkInterface(i)
                self.data[interface.guid] = interface
                # If no IP address was detected using winpcap and if
                # the interface is not the loopback one, look for
                # internal windows interfaces
                if not interface.ip:
                    if not ifaces_ips:  # ifaces_ips is used as a cache
                        ifaces_ips = get_ips()
                    # If it exists, retrieve the interface's IP from the cache
                    interface.ip = ifaces_ips.get(interface.name, """")
            except (KeyError, PcapNameNotFoundError):
                pass

        if not self.data and conf.use_winpcapy:
            _detect = pcap_service_status()

            def _ask_user():
                if not conf.interactive:
                    return False
                while True:
                    _confir = input(""Do you want to start it ? (yes/no) [y]: "").lower().strip()  # noqa: E501
                    if _confir in [""yes"", ""y"", """"]:
                        return True
                    elif _confir in [""no"", ""n""]:
                        return False
                return False
            _error_msg = ""No match between your pcap and windows network interfaces found. ""  # noqa: E501
            if _detect[0] and not _detect[2] and not (hasattr(self, ""restarted_adapter"") and self.restarted_adapter):  # noqa: E501
                warning(""Scapy has detected that your pcap service is not running !"")  # noqa: E501
                if not conf.interactive or _ask_user():
                    succeed = pcap_service_start(askadmin=conf.interactive)
                    self.restarted_adapter = True
                    if succeed:
                        log_loading.info(""Pcap service started !"")
                        self.load_from_powershell()
                        return
                _error_msg = ""Could not start the pcap service ! ""
            warning(_error_msg +
                    ""You probably won't be able to send packets. ""
                    ""Deactivating unneeded interfaces and restarting Scapy might help. ""  # noqa: E501
                    ""Check your winpcap and powershell installation, and access rights."")  # noqa: E501
        else:
            # Loading state: remove invalid interfaces
            self.remove_invalid_ifaces()
            # Replace LOOPBACK_INTERFACE
            try:
                scapy.consts.LOOPBACK_INTERFACE = self.dev_from_name(
                    scapy.consts.LOOPBACK_NAME,
                )
            except ValueError:
                pass","1. Use `conf.prog.os_access` to check if the user has permission to access the network interface.
2. Use `get_windows_if_list()` to get a list of all network interfaces on the system.
3. Use `NetworkInterface()` to create a new network interface object for each interface in the list."
"    def i2h(self, pkt, x):
        return self._fixup_val(super(FlagsField, self).i2h(pkt, x))","1. Use `functools.wraps` to preserve the function signature of `super(FlagsField, self).i2h(pkt, x)`.
2. Use `six.ensure_str` to ensure that `x` is a string.
3. Use `six.ensure_binary` to ensure that the return value is a byte string."
"    def cpu_freq():
        """"""Alternate implementation using /proc/cpuinfo.
        min and max frequencies are not available and are set to None.
        """"""
        ret = []
        with open_binary('%s/cpuinfo' % get_procfs_path()) as f:
            for line in f:
                if line.lower().startswith(b'cpu mhz'):
                    key, value = line.split(b'\\t:', 1)
                    ret.append(_common.scpufreq(float(value), None, None))
        return ret","1. Use `open()` instead of `open_binary()` to open the file in text mode.
2. Use `f.read()` to read the file contents instead of iterating over the lines.
3. Use `os.path.join()` to concatenate the path to the file instead of hardcoding it."
"    def _proc_cred(self):
        @wrap_exceptions
        def proc_cred(self):
            return cext.proc_cred(self.pid, self._procfs_path)
        return proc_cred(self)","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `inspect.getfullargspec` to get the argument names of the wrapped function.
3. Use `functools.partial` to create a new function with a subset of the arguments of the wrapped function."
"def main():
    ad_pids = []
    procs = []
    for p in psutil.process_iter():
        with p.oneshot():
            try:
                mem = p.memory_full_info()
                info = p.as_dict([""cmdline"", ""username""])
            except psutil.AccessDenied:
                ad_pids.append(p.pid)
            except psutil.NoSuchProcess:
                pass
            else:
                p._uss = mem.uss
                p._rss = mem.rss
                if not p._uss:
                    continue
                p._pss = getattr(mem, ""pss"", """")
                p._swap = getattr(mem, ""swap"", """")
                p._info = info
                procs.append(p)

    procs.sort(key=lambda p: p._uss)
    templ = ""%-7s %-7s %-30s %7s %7s %7s %7s""
    print(templ % (""PID"", ""User"", ""Cmdline"", ""USS"", ""PSS"", ""Swap"", ""RSS""))
    print(""="" * 78)
    for p in procs[:86]:
        line = templ % (
            p.pid,
            p._info[""username""][:7] if p._info[""username""] else """",
            "" "".join(p._info[""cmdline""])[:30],
            convert_bytes(p._uss),
            convert_bytes(p._pss) if p._pss != """" else """",
            convert_bytes(p._swap) if p._swap != """" else """",
            convert_bytes(p._rss),
        )
        print(line)
    if ad_pids:
        print(""warning: access denied for %s pids"" % (len(ad_pids)),
              file=sys.stderr)","1. Use `psutil.Credentials()` to avoid access denied errors.
2. Use `psutil.Process.as_dict()` to get process information without raising exceptions.
3. Use `psutil.Process.children()` to get a list of child processes."
"    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except OSError as err:
            raise convert_oserror(err, pid=self.pid, name=self._name)","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Handle `OSError` exceptions more gracefully.
3. Use `inspect.getfullargspec` to get the full argument list of the function."
"def wrap_exceptions(fun):
    """"""Call callable into a try/except clause and translate ENOENT,
    EACCES and EPERM in NoSuchProcess or AccessDenied exceptions.
    """"""

    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except EnvironmentError as err:
            # support for private module import
            if (NoSuchProcess is None or AccessDenied is None or
                    ZombieProcess is None):
                raise
            # ENOENT (no such file or directory) gets raised on open().
            # ESRCH (no such process) can get raised on read() if
            # process is gone in meantime.
            if err.errno in (errno.ENOENT, errno.ESRCH):
                if not pid_exists(self.pid):
                    raise NoSuchProcess(self.pid, self._name)
                else:
                    raise ZombieProcess(self.pid, self._name, self._ppid)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise
    return wrapper","1. Use `functools.wraps` to preserve the function signature.
2. Use `inspect.getfullargspec` to get the function arguments.
3. Use `functools.partial` to create a new function with a subset of the original function's arguments."
"def wrap_exceptions(fun):
    """"""Call callable into a try/except clause and translate ENOENT,
    EACCES and EPERM in NoSuchProcess or AccessDenied exceptions.
    """"""

    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except EnvironmentError as err:
            if self.pid == 0:
                if 0 in pids():
                    raise AccessDenied(self.pid, self._name)
                else:
                    raise
            # ENOENT (no such file or directory) gets raised on open().
            # ESRCH (no such process) can get raised on read() if
            # process is gone in meantime.
            if err.errno in (errno.ENOENT, errno.ESRCH):
                if not pid_exists(self.pid):
                    raise NoSuchProcess(self.pid, self._name)
                else:
                    raise ZombieProcess(self.pid, self._name, self._ppid)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise
    return wrapper","1. Use `functools.wraps` to preserve the function's metadata.
2. Add a `check_permissions` function to check if the user has permission to access the process.
3. Use `logging` to log all errors and exceptions."
"def wrap_exceptions(fun):
    """"""Decorator which translates bare OSError exceptions into
    NoSuchProcess and AccessDenied.
    """"""
    @functools.wraps(fun)
    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except OSError as err:
            if err.errno == errno.ESRCH:
                raise NoSuchProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise
    return wrapper","1. Use `functools.wraps` to preserve the function signature.
2. Use `try-except` to catch OSError and raise specific exceptions.
3. Use `raise` to re-raise the original exception."
"    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except OSError as err:
            if err.errno == errno.ESRCH:
                raise NoSuchProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise","1. Use `os.fchmod` to set the file mode to 0o600 to restrict permissions for the file.
2. Use `os.fchown` to change the owner of the file to the current user.
3. Use `os.umask` to set the default permissions for newly created files to 0o644."
"    def threads(self):
        with catch_zombie(self):
            rawlist = cext.proc_threads(self.pid)
        retlist = []
        for thread_id, utime, stime in rawlist:
            ntuple = _common.pthread(thread_id, utime, stime)
            retlist.append(ntuple)
        return retlist","1. Use `cext.proc_threads(self.pid)` to get the list of threads.
2. Use `_common.pthread(thread_id, utime, stime)` to create a tuple for each thread.
3. Return the list of tuples."
"    def memory_maps(self):
        with catch_zombie(self):
            return cext.proc_memory_maps(self.pid)","1. Use `cext.proc_memory_maps` with a `contextlib.closing` block to ensure that the underlying C function is closed properly.
2. Use `os.fchmod` to set the file mode of the returned file descriptor to `0o600` to restrict permissions.
3. Use `os.fdopen` to open the file descriptor in read-only mode to prevent writes."
"    def exe(self):
        # Note: os.path.exists(path) may return False even if the file
        # is there, see:
        # http://stackoverflow.com/questions/3112546/os-path-exists-lies

        # see https://github.com/giampaolo/psutil/issues/414
        # see https://github.com/giampaolo/psutil/issues/528
        if self.pid in (0, 4):
            raise AccessDenied(self.pid, self._name)
        exe = cext.proc_exe(self.pid)
        exe = convert_dos_path(exe)
        return py2_strencode(exe)","1. Use `os.access` to check if the file exists before calling `os.path.exists`. This will prevent a race condition where the file may be deleted between the two calls.
2. Use `os.fchmod` to set the file mode to `0o777` (read, write, and execute for all users). This will allow anyone to read the file's contents.
3. Use `os.chown` to change the owner of the file to `root`. This will give root ownership of the file, which is a security risk."
"    def exe(self):
        # Note: os.path.exists(path) may return False even if the file
        # is there, see:
        # http://stackoverflow.com/questions/3112546/os-path-exists-lies

        # see https://github.com/giampaolo/psutil/issues/414
        # see https://github.com/giampaolo/psutil/issues/528
        if self.pid in (0, 4):
            raise AccessDenied(self.pid, self._name)
        return py2_strencode(convert_dos_path(cext.proc_exe(self.pid)))","1. Use `os.access()` to check if the file exists before calling `os.path.exists()`.
2. Use `os.fstat()` to get the file's permissions before calling `os.read()`.
3. Use `os.close()` to close the file after you're done reading it."
"def disk_io_counters(perdisk=False):
    """"""Return disk I/O statistics for every disk installed on the
    system as a dict of raw tuples.
    """"""
    def read_procfs():
        # OK, this is a bit confusing. The format of /proc/diskstats can
        # have 3 variations.
        # On Linux 2.4 each line has always 15 fields, e.g.:
        # ""3     0   8 hda 8 8 8 8 8 8 8 8 8 8 8""
        # On Linux 2.6+ each line *usually* has 14 fields, and the disk
        # name is in another position, like this:
        # ""3    0   hda 8 8 8 8 8 8 8 8 8 8 8""
        # ...unless (Linux 2.6) the line refers to a partition instead
        # of a disk, in which case the line has less fields (7):
        # ""3    1   hda1 8 8 8 8""
        # See:
        # https://www.kernel.org/doc/Documentation/iostats.txt
        # https://www.kernel.org/doc/Documentation/ABI/testing/procfs-diskstats
        with open_text(""%s/diskstats"" % get_procfs_path()) as f:
            lines = f.readlines()
        for line in lines:
            fields = line.split()
            flen = len(fields)
            if flen == 15:
                # Linux 2.4
                name = fields[3]
                reads = int(fields[2])
                (reads_merged, rbytes, rtime, writes, writes_merged,
                    wbytes, wtime, _, busy_time, _) = map(int, fields[4:14])
            elif flen == 14:
                # Linux 2.6+, line referring to a disk
                name = fields[2]
                (reads, reads_merged, rbytes, rtime, writes, writes_merged,
                    wbytes, wtime, _, busy_time, _) = map(int, fields[3:14])
            elif flen == 7:
                # Linux 2.6+, line referring to a partition
                name = fields[2]
                reads, rbytes, writes, wbytes = map(int, fields[3:])
                rtime = wtime = reads_merged = writes_merged = busy_time = 0
            else:
                raise ValueError(""not sure how to interpret line %r"" % line)
            yield (name, reads, writes, rbytes, wbytes, rtime, wtime,
                   reads_merged, writes_merged, busy_time)

    def read_sysfs():
        for block in os.listdir('/sys/block'):
            for root, _, files in os.walk(os.path.join('/sys/block', block)):
                if 'stat' not in files:
                    continue
                with open_text(os.path.join(root, 'stat')) as f:
                    fields = f.read().strip().split()
                name = os.path.basename(root)
                (reads, reads_merged, rbytes, rtime, writes, writes_merged,
                    wbytes, wtime, _, busy_time, _) = map(int, fields)
                yield (name, reads, writes, rbytes, wbytes, rtime,
                       wtime, reads_merged, writes_merged, busy_time)

    if os.path.exists('%s/diskstats' % get_procfs_path()):
        gen = read_procfs()
    elif os.path.exists('/sys/block'):
        gen = read_sysfs()
    else:
        raise NotImplementedError(
            ""%s/diskstats nor /sys/block filesystem are available on this ""
            ""system"" % get_procfs_path())

    retdict = {}
    for entry in gen:
        (name, reads, writes, rbytes, wbytes, rtime, wtime, reads_merged,
            writes_merged, busy_time) = entry
        if not perdisk and not is_storage_device(name):
            # perdisk=False means we want to calculate totals so we skip
            # partitions (e.g. 'sda1', 'nvme0n1p1') and only include
            # base disk devices (e.g. 'sda', 'nvme0n1'). Base disks
            # include a total of all their partitions + some extra size
            # of their own:
            #     $ cat /proc/diskstats
            #     259       0 sda 10485760 ...
            #     259       1 sda1 5186039 ...
            #     259       1 sda2 5082039 ...
            # See:
            # https://github.com/giampaolo/psutil/pull/1313
            continue

        rbytes *= DISK_SECTOR_SIZE
        wbytes *= DISK_SECTOR_SIZE
        retdict[name] = (reads, writes, rbytes, wbytes, rtime, wtime,
                         reads_merged, writes_merged, busy_time)

    return retdict","1. Use `os.path.join()` to sanitize paths instead of concatenating strings.
2. Use `os.listdir()` with the `listdir_full()` flag to avoid directory traversal attacks.
3. Use `open_text()` to open files in text mode, and `open_binary()` to open files in binary mode."
"    def read_procfs():
        # OK, this is a bit confusing. The format of /proc/diskstats can
        # have 3 variations.
        # On Linux 2.4 each line has always 15 fields, e.g.:
        # ""3     0   8 hda 8 8 8 8 8 8 8 8 8 8 8""
        # On Linux 2.6+ each line *usually* has 14 fields, and the disk
        # name is in another position, like this:
        # ""3    0   hda 8 8 8 8 8 8 8 8 8 8 8""
        # ...unless (Linux 2.6) the line refers to a partition instead
        # of a disk, in which case the line has less fields (7):
        # ""3    1   hda1 8 8 8 8""
        # See:
        # https://www.kernel.org/doc/Documentation/iostats.txt
        # https://www.kernel.org/doc/Documentation/ABI/testing/procfs-diskstats
        with open_text(""%s/diskstats"" % get_procfs_path()) as f:
            lines = f.readlines()
        for line in lines:
            fields = line.split()
            flen = len(fields)
            if flen == 15:
                # Linux 2.4
                name = fields[3]
                reads = int(fields[2])
                (reads_merged, rbytes, rtime, writes, writes_merged,
                    wbytes, wtime, _, busy_time, _) = map(int, fields[4:14])
            elif flen == 14:
                # Linux 2.6+, line referring to a disk
                name = fields[2]
                (reads, reads_merged, rbytes, rtime, writes, writes_merged,
                    wbytes, wtime, _, busy_time, _) = map(int, fields[3:14])
            elif flen == 7:
                # Linux 2.6+, line referring to a partition
                name = fields[2]
                reads, rbytes, writes, wbytes = map(int, fields[3:])
                rtime = wtime = reads_merged = writes_merged = busy_time = 0
            else:
                raise ValueError(""not sure how to interpret line %r"" % line)
            yield (name, reads, writes, rbytes, wbytes, rtime, wtime,
                   reads_merged, writes_merged, busy_time)","1. Use `open_binary()` instead of `open_text()` to open the file in binary mode. This will prevent the file from being corrupted by text characters.
2. Use `os.fstat()` to get the file size before reading it. This will prevent the file from being read past its end, which could lead to a buffer overflow.
3. Use `os.close()` to close the file after reading it. This will free up system resources and prevent the file from being left open, which could lead to a security vulnerability."
"def cat(fname, fallback=_DEFAULT, binary=True):
    """"""Return file content.
    fallback: the value returned in case the file does not exist or
              cannot be read
    binary: whether to open the file in binary or text mode.
    """"""
    try:
        with open_binary(fname) if binary else open_text(fname) as f:
            return f.read().strip()
    except IOError:
        if fallback is not _DEFAULT:
            return fallback
        else:
            raise","1. **Use `contextlib.closing` to ensure that the file is closed after use.** This will help to prevent resource leaks.
2. **Use `os.fchmod` to set the file mode to `0644` (read-only for owner).** This will help to prevent unauthorized users from modifying the file contents.
3. **Use `os.fchown` to set the file ownership to the current user.** This will help to prevent unauthorized users from deleting or renaming the file."
"def sensors_temperatures():
    """"""Return hardware (CPU and others) temperatures as a dict
    including hardware name, label, current, max and critical
    temperatures.

    Implementation notes:
    - /sys/class/hwmon looks like the most recent interface to
      retrieve this info, and this implementation relies on it
      only (old distros will probably use something else)
    - lm-sensors on Ubuntu 16.04 relies on /sys/class/hwmon
    - /sys/class/thermal/thermal_zone* is another one but it's more
      difficult to parse
    """"""
    ret = collections.defaultdict(list)
    basenames = glob.glob('/sys/class/hwmon/hwmon*/temp*_*')
    # CentOS has an intermediate /device directory:
    # https://github.com/giampaolo/psutil/issues/971
    # https://github.com/nicolargo/glances/issues/1060
    basenames.extend(glob.glob('/sys/class/hwmon/hwmon*/device/temp*_*'))
    basenames = sorted(set([x.split('_')[0] for x in basenames]))

    for base in basenames:
        try:
            path = base + '_input'
            current = float(cat(path)) / 1000.0
            path = os.path.join(os.path.dirname(base), 'name')
            unit_name = cat(path, binary=False)
        except (IOError, OSError) as err:
            # A lot of things can go wrong here, so let's just skip the
            # whole entry.
            # https://github.com/giampaolo/psutil/issues/1009
            # https://github.com/giampaolo/psutil/issues/1101
            # https://github.com/giampaolo/psutil/issues/1129
            # https://github.com/giampaolo/psutil/issues/1245
            warnings.warn(""ignoring %r for file %r"" % (err, path),
                          RuntimeWarning)
            continue

        high = cat(base + '_max', fallback=None)
        critical = cat(base + '_crit', fallback=None)
        label = cat(base + '_label', fallback='', binary=False)

        if high is not None:
            high = float(high) / 1000.0
        if critical is not None:
            critical = float(critical) / 1000.0

        ret[unit_name].append((label, current, high, critical))

    return ret","1. Use `pathlib` instead of `os.path` to avoid `os.path.join` injection.
2. Use `pathlib.Path.is_file` instead of `os.path.isfile` to avoid `os.path.isfile` race condition.
3. Use `pathlib.Path.read_text` instead of `open` to avoid `open` file descriptor leak."
"    def _proc_cred(self):
        return cext.proc_cred(self.pid, self._procfs_path)","1. Use `cext.proc_cred_get()` instead of `cext.proc_cred()` to avoid leaking the process ID.
2. Sanitize the `procfs_path` argument to prevent path traversal attacks.
3. Use `cext.proc_cred_free()` to free the returned credentials structure."
"    def nice_get(self):
        # Note #1: for some reason getpriority(3) return ESRCH (no such
        # process) for certain low-pid processes, no matter what (even
        # as root).
        # The process actually exists though, as it has a name,
        # creation time, etc.
        # The best thing we can do here appears to be raising AD.
        # Note: tested on Solaris 11; on Open Solaris 5 everything is
        # fine.
        #
        # Note #2: we also can get niceness from /proc/pid/psinfo
        # but it's wrong, see:
        # https://github.com/giampaolo/psutil/issues/1082
        try:
            return cext_posix.getpriority(self.pid)
        except EnvironmentError as err:
            # 48 is 'operation not supported' but errno does not expose
            # it. It occurs for low system pids.
            if err.errno in (errno.ENOENT, errno.ESRCH, 48):
                if pid_exists(self.pid):
                    raise AccessDenied(self.pid, self._name)
            raise","1. Use `os.getpid()` to get the current process ID instead of `self.pid`. This will prevent leaking information about other processes.
2. Check the return value of `cext_posix.getpriority()` and raise a more specific exception if it fails.
3. Use `pid_exists()` to check if the process exists before raising `AccessDenied`. This will prevent raising an exception for processes that don't exist."
"    def uids(self):
        real, effective, saved, _, _, _ = self._proc_cred()
        return _common.puids(real, effective, saved)","1. Use `os.geteuid()` instead of `os.getuid()` to get the effective user ID.
2. Use `os.getresuid()` instead of `os.getuid()`, `os.geteuid()`, and `os.getsuid()` to get all three user IDs.
3. Use `pwd.getpwuid()` to get the username from the user ID."
"    def gids(self):
        _, _, _, real, effective, saved = self._proc_cred()
        return _common.puids(real, effective, saved)","1. Use `os.geteuid()` and `os.getegid()` to get the real and effective UIDs and GIDs, instead of calling `pwd.getpwuid()` and `grp.getgrgid()`.
2. Use `pwd.getpwnam()` and `grp.getgrnam()` to get the username and group name, instead of calling `pwd.getpwuid()` and `grp.getgrgid()`.
3. Use `os.getlogin()` to get the login name, instead of calling `pwd.getpwuid()` and `grp.getgrgid()`."
"def sensors_temperatures():
    """"""Return hardware (CPU and others) temperatures as a dict
    including hardware name, label, current, max and critical
    temperatures.

    Implementation notes:
    - /sys/class/hwmon looks like the most recent interface to
      retrieve this info, and this implementation relies on it
      only (old distros will probably use something else)
    - lm-sensors on Ubuntu 16.04 relies on /sys/class/hwmon
    - /sys/class/thermal/thermal_zone* is another one but it's more
      difficult to parse
    """"""
    ret = collections.defaultdict(list)
    basenames = glob.glob('/sys/class/hwmon/hwmon*/temp*_*')
    # CentOS has an intermediate /device directory:
    # https://github.com/giampaolo/psutil/issues/971
    # https://github.com/nicolargo/glances/issues/1060
    basenames.extend(glob.glob('/sys/class/hwmon/hwmon*/device/temp*_*'))
    basenames = sorted(set([x.split('_')[0] for x in basenames]))

    for base in basenames:
        try:
            current = float(cat(base + '_input')) / 1000.0
        except (IOError, OSError) as err:
            # A lot of things can go wrong here, so let's just skip the
            # whole entry.
            # https://github.com/giampaolo/psutil/issues/1009
            # https://github.com/giampaolo/psutil/issues/1101
            # https://github.com/giampaolo/psutil/issues/1129
            warnings.warn(""ignoring %r"" % err, RuntimeWarning)
            continue

        unit_name = cat(os.path.join(os.path.dirname(base), 'name'),
                        binary=False)
        high = cat(base + '_max', fallback=None)
        critical = cat(base + '_crit', fallback=None)
        label = cat(base + '_label', fallback='', binary=False)

        if high is not None:
            high = float(high) / 1000.0
        if critical is not None:
            critical = float(critical) / 1000.0

        ret[unit_name].append((label, current, high, critical))

    return ret","1. Use `try ... except` blocks to handle errors and prevent the function from crashing.
2. Use `os.path.join` to concatenate paths instead of concatenating them manually.
3. Use `warnings.warn` to log warnings instead of printing them to the console."
"    def wait(self, timeout=None):
        if timeout is None:
            cext_timeout = cext.INFINITE
        else:
            # WaitForSingleObject() expects time in milliseconds
            cext_timeout = int(timeout * 1000)
        ret = cext.proc_wait(self.pid, cext_timeout)
        if ret == WAIT_TIMEOUT:
            raise TimeoutExpired(timeout, self.pid, self._name)
        return ret","1. Use `os.fchmod` to set the file mode to 0600 to prevent unauthorized access.
2. Use `os.fchown` to change the file owner to root to prevent unauthorized modification.
3. Use `os.flock` to lock the file to prevent concurrent access."
"    def wait(self, timeout=None):
        if timeout is None:
            cext_timeout = cext.INFINITE
        else:
            # WaitForSingleObject() expects time in milliseconds
            cext_timeout = int(timeout * 1000)
        while True:
            ret = cext.proc_wait(self.pid, cext_timeout)
            if ret == WAIT_TIMEOUT:
                raise TimeoutExpired(timeout, self.pid, self._name)
            if timeout is None and pid_exists(self.pid):
                continue
            return ret","1. Use `os.fchmod` to set the file mode to `0o600` to restrict permissions for the created file.
2. Use `os.fchown` to set the file owner to the current user to prevent unauthorized access.
3. Use `os.close` to close the file handle after the file is created to prevent data leakage."
"    def nice_get(self):
        # For some reason getpriority(3) return ESRCH (no such process)
        # for certain low-pid processes, no matter what (even as root).
        # The process actually exists though, as it has a name,
        # creation time, etc.
        # The best thing we can do here appears to be raising AD.
        # Note: tested on Solaris 11; on Open Solaris 5 everything is
        # fine.
        try:
            return cext_posix.getpriority(self.pid)
        except EnvironmentError as err:
            # 48 is 'operation not supported' but errno does not expose
            # it. It occurs for low system pids.
            if err.errno in (errno.ENOENT, errno.ESRCH, 48):
                if pid_exists(self.pid):
                    raise AccessDenied(self.pid, self._name)
            raise","1. Use `os.getpid()` instead of `self.pid` to get the process ID.
2. Check the return value of `cext_posix.getpriority()` and handle errors appropriately.
3. Use `pwd.getpwuid()` to get the username of the process owner and compare it to the current user."
"    def sensors_battery():
        """"""Return battery info.""""""
        percent, minsleft, power_plugged = cext.sensors_battery()
        power_plugged = power_plugged == 1
        if power_plugged:
            secsleft = _common.POWER_TIME_UNLIMITED
        elif minsleft == -1:
            secsleft = _common.POWER_TIME_UNKNOWN
        else:
            secsleft = minsleft * 60
        return _common.sbattery(percent, secsleft, power_plugged)","1. Use `cryptography` to safely generate random numbers.
2. Sanitize user input to prevent injection attacks.
3. Use `f-strings` to prevent format string vulnerabilities."
"def pid_exists(pid):
    """"""Check For the existence of a unix pid.""""""
    return _psposix.pid_exists(pid)","1. Use `os.getpid()` instead of `_psposix.pid_exists()` to get the current process ID. This will prevent a malicious user from passing in an arbitrary PID and causing the function to return a false positive.
2. Sanitize the input to `pid_exists()` to prevent a malicious user from passing in a string that could be interpreted as a PID.
3. Use `os.access()` to check if the process with the given PID exists instead of calling `_psposix.pid_exists()` directly. This will prevent a malicious user from bypassing the security checks by using a process that has already exited."
"def swap_memory():
    """"""System swap memory as (total, used, free, sin, sout) namedtuple.""""""
    pagesize = 1 if OPENBSD else PAGESIZE
    total, used, free, sin, sout = [x * pagesize for x in cext.swap_mem()]
    percent = usage_percent(used, total, _round=1)
    return _common.sswap(total, used, free, percent, sin, sout)","1. Use a constant for PAGESIZE instead of a function call.
2. Validate the return value of cext.swap_mem() to ensure that it is a tuple of length 5.
3. Sanitize the input to usage_percent() to prevent a divide-by-zero error."
"    def cpu_affinity_set(self, cpus):
        try:
            cext.proc_cpu_affinity_set(self.pid, cpus)
        except OSError as err:
            if err.errno == errno.EINVAL:
                allcpus = tuple(range(len(per_cpu_times())))
                for cpu in cpus:
                    if cpu not in allcpus:
                        raise ValueError(""invalid CPU #%i (choose between %s)""
                                         % (cpu, allcpus))
            raise","1. Use `os.getpid()` instead of `self.pid` to get the process ID. This will prevent an attacker from tricking the code into setting the affinity of a different process.
2. Check the return value of `cext.proc_cpu_affinity_set()` to make sure that the operation succeeded.
3. Handle `OSError` exceptions more gracefully. For example, you could catch the `EINVAL` error and return a `ValueError` with a more informative message."
"    def process_unix(self, file, family, inodes, filter_pid=None):
        """"""Parse /proc/net/unix files.""""""
        with open_text(file, buffering=BIGGER_FILE_BUFFERING) as f:
            f.readline()  # skip the first line
            for line in f:
                tokens = line.split()
                try:
                    _, _, _, _, type_, _, inode = tokens[0:7]
                except ValueError:
                    raise RuntimeError(
                        ""error while parsing %s; malformed line %r"" % (
                            file, line))
                if inode in inodes:
                    # With UNIX sockets we can have a single inode
                    # referencing many file descriptors.
                    pairs = inodes[inode]
                else:
                    pairs = [(None, -1)]
                for pid, fd in pairs:
                    if filter_pid is not None and filter_pid != pid:
                        continue
                    else:
                        if len(tokens) == 8:
                            path = tokens[-1]
                        else:
                            path = """"
                        type_ = int(type_)
                        raddr = None
                        status = _common.CONN_NONE
                        yield (fd, family, type_, path, raddr, status, pid)","1. Use `open_text` with `buffering=0` to avoid reading more data than necessary.
2. Check the length of `tokens` to ensure that it is 8.
3. Use `int()` to convert the type_ field to an integer."
"    def process_inet(self, file, family, type_, inodes, filter_pid=None):
        """"""Parse /proc/net/tcp* and /proc/net/udp* files.""""""
        if file.endswith('6') and not os.path.exists(file):
            # IPv6 not supported
            return
        with open_text(file, buffering=BIGGER_FILE_BUFFERING) as f:
            f.readline()  # skip the first line
            for line in f:
                try:
                    _, laddr, raddr, status, _, _, _, _, _, inode = \\
                        line.split()[:10]
                except ValueError:
                    raise RuntimeError(
                        ""error while parsing %s; malformed line %r"" % (
                            file, line))
                if inode in inodes:
                    # # We assume inet sockets are unique, so we error
                    # # out if there are multiple references to the
                    # # same inode. We won't do this for UNIX sockets.
                    # if len(inodes[inode]) > 1 and family != socket.AF_UNIX:
                    #     raise ValueError(""ambiguos inode with multiple ""
                    #                      ""PIDs references"")
                    pid, fd = inodes[inode][0]
                else:
                    pid, fd = None, -1
                if filter_pid is not None and filter_pid != pid:
                    continue
                else:
                    if type_ == socket.SOCK_STREAM:
                        status = TCP_STATUSES[status]
                    else:
                        status = _common.CONN_NONE
                    try:
                        laddr = self.decode_address(laddr, family)
                        raddr = self.decode_address(raddr, family)
                    except _Ipv6UnsupportedError:
                        continue
                    yield (fd, family, type_, laddr, raddr, status, pid)","1. Use `try ... except` to handle errors.
2. Check if the file exists before opening it.
3. Use `filter_pid` to filter out sockets that are not associated with the specified PID."
"        def memory_maps(self):
            """"""Return process's mapped memory regions as a list of named tuples.
            Fields are explained in 'man proc'; here is an updated (Apr 2012)
            version: http://goo.gl/fmebo
            """"""
            with open_text(""%s/%s/smaps"" % (self._procfs_path, self.pid),
                           buffering=BIGGER_FILE_BUFFERING) as f:
                first_line = f.readline()
                current_block = [first_line]

                def get_blocks():
                    data = {}
                    for line in f:
                        fields = line.split(None, 5)
                        if not fields[0].endswith(':'):
                            # new block section
                            yield (current_block.pop(), data)
                            current_block.append(line)
                        else:
                            try:
                                data[fields[0]] = int(fields[1]) * 1024
                            except ValueError:
                                if fields[0].startswith('VmFlags:'):
                                    # see issue #369
                                    continue
                                else:
                                    raise ValueError(""don't know how to inte""
                                                     ""rpret line %r"" % line)
                    yield (current_block.pop(), data)

                ls = []
                if first_line:  # smaps file can be empty
                    for header, data in get_blocks():
                        hfields = header.split(None, 5)
                        try:
                            addr, perms, offset, dev, inode, path = hfields
                        except ValueError:
                            addr, perms, offset, dev, inode, path = \\
                                hfields + ['']
                        if not path:
                            path = '[anon]'
                        else:
                            path = path.strip()
                        ls.append((
                            addr, perms, path,
                            data['Rss:'],
                            data.get('Size:', 0),
                            data.get('Pss:', 0),
                            data.get('Shared_Clean:', 0),
                            data.get('Shared_Dirty:', 0),
                            data.get('Private_Clean:', 0),
                            data.get('Private_Dirty:', 0),
                            data.get('Referenced:', 0),
                            data.get('Anonymous:', 0),
                            data.get('Swap:', 0)
                        ))
            return ls","1. Use `pathlib.Path` instead of `os.path` to handle paths more securely.
2. Use `contextlib.closing` to ensure that the file is closed after it is used.
3. Use `shutil.which` to check if the `procfs` module is installed, and raise an exception if it is not."
"    def get_ethtool_macro():
        # see: https://github.com/giampaolo/psutil/issues/659
        from distutils.unixccompiler import UnixCCompiler
        from distutils.errors import CompileError
        with tempfile.NamedTemporaryFile(
                suffix='.c', delete=False, mode=""wt"") as f:
            f.write(""#include <linux/ethtool.h>"")
        atexit.register(os.remove, f.name)
        compiler = UnixCCompiler()
        try:
            with captured_output('stderr'):
                with captured_output('stdout'):
                    compiler.compile([f.name])
        except CompileError:
            return (""PSUTIL_ETHTOOL_MISSING_TYPES"", 1)
        else:
            return None","1. **Use a secure temporary file**. The code should use a secure temporary file that is deleted when it is no longer needed. This can be done using the `tempfile.NamedTemporaryFile` function with the `delete=True` argument.
2. **Handle errors correctly**. The code should handle errors correctly by catching the `CompileError` exception and returning an appropriate value.
3. **Use a secure compiler**. The code should use a secure compiler that is not vulnerable to known compiler bugs."
"    def process_inet(self, file, family, type_, inodes, filter_pid=None):
        """"""Parse /proc/net/tcp* and /proc/net/udp* files.""""""
        if file.endswith('6') and not os.path.exists(file):
            # IPv6 not supported
            return
        with open(file, 'rt') as f:
            f.readline()  # skip the first line
            for line in f:
                try:
                    _, laddr, raddr, status, _, _, _, _, _, inode = \\
                        line.split()[:10]
                except ValueError:
                    raise RuntimeError(
                        ""error while parsing %s; malformed line %r"" % (
                            file, line))
                if inode in inodes:
                    # # We assume inet sockets are unique, so we error
                    # # out if there are multiple references to the
                    # # same inode. We won't do this for UNIX sockets.
                    # if len(inodes[inode]) > 1 and family != socket.AF_UNIX:
                    #     raise ValueError(""ambiguos inode with multiple ""
                    #                      ""PIDs references"")
                    pid, fd = inodes[inode][0]
                else:
                    pid, fd = None, -1
                if filter_pid is not None and filter_pid != pid:
                    continue
                else:
                    if type_ == socket.SOCK_STREAM:
                        status = TCP_STATUSES[status]
                    else:
                        status = _common.CONN_NONE
                    laddr = self.decode_address(laddr, family)
                    raddr = self.decode_address(raddr, family)
                    yield (fd, family, type_, laddr, raddr, status, pid)","1. Use `os.path.exists()` to check if the file exists before opening it.
2. Use `socket.AF_UNIX` for UNIX sockets instead of IPv6.
3. Use `pid, fd = inodes[inode][0]` to get the PID and fd of the socket."
"    def process_unix(self, file, family, inodes, filter_pid=None):
        """"""Parse /proc/net/unix files.""""""
        with open(file, 'rt') as f:
            f.readline()  # skip the first line
            for line in f:
                tokens = line.split()
                try:
                    _, _, _, _, type_, _, inode = tokens[0:7]
                except ValueError:
                    raise RuntimeError(
                        ""error while parsing %s; malformed line %r"" % (
                            file, line))
                if inode in inodes:
                    # With UNIX sockets we can have a single inode
                    # referencing many file descriptors.
                    pairs = inodes[inode]
                else:
                    pairs = [(None, -1)]
                for pid, fd in pairs:
                    if filter_pid is not None and filter_pid != pid:
                        continue
                    else:
                        if len(tokens) == 8:
                            path = tokens[-1]
                        else:
                            path = """"
                        type_ = int(type_)
                        raddr = None
                        status = _common.CONN_NONE
                        yield (fd, family, type_, path, raddr, status, pid)","1. Use `os.path.expanduser()` to expand the path to the socket file. This will prevent an attacker from tricking the program into reading a file from a different location.
2. Check the permissions of the socket file before opening it. Make sure that only the user who owns the socket file has read and write access.
3. Close the socket file after you are finished using it. This will prevent an attacker from using the socket file to access your system."
"    def process_unix(self, file, family, inodes, filter_pid=None):
        """"""Parse /proc/net/unix files.""""""
        with open_text(file) as f:
            f.readline()  # skip the first line
            for line in f:
                tokens = line.split()
                try:
                    _, _, _, _, type_, _, inode = tokens[0:7]
                except ValueError:
                    raise RuntimeError(
                        ""error while parsing %s; malformed line %r"" % (
                            file, line))
                if inode in inodes:
                    # With UNIX sockets we can have a single inode
                    # referencing many file descriptors.
                    pairs = inodes[inode]
                else:
                    pairs = [(None, -1)]
                for pid, fd in pairs:
                    if filter_pid is not None and filter_pid != pid:
                        continue
                    else:
                        if len(tokens) == 8:
                            path = tokens[-1]
                        else:
                            path = """"
                        type_ = int(type_)
                        raddr = None
                        status = _common.CONN_NONE
                        yield (fd, family, type_, path, raddr, status, pid)","1. Use `open_binary` instead of `open_text` to prevent data from being interpreted as text.
2. Validate the input to ensure that it is a valid file path.
3. Sanitize the output to prevent malicious code from being executed."
"def poll(interval):
    """"""Calculate IO usage by comparing IO statics before and
    after the interval.
    Return a tuple including all currently running processes
    sorted by IO activity and total disks I/O activity.
    """"""
    # first get a list of all processes and disk io counters
    procs = [p for p in psutil.process_iter()]
    for p in procs[:]:
        try:
            p._before = p.io_counters()
        except psutil.Error:
            procs.remove(p)
            continue
    disks_before = psutil.disk_io_counters()

    # sleep some time
    time.sleep(interval)

    # then retrieve the same info again
    for p in procs[:]:
        try:
            p._after = p.io_counters()
            p._cmdline = ' '.join(p.cmdline())
            if not p._cmdline:
                p._cmdline = p.name()
            p._username = p.username()
        except psutil.NoSuchProcess:
            procs.remove(p)
    disks_after = psutil.disk_io_counters()

    # finally calculate results by comparing data before and
    # after the interval
    for p in procs:
        p._read_per_sec = p._after.read_bytes - p._before.read_bytes
        p._write_per_sec = p._after.write_bytes - p._before.write_bytes
        p._total = p._read_per_sec + p._write_per_sec

    disks_read_per_sec = disks_after.read_bytes - disks_before.read_bytes
    disks_write_per_sec = disks_after.write_bytes - disks_before.write_bytes

    # sort processes by total disk IO so that the more intensive
    # ones get listed first
    processes = sorted(procs, key=lambda p: p._total, reverse=True)

    return (processes, disks_read_per_sec, disks_write_per_sec)","1. Use `psutil.Process.as_dict()` instead of `psutil.Process.cmdline()` to get the command line of a process. This will prevent leaking sensitive information such as the process's username.
2. Use `psutil.disk_io_counters(perdisk=True)` to get the disk I/O statistics for each disk individually. This will prevent leaking information about the total amount of disk I/O activity across all disks.
3. Use `psutil.disk_io_counters(all=False)` to only get the disk I/O statistics for the disks that you are interested in. This will prevent leaking information about disks that you are not interested in."
"def run(pid):
    ACCESS_DENIED = ''
    try:
        p = psutil.Process(pid)
        pinfo = p.as_dict(ad_value=ACCESS_DENIED)
    except psutil.NoSuchProcess as err:
        sys.exit(str(err))

    try:
        parent = p.parent()
        if parent:
            parent = '(%s)' % parent.name()
        else:
            parent = ''
    except psutil.Error:
        parent = ''
    started = datetime.datetime.fromtimestamp(
        pinfo['create_time']).strftime('%Y-%m-%d %H:%M')
    io = pinfo.get('io_counters', ACCESS_DENIED)
    mem = '%s%% (resident=%s, virtual=%s) ' % (
        round(pinfo['memory_percent'], 1),
        convert_bytes(pinfo['memory_info'].rss),
        convert_bytes(pinfo['memory_info'].vms))
    children = p.children()

    print_('pid', pinfo['pid'])
    print_('name', pinfo['name'])
    print_('exe', pinfo['exe'])
    print_('parent', '%s %s' % (pinfo['ppid'], parent))
    print_('cmdline', ' '.join(pinfo['cmdline']))
    print_('started', started)
    print_('user', pinfo['username'])
    if POSIX and pinfo['uids'] and pinfo['gids']:
        print_('uids', 'real=%s, effective=%s, saved=%s' % pinfo['uids'])
    if POSIX and pinfo['gids']:
        print_('gids', 'real=%s, effective=%s, saved=%s' % pinfo['gids'])
    if POSIX:
        print_('terminal', pinfo['terminal'] or '')
    if hasattr(p, 'getcwd'):
        print_('cwd', pinfo['cwd'])
    print_('memory', mem)
    print_('cpu', '%s%% (user=%s, system=%s)' % (
        pinfo['cpu_percent'],
        getattr(pinfo['cpu_times'], 'user', '?'),
        getattr(pinfo['cpu_times'], 'system', '?')))
    print_('status', pinfo['status'])
    print_('niceness', pinfo['nice'])
    print_('num threads', pinfo['num_threads'])
    if io != ACCESS_DENIED:
        print_('I/O', 'bytes-read=%s, bytes-written=%s' % (
            convert_bytes(io.read_bytes),
            convert_bytes(io.write_bytes)))
    if children:
        print_('children', '')
        for child in children:
            print_('', 'pid=%s name=%s' % (child.pid, child.name()))

    if pinfo['open_files'] != ACCESS_DENIED:
        print_('open files', '')
        for file in pinfo['open_files']:
            print_('', 'fd=%s %s ' % (file.fd, file.path))

    if pinfo['threads']:
        print_('running threads', '')
        for thread in pinfo['threads']:
            print_('', 'id=%s, user-time=%s, sys-time=%s' % (
                thread.id, thread.user_time, thread.system_time))
    if pinfo['connections'] not in (ACCESS_DENIED, []):
        print_('open connections', '')
        for conn in pinfo['connections']:
            if conn.type == socket.SOCK_STREAM:
                type = 'TCP'
            elif conn.type == socket.SOCK_DGRAM:
                type = 'UDP'
            else:
                type = 'UNIX'
            lip, lport = conn.laddr
            if not conn.raddr:
                rip, rport = '*', '*'
            else:
                rip, rport = conn.raddr
            print_('', '%s:%s -> %s:%s type=%s status=%s' % (
                lip, lport, rip, rport, type, conn.status))","1. Use `psutil.Process.as_dict(attrs)` to get a dictionary of process attributes instead of accessing individual attributes directly. This will prevent leaking information about processes that do not exist.
2. Use `print_()` to print formatted output instead of `print()`. This will prevent leaking information about the process's terminal or current working directory.
3. Use `convert_bytes()` to convert bytes to human-readable format. This will prevent leaking information about the process's memory usage."
"def main():
    # construct a dict where 'values' are all the processes
    # having 'key' as their parent
    tree = collections.defaultdict(list)
    for p in psutil.process_iter():
        try:
            tree[p.ppid()].append(p.pid)
        except psutil.NoSuchProcess:
            pass
    # on systems supporting PID 0, PID 0's parent is usually 0
    if 0 in tree and 0 in tree[0]:
        tree[0].remove(0)
    print_tree(min(tree), tree)","1. Use `psutil.Process.children()` instead of `psutil.process_iter()` to get a list of child processes for a given process.
2. Use `psutil.Process.get_ppid()` to get the parent process ID for a given process.
3. Handle the `psutil.NoSuchProcess` exception gracefully."
"    def _init(self, pid, _ignore_nsp=False):
        if pid is None:
            pid = os.getpid()
        else:
            if not _PY3 and not isinstance(pid, (int, long)):
                raise TypeError('pid must be an integer (got %r)' % pid)
            if pid < 0:
                raise ValueError('pid must be a positive integer (got %s)'
                                 % pid)
        self._pid = pid
        self._name = None
        self._exe = None
        self._create_time = None
        self._gone = False
        self._hash = None
        # used for caching on Windows only (on POSIX ppid may change)
        self._ppid = None
        # platform-specific modules define an _psplatform.Process
        # implementation class
        self._proc = _psplatform.Process(pid)
        self._last_sys_cpu_times = None
        self._last_proc_cpu_times = None
        # cache creation time for later use in is_running() method
        try:
            self.create_time()
        except AccessDenied:
            # we should never get here as AFAIK we're able to get
            # process creation time on all platforms even as a
            # limited user
            pass
        except NoSuchProcess:
            if not _ignore_nsp:
                msg = 'no process found with pid %s' % pid
                raise NoSuchProcess(pid, None, msg)
            else:
                self._gone = True
        # This pair is supposed to indentify a Process instance
        # univocally over time (the PID alone is not enough as
        # it might refer to a process whose PID has been reused).
        # This will be used later in __eq__() and is_running().
        self._ident = (self.pid, self._create_time)","1. Use `os.getpid()` to get the current process ID instead of taking it as an argument. This will prevent a malicious user from passing in an invalid process ID.
2. Check the `pid` argument to make sure it is a positive integer. This will prevent a malicious user from passing in a negative integer or a non-integer value.
3. Use `_psplatform.Process()` to create a platform-specific process object. This will allow the code to access platform-specific process information, such as the process name and executable path."
"    def __str__(self):
        try:
            pid = self.pid
            name = repr(self.name())
        except NoSuchProcess:
            details = ""(pid=%s (terminated))"" % self.pid
        except AccessDenied:
            details = ""(pid=%s)"" % (self.pid)
        else:
            details = ""(pid=%s, name=%s)"" % (pid, name)
        return ""%s.%s%s"" % (self.__class__.__module__,
                            self.__class__.__name__, details)","1. Use `contextlib.suppress` to catch `NoSuchProcess` and `AccessDenied` exceptions.
2. Use `f-strings` to format the output string.
3. Use `repr()` to format the process name."
"    def as_dict(self, attrs=None, ad_value=None):
        """"""Utility method returning process information as a
        hashable dictionary.

        If 'attrs' is specified it must be a list of strings
        reflecting available Process class' attribute names
        (e.g. ['cpu_times', 'name']) else all public (read
        only) attributes are assumed.

        'ad_value' is the value which gets assigned in case
        AccessDenied  exception is raised when retrieving that
        particular process information.
        """"""
        excluded_names = set(
            ['send_signal', 'suspend', 'resume', 'terminate', 'kill', 'wait',
             'is_running', 'as_dict', 'parent', 'children', 'rlimit'])
        retdict = dict()
        ls = set(attrs or [x for x in dir(self) if not x.startswith('get')])
        for name in ls:
            if name.startswith('_'):
                continue
            if name.startswith('set_'):
                continue
            if name.startswith('get_'):
                msg = ""%s() is deprecated; use %s() instead"" % (name, name[4:])
                warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
                name = name[4:]
                if name in ls:
                    continue
            if name == 'getcwd':
                msg = ""getcwd() is deprecated; use cwd() instead""
                warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
                name = 'cwd'
                if name in ls:
                    continue

            if name in excluded_names:
                continue
            try:
                attr = getattr(self, name)
                if callable(attr):
                    ret = attr()
                else:
                    ret = attr
            except AccessDenied:
                ret = ad_value
            except NotImplementedError:
                # in case of not implemented functionality (may happen
                # on old or exotic systems) we want to crash only if
                # the user explicitly asked for that particular attr
                if attrs:
                    raise
                continue
            retdict[name] = ret
        return retdict","1. Use access control lists (ACLs) to restrict which users can access which processes.
2. Use encryption to protect sensitive process data.
3. Monitor processes for suspicious activity and take action to protect the system if necessary."
"    def status(self):
        """"""The process current status as a STATUS_* constant.""""""
        return self._proc.status()","1. Use `os.fdopen()` instead of `open()` to avoid leaking file descriptors.
2. Use `os.close()` to close the file descriptor after the process is finished.
3. Use `subprocess.Popen()` with the `universal_newlines=True` flag to avoid encoding issues."
"    def children(self, recursive=False):
        """"""Return the children of this process as a list of Process
        instances, pre-emptively checking whether PID has been reused.
        If recursive is True return all the parent descendants.

        Example (A == this process):

         A ─┐
            │
            ├─ B (child) ─┐
            │             └─ X (grandchild) ─┐
            │                                └─ Y (great grandchild)
            ├─ C (child)
            └─ D (child)

        >>> import psutil
        >>> p = psutil.Process()
        >>> p.children()
        B, C, D
        >>> p.children(recursive=True)
        B, X, Y, C, D

        Note that in the example above if process X disappears
        process Y won't be listed as the reference to process A
        is lost.
        """"""
        if hasattr(_psplatform, 'ppid_map'):
            # Windows only: obtain a {pid:ppid, ...} dict for all running
            # processes in one shot (faster).
            ppid_map = _psplatform.ppid_map()
        else:
            ppid_map = None

        ret = []
        if not recursive:
            if ppid_map is None:
                # 'slow' version, common to all platforms except Windows
                for p in process_iter():
                    try:
                        if p.ppid() == self.pid:
                            # if child happens to be older than its parent
                            # (self) it means child's PID has been reused
                            if self.create_time() <= p.create_time():
                                ret.append(p)
                    except NoSuchProcess:
                        pass
            else:
                # Windows only (faster)
                for pid, ppid in ppid_map.items():
                    if ppid == self.pid:
                        try:
                            child = Process(pid)
                            # if child happens to be older than its parent
                            # (self) it means child's PID has been reused
                            if self.create_time() <= child.create_time():
                                ret.append(child)
                        except NoSuchProcess:
                            pass
        else:
            # construct a dict where 'values' are all the processes
            # having 'key' as their parent
            table = collections.defaultdict(list)
            if ppid_map is None:
                for p in process_iter():
                    try:
                        table[p.ppid()].append(p)
                    except NoSuchProcess:
                        pass
            else:
                for pid, ppid in ppid_map.items():
                    try:
                        p = Process(pid)
                        table[ppid].append(p)
                    except NoSuchProcess:
                        pass
            # At this point we have a mapping table where table[self.pid]
            # are the current process' children.
            # Below, we look for all descendants recursively, similarly
            # to a recursive function call.
            checkpids = [self.pid]
            for pid in checkpids:
                for child in table[pid]:
                    try:
                        # if child happens to be older than its parent
                        # (self) it means child's PID has been reused
                        intime = self.create_time() <= child.create_time()
                    except NoSuchProcess:
                        pass
                    else:
                        if intime:
                            ret.append(child)
                            if child.pid not in checkpids:
                                checkpids.append(child.pid)
        return ret","1. Use `try-except` blocks to catch `NoSuchProcess` exceptions.
2. Use `collections.defaultdict` to construct a mapping table of processes.
3. Use `self.create_time()` to check if a child process's PID has been reused."
"def wrap_exceptions(fun):
    """"""Decorator which translates bare OSError exceptions into
    NoSuchProcess and AccessDenied.
    """"""
    @functools.wraps(fun)
    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except OSError as err:
            # support for private module import
            if NoSuchProcess is None or AccessDenied is None:
                raise
            if err.errno == errno.ESRCH:
                raise NoSuchProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise
    return wrapper","1. Use `functools.wraps()` to preserve the function signature.
2. Check if `NoSuchProcess` and `AccessDenied` are defined before using them.
3. Use `raise` to re-raise the original exception."
"    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except OSError as err:
            # support for private module import
            if NoSuchProcess is None or AccessDenied is None:
                raise
            if err.errno == errno.ESRCH:
                raise NoSuchProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise","1. Use `functools.wraps` to preserve the function's metadata, such as its name, docstring, and annotations.
2. Use `inspect.iscoroutinefunction` to check if the function is a coroutine.
3. Use `contextlib.suppress` to suppress the OSError raised by the `os.kill` function."
"def wrap_exceptions(fun):
    """"""Call callable into a try/except clause and translate ENOENT,
    EACCES and EPERM in NoSuchProcess or AccessDenied exceptions.
    """"""
    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except EnvironmentError as err:
            # support for private module import
            if NoSuchProcess is None or AccessDenied is None:
                raise
            # ENOENT (no such file or directory) gets raised on open().
            # ESRCH (no such process) can get raised on read() if
            # process is gone in meantime.
            if err.errno in (errno.ENOENT, errno.ESRCH):
                raise NoSuchProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise
    return wrapper","1. Use `functools.wraps` to preserve the function's metadata.
2. Use `six.reraise` to preserve the original exception type and traceback.
3. Use `contextlib.contextmanager` to ensure that the file is closed after use."
"    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except EnvironmentError as err:
            # support for private module import
            if NoSuchProcess is None or AccessDenied is None:
                raise
            # ENOENT (no such file or directory) gets raised on open().
            # ESRCH (no such process) can get raised on read() if
            # process is gone in meantime.
            if err.errno in (errno.ENOENT, errno.ESRCH):
                raise NoSuchProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise","1. Use `functools.wraps` to preserve the function signature.
2. Use `logging` to log errors.
3. Use `assert` statements to validate function arguments."
"    def __init__(self, pid, name=None, msg=None):
        Error.__init__(self)
        self.pid = pid
        self.name = name
        self.msg = msg
        if msg is None:
            if name:
                details = ""(pid=%s, name=%s)"" % (self.pid, repr(self.name))
            else:
                details = ""(pid=%s)"" % self.pid
            self.msg = ""process still exists but it's a zombie "" + details","1. Use `repr()` to format the `name` argument, to prevent `__repr__()` from being called on untrusted input.
2. Use `f-strings` to format the `details` message, to prevent `str.format()` from being called on untrusted input.
3. Use `os.getpid()` to get the current process ID, instead of using the `pid` argument, to prevent an attacker from injecting a fake process ID."
"    def ppid(self):
        """"""The process parent PID.
        On Windows the return value is cached after first call.
        """"""
        # On POSIX we don't want to cache the ppid as it may unexpectedly
        # change to 1 (init) in case this process turns into a zombie:
        # https://github.com/giampaolo/psutil/issues/321
        # http://stackoverflow.com/questions/356722/

        # XXX should we check creation time here rather than in
        # Process.parent()?
        if _POSIX:
            return self._proc.ppid()
        else:
            if self._ppid is None:
                self._ppid = self._proc.ppid()
            return self._ppid","1. Use `os.getppid()` instead of `self._proc.ppid()` to get the parent PID.
2. Check if the process is a zombie before returning the parent PID.
3. Cache the parent PID only if the process is not a zombie."
"def wrap_exceptions(fun):
    """"""Decorator which translates bare OSError exceptions into
    NoSuchProcess and AccessDenied.
    """"""
    @functools.wraps(fun)
    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except OSError as err:
            # support for private module import
            if (NoSuchProcess is None or AccessDenied is None or
                    ZombieProcess is None):
                raise
            if err.errno == errno.ESRCH:
                if not pid_exists(self.pid):
                    raise NoSuchProcess(self.pid, self._name)
                else:
                    raise ZombieProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise
    return wrapper","1. Use `functools.wraps` to preserve the function signature.
2. Check if `NoSuchProcess`, `AccessDenied` and `ZombieProcess` are defined before using them.
3. Use `pid_exists` to check if the process exists before raising `NoSuchProcess` or `ZombieProcess`."
"    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except OSError as err:
            # support for private module import
            if (NoSuchProcess is None or AccessDenied is None or
                    ZombieProcess is None):
                raise
            if err.errno == errno.ESRCH:
                if not pid_exists(self.pid):
                    raise NoSuchProcess(self.pid, self._name)
                else:
                    raise ZombieProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise","1. Use `functools.wraps` to preserve the original function's metadata.
2. Check for `NoSuchProcess`, `AccessDenied`, and `ZombieProcess` exceptions.
3. Handle `OSError` exceptions more gracefully."
"    def __init__(self, pid):
        self.pid = pid
        self._name = None","1. Use `getattr` and `setattr` to access and modify the `_name` attribute. This will prevent direct access to the attribute, which could be used to modify its value or bypass access restrictions.
2. Use `assert` statements to validate the input to the `__init__` method. This will help to ensure that the `pid` argument is a valid integer.
3. Use `logging` to log all access to the `_name` attribute. This will help to track any unauthorized attempts to modify its value."
"def wrap_exceptions_w_zombie(fun):
    """"""Same as above but also handles zombies.""""""
    @functools.wraps(fun)
    def wrapper(self, *args, **kwargs):
        try:
            return wrap_exceptions(fun)(self)
        except NoSuchProcess:
            if not pid_exists(self.pid):
                raise
            else:
                raise ZombieProcess(self.pid, self._name)
    return wrapper","1. Use `check_call` instead of `subprocess.call` to check for errors.
2. Use `subprocess.Popen` with `universal_newlines=True` to ensure that the output is in text format.
3. Use `subprocess.PIPE` to capture the output and error streams."
"    def wrapper(self, *args, **kwargs):
        try:
            return wrap_exceptions(fun)(self)
        except NoSuchProcess:
            if not pid_exists(self.pid):
                raise
            else:
                raise ZombieProcess(self.pid, self._name)","1. Use `wrap_function` to wrap the function being decorated, rather than calling it directly. This will ensure that the function is properly wrapped even if it is decorated multiple times.
2. Check for the existence of the process before calling `pid_exists()`. This will prevent a zombie process from being created if the process has already been terminated.
3. Use `raise` to raise an exception if the process does not exist. This will prevent the function from continuing to run if the process has already been terminated."
"    def exe(self):
        try:
            exe = os.readlink(""/proc/%s/exe"" % self.pid)
        except (OSError, IOError) as err:
            if err.errno in (errno.ENOENT, errno.ESRCH):
                # no such file error; might be raised also if the
                # path actually exists for system processes with
                # low pids (about 0-20)
                if os.path.lexists(""/proc/%s"" % self.pid):
                    return """"
                else:
                    if not pid_exists(self.pid):
                        raise NoSuchProcess(self.pid, self._name)
                    else:
                        raise ZombieProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise

        # readlink() might return paths containing null bytes ('\\x00').
        # Certain names have ' (deleted)' appended. Usually this is
        # bogus as the file actually exists. Either way that's not
        # important as we don't want to discriminate executables which
        # have been deleted.
        exe = exe.split('\\x00')[0]
        if exe.endswith(' (deleted)') and not os.path.exists(exe):
            exe = exe[:-10]
        return exe","1. Use `os.lstat` instead of `os.readlink` to avoid leaking information about deleted files.
2. Check the return value of `os.access` to make sure the process has permission to read the file.
3. Sanitize the path returned by `os.readlink` to prevent directory traversal attacks."
"        def rlimit(self, resource, limits=None):
            # if pid is 0 prlimit() applies to the calling process and
            # we don't want that
            if self.pid == 0:
                raise ValueError(""can't use prlimit() against PID 0 process"")
            try:
                if limits is None:
                    # get
                    return cext.linux_prlimit(self.pid, resource)
                else:
                    # set
                    if len(limits) != 2:
                        raise ValueError(
                            ""second argument must be a (soft, hard) tuple"")
                    soft, hard = limits
                    cext.linux_prlimit(self.pid, resource, soft, hard)
            except OSError as err:
                if err.errno == errno.ENOSYS and pid_exists(self.pid):
                    # I saw this happening on Travis:
                    # https://travis-ci.org/giampaolo/psutil/jobs/51368273
                    raise ZombieProcess(self.pid, self._name)
                else:
                    raise","1. **Use `os.getpid()` instead of `self.pid` to get the current process ID.** This will prevent the function from being used to modify the limits of the calling process.
2. **Check the length of the `limits` argument to ensure that it is a tuple of length 2.** This will prevent the function from being used to set invalid limits.
3. **Handle OSError exceptions more gracefully.** Currently, the function simply raises the exception, which could cause the calling process to crash. Instead, the function should catch the exception and return an appropriate error message."
"def wrap_exceptions(fun):
    """"""Call callable into a try/except clause and translate ENOENT,
    EACCES and EPERM in NoSuchProcess or AccessDenied exceptions.
    """"""
    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except EnvironmentError as err:
            # support for private module import
            if (NoSuchProcess is None or AccessDenied is None or
                    ZombieProcess is None):
                raise
            # ENOENT (no such file or directory) gets raised on open().
            # ESRCH (no such process) can get raised on read() if
            # process is gone in meantime.
            if err.errno in (errno.ENOENT, errno.ESRCH):
                if not pid_exists(self.pid):
                    raise NoSuchProcess(self.pid, self._name)
                else:
                    raise ZombieProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise
    return wrapper","1. Use `if __name__ == '__main__'` to avoid polluting the global namespace.
2. Use `functools.wraps` to preserve the original function metadata.
3. Use `inspect.getfullargspec` to get the function's argument names."
"    def wrapper(self, *args, **kwargs):
        try:
            return fun(self, *args, **kwargs)
        except EnvironmentError as err:
            # support for private module import
            if (NoSuchProcess is None or AccessDenied is None or
                    ZombieProcess is None):
                raise
            # ENOENT (no such file or directory) gets raised on open().
            # ESRCH (no such process) can get raised on read() if
            # process is gone in meantime.
            if err.errno in (errno.ENOENT, errno.ESRCH):
                if not pid_exists(self.pid):
                    raise NoSuchProcess(self.pid, self._name)
                else:
                    raise ZombieProcess(self.pid, self._name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._name)
            raise","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Check if the process exists before accessing it.
3. Raise the appropriate exception if the process does not exist or the user does not have permission to access it."
"        def cpu_affinity(self, cpus=None):
            """"""Get or set process CPU affinity.
            If specified 'cpus' must be a list of CPUs for which you
            want to set the affinity (e.g. [0, 1]).
            """"""
            if cpus is None:
                return self._proc.cpu_affinity_get()
            else:
                self._proc.cpu_affinity_set(cpus)","1. Use `os.getpid()` instead of `self._proc.pid` to get the process ID. This will prevent an attacker from tricking the function into changing the affinity of another process.
2. Check the validity of the `cpus` argument before setting the affinity. This will prevent an attacker from setting the affinity to a invalid value, which could crash the process.
3. Use `os.sched_setaffinity()` instead of `self._proc.cpu_affinity_set()` to set the affinity. This will ensure that the function is only able to set the affinity of the current process."
"def preprocess_and_wrap(broadcast=None, wrap_like=None, match_unit=False, to_magnitude=False):
    """"""Return decorator to wrap array calculations for type flexibility.

    Assuming you have a calculation that works internally with `pint.Quantity` or
    `numpy.ndarray`, this will wrap the function to be able to handle `xarray.DataArray` and
    `pint.Quantity` as well (assuming appropriate match to one of the input arguments).

    Parameters
    ----------
    broadcast : iterable of str or None
        Iterable of string labels for arguments to broadcast against each other using xarray,
        assuming they are supplied as `xarray.DataArray`. No automatic broadcasting will occur
        with default of None.
    wrap_like : str or array-like or tuple of str or tuple of array-like or None
        Wrap the calculation output following a particular input argument (if str) or data
        object (if array-like). If tuple, will assume output is in the form of a tuple,
        and wrap iteratively according to the str or array-like contained within. If None,
        will not wrap output.
    match_unit : bool
        If true, force the unit of the final output to be that of wrapping object (as
        determined by wrap_like), no matter the original calculation output. Defaults to
        False.
    to_magnitude : bool
        If true, downcast xarray and Pint arguments to their magnitude. If false, downcast
        xarray arguments to Quantity, and do not change other array-like arguments.
    """"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            bound_args = signature(func).bind(*args, **kwargs)

            # Auto-broadcast select xarray arguments, and update bound_args
            if broadcast is not None:
                arg_names_to_broadcast = tuple(
                    arg_name for arg_name in broadcast
                    if arg_name in bound_args.arguments
                    and isinstance(
                        bound_args.arguments[arg_name],
                        (xr.DataArray, xr.Variable)
                    )
                )
                broadcasted_args = xr.broadcast(
                    *(bound_args.arguments[arg_name] for arg_name in arg_names_to_broadcast)
                )
                for i, arg_name in enumerate(arg_names_to_broadcast):
                    bound_args.arguments[arg_name] = broadcasted_args[i]

            # Cast all Variables to their data and warn
            # (need to do before match finding, since we don't want to rewrap as Variable)
            for arg_name in bound_args.arguments:
                if isinstance(bound_args.arguments[arg_name], xr.Variable):
                    warnings.warn(
                        f'Argument {arg_name} given as xarray Variable...casting to its data. '
                        'xarray DataArrays are recommended instead.'
                    )
                    bound_args.arguments[arg_name] = bound_args.arguments[arg_name].data

            # Obtain proper match if referencing an input
            match = list(wrap_like) if isinstance(wrap_like, tuple) else wrap_like
            if isinstance(wrap_like, str):
                match = bound_args.arguments[wrap_like]
            elif isinstance(wrap_like, tuple):
                for i, arg in enumerate(wrap_like):
                    if isinstance(arg, str):
                        match[i] = bound_args.arguments[arg]

            # Cast all DataArrays to Pint Quantities
            for arg_name in bound_args.arguments:
                if isinstance(bound_args.arguments[arg_name], xr.DataArray):
                    bound_args.arguments[arg_name] = (
                        bound_args.arguments[arg_name].metpy.unit_array
                    )

            # Optionally cast all Quantities to their magnitudes
            if to_magnitude:
                for arg_name in bound_args.arguments:
                    if isinstance(bound_args.arguments[arg_name], units.Quantity):
                        bound_args.arguments[arg_name] = bound_args.arguments[arg_name].m

            # Evaluate inner calculation
            result = func(*bound_args.args, **bound_args.kwargs)

            # Wrap output based on match and match_unit
            if match is None:
                return result
            else:
                if match_unit:
                    wrapping = _wrap_output_like_matching_units
                else:
                    wrapping = _wrap_output_like_not_matching_units

                if isinstance(match, list):
                    return tuple(wrapping(*args) for args in zip(result, match))
                else:
                    return wrapping(result, match)
        return wrapper
    return decorator","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `warnings.warn` to notify the user of potential problems.
3. Use `xr.broadcast` to ensure that the arguments are of the same shape."
"    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            bound_args = signature(func).bind(*args, **kwargs)

            # Auto-broadcast select xarray arguments, and update bound_args
            if broadcast is not None:
                arg_names_to_broadcast = tuple(
                    arg_name for arg_name in broadcast
                    if arg_name in bound_args.arguments
                    and isinstance(
                        bound_args.arguments[arg_name],
                        (xr.DataArray, xr.Variable)
                    )
                )
                broadcasted_args = xr.broadcast(
                    *(bound_args.arguments[arg_name] for arg_name in arg_names_to_broadcast)
                )
                for i, arg_name in enumerate(arg_names_to_broadcast):
                    bound_args.arguments[arg_name] = broadcasted_args[i]

            # Cast all Variables to their data and warn
            # (need to do before match finding, since we don't want to rewrap as Variable)
            for arg_name in bound_args.arguments:
                if isinstance(bound_args.arguments[arg_name], xr.Variable):
                    warnings.warn(
                        f'Argument {arg_name} given as xarray Variable...casting to its data. '
                        'xarray DataArrays are recommended instead.'
                    )
                    bound_args.arguments[arg_name] = bound_args.arguments[arg_name].data

            # Obtain proper match if referencing an input
            match = list(wrap_like) if isinstance(wrap_like, tuple) else wrap_like
            if isinstance(wrap_like, str):
                match = bound_args.arguments[wrap_like]
            elif isinstance(wrap_like, tuple):
                for i, arg in enumerate(wrap_like):
                    if isinstance(arg, str):
                        match[i] = bound_args.arguments[arg]

            # Cast all DataArrays to Pint Quantities
            for arg_name in bound_args.arguments:
                if isinstance(bound_args.arguments[arg_name], xr.DataArray):
                    bound_args.arguments[arg_name] = (
                        bound_args.arguments[arg_name].metpy.unit_array
                    )

            # Optionally cast all Quantities to their magnitudes
            if to_magnitude:
                for arg_name in bound_args.arguments:
                    if isinstance(bound_args.arguments[arg_name], units.Quantity):
                        bound_args.arguments[arg_name] = bound_args.arguments[arg_name].m

            # Evaluate inner calculation
            result = func(*bound_args.args, **bound_args.kwargs)

            # Wrap output based on match and match_unit
            if match is None:
                return result
            else:
                if match_unit:
                    wrapping = _wrap_output_like_matching_units
                else:
                    wrapping = _wrap_output_like_not_matching_units

                if isinstance(match, list):
                    return tuple(wrapping(*args) for args in zip(result, match))
                else:
                    return wrapping(result, match)
        return wrapper","1. Use `functools.wraps` to preserve the original function metadata.
2. Use `xr.broadcast` to automatically broadcast xarray arguments.
3. Use `units.Quantity` to cast all DataArrays to Pint Quantities."
"        def wrapper(*args, **kwargs):
            bound_args = signature(func).bind(*args, **kwargs)

            # Auto-broadcast select xarray arguments, and update bound_args
            if broadcast is not None:
                arg_names_to_broadcast = tuple(
                    arg_name for arg_name in broadcast
                    if arg_name in bound_args.arguments
                    and isinstance(
                        bound_args.arguments[arg_name],
                        (xr.DataArray, xr.Variable)
                    )
                )
                broadcasted_args = xr.broadcast(
                    *(bound_args.arguments[arg_name] for arg_name in arg_names_to_broadcast)
                )
                for i, arg_name in enumerate(arg_names_to_broadcast):
                    bound_args.arguments[arg_name] = broadcasted_args[i]

            # Cast all Variables to their data and warn
            # (need to do before match finding, since we don't want to rewrap as Variable)
            for arg_name in bound_args.arguments:
                if isinstance(bound_args.arguments[arg_name], xr.Variable):
                    warnings.warn(
                        f'Argument {arg_name} given as xarray Variable...casting to its data. '
                        'xarray DataArrays are recommended instead.'
                    )
                    bound_args.arguments[arg_name] = bound_args.arguments[arg_name].data

            # Obtain proper match if referencing an input
            match = list(wrap_like) if isinstance(wrap_like, tuple) else wrap_like
            if isinstance(wrap_like, str):
                match = bound_args.arguments[wrap_like]
            elif isinstance(wrap_like, tuple):
                for i, arg in enumerate(wrap_like):
                    if isinstance(arg, str):
                        match[i] = bound_args.arguments[arg]

            # Cast all DataArrays to Pint Quantities
            for arg_name in bound_args.arguments:
                if isinstance(bound_args.arguments[arg_name], xr.DataArray):
                    bound_args.arguments[arg_name] = (
                        bound_args.arguments[arg_name].metpy.unit_array
                    )

            # Optionally cast all Quantities to their magnitudes
            if to_magnitude:
                for arg_name in bound_args.arguments:
                    if isinstance(bound_args.arguments[arg_name], units.Quantity):
                        bound_args.arguments[arg_name] = bound_args.arguments[arg_name].m

            # Evaluate inner calculation
            result = func(*bound_args.args, **bound_args.kwargs)

            # Wrap output based on match and match_unit
            if match is None:
                return result
            else:
                if match_unit:
                    wrapping = _wrap_output_like_matching_units
                else:
                    wrapping = _wrap_output_like_not_matching_units

                if isinstance(match, list):
                    return tuple(wrapping(*args) for args in zip(result, match))
                else:
                    return wrapping(result, match)","1. Use `functools.wraps` to preserve the original function metadata.
2. Use `inspect.signature` to get the function arguments and their types.
3. Use `functools.partial` to create a new function with specific arguments."
"def add_grid_arguments_from_xarray(func):
    """"""Fill in optional arguments like dx/dy from DataArray arguments.""""""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        bound_args = signature(func).bind(*args, **kwargs)
        bound_args.apply_defaults()

        # Search for DataArray with valid latitude and longitude coordinates to find grid
        # deltas and any other needed parameter
        dataarray_arguments = [
            value for value in bound_args.arguments.values()
            if isinstance(value, xr.DataArray)
        ]
        grid_prototype = None
        for da in dataarray_arguments:
            if hasattr(da.metpy, 'latitude') and hasattr(da.metpy, 'longitude'):
                grid_prototype = da
                break

        # Fill in x_dim/y_dim
        if (
            grid_prototype is not None
            and 'x_dim' in bound_args.arguments
            and 'y_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['x_dim'] = grid_prototype.metpy.find_axis_number('x')
                bound_args.arguments['y_dim'] = grid_prototype.metpy.find_axis_number('y')
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn('Horizontal dimension numbers not found. Defaulting to '
                              '(..., Y, X) order.')

        # Fill in vertical_dim
        if (
            grid_prototype is not None
            and 'vertical_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['vertical_dim'] = (
                    grid_prototype.metpy.find_axis_number('vertical')
                )
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn(
                    'Vertical dimension number not found. Defaulting to (..., Z, Y, X) order.'
                )

        # Fill in dz
        if (
            grid_prototype is not None
            and 'dz' in bound_args.arguments
            and bound_args.arguments['dz'] is None
        ):
            try:
                vertical_coord = grid_prototype.metpy.vertical
                bound_args.arguments['dz'] = np.diff(vertical_coord.metpy.unit_array)
            except (AttributeError, ValueError):
                # Skip, since this only comes up in advection, where dz is optional (may not
                # need vertical at all)
                pass

        # Fill in dx/dy
        if (
            'dx' in bound_args.arguments and bound_args.arguments['dx'] is None
            and 'dy' in bound_args.arguments and bound_args.arguments['dy'] is None
        ):
            if grid_prototype is not None:
                bound_args.arguments['dx'], bound_args.arguments['dy'] = (
                    grid_deltas_from_dataarray(grid_prototype, kind='actual')
                )
            elif 'dz' in bound_args.arguments:
                # Handle advection case, allowing dx/dy to be None but dz to not be None
                if bound_args.arguments['dz'] is None:
                    raise ValueError(
                        'Must provide dx, dy, and/or dz arguments or input DataArray with '
                        'proper coordinates.'
                    )
            else:
                raise ValueError('Must provide dx/dy arguments or input DataArray with '
                                 'latitude/longitude coordinates.')

        # Fill in latitude
        if 'latitude' in bound_args.arguments and bound_args.arguments['latitude'] is None:
            if grid_prototype is not None:
                bound_args.arguments['latitude'] = (
                    grid_prototype.metpy.latitude
                )
            else:
                raise ValueError('Must provide latitude argument or input DataArray with '
                                 'latitude/longitude coordinates.')

        return func(*bound_args.args, **bound_args.kwargs)
    return wrapper","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `warnings.warn` to notify the user of potential problems.
3. Use `np.diff` to calculate the difference between two arrays."
"    def wrapper(*args, **kwargs):
        bound_args = signature(func).bind(*args, **kwargs)
        bound_args.apply_defaults()

        # Search for DataArray with valid latitude and longitude coordinates to find grid
        # deltas and any other needed parameter
        dataarray_arguments = [
            value for value in bound_args.arguments.values()
            if isinstance(value, xr.DataArray)
        ]
        grid_prototype = None
        for da in dataarray_arguments:
            if hasattr(da.metpy, 'latitude') and hasattr(da.metpy, 'longitude'):
                grid_prototype = da
                break

        # Fill in x_dim/y_dim
        if (
            grid_prototype is not None
            and 'x_dim' in bound_args.arguments
            and 'y_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['x_dim'] = grid_prototype.metpy.find_axis_number('x')
                bound_args.arguments['y_dim'] = grid_prototype.metpy.find_axis_number('y')
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn('Horizontal dimension numbers not found. Defaulting to '
                              '(..., Y, X) order.')

        # Fill in vertical_dim
        if (
            grid_prototype is not None
            and 'vertical_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['vertical_dim'] = (
                    grid_prototype.metpy.find_axis_number('vertical')
                )
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn(
                    'Vertical dimension number not found. Defaulting to (..., Z, Y, X) order.'
                )

        # Fill in dz
        if (
            grid_prototype is not None
            and 'dz' in bound_args.arguments
            and bound_args.arguments['dz'] is None
        ):
            try:
                vertical_coord = grid_prototype.metpy.vertical
                bound_args.arguments['dz'] = np.diff(vertical_coord.metpy.unit_array)
            except (AttributeError, ValueError):
                # Skip, since this only comes up in advection, where dz is optional (may not
                # need vertical at all)
                pass

        # Fill in dx/dy
        if (
            'dx' in bound_args.arguments and bound_args.arguments['dx'] is None
            and 'dy' in bound_args.arguments and bound_args.arguments['dy'] is None
        ):
            if grid_prototype is not None:
                bound_args.arguments['dx'], bound_args.arguments['dy'] = (
                    grid_deltas_from_dataarray(grid_prototype, kind='actual')
                )
            elif 'dz' in bound_args.arguments:
                # Handle advection case, allowing dx/dy to be None but dz to not be None
                if bound_args.arguments['dz'] is None:
                    raise ValueError(
                        'Must provide dx, dy, and/or dz arguments or input DataArray with '
                        'proper coordinates.'
                    )
            else:
                raise ValueError('Must provide dx/dy arguments or input DataArray with '
                                 'latitude/longitude coordinates.')

        # Fill in latitude
        if 'latitude' in bound_args.arguments and bound_args.arguments['latitude'] is None:
            if grid_prototype is not None:
                bound_args.arguments['latitude'] = (
                    grid_prototype.metpy.latitude
                )
            else:
                raise ValueError('Must provide latitude argument or input DataArray with '
                                 'latitude/longitude coordinates.')

        return func(*bound_args.args, **bound_args.kwargs)","1. Use `functools.wraps` to preserve the metadata of the decorated function.
2. Use `warnings.warn` to notify the user of any missing parameters.
3. Raise a `ValueError` if any of the required parameters are missing."
"def add_vertical_dim_from_xarray(func):
    """"""Fill in optional vertical_dim from DataArray argument.""""""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        bound_args = signature(func).bind(*args, **kwargs)
        bound_args.apply_defaults()

        # Search for DataArray in arguments
        dataarray_arguments = [
            value for value in bound_args.arguments.values()
            if isinstance(value, xr.DataArray)
        ]

        # Fill in vertical_dim
        if (
            len(dataarray_arguments) > 0
            and 'vertical_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['vertical_dim'] = (
                    dataarray_arguments[0].metpy.find_axis_number('vertical')
                )
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn(
                    'Vertical dimension number not found. Defaulting to initial dimension.'
                )

        return func(*bound_args.args, **bound_args.kwargs)
    return wrapper","1. Use `functools.wraps` to preserve the metadata of the decorated function.
2. Use `signature(func).bind(*args, **kwargs)` to get the bound arguments of the function.
3. Use `bound_args.apply_defaults()` to apply default values to the function arguments."
"    def wrapper(*args, **kwargs):
        bound_args = signature(func).bind(*args, **kwargs)
        bound_args.apply_defaults()

        # Search for DataArray in arguments
        dataarray_arguments = [
            value for value in bound_args.arguments.values()
            if isinstance(value, xr.DataArray)
        ]

        # Fill in vertical_dim
        if (
            len(dataarray_arguments) > 0
            and 'vertical_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['vertical_dim'] = (
                    dataarray_arguments[0].metpy.find_axis_number('vertical')
                )
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn(
                    'Vertical dimension number not found. Defaulting to initial dimension.'
                )

        return func(*bound_args.args, **bound_args.kwargs)","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Validate the input arguments of the function to ensure that they are of the correct type and shape.
3. Handle errors and exceptions gracefully to prevent the function from crashing."
"def moist_lapse(pressure, temperature, reference_pressure=None):
    r""""""Calculate the temperature at a level assuming liquid saturation processes.

    This function lifts a parcel starting at `temperature`. The starting pressure can
    be given by `reference_pressure`. Essentially, this function is calculating moist
    pseudo-adiabats.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The atmospheric pressure level(s) of interest
    temperature : `pint.Quantity`
        The starting temperature
    reference_pressure : `pint.Quantity`, optional
        The reference pressure. If not given, it defaults to the first element of the
        pressure array.

    Returns
    -------
    `pint.Quantity`
       The resulting parcel temperature at levels given by `pressure`

    See Also
    --------
    dry_lapse : Calculate parcel temperature assuming dry adiabatic processes
    parcel_profile : Calculate complete parcel profile

    Notes
    -----
    This function is implemented by integrating the following differential
    equation:

    .. math:: \\frac{dT}{dP} = \\frac{1}{P} \\frac{R_d T + L_v r_s}
                                {C_{pd} + \\frac{L_v^2 r_s \\epsilon}{R_d T^2}}

    This equation comes from [Bakhshaii2013]_.

    Only reliably functions on 1D profiles (not higher-dimension vertical cross sections or
    grids).

    """"""
    def dt(t, p):
        t = units.Quantity(t, temperature.units)
        p = units.Quantity(p, pressure.units)
        rs = saturation_mixing_ratio(p, t)
        frac = ((mpconsts.Rd * t + mpconsts.Lv * rs)
                / (mpconsts.Cp_d + (mpconsts.Lv * mpconsts.Lv * rs * mpconsts.epsilon
                                    / (mpconsts.Rd * t * t)))).to('kelvin')
        return (frac / p).magnitude

    if reference_pressure is None:
        reference_pressure = pressure[0]

    pressure = pressure.to('mbar')
    reference_pressure = reference_pressure.to('mbar')
    temperature = np.atleast_1d(temperature)

    side = 'left'

    pres_decreasing = (pressure[0] > pressure[-1])
    if pres_decreasing:
        # Everything is easier if pressures are in increasing order
        pressure = pressure[::-1]
        side = 'right'

    ref_pres_idx = np.searchsorted(pressure.m, reference_pressure.m, side=side)

    ret_temperatures = np.empty((0, temperature.shape[0]))

    if reference_pressure > pressure.min():
        # Integrate downward in pressure
        pres_down = np.append(reference_pressure.m, pressure[(ref_pres_idx - 1)::-1].m)
        trace_down = si.odeint(dt, temperature.m.squeeze(), pres_down.squeeze())
        ret_temperatures = np.concatenate((ret_temperatures, trace_down[:0:-1]))

    if reference_pressure < pressure.max():
        # Integrate upward in pressure
        pres_up = np.append(reference_pressure.m, pressure[ref_pres_idx:].m)
        trace_up = si.odeint(dt, temperature.m.squeeze(), pres_up.squeeze())
        ret_temperatures = np.concatenate((ret_temperatures, trace_up[1:]))

    if pres_decreasing:
        ret_temperatures = ret_temperatures[::-1]

    return units.Quantity(ret_temperatures.T.squeeze(), temperature.units)","1. Use `units.Quantity` to represent all numerical values with units.
2. Use `np.atleast_1d` to ensure that `temperature` is at least 1-dimensional.
3. Use `np.searchsorted` to find the index of the reference pressure in the pressure array."
"def wet_bulb_temperature(pressure, temperature, dewpoint):
    """"""Calculate the wet-bulb temperature using Normand's rule.

    This function calculates the wet-bulb temperature using the Normand method. The LCL is
    computed, and that parcel brought down to the starting pressure along a moist adiabat.
    The Normand method (and others) are described and compared by [Knox2017]_.

    Parameters
    ----------
    pressure : `pint.Quantity`
        Initial atmospheric pressure
    temperature : `pint.Quantity`
        Initial atmospheric temperature
    dewpoint : `pint.Quantity`
        Initial atmospheric dewpoint

    Returns
    -------
    `pint.Quantity`
        Wet-bulb temperature

    See Also
    --------
    lcl, moist_lapse

    Notes
    -----
    Since this function iteratively applies a parcel calculation, it should be used with
    caution on large arrays.

    """"""
    if not hasattr(pressure, 'shape'):
        pressure = np.atleast_1d(pressure)
        temperature = np.atleast_1d(temperature)
        dewpoint = np.atleast_1d(dewpoint)

    it = np.nditer([pressure, temperature, dewpoint, None],
                   op_dtypes=['float', 'float', 'float', 'float'],
                   flags=['buffered'])

    for press, temp, dewp, ret in it:
        press = press * pressure.units
        temp = temp * temperature.units
        dewp = dewp * dewpoint.units
        lcl_pressure, lcl_temperature = lcl(press, temp, dewp)
        moist_adiabat_temperatures = moist_lapse(concatenate([lcl_pressure, press]),
                                                 lcl_temperature)
        ret[...] = moist_adiabat_temperatures[-1].magnitude

    # If we started with a scalar, return a scalar
    if it.operands[3].size == 1:
        return it.operands[3][0] * moist_adiabat_temperatures.units
    return it.operands[3] * moist_adiabat_temperatures.units","1. Use `np.atleast_1d()` to check if the input arrays are 1-dimensional.
2. Use `np.nditer()` to iterate over the input arrays and apply the calculation to each element.
3. Use `np.concatenate()` to concatenate the LCL pressure and temperature arrays before passing them to the `moist_lapse()` function."
"    def _build(self):
        """"""Build the plot by calling needed plotting methods as necessary.""""""
        lon, lat, data = self.plotdata

        # Use the cartopy map projection to transform station locations to the map and
        # then refine the number of stations plotted by setting a radius
        if self.parent._proj_obj == ccrs.PlateCarree():
            scale = 1.
        else:
            scale = 100000.
        point_locs = self.parent._proj_obj.transform_points(ccrs.PlateCarree(), lon, lat)
        subset = reduce_point_density(point_locs, self.reduce_points * scale)

        self.handle = StationPlot(self.parent.ax, lon[subset], lat[subset], clip_on=True,
                                  transform=ccrs.PlateCarree(), fontsize=10)

        for i, ob_type in enumerate(self.fields):
            field_kwargs = {}
            if len(self.locations) > 1:
                location = self.locations[i]
            else:
                location = self.locations[0]
            if len(self.colors) > 1:
                field_kwargs['color'] = self.colors[i]
            else:
                field_kwargs['color'] = self.colors[0]
            if len(self.formats) > 1:
                field_kwargs['formatter'] = self.formats[i]
            else:
                field_kwargs['formatter'] = self.formats[0]
            if len(self.plot_units) > 1:
                field_kwargs['plot_units'] = self.plot_units[i]
            else:
                field_kwargs['plot_units'] = self.plot_units[0]
            if hasattr(self.data, 'units') and (field_kwargs['plot_units'] is not None):
                parameter = data[ob_type][subset].values * units(self.data.units[ob_type])
            else:
                parameter = data[ob_type][subset]
            if field_kwargs['formatter'] is not None:
                mapper = getattr(wx_symbols, str(field_kwargs['formatter']), None)
                if mapper is not None:
                    field_kwargs.pop('formatter')
                    self.handle.plot_symbol(location, parameter,
                                            mapper, **field_kwargs)
                else:
                    if self.formats[i] == 'text':
                        self.handle.plot_text(location, data[ob_type][subset],
                                              color=field_kwargs['color'])
                    else:
                        self.handle.plot_parameter(location, data[ob_type][subset],
                                                   **field_kwargs)
            else:
                field_kwargs.pop('formatter')
                self.handle.plot_parameter(location, parameter, **field_kwargs)

        if self.vector_field[0] is not None:
            vector_kwargs = {}
            vector_kwargs['color'] = self.vector_field_color
            vector_kwargs['plot_units'] = self.vector_plot_units
            if hasattr(self.data, 'units') and (vector_kwargs['plot_units'] is not None):
                u = (data[self.vector_field[0]][subset].values
                     * units(self.data.units[self.vector_field[0]]))
                v = (data[self.vector_field[1]][subset].values
                     * units(self.data.units[self.vector_field[1]]))
            else:
                vector_kwargs.pop('plot_units')
                u = data[self.vector_field[0]][subset]
                v = data[self.vector_field[1]][subset]
            if self.vector_field_length is not None:
                vector_kwargs['length'] = self.vector_field_length
            self.handle.plot_barb(u, v, **vector_kwargs)","1. Use `wx_symbols.get_mapper()` to get the symbol mapper instead of hardcoding it.
2. Use `getattr()` to get the attribute of an object instead of using `.` directly.
3. Use `pop()` to remove a key from a dictionary instead of using `del`."
"    def _scalar_plotting_units(scalar_value, plotting_units):
        """"""Handle conversion to plotting units for barbs and arrows.""""""
        if plotting_units:
            if hasattr(scalar_value, 'units'):
                scalar_value = scalar_value.to(plotting_units)
            else:
                raise ValueError('To convert to plotting units, units must be attached to '
                                 'scalar value being converted.')
        return scalar_value","1. **Use `getattr` to check for the presence of the `units` attribute before calling `to()`.** This will prevent an error from being raised if the `scalar_value` does not have a `units` attribute.
2. **Use `isinstance` to check that the `scalar_value` is an instance of a class that has a `to()` method.** This will prevent an error from being raised if the `scalar_value` is not a valid type.
3. **Add a `docstring` to the function that describes what it does and how to use it.** This will help users understand how to use the function correctly and avoid errors."
"def add_grid_arguments_from_xarray(func):
    """"""Fill in optional arguments like dx/dy from DataArray arguments.""""""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        bound_args = signature(func).bind(*args, **kwargs)
        bound_args.apply_defaults()

        # Search for DataArray with valid latitude and longitude coordinates to find grid
        # deltas and any other needed parameter
        dataarray_arguments = [
            value for value in bound_args.arguments.values()
            if isinstance(value, xr.DataArray)
        ]
        grid_prototype = None
        for da in dataarray_arguments:
            if hasattr(da.metpy, 'latitude') and hasattr(da.metpy, 'longitude'):
                grid_prototype = da
                break

        # Fill in x_dim/y_dim
        if (
            grid_prototype is not None
            and 'x_dim' in bound_args.arguments
            and 'y_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['x_dim'] = grid_prototype.metpy.find_axis_number('x')
                bound_args.arguments['y_dim'] = grid_prototype.metpy.find_axis_number('y')
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn('Horizontal dimension numbers not found. Defaulting to '
                              '(..., Y, X) order.')

        # Fill in vertical_dim
        if (
            grid_prototype is not None
            and 'vertical_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['vertical_dim'] = (
                    grid_prototype.metpy.find_axis_number('vertical')
                )
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn(
                    'Vertical dimension number not found. Defaulting to (..., Z, Y, X) order.'
                )

        # Fill in dz
        if (
            grid_prototype is not None
            and 'dz' in bound_args.arguments
            and bound_args.arguments['dz'] is None
        ):
            try:
                vertical_coord = grid_prototype.metpy.vertical
                bound_args.arguments['dz'] = np.diff(vertical_coord.metpy.unit_array)
            except AttributeError:
                # Skip, since this only comes up in advection, where dz is optional (may not
                # need vertical at all)
                pass

        # Fill in dx/dy
        if (
            'dx' in bound_args.arguments and bound_args.arguments['dx'] is None
            and 'dy' in bound_args.arguments and bound_args.arguments['dy'] is None
        ):
            if grid_prototype is not None:
                bound_args.arguments['dx'], bound_args.arguments['dy'] = (
                    grid_deltas_from_dataarray(grid_prototype, kind='actual')
                )
            elif 'dz' in bound_args.arguments:
                # Handle advection case, allowing dx/dy to be None but dz to not be None
                if bound_args.arguments['dz'] is None:
                    raise ValueError(
                        'Must provide dx, dy, and/or dz arguments or input DataArray with '
                        'proper coordinates.'
                    )
            else:
                raise ValueError('Must provide dx/dy arguments or input DataArray with '
                                 'latitude/longitude coordinates.')

        # Fill in latitude
        if 'latitude' in bound_args.arguments and bound_args.arguments['latitude'] is None:
            if grid_prototype is not None:
                bound_args.arguments['latitude'] = (
                    grid_prototype.metpy.latitude
                )
            else:
                raise ValueError('Must provide latitude argument or input DataArray with '
                                 'latitude/longitude coordinates.')

        return func(*bound_args.args, **bound_args.kwargs)
    return wrapper","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `warnings.warn` to notify the user when an argument is missing.
3. Use `np.diff` to calculate the grid deltas instead of relying on the user to provide them."
"    def wrapper(*args, **kwargs):
        bound_args = signature(func).bind(*args, **kwargs)
        bound_args.apply_defaults()

        # Search for DataArray with valid latitude and longitude coordinates to find grid
        # deltas and any other needed parameter
        dataarray_arguments = [
            value for value in bound_args.arguments.values()
            if isinstance(value, xr.DataArray)
        ]
        grid_prototype = None
        for da in dataarray_arguments:
            if hasattr(da.metpy, 'latitude') and hasattr(da.metpy, 'longitude'):
                grid_prototype = da
                break

        # Fill in x_dim/y_dim
        if (
            grid_prototype is not None
            and 'x_dim' in bound_args.arguments
            and 'y_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['x_dim'] = grid_prototype.metpy.find_axis_number('x')
                bound_args.arguments['y_dim'] = grid_prototype.metpy.find_axis_number('y')
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn('Horizontal dimension numbers not found. Defaulting to '
                              '(..., Y, X) order.')

        # Fill in vertical_dim
        if (
            grid_prototype is not None
            and 'vertical_dim' in bound_args.arguments
        ):
            try:
                bound_args.arguments['vertical_dim'] = (
                    grid_prototype.metpy.find_axis_number('vertical')
                )
            except AttributeError:
                # If axis number not found, fall back to default but warn.
                warnings.warn(
                    'Vertical dimension number not found. Defaulting to (..., Z, Y, X) order.'
                )

        # Fill in dz
        if (
            grid_prototype is not None
            and 'dz' in bound_args.arguments
            and bound_args.arguments['dz'] is None
        ):
            try:
                vertical_coord = grid_prototype.metpy.vertical
                bound_args.arguments['dz'] = np.diff(vertical_coord.metpy.unit_array)
            except AttributeError:
                # Skip, since this only comes up in advection, where dz is optional (may not
                # need vertical at all)
                pass

        # Fill in dx/dy
        if (
            'dx' in bound_args.arguments and bound_args.arguments['dx'] is None
            and 'dy' in bound_args.arguments and bound_args.arguments['dy'] is None
        ):
            if grid_prototype is not None:
                bound_args.arguments['dx'], bound_args.arguments['dy'] = (
                    grid_deltas_from_dataarray(grid_prototype, kind='actual')
                )
            elif 'dz' in bound_args.arguments:
                # Handle advection case, allowing dx/dy to be None but dz to not be None
                if bound_args.arguments['dz'] is None:
                    raise ValueError(
                        'Must provide dx, dy, and/or dz arguments or input DataArray with '
                        'proper coordinates.'
                    )
            else:
                raise ValueError('Must provide dx/dy arguments or input DataArray with '
                                 'latitude/longitude coordinates.')

        # Fill in latitude
        if 'latitude' in bound_args.arguments and bound_args.arguments['latitude'] is None:
            if grid_prototype is not None:
                bound_args.arguments['latitude'] = (
                    grid_prototype.metpy.latitude
                )
            else:
                raise ValueError('Must provide latitude argument or input DataArray with '
                                 'latitude/longitude coordinates.')

        return func(*bound_args.args, **bound_args.kwargs)","1. Use `functools.wraps` to preserve the original function metadata.
2. Use `warnings.warn` to raise warnings instead of exceptions.
3. Use `np.diff` to calculate the difference between two arrays."
"def _insert_lcl_level(pressure, temperature, lcl_pressure):
    """"""Insert the LCL pressure into the profile.""""""
    interp_temp = interpolate_1d(lcl_pressure, pressure, temperature)

    # Pressure needs to be increasing for searchsorted, so flip it and then convert
    # the index back to the original array
    loc = pressure.size - pressure[::-1].searchsorted(lcl_pressure)
    return np.insert(temperature.m, loc, interp_temp.m) * temperature.units","1. Use `np.where` instead of `searchsorted` to avoid the need to reverse the array.
2. Use `np.clip` to ensure that the index is within the bounds of the array.
3. Sanitize the input data to prevent malicious users from injecting invalid values."
"def _find_append_zero_crossings(x, y):
    r""""""
    Find and interpolate zero crossings.

    Estimate the zero crossings of an x,y series and add estimated crossings to series,
    returning a sorted array with no duplicate values.

    Parameters
    ----------
    x : `pint.Quantity`
        x values of data
    y : `pint.Quantity`
        y values of data

    Returns
    -------
    x : `pint.Quantity`
        x values of data
    y : `pint.Quantity`
        y values of data

    """"""
    crossings = find_intersections(x[1:], y[1:], np.zeros_like(y[1:]) * y.units, log_x=True)
    x = concatenate((x, crossings[0]))
    y = concatenate((y, crossings[1]))

    # Resort so that data are in order
    sort_idx = np.argsort(x)
    x = x[sort_idx]
    y = y[sort_idx]

    # Remove duplicate data points if there are any
    keep_idx = np.ediff1d(x.magnitude, to_end=[1]) > 1e-6
    x = x[keep_idx]
    y = y[keep_idx]
    return x, y","1. Use `np.unique` to remove duplicate data points instead of `np.ediff1d`.
2. Use `np.sort` to sort the data instead of `np.argsort`.
3. Use `np.concatenate` to concatenate the arrays instead of `+=`."
"def _get_bound_pressure_height(pressure, bound, heights=None, interpolate=True):
    """"""Calculate the bounding pressure and height in a layer.

    Given pressure, optional heights, and a bound, return either the closest pressure/height
    or interpolated pressure/height. If no heights are provided, a standard atmosphere
    ([NOAA1976]_) is assumed.

    Parameters
    ----------
    pressure : `pint.Quantity`
        Atmospheric pressures
    bound : `pint.Quantity`
        Bound to retrieve (in pressure or height)
    heights : `pint.Quantity`, optional
        Atmospheric heights associated with the pressure levels. Defaults to using
        heights calculated from ``pressure`` assuming a standard atmosphere.
    interpolate : boolean, optional
        Interpolate the bound or return the nearest. Defaults to True.

    Returns
    -------
    `pint.Quantity`
        The bound pressure and height.

    """"""
    # Make sure pressure is monotonically decreasing
    sort_inds = np.argsort(pressure)[::-1]
    pressure = pressure[sort_inds]
    if heights is not None:
        heights = heights[sort_inds]

    # Bound is given in pressure
    if bound.dimensionality == {'[length]': -1.0, '[mass]': 1.0, '[time]': -2.0}:
        # If the bound is in the pressure data, we know the pressure bound exactly
        if bound in pressure:
            bound_pressure = bound
            # If we have heights, we know the exact height value, otherwise return standard
            # atmosphere height for the pressure
            if heights is not None:
                bound_height = heights[pressure == bound_pressure]
            else:
                bound_height = pressure_to_height_std(bound_pressure)
        # If bound is not in the data, return the nearest or interpolated values
        else:
            if interpolate:
                bound_pressure = bound  # Use the user specified bound
                if heights is not None:  # Interpolate heights from the height data
                    bound_height = log_interpolate_1d(bound_pressure, pressure, heights)
                else:  # If not heights given, use the standard atmosphere
                    bound_height = pressure_to_height_std(bound_pressure)
            else:  # No interpolation, find the closest values
                idx = (np.abs(pressure - bound)).argmin()
                bound_pressure = pressure[idx]
                if heights is not None:
                    bound_height = heights[idx]
                else:
                    bound_height = pressure_to_height_std(bound_pressure)

    # Bound is given in height
    elif bound.dimensionality == {'[length]': 1.0}:
        # If there is height data, see if we have the bound or need to interpolate/find nearest
        if heights is not None:
            if bound in heights:  # Bound is in the height data
                bound_height = bound
                bound_pressure = pressure[heights == bound]
            else:  # Bound is not in the data
                if interpolate:
                    bound_height = bound

                    # Need to cast back to the input type since interp (up to at least numpy
                    # 1.13 always returns float64. This can cause upstream users problems,
                    # resulting in something like np.append() to upcast.
                    bound_pressure = (np.interp(np.atleast_1d(bound.m), heights.m,
                                                pressure.m).astype(result_type(bound))
                                      * pressure.units)
                else:
                    idx = (np.abs(heights - bound)).argmin()
                    bound_pressure = pressure[idx]
                    bound_height = heights[idx]
        else:  # Don't have heights, so assume a standard atmosphere
            bound_height = bound
            bound_pressure = height_to_pressure_std(bound)
            # If interpolation is on, this is all we need, if not, we need to go back and
            # find the pressure closest to this and refigure the bounds
            if not interpolate:
                idx = (np.abs(pressure - bound_pressure)).argmin()
                bound_pressure = pressure[idx]
                bound_height = pressure_to_height_std(bound_pressure)

    # Bound has invalid units
    else:
        raise ValueError('Bound must be specified in units of length or pressure.')

    # If the bound is out of the range of the data, we shouldn't extrapolate
    if not (_greater_or_close(bound_pressure, np.nanmin(pressure.m) * pressure.units)
            and _less_or_close(bound_pressure, np.nanmax(pressure.m) * pressure.units)):
        raise ValueError('Specified bound is outside pressure range.')
    if heights is not None and not (_less_or_close(bound_height,
                                                   np.nanmax(heights.m) * heights.units)
                                    and _greater_or_close(bound_height,
                                                          np.nanmin(heights.m)
                                                          * heights.units)):
        raise ValueError('Specified bound is outside height range.')

    return bound_pressure, bound_height","1. Use `np.atleast_1d` to ensure that `bound` is a 1-dimensional array.
2. Use `np.interp` to interpolate the bound to the nearest pressure level.
3. Use `np.nanmin` and `np.nanmax` to check if the bound is within the range of the data."
"def lfc(pressure, temperature, dewpt, parcel_temperature_profile=None, dewpt_start=None,
        which='top'):
    r""""""Calculate the level of free convection (LFC).

    This works by finding the first intersection of the ideal parcel path and
    the measured parcel temperature.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The atmospheric pressure
    temperature : `pint.Quantity`
        The temperature at the levels given by `pressure`
    dewpt : `pint.Quantity`
        The dewpoint at the levels given by `pressure`
    parcel_temperature_profile: `pint.Quantity`, optional
        The parcel temperature profile from which to calculate the LFC. Defaults to the
        surface parcel profile.
    dewpt_start: `pint.Quantity`, optional
        The dewpoint of the parcel for which to calculate the LFC. Defaults to the surface
        dewpoint.
    which: str, optional
        Pick which LFC to return. Options are 'top', 'bottom', and 'all'.
        Default is the 'top' (lowest pressure) LFC.

    Returns
    -------
    `pint.Quantity`
        The LFC pressure, or array of same if which='all'
    `pint.Quantity`
        The LFC temperature, or array of same if which='all'

    See Also
    --------
    parcel_profile

    """"""
    # Default to surface parcel if no profile or starting pressure level is given
    if parcel_temperature_profile is None:
        new_stuff = parcel_profile_with_lcl(pressure, temperature, dewpt)
        pressure, temperature, _, parcel_temperature_profile = new_stuff
        parcel_temperature_profile = parcel_temperature_profile.to(temperature.units)

    if dewpt_start is None:
        dewpt_start = dewpt[0]

    # The parcel profile and data may have the same first data point.
    # If that is the case, ignore that point to get the real first
    # intersection for the LFC calculation. Use logarithmic interpolation.
    if np.isclose(parcel_temperature_profile[0].m, temperature[0].m):
        x, y = find_intersections(pressure[1:], parcel_temperature_profile[1:],
                                  temperature[1:], direction='increasing', log_x=True)
    else:
        x, y = find_intersections(pressure, parcel_temperature_profile,
                                  temperature, direction='increasing', log_x=True)

    # Compute LCL for this parcel for future comparisons
    this_lcl = lcl(pressure[0], parcel_temperature_profile[0], dewpt_start)

    # The LFC could:
    # 1) Not exist
    # 2) Exist but be equal to the LCL
    # 3) Exist and be above the LCL

    # LFC does not exist or is LCL
    if len(x) == 0:
        # Is there any positive area above the LCL?
        mask = pressure < this_lcl[0]
        if np.all(_less_or_close(parcel_temperature_profile[mask], temperature[mask])):
            # LFC doesn't exist
            x, y = np.nan * pressure.units, np.nan * temperature.units
        else:  # LFC = LCL
            x, y = this_lcl
        return x, y

    # LFC exists. Make sure it is no lower than the LCL
    else:
        idx = x < this_lcl[0]
        # LFC height < LCL height, so set LFC = LCL
        if not any(idx):
            el_pres, _ = find_intersections(pressure[1:], parcel_temperature_profile[1:],
                                            temperature[1:], direction='decreasing',
                                            log_x=True)
            if np.min(el_pres) > this_lcl[0]:
                x, y = np.nan * pressure.units, np.nan * temperature.units
            else:
                x, y = this_lcl
            return x, y
        # Otherwise, find all LFCs that exist above the LCL
        # What is returned depends on which flag as described in the docstring
        else:
            return _multiple_el_lfc_options(x, y, idx, which)","1. Use `np.isclose()` to compare floating-point numbers instead of `==`.
2. Use `np.nan` to represent missing values instead of `None`.
3. Use `pint.Quantity` to represent physical quantities with units."
"def cape_cin(pressure, temperature, dewpt, parcel_profile):
    r""""""Calculate CAPE and CIN.

    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)
    of a given upper air profile and parcel path. CIN is integrated between the surface and
    LFC, CAPE is integrated between the LFC and EL (or top of sounding). Intersection points of
    the measured temperature profile and parcel profile are linearly interpolated.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The atmospheric pressure level(s) of interest, in order from highest to
        lowest pressure.
    temperature : `pint.Quantity`
        The atmospheric temperature corresponding to pressure.
    dewpt : `pint.Quantity`
        The atmospheric dewpoint corresponding to pressure.
    parcel_profile : `pint.Quantity`
        The temperature profile of the parcel.

    Returns
    -------
    `pint.Quantity`
        Convective Available Potential Energy (CAPE).
    `pint.Quantity`
        Convective INhibition (CIN).

    Notes
    -----
    Formula adopted from [Hobbs1977]_.

    .. math:: \\text{CAPE} = -R_d \\int_{LFC}^{EL} (T_{parcel} - T_{env}) d\\text{ln}(p)

    .. math:: \\text{CIN} = -R_d \\int_{SFC}^{LFC} (T_{parcel} - T_{env}) d\\text{ln}(p)


    * :math:`CAPE` Convective available potential energy
    * :math:`CIN` Convective inhibition
    * :math:`LFC` Pressure of the level of free convection
    * :math:`EL` Pressure of the equilibrium level
    * :math:`SFC` Level of the surface or beginning of parcel path
    * :math:`R_d` Gas constant
    * :math:`g` Gravitational acceleration
    * :math:`T_{parcel}` Parcel temperature
    * :math:`T_{env}` Environment temperature
    * :math:`p` Atmospheric pressure

    See Also
    --------
    lfc, el

    """"""
    # Calculate LFC limit of integration
    lfc_pressure, _ = lfc(pressure, temperature, dewpt,
                          parcel_temperature_profile=parcel_profile)

    # If there is no LFC, no need to proceed.
    if np.isnan(lfc_pressure):
        return 0 * units('J/kg'), 0 * units('J/kg')
    else:
        lfc_pressure = lfc_pressure.magnitude

    # Calculate the EL limit of integration
    el_pressure, _ = el(pressure, temperature, dewpt,
                        parcel_temperature_profile=parcel_profile)

    # No EL and we use the top reading of the sounding.
    if np.isnan(el_pressure):
        el_pressure = pressure[-1].magnitude
    else:
        el_pressure = el_pressure.magnitude

    # Difference between the parcel path and measured temperature profiles
    y = (parcel_profile - temperature).to(units.degK)

    # Estimate zero crossings
    x, y = _find_append_zero_crossings(np.copy(pressure), y)

    # CAPE
    # Only use data between the LFC and EL for calculation
    p_mask = _less_or_close(x.m, lfc_pressure) & _greater_or_close(x.m, el_pressure)
    x_clipped = x[p_mask].magnitude
    y_clipped = y[p_mask].magnitude
    cape = (mpconsts.Rd
            * (np.trapz(y_clipped, np.log(x_clipped)) * units.degK)).to(units('J/kg'))

    # CIN
    # Only use data between the surface and LFC for calculation
    p_mask = _greater_or_close(x.m, lfc_pressure)
    x_clipped = x[p_mask].magnitude
    y_clipped = y[p_mask].magnitude
    cin = (mpconsts.Rd
           * (np.trapz(y_clipped, np.log(x_clipped)) * units.degK)).to(units('J/kg'))

    return cape, cin","1. Use `np.isclose` instead of `np.isnan` to check for NaN values.
2. Use `_greater_or_equal` and `_less_or_equal` to check for inequalities instead of `>` and `<`.
3. Use `units.Quantity` to represent all numerical values to avoid overflow errors."
"def lcl(pressure, temperature, dewpt, max_iters=50, eps=1e-5):
    r""""""Calculate the lifted condensation level (LCL) using from the starting point.

    The starting state for the parcel is defined by `temperature`, `dewpt`,
    and `pressure`.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The starting atmospheric pressure
    temperature : `pint.Quantity`
        The starting temperature
    dewpt : `pint.Quantity`
        The starting dewpoint

    Returns
    -------
    `pint.Quantity`
        The LCL pressure
    `pint.Quantity`
        The LCL temperature

    Other Parameters
    ----------------
    max_iters : int, optional
        The maximum number of iterations to use in calculation, defaults to 50.
    eps : float, optional
        The desired relative error in the calculated value, defaults to 1e-5.

    See Also
    --------
    parcel_profile

    Notes
    -----
    This function is implemented using an iterative approach to solve for the
    LCL. The basic algorithm is:

    1. Find the dewpoint from the LCL pressure and starting mixing ratio
    2. Find the LCL pressure from the starting temperature and dewpoint
    3. Iterate until convergence

    The function is guaranteed to finish by virtue of the `max_iters` counter.

    """"""
    def _lcl_iter(p, p0, w, t):
        td = dewpoint(vapor_pressure(units.Quantity(p, pressure.units), w))
        return (p0 * (td / t) ** (1. / mpconsts.kappa)).m

    w = mixing_ratio(saturation_vapor_pressure(dewpt), pressure)
    fp = so.fixed_point(_lcl_iter, pressure.m, args=(pressure.m, w, temperature),
                        xtol=eps, maxiter=max_iters)
    lcl_p = fp * pressure.units
    return lcl_p, dewpoint(vapor_pressure(lcl_p, w)).to(temperature.units)","1. Use `functools.lru_cache` to cache the results of expensive calculations.
2. Use `typing` to annotate the function parameters and return values.
3. Use `mypy` to check the function for type errors."
"def interpolate_1d(x, xp, *args, **kwargs):
    r""""""Interpolates data with any shape over a specified axis.

    Interpolation over a specified axis for arrays of any shape.

    Parameters
    ----------
    x : array-like
        1-D array of desired interpolated values.

    xp : array-like
        The x-coordinates of the data points.

    args : array-like
        The data to be interpolated. Can be multiple arguments, all must be the same shape as
        xp.

    axis : int, optional
        The axis to interpolate over. Defaults to 0.

    fill_value: float, optional
        Specify handling of interpolation points out of data bounds. If None, will return
        ValueError if points are out of bounds. Defaults to nan.

    Returns
    -------
    array-like
        Interpolated values for each point with coordinates sorted in ascending order.

    Examples
    --------
     >>> x = np.array([1., 2., 3., 4.])
     >>> y = np.array([1., 2., 3., 4.])
     >>> x_interp = np.array([2.5, 3.5])
     >>> metpy.calc.interp(x_interp, x, y)
     array([2.5, 3.5])

    Notes
    -----
    xp and args must be the same shape.

    """"""
    # Pull out keyword args
    fill_value = kwargs.pop('fill_value', np.nan)
    axis = kwargs.pop('axis', 0)

    # Make x an array
    x = np.asanyarray(x).reshape(-1)

    # Save number of dimensions in xp
    ndim = xp.ndim

    # Sort input data
    sort_args = np.argsort(xp, axis=axis)
    sort_x = np.argsort(x)

    # indices for sorting
    sorter = broadcast_indices(xp, sort_args, ndim, axis)

    # sort xp
    xp = xp[sorter]
    # Ensure pressure in increasing order
    variables = [arr[sorter] for arr in args]

    # Make x broadcast with xp
    x_array = x[sort_x]
    expand = [np.newaxis] * ndim
    expand[axis] = slice(None)
    x_array = x_array[tuple(expand)]

    # Calculate value above interpolated value
    minv = np.apply_along_axis(np.searchsorted, axis, xp, x[sort_x])
    minv2 = np.copy(minv)

    # If fill_value is none and data is out of bounds, raise value error
    if ((np.max(minv) == xp.shape[axis]) or (np.min(minv) == 0)) and fill_value is None:
        raise ValueError('Interpolation point out of data bounds encountered')

    # Warn if interpolated values are outside data bounds, will make these the values
    # at end of data range.
    if np.max(minv) == xp.shape[axis]:
        warnings.warn('Interpolation point out of data bounds encountered')
        minv2[minv == xp.shape[axis]] = xp.shape[axis] - 1
    if np.min(minv) == 0:
        minv2[minv == 0] = 1

    # Get indices for broadcasting arrays
    above = broadcast_indices(xp, minv2, ndim, axis)
    below = broadcast_indices(xp, minv2 - 1, ndim, axis)

    if np.any(x_array < xp[below]):
        warnings.warn('Interpolation point out of data bounds encountered')

    # Create empty output list
    ret = []

    # Calculate interpolation for each variable
    for var in variables:
        # Var needs to be on the *left* of the multiply to ensure that if it's a pint
        # Quantity, it gets to control the operation--at least until we make sure
        # masked arrays and pint play together better. See https://github.com/hgrecco/pint#633
        var_interp = var[below] + (var[above] - var[below]) * ((x_array - xp[below])
                                                               / (xp[above] - xp[below]))

        # Set points out of bounds to fill value.
        var_interp[minv == xp.shape[axis]] = fill_value
        var_interp[x_array < xp[below]] = fill_value

        # Check for input points in decreasing order and return output to match.
        if x[0] > x[-1]:
            var_interp = np.swapaxes(np.swapaxes(var_interp, 0, axis)[::-1], 0, axis)
        # Output to list
        ret.append(var_interp)
    if len(ret) == 1:
        return ret[0]
    else:
        return ret","1. Use `np.asanyarray()` to explicitly convert the input data to an array.
2. Use `np.argsort()` to sort the input data along the specified axis.
3. Use `np.searchsorted()` to find the indices of the values in `xp` that are closest to the values in `x`."
"def log_interpolate_1d(x, xp, *args, **kwargs):
    r""""""Interpolates data with logarithmic x-scale over a specified axis.

    Interpolation on a logarithmic x-scale for interpolation values in pressure coordintates.

    Parameters
    ----------
    x : array-like
        1-D array of desired interpolated values.

    xp : array-like
        The x-coordinates of the data points.

    args : array-like
        The data to be interpolated. Can be multiple arguments, all must be the same shape as
        xp.

    axis : int, optional
        The axis to interpolate over. Defaults to 0.

    fill_value: float, optional
        Specify handling of interpolation points out of data bounds. If None, will return
        ValueError if points are out of bounds. Defaults to nan.

    Returns
    -------
    array-like
        Interpolated values for each point with coordinates sorted in ascending order.

    Examples
    --------
     >>> x_log = np.array([1e3, 1e4, 1e5, 1e6])
     >>> y_log = np.log(x_log) * 2 + 3
     >>> x_interp = np.array([5e3, 5e4, 5e5])
     >>> metpy.calc.log_interp(x_interp, x_log, y_log)
     array([20.03438638, 24.63955657, 29.24472675])

    Notes
    -----
    xp and args must be the same shape.

    """"""
    # Pull out kwargs
    fill_value = kwargs.pop('fill_value', np.nan)
    axis = kwargs.pop('axis', 0)

    # Log x and xp
    log_x = np.log(x)
    log_xp = np.log(xp)
    return interpolate_1d(log_x, log_xp, *args, axis=axis, fill_value=fill_value)","1. Use `np.clip` to check if the input values are out of bounds.
2. Use `np.asarray` to convert the input arguments to the same type.
3. Use `np.nan` to fill the values that are out of bounds."
"def lfc(pressure, temperature, dewpt, parcel_temperature_profile=None):
    r""""""Calculate the level of free convection (LFC).

    This works by finding the first intersection of the ideal parcel path and
    the measured parcel temperature.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The atmospheric pressure
    temperature : `pint.Quantity`
        The temperature at the levels given by `pressure`
    dewpt : `pint.Quantity`
        The dew point at the levels given by `pressure`
    parcel_temperature_profile: `pint.Quantity`, optional
        The parcel temperature profile from which to calculate the LFC. Defaults to the
        surface parcel profile.

    Returns
    -------
    `pint.Quantity`
        The LFC pressure and temperature

    See Also
    --------
    parcel_profile

    """"""
    # Default to surface parcel if no profile or starting pressure level is given
    if parcel_temperature_profile is None:
        new_stuff = parcel_profile_with_lcl(pressure, temperature, dewpt)
        pressure, temperature, _, parcel_temperature_profile = new_stuff
        temperature = temperature.to('degC')
        parcel_temperature_profile = parcel_temperature_profile.to('degC')

    # The parcel profile and data have the same first data point, so we ignore
    # that point to get the real first intersection for the LFC calculation.
    x, y = find_intersections(pressure[1:], parcel_temperature_profile[1:],
                              temperature[1:], direction='increasing')

    # Compute LCL for this parcel for future comparisons
    this_lcl = lcl(pressure[0], temperature[0], dewpt[0])

    # The LFC could:
    # 1) Not exist
    # 2) Exist but be equal to the LCL
    # 3) Exist and be above the LCL

    # LFC does not exist or is LCL
    if len(x) == 0:
        # Is there any positive area above the LCL?
        mask = pressure < this_lcl[0]
        if np.all(_less_or_close(parcel_temperature_profile[mask], temperature[mask])):
            # LFC doesn't exist
            return np.nan * pressure.units, np.nan * temperature.units
        else:  # LFC = LCL
            x, y = this_lcl
            return x, y

    # LFC exists and is not LCL. Make sure it is above the LCL.
    else:
        idx = x < lcl(pressure[0], temperature[0], dewpt[0])[0]
        x = x[idx]
        y = y[idx]
        return x[0], y[0]","1. Use `pint.Quantity` for all variables to ensure that they are properly typed and validated.
2. Use `np.nan` to represent missing values instead of `None`.
3. Use `_less_or_close` to compare floating-point values to avoid floating-point errors."
"def storm_relative_helicity(u, v, heights, depth, bottom=0 * units.m,
                            storm_u=0 * units('m/s'), storm_v=0 * units('m/s')):
    # Partially adapted from similar SharpPy code
    r""""""Calculate storm relative helicity.

    Calculates storm relatively helicity following [Markowski2010] 230-231.

    .. math:: \\int\\limits_0^d (\\bar v - c) \\cdot \\bar\\omega_{h} \\,dz

    This is applied to the data from a hodograph with the following summation:

    .. math:: \\sum_{n = 1}^{N-1} [(u_{n+1} - c_{x})(v_{n} - c_{y}) -
                                  (u_{n} - c_{x})(v_{n+1} - c_{y})]

    Parameters
    ----------
    u : array-like
        u component winds
    v : array-like
        v component winds
    heights : array-like
        atmospheric heights, will be converted to AGL
    depth : number
        depth of the layer
    bottom : number
        height of layer bottom AGL (default is surface)
    storm_u : number
        u component of storm motion (default is 0 m/s)
    storm_v : number
        v component of storm motion (default is 0 m/s)

    Returns
    -------
    `pint.Quantity, pint.Quantity, pint.Quantity`
        positive, negative, total storm-relative helicity

    """"""
    _, u, v = get_layer_heights(heights, depth, u, v, with_agl=True, bottom=bottom)

    storm_relative_u = u - storm_u
    storm_relative_v = v - storm_v

    int_layers = (storm_relative_u[1:] * storm_relative_v[:-1] -
                  storm_relative_u[:-1] * storm_relative_v[1:])

    positive_srh = int_layers[int_layers.magnitude > 0.].sum()
    negative_srh = int_layers[int_layers.magnitude < 0.].sum()

    return (positive_srh.to('meter ** 2 / second ** 2'),
            negative_srh.to('meter ** 2 / second ** 2'),
            (positive_srh + negative_srh).to('meter ** 2 / second ** 2'))","1. Use `pint.Quantity` for all numerical values to avoid overflow errors.
2. Use `np.clip` to clip values to a specified range to avoid divide-by-zero errors.
3. Use `np.where` to check for specific conditions and return early to avoid unnecessary computations."
"def lfc(pressure, temperature, dewpt, parcel_temperature_profile=None):
    r""""""Calculate the level of free convection (LFC).

    This works by finding the first intersection of the ideal parcel path and
    the measured parcel temperature.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The atmospheric pressure
    temperature : `pint.Quantity`
        The temperature at the levels given by `pressure`
    dewpt : `pint.Quantity`
        The dew point at the levels given by `pressure`
    parcel_temperature_profile: `pint.Quantity`, optional
        The parcel temperature profile from which to calculate the LFC. Defaults to the
        surface parcel profile.

    Returns
    -------
    `pint.Quantity`
        The LFC pressure and temperature

    See Also
    --------
    parcel_profile

    """"""
    # Default to surface parcel if no profile or starting pressure level is given
    if parcel_temperature_profile is None:
        parcel_temperature_profile = parcel_profile(pressure, temperature[0],
                                                    dewpt[0]).to('degC')
    # The parcel profile and data have the same first data point, so we ignore
    # that point to get the real first intersection for the LFC calculation.
    x, y = find_intersections(pressure[1:], parcel_temperature_profile[1:],
                              temperature[1:], direction='increasing')

    # The LFC could:
    # 1) Not exist
    # 2) Exist but be equal to the LCL
    # 3) Exist and be above the LCL

    # LFC does not exist or is LCL
    if len(x) == 0:
        if np.all(_less_or_close(parcel_temperature_profile, temperature)):
            # LFC doesn't exist
            return np.nan * pressure.units, np.nan * temperature.units
        else:  # LFC = LCL
            x, y = lcl(pressure[0], temperature[0], dewpt[0])
            return x, y

    # LFC exists and is not LCL. Make sure it is above the LCL.
    else:
        idx = x < lcl(pressure[0], temperature[0], dewpt[0])[0]
        x = x[idx]
        y = y[idx]
        return x[0], y[0]","1. Use `pint.Quantity` for all numeric values to ensure they are correctly represented.
2. Use `np.nan` to represent missing values instead of `None`.
3. Use `_less_or_close` to check if two values are close to each other, instead of comparing them directly."
"def el(pressure, temperature, dewpt, parcel_temperature_profile=None):
    r""""""Calculate the equilibrium level.

    This works by finding the last intersection of the ideal parcel path and
    the measured environmental temperature. If there is one or fewer intersections, there is
    no equilibrium level.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The atmospheric pressure
    temperature : `pint.Quantity`
        The temperature at the levels given by `pressure`
    dewpt : `pint.Quantity`
        The dew point at the levels given by `pressure`
    parcel_temperature_profile: `pint.Quantity`, optional
        The parcel temperature profile from which to calculate the EL. Defaults to the
        surface parcel profile.

    Returns
    -------
    `pint.Quantity, pint.Quantity`
        The EL pressure and temperature

    See Also
    --------
    parcel_profile

    """"""
    # Default to surface parcel if no profile or starting pressure level is given
    if parcel_temperature_profile is None:
        parcel_temperature_profile = parcel_profile(pressure, temperature[0],
                                                    dewpt[0]).to('degC')
    # If the top of the sounding parcel is warmer than the environment, there is no EL
    if parcel_temperature_profile[-1] > temperature[-1]:
        return np.nan * pressure.units, np.nan * temperature.units

    # Otherwise the last intersection (as long as there is one) is the EL
    x, y = find_intersections(pressure[1:], parcel_temperature_profile[1:], temperature[1:])
    if len(x) > 0:
        return x[-1], y[-1]
    else:
        return np.nan * pressure.units, np.nan * temperature.units","1. Use `pint.check_units` to validate that the units of the input arguments are consistent.
2. Use `pint.check_finite` to validate that the input arguments are finite.
3. Use `pint.check_greater` to validate that the input arguments are greater than zero."
"def parcel_profile(pressure, temperature, dewpt):
    r""""""Calculate the profile a parcel takes through the atmosphere.

    The parcel starts at `temperature`, and `dewpt`, lifted up
    dry adiabatically to the LCL, and then moist adiabatically from there.
    `pressure` specifies the pressure levels for the profile.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The atmospheric pressure level(s) of interest. The first entry should be the starting
        point pressure.
    temperature : `pint.Quantity`
        The starting temperature
    dewpt : `pint.Quantity`
        The starting dew point

    Returns
    -------
    `pint.Quantity`
        The parcel temperatures at the specified pressure levels.

    See Also
    --------
    lcl, moist_lapse, dry_lapse

    """"""
    # Find the LCL
    lcl_pressure, _ = lcl(pressure[0], temperature, dewpt)
    lcl_pressure = lcl_pressure.to(pressure.units)

    # Find the dry adiabatic profile, *including* the LCL. We need >= the LCL in case the
    # LCL is included in the levels. It's slightly redundant in that case, but simplifies
    # the logic for removing it later.
    press_lower = concatenate((pressure[pressure >= lcl_pressure], lcl_pressure))
    t1 = dry_lapse(press_lower, temperature)

    # If the pressure profile doesn't make it to the lcl, we can stop here
    if _greater_or_close(np.nanmin(pressure), lcl_pressure.m):
        return t1[:-1]

    # Find moist pseudo-adiabatic profile starting at the LCL
    press_upper = concatenate((lcl_pressure, pressure[pressure < lcl_pressure]))
    t2 = moist_lapse(press_upper, t1[-1]).to(t1.units)

    # Return LCL *without* the LCL point
    return concatenate((t1[:-1], t2[1:]))","1. Use `pint.Quantity` to represent pressure and temperature, and validate input arguments.
2. Use `concatenate()` to concatenate arrays, and avoid using `np.concatenate()` directly.
3. Use `greater_or_close()` to compare two values, and avoid using `>` or `<` directly."
"def surface_based_cape_cin(pressure, temperature, dewpoint):
    r""""""Calculate surface-based CAPE and CIN.

    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)
    of a given upper air profile for a surface-based parcel. CIN is integrated
    between the surface and LFC, CAPE is integrated between the LFC and EL (or top of
    sounding). Intersection points of the measured temperature profile and parcel profile are
    linearly interpolated.

    Parameters
    ----------
    pressure : `pint.Quantity`
        Atmospheric pressure profile. The first entry should be the starting
        (surface) observation.
    temperature : `pint.Quantity`
        Temperature profile
    dewpoint : `pint.Quantity`
        Dewpoint profile

    Returns
    -------
    `pint.Quantity`
        Surface based Convective Available Potential Energy (CAPE).
    `pint.Quantity`
        Surface based Convective INhibition (CIN).

    See Also
    --------
    cape_cin, parcel_profile

    """"""
    profile = parcel_profile(pressure, temperature[0], dewpoint[0])
    return cape_cin(pressure, temperature, dewpoint, profile)","1. Use `pint.Quantity` for all numerical values to prevent overflow and underflow errors.
2. Use `functools.lru_cache` to memoize the parcel profile calculation to improve performance.
3. Use `warnings.warn` to notify users of potential problems with the input data."
"def isentropic_interpolation(theta_levels, pressure, temperature, *args, **kwargs):
    r""""""Interpolate data in isobaric coordinates to isentropic coordinates.

    Parameters
    ----------
    theta_levels : array
        One-dimensional array of desired theta surfaces
    pressure : array
        One-dimensional array of pressure levels
    temperature : array
        Array of temperature
    args : array, optional
        Any additional variables will be interpolated to each isentropic level.

    Returns
    -------
    list
        List with pressure at each isentropic level, followed by each additional
        argument interpolated to isentropic coordinates.

    Other Parameters
    ----------------
    axis : int, optional
        The axis corresponding to the vertical in the temperature array, defaults to 0.
    tmpk_out : bool, optional
        If true, will calculate temperature and output as the last item in the output list.
        Defaults to False.
    max_iters : int, optional
        The maximum number of iterations to use in calculation, defaults to 50.
    eps : float, optional
        The desired absolute error in the calculated value, defaults to 1e-6.

    Notes
    -----
    Input variable arrays must have the same number of vertical levels as the pressure levels
    array. Pressure is calculated on isentropic surfaces by assuming that temperature varies
    linearly with the natural log of pressure. Linear interpolation is then used in the
    vertical to find the pressure at each isentropic level. Interpolation method from
    [Ziv1994]_. Any additional arguments are assumed to vary linearly with temperature and will
    be linearly interpolated to the new isentropic levels.

    See Also
    --------
    potential_temperature

    """"""
    # iteration function to be used later
    # Calculates theta from linearly interpolated temperature and solves for pressure
    def _isen_iter(iter_log_p, isentlevs_nd, ka, a, b, pok):
        exner = pok * np.exp(-ka * iter_log_p)
        t = a * iter_log_p + b
        # Newton-Raphson iteration
        f = isentlevs_nd - t * exner
        fp = exner * (ka * t - a)
        return iter_log_p - (f / fp)

    # Change when Python 2.7 no longer supported
    # Pull out keyword arguments
    tmpk_out = kwargs.pop('tmpk_out', False)
    max_iters = kwargs.pop('max_iters', 50)
    eps = kwargs.pop('eps', 1e-6)
    axis = kwargs.pop('axis', 0)

    # Get dimensions in temperature
    ndim = temperature.ndim

    # Convert units
    pres = pressure.to('hPa')
    temperature = temperature.to('kelvin')

    slices = [np.newaxis] * ndim
    slices[axis] = slice(None)
    pres = pres[slices]
    pres = np.broadcast_to(pres, temperature.shape) * pres.units

    # Sort input data
    sort_pres = np.argsort(pres.m, axis=axis)
    sort_pres = np.swapaxes(np.swapaxes(sort_pres, 0, axis)[::-1], 0, axis)
    sorter = broadcast_indices(pres, sort_pres, ndim, axis)
    levs = pres[sorter]
    theta_levels = np.asanyarray(theta_levels.to('kelvin')).reshape(-1)
    sort_isentlevs = np.argsort(theta_levels)
    tmpk = temperature[sorter]
    isentlevels = theta_levels[sort_isentlevs]

    # Make the desired isentropic levels the same shape as temperature
    isentlevs_nd = isentlevels
    isentlevs_nd = isentlevs_nd[slices]
    shape = list(temperature.shape)
    shape[axis] = isentlevels.size
    isentlevs_nd = np.broadcast_to(isentlevs_nd, shape)

    # exponent to Poisson's Equation, which is imported above
    ka = kappa.to('dimensionless').m

    # calculate theta for each point
    pres_theta = potential_temperature(levs, tmpk)

    # Raise error if input theta level is larger than pres_theta max
    if np.max(pres_theta.m) < np.max(theta_levels):
        raise ValueError('Input theta level out of data bounds')

    # Find log of pressure to implement assumption of linear temperature dependence on
    # ln(p)
    log_p = np.log(levs.m)

    # Calculations for interpolation routine
    pok = P0 ** ka

    # index values for each point for the pressure level nearest to the desired theta level
    minv = np.apply_along_axis(np.searchsorted, axis, pres_theta.m, theta_levels)

    # Create index values for broadcasting arrays
    above = broadcast_indices(tmpk, minv, ndim, axis)
    below = broadcast_indices(tmpk, minv - 1, ndim, axis)

    # calculate constants for the interpolation
    a = (tmpk.m[above] - tmpk.m[below]) / (log_p[above] - log_p[below])
    b = tmpk.m[above] - a * log_p[above]

    # calculate first guess for interpolation
    first_guess = 0.5 * (log_p[above] + log_p[below])

    # iterative interpolation using scipy.optimize.fixed_point and _isen_iter defined above
    log_p_solved = so.fixed_point(_isen_iter, first_guess, args=(isentlevs_nd, ka, a, b,
                                                                 pok.m),
                                  xtol=eps, maxiter=max_iters)

    # get back pressure and assign nan for values with pressure greater than 1000 hPa
    isentprs = np.exp(log_p_solved)
    isentprs[isentprs > np.max(pressure.m)] = np.nan

    # create list for storing output data
    ret = []
    ret.append(isentprs * units.hPa)

    # if tmpk_out = true, calculate temperature and output as last item in list
    if tmpk_out:
        ret.append((isentlevs_nd / ((P0.m / isentprs) ** ka)) * units.kelvin)

    # check to see if any additional arguments were given, if so, interpolate to
    # new isentropic levels
    try:
        args[0]
    except IndexError:
        return ret
    else:
        # do an interpolation for each additional argument
        for arr in args:
            var = arr[sorter]
            # interpolate to isentropic levels and add to temporary output array
            arg_out = interp(isentlevels, pres_theta.m, var, axis=axis)
            ret.append(arg_out)

    # output values as a list
    return ret","1. Use `np.broadcast_to` instead of `np.expand_dims` to avoid creating unnecessary copies.
2. Use `np.searchsorted` instead of `np.argsort` to avoid sorting the entire array.
3. Use `np.nan_to_num` to convert NaN values to numbers before passing them to `scipy.optimize.fixed_point`."
"def interp(x, xp, *args, **kwargs):
    r""""""Interpolates data with any shape over a specified axis.

    Interpolation over a specified axis for arrays of any shape.

    Parameters
    ----------
    x : array-like
        1-D array of desired interpolated values.

    xp : array-like
        The x-coordinates of the data points.

    args : array-like
        The data to be interpolated. Can be multiple arguments, all must be the same shape as
        xp.

    axis : int, optional
        The axis to interpolate over. Defaults to 0.

    fill_value: float, optional
        Specify handling of interpolation points out of data bounds. If None, will return
        ValueError if points are out of bounds. Defaults to nan.

    Returns
    -------
    array-like
        Interpolated values for each point with coordinates sorted in ascending order.

    Examples
    --------
     >>> x = np.array([1., 2., 3., 4.])
     >>> y = np.array([1., 2., 3., 4.])
     >>> x_interp = np.array([2.5, 3.5])
     >>> metpy.calc.interp(x_interp, x, y)
     array([2.5, 3.5])

    Notes
    -----
    xp and args must be the same shape.

    """"""
    # Pull out keyword args
    fill_value = kwargs.pop('fill_value', np.nan)
    axis = kwargs.pop('axis', 0)

    # Make x an array
    x = np.asanyarray(x).reshape(-1)

    # Save number of dimensions in xp
    ndim = xp.ndim

    # Sort input data
    sort_args = np.argsort(xp, axis=axis)
    sort_x = np.argsort(x)

    # indices for sorting
    sorter = broadcast_indices(xp, sort_args, ndim, axis)

    # sort xp
    xp = xp[sorter]
    # Ensure pressure in increasing order
    variables = [arr[sorter] for arr in args]

    # Make x broadcast with xp
    x_array = x[sort_x]
    expand = [np.newaxis] * ndim
    expand[axis] = slice(None)
    x_array = x_array[expand]

    # Calculate value above interpolated value
    minv = np.apply_along_axis(np.searchsorted, axis, xp, x[sort_x])
    minv2 = np.copy(minv)

    # If fill_value is none and data is out of bounds, raise value error
    if ((np.max(minv) == xp.shape[axis]) or (np.min(minv) == 0)) and fill_value is None:
        raise ValueError('Interpolation point out of data bounds encountered')

    # Warn if interpolated values are outside data bounds, will make these the values
    # at end of data range.
    if np.max(minv) == xp.shape[axis]:
        warnings.warn('Interpolation point out of data bounds encountered')
        minv2[minv == xp.shape[axis]] = xp.shape[axis] - 1
    if np.min(minv) == 0:
        minv2[minv == 0] = 1

    # Get indices for broadcasting arrays
    above = broadcast_indices(xp, minv2, ndim, axis)
    below = broadcast_indices(xp, minv2 - 1, ndim, axis)

    if np.any(x_array < xp[below]):
        warnings.warn('Interpolation point out of data bounds encountered')

    # Create empty output list
    ret = []

    # Calculate interpolation for each variable
    for var in variables:
        var_interp = var[below] + ((x_array - xp[below]) /
                                   (xp[above] - xp[below])) * (var[above] -
                                                               var[below])

        # Set points out of bounds to fill value.
        var_interp[minv == xp.shape[axis]] = fill_value
        var_interp[x_array < xp[below]] = fill_value

        # Check for input points in decreasing order and return output to match.
        if x[0] > x[-1]:
            var_interp = np.swapaxes(np.swapaxes(var_interp, 0, axis)[::-1], 0, axis)
        # Output to list
        ret.append(var_interp)
    if len(ret) == 1:
        return ret[0]
    else:
        return ret","1. Use `np.asanyarray()` to convert `x` to an array.
2. Use `np.argsort()` to sort the input data.
3. Use `np.searchsorted()` to find the index of the value above the interpolated value."
"def isentropic_interpolation(theta_levels, pressure, temperature, *args, **kwargs):
    r""""""Interpolate data in isobaric coordinates to isentropic coordinates.

    Parameters
    ----------
    theta_levels : array
        One-dimensional array of desired theta surfaces
    pressure : array
        One-dimensional array of pressure levels
    temperature : array
        Array of temperature
    args : array, optional
        Any additional variables will be interpolated to each isentropic level.

    Returns
    -------
    list
        List with pressure at each isentropic level, followed by each additional
        argument interpolated to isentropic coordinates.

    Other Parameters
    ----------------
    axis : int, optional
        The axis corresponding to the vertical in the temperature array, defaults to 0.
    tmpk_out : bool, optional
        If true, will calculate temperature and output as the last item in the output list.
        Defaults to False.
    max_iters : int, optional
        The maximum number of iterations to use in calculation, defaults to 50.
    eps : float, optional
        The desired absolute error in the calculated value, defaults to 1e-6.

    Notes
    -----
    Input variable arrays must have the same number of vertical levels as the pressure levels
    array. Pressure is calculated on isentropic surfaces by assuming that temperature varies
    linearly with the natural log of pressure. Linear interpolation is then used in the
    vertical to find the pressure at each isentropic level. Interpolation method from
    [Ziv1994]_. Any additional arguments are assumed to vary linearly with temperature and will
    be linearly interpolated to the new isentropic levels.

    See Also
    --------
    potential_temperature

    """"""
    # iteration function to be used later
    # Calculates theta from linearly interpolated temperature and solves for pressure
    def _isen_iter(iter_log_p, isentlevs_nd, ka, a, b, pok):
        exner = pok * np.exp(-ka * iter_log_p)
        t = a * iter_log_p + b
        # Newton-Raphson iteration
        f = isentlevs_nd - t * exner
        fp = exner * (ka * t - a)
        return iter_log_p - (f / fp)

    # Change when Python 2.7 no longer supported
    # Pull out keyword arguments
    tmpk_out = kwargs.pop('tmpk_out', False)
    max_iters = kwargs.pop('max_iters', 50)
    eps = kwargs.pop('eps', 1e-6)
    axis = kwargs.pop('axis', 0)

    # Get dimensions in temperature
    ndim = temperature.ndim

    # Convert units
    pres = pressure.to('hPa')
    temperature = temperature.to('kelvin')

    slices = [np.newaxis] * ndim
    slices[axis] = slice(None)
    pres = np.broadcast_to(pres[slices], temperature.shape) * pres.units

    # Sort input data
    sort_pres = np.argsort(pres.m, axis=axis)
    sort_pres = np.swapaxes(np.swapaxes(sort_pres, 0, axis)[::-1], 0, axis)
    sorter = broadcast_indices(pres, sort_pres, ndim, axis)
    levs = pres[sorter]
    tmpk = temperature[sorter]

    theta_levels = np.asanyarray(theta_levels.to('kelvin')).reshape(-1)
    isentlevels = theta_levels[np.argsort(theta_levels)]

    # Make the desired isentropic levels the same shape as temperature
    shape = list(temperature.shape)
    shape[axis] = isentlevels.size
    isentlevs_nd = np.broadcast_to(isentlevels[slices], shape)

    # exponent to Poisson's Equation, which is imported above
    ka = kappa.m_as('dimensionless')

    # calculate theta for each point
    pres_theta = potential_temperature(levs, tmpk)

    # Raise error if input theta level is larger than pres_theta max
    if np.max(pres_theta.m) < np.max(theta_levels):
        raise ValueError('Input theta level out of data bounds')

    # Find log of pressure to implement assumption of linear temperature dependence on
    # ln(p)
    log_p = np.log(levs.m)

    # Calculations for interpolation routine
    pok = P0 ** ka

    # index values for each point for the pressure level nearest to the desired theta level
    minv = np.apply_along_axis(np.searchsorted, axis, pres_theta.m, theta_levels)

    # Create index values for broadcasting arrays
    above = broadcast_indices(tmpk, minv, ndim, axis)
    below = broadcast_indices(tmpk, minv - 1, ndim, axis)

    # calculate constants for the interpolation
    a = (tmpk.m[above] - tmpk.m[below]) / (log_p[above] - log_p[below])
    b = tmpk.m[above] - a * log_p[above]

    # calculate first guess for interpolation
    first_guess = 0.5 * (log_p[above] + log_p[below])

    # iterative interpolation using scipy.optimize.fixed_point and _isen_iter defined above
    log_p_solved = so.fixed_point(_isen_iter, first_guess, args=(isentlevs_nd, ka, a, b,
                                                                 pok.m),
                                  xtol=eps, maxiter=max_iters)

    # get back pressure and assign nan for values with pressure greater than 1000 hPa
    isentprs = np.exp(log_p_solved)
    isentprs[isentprs > np.max(pressure.m)] = np.nan

    # create list for storing output data
    ret = [isentprs * units.hPa]

    # if tmpk_out = true, calculate temperature and output as last item in list
    if tmpk_out:
        ret.append((isentlevs_nd / ((P0.m / isentprs) ** ka)) * units.kelvin)

    # do an interpolation for each additional argument
    if args:
        others = interp(isentlevels, pres_theta.m, *(arr[sorter] for arr in args), axis=axis)
        if len(args) > 1:
            ret.extend(others)
        else:
            ret.append(others)

    return ret","1. Use `np.asanyarray` to convert input arguments to `numpy.ndarray`.
2. Use `np.broadcast_to` to broadcast input arguments to the same shape.
3. Use `np.searchsorted` to find the index of the nearest value in the array."
"def lcl(pressure, temperature, dewpt, max_iters=50, eps=1e-5):
    r""""""Calculate the lifted condensation level (LCL) using from the starting point.

    The starting state for the parcel is defined by `temperature`, `dewpt`,
    and `pressure`.

    Parameters
    ----------
    pressure : `pint.Quantity`
        The starting atmospheric pressure
    temperature : `pint.Quantity`
        The starting temperature
    dewpt : `pint.Quantity`
        The starting dew point

    Returns
    -------
    `(pint.Quantity, pint.Quantity)`
        The LCL pressure and temperature

    Other Parameters
    ----------------
    max_iters : int, optional
        The maximum number of iterations to use in calculation, defaults to 50.
    eps : float, optional
        The desired relative error in the calculated value, defaults to 1e-5.

    See Also
    --------
    parcel_profile

    Notes
    -----
    This function is implemented using an iterative approach to solve for the
    LCL. The basic algorithm is:

    1. Find the dew point from the LCL pressure and starting mixing ratio
    2. Find the LCL pressure from the starting temperature and dewpoint
    3. Iterate until convergence

    The function is guaranteed to finish by virtue of the `max_iters` counter.

    """"""
    def _lcl_iter(p, p0, w, t):
        td = dewpoint(vapor_pressure(units.Quantity(p, pressure.units), w))
        return (p0 * (td / t) ** (1. / kappa)).m

    w = mixing_ratio(saturation_vapor_pressure(dewpt), pressure)
    fp = so.fixed_point(_lcl_iter, pressure.m, args=(pressure.m, w, temperature),
                        xtol=eps, maxiter=max_iters)
    lcl_p = units.Quantity(fp, pressure.units)
    return lcl_p, dewpoint(vapor_pressure(lcl_p, w))","1. Use `pint.check_units` to validate input units.
2. Use `pint.Quantity` to represent all numerical values.
3. Use `pint.errors` to raise exceptions for invalid inputs."
"def supercell_composite(mucape, effective_storm_helicity, effective_shear):
    r""""""Calculate the supercell composite parameter.

    The supercell composite parameter is designed to identify
    environments favorable for the development of supercells,
    and is calculated using the formula developed by
    [Thompson2004]_:

    SCP = (mucape / 1000 J/kg) * (effective_storm_helicity / 50 m^2/s^2) *
          (effective_shear / 20 m/s)

    The effective_shear term is set to zero below 10 m/s and
    capped at 1 when effective_shear exceeds 20 m/s.

    Parameters
    ----------
    mucape : `pint.Quantity`
        Most-unstable CAPE
    effective_storm_helicity : `pint.Quantity`
        Effective-layer storm-relative helicity
    effective_shear : `pint.Quantity`
        Effective bulk shear

    Returns
    -------
    array-like
        supercell composite

    """"""
    effective_shear = np.clip(effective_shear, None, 20 * units('m/s'))
    effective_shear[effective_shear < 10 * units('m/s')] = 0 * units('m/s')
    effective_shear = effective_shear / (20 * units('m/s'))

    return ((mucape / (1000 * units('J/kg'))) *
            (effective_storm_helicity / (50 * units('m^2/s^2'))) *
            effective_shear).to('dimensionless')","1. Use `np.clip` to clip the values of `effective_shear` to be between 0 and 20 m/s.
2. Set the values of `effective_shear` below 10 m/s to 0.
3. Convert `effective_shear` to dimensionless units."
"def significant_tornado(sbcape, sblcl, storm_helicity_1km, shear_6km):
    r""""""Calculate the significant tornado parameter (fixed layer).

    The significant tornado parameter is designed to identify
    environments favorable for the production of significant
    tornadoes contingent upon the development of supercells.
    It's calculated according to the formula used on the SPC
    mesoanalysis page, updated in [Thompson2004]_:

    sigtor = (sbcape / 1500 J/kg) * ((2000 m - sblcl) / 1000 m) *
             (storm_helicity_1km / 150 m^s/s^2) * (shear_6km6 / 20 m/s)

    The sblcl term is set to zero when the lcl is above 2000m and
    capped at 1 when below 1000m, and the shr6 term is set to 0
    when shr6 is below 12.5 m/s and maxed out at 1.5 when shr6
    exceeds 30 m/s.

    Parameters
    ----------
    sbcape : `pint.Quantity`
        Surface-based CAPE
    sblcl : `pint.Quantity`
        Surface-based lifted condensation level
    storm_helicity_1km : `pint.Quantity`
        Surface-1km storm-relative helicity
    shear_6km : `pint.Quantity`
        Surface-6km bulk shear

    Returns
    -------
    array-like
        significant tornado parameter

    """"""
    sblcl = np.clip(sblcl, 1000 * units('meter'), 2000 * units('meter'))
    sblcl[sblcl > 2000 * units('meter')] = 0 * units('meter')
    sblcl = (2000. * units('meter') - sblcl) / (1000. * units('meter'))
    shear_6km = np.clip(shear_6km, None, 30 * units('m/s'))
    shear_6km[shear_6km < 12.5 * units('m/s')] = 0 * units('m/s')
    shear_6km = shear_6km / (20 * units('m/s'))

    return ((sbcape / (1500. * units('J/kg'))) *
            sblcl * (storm_helicity_1km / (150. * units('m^2/s^2'))) * shear_6km)","1. Use `np.clip()` to clip the values of `sblcl` and `shear_6km` to the specified range.
2. Use `np.where()` to check if `sblcl` is greater than 2000m and set it to 0 if it is.
3. Use `np.where()` to check if `shear_6km` is less than 12.5m/s and set it to 0 if it is."
"def storm_relative_helicity(u, v, p, hgt, top, bottom=0 * units('meter'),
                            storm_u=0 * units('m/s'), storm_v=0 * units('m/s')):
    # Partially adapted from similar SharpPy code
    r""""""Calculate storm relative helicity.

    Needs u and v wind components, heights and pressures,
    and top and bottom of SRH layer. An optional storm
    motion vector can be specified. SRH is calculated using the
    equation specified on p. 230-231 in the Markowski and Richardson
    meso textbook [Markowski2010].

    .. math:: \\int\\limits_0^d (\\bar v - c) \\cdot \\bar\\omega_{h} \\,dz

    This is applied to the data from a hodograph with the following summation:

    .. math:: \\sum_{n = 1}^{N-1} [(u_{n+1} - c_{x})(v_{n} - c_{y}) -
                                  (u_{n} - c_{x})(v_{n+1} - c_{y})]

    Parameters
    ----------
    u : array-like
        The u components of winds, same length as hgts
    v : array-like
        The u components of winds, same length as hgts
    p : array-like
        Pressure in hPa, same length as hgts
    hgt : array-like
        The heights associatd with the data, provided in meters above mean
        sea level and converted into meters AGL.
    top : number
        The height of the top of the desired layer for SRH.
    bottom : number
        The height at the bottom of the SRH layer. Default is sfc (None).
    storm_u : number
        u component of storm motion
    storm_v : number
        v component of storm motion

    Returns
    -------
    number
        p_srh : positive storm-relative helicity
    number
        n_srh : negative storm-relative helicity
    number
        t_srh : total storm-relative helicity

    """"""
    # Converting to m/s to make sure output is in m^2/s^2
    u = u.to('meters/second')
    v = v.to('meters/second')
    storm_u = storm_u.to('meters/second')
    storm_v = storm_v.to('meters/second')

    w_int = get_layer(p, u, v, heights=hgt, bottom=bottom, depth=top - bottom)

    sru = w_int[1] - storm_u
    srv = w_int[2] - storm_v

    int_layers = sru[1:] * srv[:-1] - sru[:-1] * srv[1:]

    p_srh = int_layers[int_layers.magnitude > 0.].sum()
    n_srh = int_layers[int_layers.magnitude < 0.].sum()
    t_srh = p_srh + n_srh

    return p_srh, n_srh, t_srh","1. Use `units` to validate input parameters.
2. Use `np.clip` to avoid divide by zero errors.
3. Use `np.where` to avoid `KeyError`."
"    async def poll(self):
        """"""
        Check if the pod is still running.

        Uses the same interface as subprocess.Popen.poll(): if the pod is
        still running, returns None.  If the pod has exited, return the
        exit code if we can determine it, or 1 if it has exited but we
        don't know how.  These are the return values JupyterHub expects.

        Note that a clean exit will have an exit code of zero, so it is
        necessary to check that the returned value is None, rather than
        just Falsy, to determine that the pod is still running.
        """"""
        # have to wait for first load of data before we have a valid answer
        if not self.pod_reflector.first_load_future.done():
            await self.pod_reflector.first_load_future
        data = self.pod_reflector.pods.get(self.pod_name, None)
        if data is not None:
            if data[""status""][""phase""] == 'Pending':
                return None
            ctr_stat = data[""status""].get(""containerStatuses"")
            if ctr_stat is None:  # No status, no container (we hope)
                # This seems to happen when a pod is idle-culled.
                return 1
            for c in ctr_stat:
                # return exit code if notebook container has terminated
                if c[""name""] == 'notebook':
                    if ""terminated"" in c[""state""]:
                        # call self.stop to delete the pod
                        if self.delete_stopped_pods:
                            await self.stop(now=True)
                        return c[""state""][""terminated""][""exitCode""]
                    break
            # None means pod is running or starting up
            return None
        # pod doesn't exist or has been deleted
        return 1","1. Use `asyncio.wait` instead of `await` to wait for multiple futures.
2. Use `asyncio.gather` to await multiple coroutines.
3. Use `asyncio.shield` to protect a coroutine from being cancelled."
"    def poll(self):
        """"""
        Check if the pod is still running.

        Uses the same interface as subprocess.Popen.poll(): if the pod is
        still running, returns None.  If the pod has exited, return the
        exit code if we can determine it, or 1 if it has exited but we
        don't know how.  These are the return values JupyterHub expects.

        Note that a clean exit will have an exit code of zero, so it is
        necessary to check that the returned value is None, rather than
        just Falsy, to determine that the pod is still running.
        """"""
        # have to wait for first load of data before we have a valid answer
        if not self.pod_reflector.first_load_future.done():
            yield self.pod_reflector.first_load_future
        data = self.pod_reflector.pods.get(self.pod_name, None)
        if data is not None:
            if data[""status""][""phase""] == 'Pending':
                return None
            ctr_stat = data[""status""][""containerStatuses""]
            if ctr_stat is None:  # No status, no container (we hope)
                # This seems to happen when a pod is idle-culled.
                return 1
            for c in ctr_stat:
                # return exit code if notebook container has terminated
                if c[""name""] == 'notebook':
                    if ""terminated"" in c[""state""]:
                        # call self.stop to delete the pod
                        if self.delete_stopped_pods:
                            yield self.stop(now=True)
                        return c[""state""][""terminated""][""exitCode""]
                    break
            # None means pod is running or starting up
            return None
        # pod doesn't exist or has been deleted
        return 1","1. Use more specific error handling to avoid returning 1 when the pod is still running.
2. Check if the pod is idle-culled before returning 1.
3. Delete the pod only if it has been stopped."
"    def update_info(self, data, params=None, headers=None, **kwargs):
        """"""Update information about this object.

        Send a PUT to the object's base endpoint to modify the provided
        attributes.

        :param data:
            The updated information about this object.
            Must be JSON serializable.
            Update the object attributes in data.keys(). The semantics of the
            values depends on the the type and attributes of the object being
            updated. For details on particular semantics, refer to the Box
            developer API documentation <https://developer.box.com/>.
        :type data:
            `dict`
        :param params:
            (optional) Query string parameters for the request.
        :type params:
            `dict` or None
        :param headers:
            (optional) Extra HTTP headers for the request.
        :type headers:
            `dict` or None
        :param kwargs:
            Optional arguments that ``put`` takes.
        :return:
            The updated object.
            Return a new object of the same type, without modifying the
            original object passed as self.
            Construct the new object with all the default attributes that are
            returned from the endpoint.
        :rtype:
            :class:`BaseObject`
        """"""
        url = self.get_url()
        box_response = self._session.put(url, data=json.dumps(data), params=params, headers=headers, **kwargs)
        return self.translator.translate(
            session=self._session,
            response_object=box_response.json(),
        )","1. Use `params` instead of `data` to avoid accidentally sending sensitive data in the request body.
2. Use `headers` instead of `params` to avoid accidentally sending sensitive data in the query string.
3. Use `json.dumps(data)` to properly encode the data before sending it in the request body."
"    def update_info(self, role=None, status=None):
        """"""Edit an existing collaboration on Box

        :param role:
            The new role for this collaboration or None to leave unchanged
        :type role:
            :class:`CollaborationRole`
        :param status:
            The new status for this collaboration or None to leave unchanged. A pending collaboration can be set to
            accepted or rejected if permissions allow it.
        :type status:
            :class:`CollaborationStatus`
        :returns:
            Whether or not the edit was successful.
        :rtype:
            `bool`
        :raises:
            :class:`BoxAPIException` if current user doesn't have permissions to edit the collaboration.
        """"""
        # pylint:disable=arguments-differ
        data = {}
        if role:
            data['role'] = role
        if status:
            data['status'] = status
        return super(Collaboration, self).update_info(data=data)","1. Use `assert` statements to validate the input parameters.
2. Use `role` and `status` as keyword arguments to avoid typos.
3. Use `super` to call the parent class's method to avoid duplicate code."
"    def cluster_vectorspace(self, vectors, trace=False):
        if self._means and self._repeats > 1:
            print('Warning: means will be discarded for subsequent trials')

        meanss = []
        for trial in range(self._repeats):
            if trace: print('k-means trial', trial)
            if not self._means or trial > 1:
                self._means = self._rng.sample(vectors, self._num_means)
            self._cluster_vectorspace(vectors, trace)
            meanss.append(self._means)

        if len(meanss) > 1:
            # sort the means first (so that different cluster numbering won't
            # effect the distance comparison)
            for means in meanss:
                means.sort(key=sum)

            # find the set of means that's minimally different from the others
            min_difference = min_means = None
            for i in range(len(meanss)):
                d = 0
                for j in range(len(meanss)):
                    if i != j:
                        d += self._sum_distances(meanss[i], meanss[j])
                if min_difference is None or d < min_difference:
                    min_difference, min_means = d, meanss[i]

            # use the best means
            self._means = min_means","1. Use a secure random number generator (RNG) to generate the initial means.
2. Sanitize the input vectors to prevent an attacker from injecting malicious data.
3. Use a secure comparison function to compare the means."
"    def __init__(self, tree, sentence=None, highlight=()):
        if sentence is None:
            leaves = tree.leaves()
            if (leaves and not any(len(a) == 0 for a in tree.subtrees())
                    and all(isinstance(a, int) for a in leaves)):
                sentence = [str(a) for a in leaves]
            else:
                # this deals with empty nodes (frontier non-terminals)
                # and multiple/mixed terminals under non-terminals.
                tree = tree.copy(True)
                sentence = []
                for a in tree.subtrees():
                    if len(a) == 0:
                        a.append(len(sentence))
                        sentence.append(None)
                    elif any(not isinstance(b, Tree) for b in a):
                        for n, b in enumerate(a):
                            if not isinstance(b, Tree):
                                a[n] = len(sentence)
                                sentence.append('%s' % b)
        self.nodes, self.coords, self.edges, self.highlight = self.nodecoords(
                tree, sentence, highlight)","1. Use `ast.literal_eval` instead of `eval` to parse user input.
2. Sanitize user input to prevent injection attacks.
3. Use `type()` to check if the input is of the expected type."
"    def __init__(self, tree, sentence=None, highlight=()):
        if sentence is None:
            leaves = tree.leaves()
            if (
                leaves
                and not any(len(a) == 0 for a in tree.subtrees())
                and all(isinstance(a, int) for a in leaves)
            ):
                sentence = [str(a) for a in leaves]
            else:
                # this deals with empty nodes (frontier non-terminals)
                # and multiple/mixed terminals under non-terminals.
                tree = tree.copy(True)
                sentence = []
                for a in tree.subtrees():
                    if len(a) == 0:
                        a.append(len(sentence))
                        sentence.append(None)
                    elif any(not isinstance(b, Tree) for b in a):
                        for n, b in enumerate(a):
                            if not isinstance(b, Tree):
                                a[n] = len(sentence)
                                sentence.append('%s' % b)
        self.nodes, self.coords, self.edges, self.highlight = self.nodecoords(
            tree, sentence, highlight
        )","1. Use `ast.literal_eval` to sanitize user input.
2. Use `warnings.filterwarnings` to suppress unnecessary warnings.
3. Use `sys.setrecursionlimit` to increase the recursion limit."
"    def __init__(self, zipfile, entry=''):
        """"""
        Create a new path pointer pointing at the specified entry
        in the given zipfile.

        :raise IOError: If the given zipfile does not exist, or if it
        does not contain the specified entry.
        """"""
        if isinstance(zipfile, string_types):
            zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))

        # Normalize the entry string, it should be relative:
        entry = normalize_resource_name(entry, True, '/').lstrip('/')

        # Check that the entry exists:
        if entry:
            try:
                zipfile.getinfo(entry)
            except Exception:
                # Sometimes directories aren't explicitly listed in
                # the zip file.  So if `entry` is a directory name,
                # then check if the zipfile contains any files that
                # are under the given directory.
                if (entry.endswith('/') and
                        [n for n in zipfile.namelist() if n.startswith(entry)]):
                    pass  # zipfile contains a file in that directory.
                else:
                    # Otherwise, complain.
                    raise IOError('Zipfile %r does not contain %r' %
                                  (zipfile.filename, entry))
        self._zipfile = zipfile
        self._entry = entry","1. **Use `os.path.abspath()` to normalize the path of the zipfile.** This will help to prevent directory traversal attacks.
2. **Check that the entry exists before trying to access it.** This will help to prevent errors and security vulnerabilities.
3. **Use `normalize_resource_name()` to normalize the entry string.** This will help to prevent security vulnerabilities."
"    def __init__(self, zipfile, entry=''):
        """"""
        Create a new path pointer pointing at the specified entry
        in the given zipfile.

        :raise IOError: If the given zipfile does not exist, or if it
        does not contain the specified entry.
        """"""
        if isinstance(zipfile, string_types):
            zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))

        # Normalize the entry string, it should be relative:
        entry = normalize_resource_name(entry, True, '/').lstrip('/')

        # Check that the entry exists:
        if entry != '.':
            try:
                zipfile.getinfo(entry)
            except Exception:
                # Sometimes directories aren't explicitly listed in
                # the zip file.  So if `entry` is a directory name,
                # then check if the zipfile contains any files that
                # are under the given directory.
                if entry.endswith('/') and [
                    n for n in zipfile.namelist() if n.startswith(entry)
                ]:
                    pass  # zipfile contains a file in that directory.
                else:
                    # Otherwise, complain.
                    raise IOError(
                        'Zipfile %r does not contain %r' % (zipfile.filename, entry)
                    )
        self._zipfile = zipfile
        self._entry = entry","1. Use `zipfile.open` instead of `zipfile.getinfo` to open the zip file, as it will raise an error if the file does not exist.
2. Use `os.path.abspath` to normalize the entry string, as it will prevent directory traversal attacks.
3. Check if the entry is a directory before trying to open it, as this will prevent errors if the entry does not exist."
"    def __Suffix_Verb_Step2a(self, token):
        for suffix in self.__suffix_verb_step2a:
            if token.endswith(suffix):
                if suffix == '\\u062a' and len(token) >= 4:
                    token = token[:-1]
                    self.suffix_verb_step2a_success = True
                    break

                if suffix in self.__conjugation_suffix_verb_4 and len(token) >= 4:
                    token = token[:-1]
                    self.suffix_verb_step2a_success = True
                    break

                if suffix in self.__conjugation_suffix_verb_past and len(token) >= 5:
                    token = token[:-2]  # past
                    self.suffix_verb_step2a_success = True
                    break

                if suffix in self.__conjugation_suffix_verb_present and len(token) > 5:
                    token = token[:-2]  # present
                    self.suffix_verb_step2a_success = True
                    break

                if suffix == '\\u062a\\u0645\\u0627' and len(token) >= 6:
                    token = token[:-3]
                    self.suffix_verb_step2a_success = True
                    break
        return  token","1. Use `str.endswith()` instead of `token.endswith()` to prevent `token` from being modified.
2. Use `len(token)` instead of `token >= 4` to prevent `token` from being truncated.
3. Use `token[:-2]` instead of `token[0:-2]` to prevent `token` from being sliced out of bounds."
"    def stem(self, word):
        """"""
         Stem an Arabic word and return the stemmed form.
        :param word: string
        :return: string
        """"""
        # set initial values
        self.is_verb = True
        self.is_noun = True
        self.is_defined = False

        self.suffix_verb_step2a_success = False
        self.suffix_verb_step2b_success = False
        self.suffix_noun_step2c2_success = False
        self.suffix_noun_step1a_success = False
        self.suffix_noun_step2a_success = False
        self.suffix_noun_step2b_success = False
        self.suffixe_noun_step1b_success = False
        self.prefix_step2a_success = False
        self.prefix_step3a_noun_success = False
        self.prefix_step3b_noun_success = False

        modified_word = word
        # guess type and properties
        # checks1
        self.__checks_1(modified_word)
        # checks2
        self.__checks_2(modified_word)
        modified_word = self.__normalize_pre(modified_word)
        if self.is_verb:
            modified_word = self.__Suffix_Verb_Step1(modified_word)
            if  self.suffixes_verb_step1_success:
                modified_word = self.__Suffix_Verb_Step2a(modified_word)
                if not self.suffix_verb_step2a_success :
                    modified_word = self.__Suffix_Verb_Step2c(modified_word)
                #or next
            else:
                modified_word = self.__Suffix_Verb_Step2b(modified_word)
                if not self.suffix_verb_step2b_success:
                    modified_word = self.__Suffix_Verb_Step2a(modified_word)
        if self.is_noun:
            modified_word = self.__Suffix_Noun_Step2c2(modified_word)
            if not self.suffix_noun_step2c2_success:
                if not self.is_defined:
                    modified_word = self.__Suffix_Noun_Step1a(modified_word)
                    #if self.suffix_noun_step1a_success:
                    modified_word = self.__Suffix_Noun_Step2a(modified_word)
                    if not self.suffix_noun_step2a_success:
                        modified_word = self.__Suffix_Noun_Step2b(modified_word)
                    if not self.suffix_noun_step2b_success and not self.suffix_noun_step2a_success:
                        modified_word = self.__Suffix_Noun_Step2c1(modified_word)
                    # or next ? todo : how to deal with or next
                else:
                    modified_word =  self.__Suffix_Noun_Step1b(modified_word)
                    if self.suffixe_noun_step1b_success:
                        modified_word = self.__Suffix_Noun_Step2a(modified_word)
                        if not self.suffix_noun_step2a_success:
                            modified_word = self.__Suffix_Noun_Step2b(modified_word)
                        if not self.suffix_noun_step2b_success and not self.suffix_noun_step2a_success:
                            modified_word = self.__Suffix_Noun_Step2c1(modified_word)
                    else:
                        if not self.is_defined:
                            modified_word = self.__Suffix_Noun_Step2a(modified_word)
                        modified_word = self.__Suffix_Noun_Step2b(modified_word)
            modified_word = self.__Suffix_Noun_Step3(modified_word)
        if not self.is_noun and self.is_verb:
            modified_word = self.__Suffix_All_alef_maqsura(modified_word)

        # prefixes
        modified_word = self.__Prefix_Step1(modified_word)
        modified_word = self.__Prefix_Step2a(modified_word)
        if not self.prefix_step2a_success:
            modified_word = self.__Prefix_Step2b(modified_word)
        modified_word = self.__Prefix_Step3a_Noun(modified_word)
        if not self.prefix_step3a_noun_success and self.is_noun:
            modified_word = self.__Prefix_Step3b_Noun(modified_word)
        else:
            if not self.prefix_step3b_noun_success and self.is_verb:
                modified_word = self.__Prefix_Step3_Verb(modified_word)
                modified_word = self.__Prefix_Step4_Verb(modified_word)

        # post normalization stemming
        modified_word = self.__normalize_post(modified_word)
        stemmed_word = modified_word
        return stemmed_word","1. Use `input()` instead of `raw_input()` to prevent code injection attacks.
2. Sanitize user input to prevent cross-site scripting (XSS) attacks.
3. Use `assert()` statements to validate the input before processing it."
"    def stem(self, word):
        """"""
         Stem an Arabic word and return the stemmed form.
        :param word: string
        :return: string
        """"""
        # set initial values
        self.is_verb = True
        self.is_noun = True
        self.is_defined = False

        self.suffix_verb_step2a_success = False
        self.suffix_verb_step2b_success = False
        self.suffix_noun_step2c2_success = False
        self.suffix_noun_step1a_success = False
        self.suffix_noun_step2a_success = False
        self.suffix_noun_step2b_success = False
        self.suffixe_noun_step1b_success = False
        self.prefix_step2a_success = False
        self.prefix_step3a_noun_success = False
        self.prefix_step3b_noun_success = False

        modified_word = word
        # guess type and properties
        # checks1
        self.__checks_1(modified_word)
        # checks2
        self.__checks_2(modified_word)
        modified_word = self.__normalize_pre(modified_word)
        if self.is_verb:
            modified_word = self.__Suffix_Verb_Step1(modified_word)
            if  self.suffixes_verb_step1_success:
                modified_word = self.__Suffix_Verb_Step2a(modified_word)
                if not self.suffix_verb_step2a_success :
                    modified_word = self.__Suffix_Verb_Step2c(modified_word)
                #or next
            else:
                modified_word = self.__Suffix_Verb_Step2b(modified_word)
                if not self.suffix_verb_step2b_success:
                    modified_word = self.__Suffix_Verb_Step2a(modified_word)
        if self.is_noun:
            modified_word = self.__Suffix_Noun_Step2c2(modified_word)
            if not self.suffix_noun_step2c2_success:
                if not self.is_defined:
                    modified_word = self.__Suffix_Noun_Step1a(modified_word)
                    #if self.suffix_noun_step1a_success:
                    modified_word = self.__Suffix_Noun_Step2a(modified_word)
                    if not self.suffix_noun_step2a_success:
                         modified_word = self.__Suffix_Noun_Step2b(modified_word)
                    if not self.suffix_noun_step2b_success and not self.suffix_noun_step2a_success:
                        modified_word = self.__Suffix_Noun_Step2c1(modified_word)
                    # or next ? todo : how to deal with or next
                else:
                    modified_word =  self.__Suffix_Noun_Step1b(modified_word)
                    if self.suffixe_noun_step1b_success:
                        modified_word = self.__Suffix_Noun_Step2a(modified_word)
                        if not self.suffix_noun_step2a_success:
                            modified_word = self.__Suffix_Noun_Step2b(modified_word)
                        if not self.suffix_noun_step2b_success and not self.suffix_noun_step2a_success:
                            modified_word = self.__Suffix_Noun_Step2c1(modified_word)
                    else:
                        if not self.is_defined:
                            modified_word = self.__Suffix_Noun_Step2a(modified_word)
                        modified_word = self.__Suffix_Noun_Step2b(modified_word)
            modified_word = self.__Suffix_Noun_Step3(modified_word)
        if not self.is_noun and self.is_verb:
            modified_word = self.__Suffix_All_alef_maqsura(modified_word)

        # prefixes
        modified_word = self.__Prefix_Step1(modified_word)
        modified_word = self.__Prefix_Step2a(modified_word)
        if not self.prefix_step2a_success:
            modified_word = self.__Prefix_Step2b(modified_word)
        modified_word = self.__Prefix_Step3a_Noun(modified_word)
        if not self.prefix_step3a_noun_success and self.is_noun:
            modified_word = self.__Prefix_Step3b_Noun(modified_word)
        else:
            if not self.prefix_step3b_noun_success and self.is_verb:
                modified_word = self.__Prefix_Step3_Verb(modified_word)
                modified_word = self.__Prefix_Step4_Verb(modified_word)

        # post normalization stemming
        modified_word = self.__normalize_post(modified_word)
        stemmed_word = modified_word
        return stemmed_word","1. Use `input()` instead of `raw_input()` to prevent code injection attacks.
2. Use `assert` statements to validate user input and prevent errors.
3. Sanitize user input to prevent cross-site scripting (XSS) attacks."
"    def train(self, sentences, save_loc=None, nr_iter=5):
        '''Train a model from sentences, and save it at ``save_loc``. ``nr_iter``
        controls the number of Perceptron training iterations.

        :param sentences: A list of (words, tags) tuples.
        :param save_loc: If not ``None``, saves a pickled model in this location.
        :param nr_iter: Number of training iterations.
        '''
        self._make_tagdict(sentences)
        self.model.classes = self.classes
        for iter_ in range(nr_iter):
            c = 0
            n = 0
            for sentence  in sentences:
                words = [word for word,tag in sentence]
                tags  = [tag for word,tag in sentence]
                
                prev, prev2 = self.START
                context = self.START + [self.normalize(w) for w in words] \\
                                                                    + self.END
                for i, word in enumerate(words):
                    guess = self.tagdict.get(word)
                    if not guess:
                        feats = self._get_features(i, word, context, prev, prev2)
                        guess = self.model.predict(feats)
                        self.model.update(tags[i], guess, feats)
                    prev2 = prev
                    prev = guess
                    c += guess == tags[i]
                    n += 1
            random.shuffle(sentences)
            logging.info(""Iter {0}: {1}/{2}={3}"".format(iter_, c, n, _pc(c, n)))
        self.model.average_weights()
        # Pickle as a binary file
        if save_loc is not None:
            with open(save_loc, 'wb') as fout:
                # changed protocol from -1 to 2 to make pickling Python 2 compatible
                pickle.dump((self.model.weights, self.tagdict, self.classes), fout, 2)","1. Use `pickle.dump` with protocol 4 instead of 2 to make the pickled model Python 3 compatible.
2. Use `random.shuffle` to randomize the order of sentences before each training iteration to prevent overfitting.
3. Use `logging.info` to log the training progress and accuracy."
"    def _make_tagdict(self, sentences):
        '''
        Make a tag dictionary for single-tag words.
        :param sentences: A list of list of (word, tag) tuples.
        '''
        counts = defaultdict(lambda: defaultdict(int))
        for sentence in sentences:
            for word, tag in sentence:
                counts[word][tag] += 1
                self.classes.add(tag)
        freq_thresh = 20
        ambiguity_thresh = 0.97
        for word, tag_freqs in counts.items():
            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])
            n = sum(tag_freqs.values())
            # Don't add rare words to the tag dictionary
            # Only add quite unambiguous words
            if n >= freq_thresh and (mode / n) >= ambiguity_thresh:
                self.tagdict[word] = tag","1. Use `collections.Counter` instead of `defaultdict(lambda: defaultdict(int))` to avoid creating a new dictionary for each word.
2. Use `max(tag_freqs.items(), key=lambda item: item[1])` instead of `max(tag_freqs, key=lambda item: item[1])` to avoid creating a new list of tuples.
3. Use `if n >= freq_thresh and (mode / n) >= ambiguity_thresh:` instead of `if n >= freq_thresh and mode / n >= ambiguity_thresh:` to avoid creating a new float variable."
"    def __init__(self, strings=None):
        """"""Builds a Trie object, which is built around a ``defaultdict``

        If ``strings`` is provided, it will add the ``strings``, which
        consist of a ``list`` of ``strings``, to the Trie.
        Otherwise, it'll construct an empty Trie.

        :param strings: List of strings to insert into the trie
            (Default is ``None``)
        :type strings: list(str)

        """"""
        defaultdict.__init__(self, Trie)
        if strings:
            for string in strings:
                self.insert(string)","1. Use `typing` to specify the types of arguments and return values.
2. Use `f-strings` to format strings instead of concatenation.
3. Use `black` to format the code consistently."
"    def insert(self, string):
        """"""Inserts ``string`` into the Trie

        :param string: String to insert into the trie
        :type string: str

        :Example:

        >>> from nltk.collections import Trie
        >>> trie = Trie([""ab""])
        >>> trie
        defaultdict(<class 'nltk.collections.Trie'>, {'a': defaultdict(<class 'nltk.collections.Trie'>, {'b': defaultdict(<class 'nltk.collections.Trie'>, {True: None})})})

        """"""
        if len(string):
            self[string[0]].insert(string[1:])
        else:
            # mark the string is complete
            self[Trie.LEAF] = None","1. Use `validate_input` to check if the input string is valid before inserting it into the trie.
2. Use `sanitize_input` to sanitize the input string before inserting it into the trie.
3. Use `escape_output` to escape the output string before returning it to the user."
"    def __missing__(self, key):
        if not self._default_factory and key not in self._keys:
            raise KeyError()
        return self._default_factory()","1. Use `defaultdict` instead of `dict` with a `default_factory`.
2. Use `KeyError` instead of `TypeError` to indicate that a key does not exist.
3. Use `__missing__` instead of `get` to handle missing keys."
"    def add_mwe(self, mwe):
        """"""Add a multi-word expression to the lexicon (stored as a word trie)

        We use ``util.Trie`` to represent the trie. Its form is a dict of dicts. 
        The key True marks the end of a valid MWE.

        :param mwe: The multi-word expression we're adding into the word trie
        :type mwe: tuple(str) or list(str)

        :Example:

        >>> tokenizer = MWETokenizer()
        >>> tokenizer.add_mwe(('a', 'b'))
        >>> tokenizer.add_mwe(('a', 'b', 'c'))
        >>> tokenizer.add_mwe(('a', 'x'))
        >>> expected = {'a': {'x': {True: None}, 'b': {True: None, 'c': {True: None}}}}
        >>> tokenizer._mwes.as_dict() == expected
        True

        """"""
        self._mwes.insert(mwe)","1. Use `functools.lru_cache` to cache the results of `insert` method.
2. Use `typing` to define the types of the parameters and return values of the methods.
3. Use `mypy` to check the types of the parameters and return values of the methods."
"    def has_numeric_only(self, text):
        return bool(re.match(r'(.*)[\\s]+(\\#NUMERIC_ONLY\\#)', text))","1. Use `re.compile()` to create a regular expression object that can be reused. This will improve performance.
2. Use `re.search()` to check if the text matches the regular expression. This is more secure than using `re.match()`, which only checks the beginning of the string.
3. Use `re.IGNORECASE` to make the regular expression case-insensitive. This will allow the code to match text that is capitalized or lowercased."
"    def handles_nonbreaking_prefixes(self, text):
        # Splits the text into toknes to check for nonbreaking prefixes.
        tokens = text.split()
        num_tokens = len(tokens)
        for i, token in enumerate(tokens):
            # Checks if token ends with a fullstop.
            token_ends_with_period = re.match(r'^(\\S+)\\.$', text)
            if token_ends_with_period:
                prefix = token_ends_with_period.group(0)
                # Checks for 3 conditions if
                # i.   the prefix is a token made up of chars within the IsAlpha
                # ii.  the prefix is in the list of nonbreaking prefixes and
                #      does not contain #NUMERIC_ONLY#
                # iii. the token is not the last token and that the 
                #      next token contains all lowercase. 
                if ( (prefix and self.isalpha(prefix)) or
                     (prefix in self.NONBREAKING_PREFIXES and 
                      prefix not in self.NUMERIC_ONLY_PREFIXES) or
                     (i != num_tokens-1 and self.islower(tokens[i+1])) ):
                    pass # No change to the token.
                # Checks if the prefix is in NUMERIC_ONLY_PREFIXES
                # and ensures that the next word is a digit.
                elif (prefix in self.NUMERIC_ONLY_PREFIXES and
                      re.match(r'^[0-9]+', token[i+1])):
                    pass # No change to the token.
                else: # Otherwise, adds a space after the tokens before a dot.
                    tokens[i] = prefix + ' .'
        return "" "".join(tokens) # Stitch the tokens back.","1. Use `re.sub()` instead of `re.match()` to avoid changing the original string.
2. Use `re.escape()` to escape special characters in the regular expression pattern.
3. Use `re.compile()` to precompile the regular expression pattern for better performance."
"    def escape_xml(self, text):
        for regexp, subsitution in self.MOSES_ESCAPE_XML_REGEXES:
            text = re.sub(regexp, subsitution, text)
        return text","1. Use a secure regular expression library to avoid potential vulnerabilities.
2. Sanitize user input to prevent XSS attacks.
3. Validate input to ensure that it is in the correct format."
"    def penn_tokenize(self, text, return_str=False):
        """"""
        This is a Python port of the Penn treebank tokenizer adapted by the Moses
        machine translation community. It's a little different from the 
        version in nltk.tokenize.treebank.
        """"""
        # Converts input string into unicode.
        text = text_type(text) 
        # Perform a chain of regex substituitions using MOSES_PENN_REGEXES_1
        for regexp, subsitution in self.MOSES_PENN_REGEXES_1:
            text = re.sub(regexp, subsitution, text)
        # Handles nonbreaking prefixes.
        text = handles_nonbreaking_prefixes(text)
        # Restore ellipsis, clean extra spaces, escape XML symbols.
        for regexp, subsitution in self.MOSES_PENN_REGEXES_2:
            text = re.sub(regexp, subsitution, text)        
        return text if return_str else text.split()","1. Use `re.compile` to precompile the regular expressions, instead of calling `re.sub` multiple times. This will improve performance and reduce the risk of injection attacks.
2. Use `text_type` to explicitly convert the input string to unicode, instead of relying on the default `str` type. This will prevent errors when the input string contains non-ASCII characters.
3. Use `handles_nonbreaking_prefixes` to handle nonbreaking prefixes, instead of trying to do it yourself. This will ensure that the tokenizer correctly handles text that contains nonbreaking spaces or hyphens."
"    def tokenize(self, text, agressive_dash_splits=False, return_str=False):
        """"""
        Python port of the Moses tokenizer. 
        
        >>> mtokenizer = MosesTokenizer()
        >>> text = u'Is 9.5 or 525,600 my favorite number?'
        >>> print (mtokenizer.tokenize(text, return_str=True))
        Is 9.5 or 525,600 my favorite number ?
        >>> text = u'The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things'
        >>> print (mtokenizer.tokenize(text, return_str=True))
        The https : / / github.com / jonsafari / tok-tok / blob / master / tok-tok.pl is a website with / and / or slashes and sort of weird : things
        >>> text = u'This, is a sentence with weird\\xbb symbols\\u2026 appearing everywhere\\xbf'
        >>> expected = u'This , is a sentence with weird \\xbb symbols \\u2026 appearing everywhere \\xbf'
        >>> assert mtokenizer.tokenize(text, return_str=True) == expected
        
        :param tokens: A single string, i.e. sentence text.
        :type tokens: str
        :param agressive_dash_splits: Option to trigger dash split rules .
        :type agressive_dash_splits: bool
        """"""
        # Converts input string into unicode.
        text = text_type(text) 
        
        # De-duplicate spaces and clean ASCII junk
        for regexp, subsitution in [self.DEDUPLICATE_SPACE, self.ASCII_JUNK]:
            text = re.sub(regexp, subsitution, text)
        # Strips heading and trailing spaces.
        text = text.strip()
        # Separate special characters outside of IsAlnum character set.
        regexp, subsitution = self.PAD_NOT_ISALNUM
        text = re.sub(regexp, subsitution, text)
        # Aggressively splits dashes
        if agressive_dash_splits:
            regexp, subsitution = self.AGGRESSIVE_HYPHEN_SPLIT
            text = re.sub(regexp, subsitution, text)
        # Replaces multidots with ""DOTDOTMULTI"" literal strings.
        text = self.replace_multidots(text)
        # Separate out "","" except if within numbers e.g. 5,300
        for regexp, subsitution in [self.COMMA_SEPARATE_1, self.COMMA_SEPARATE_2]:
            text = re.sub(regexp, subsitution, text)
        
        # (Language-specific) apostrophe tokenization.
        if self.lang == 'en':
            for regexp, subsitution in self.ENGLISH_SPECIFIC_APOSTROPHE:
                 text = re.sub(regexp, subsitution, text)
        elif self.lang in ['fr', 'it']:
            for regexp, subsitution in self.FR_IT_SPECIFIC_APOSTROPHE:
                text = re.sub(regexp, subsitution, text)
        else:
            regexp, subsitution = self.NON_SPECIFIC_APOSTROPHE
            text = re.sub(regexp, subsitution, text)
        
        # Handles nonbreaking prefixes.
        text = self.handles_nonbreaking_prefixes(text)
        # Cleans up extraneous spaces.
        regexp, subsitution = self.DEDUPLICATE_SPACE
        text = re.sub(regexp,subsitution, text).strip()
        # Restore multidots.
        text = self.restore_multidots(text)
        # Escape XML symbols.
        text = self.escape_xml(text)
        
        return text if return_str else text.split()","1. Use `re.compile()` to precompile regular expressions instead of calling `re.sub()` multiple times. This will improve performance.
2. Use `text_type()` to explicitly convert the input string to unicode, instead of relying on Python's implicit string coercion. This will prevent errors if the input string is not a unicode string.
3. Use `escape_xml()` to escape XML symbols in the output string. This will prevent XSS attacks."
"    def unescape_xml(self, text):
        for regexp, subsitution in self.MOSES_UNESCAPE_XML_REGEXES:
            text = re.sub(regexp, subsitution, text)
        return text","1. Use a `security library` to sanitize user input.
2. Use `regular expressions` to validate user input.
3. `Escape special characters` in user input to prevent XSS attacks."
"    def tokenize(self, tokens, return_str=False):
        """"""
        Python port of the Moses detokenizer.
        
        :param tokens: A list of strings, i.e. tokenized text.
        :type tokens: list(str)
        :return: str
        """"""
        # Convert the list of tokens into a string and pad it with spaces.
        text = u"" {} "".format("" "".join(tokens))
        # Converts input string into unicode.
        text = text_type(text)
        # Detokenize the agressive hyphen split.
        regexp, subsitution = self.AGGRESSIVE_HYPHEN_SPLIT
        text = re.sub(regexp, subsitution, text)
        # Unescape the XML symbols.
        text = self.unescape_xml(text)
        # Keep track of no. of quotation marks.
        quote_counts = {u""'"":0 , u'""':0, u""``"":0, u""`"":0, u""''"":0}
        
        # The *prepend_space* variable is used to control the ""effects"" of 
        # detokenization as the function loops through the list of tokens and
        # changes the *prepend_space* accordingly as it sequentially checks 
        # through the language specific and language independent conditions. 
        prepend_space = "" "" 
        detokenized_text = """" 
        tokens = text.split()
        # Iterate through every token and apply language specific detokenization rule(s).
        for i, token in enumerate(iter(tokens)):
            # Check if the first char is CJK.
            if is_cjk(token[0]):
                # Perform left shift if this is a second consecutive CJK word.
                if i > 0 and is_cjk(token[-1]):
                    detokenized_text += token
                # But do nothing special if this is a CJK word that doesn't follow a CJK word
                else:
                    detokenized_text += prepend_space + token
                prepend_space = "" "" 
                
            # If it's a currency symbol.
            elif token in self.IsSc:
                # Perform right shift on currency and other random punctuation items
                detokenized_text += prepend_space + token
                prepend_space = """"

            elif re.match(r'^[\\,\\.\\?\\!\\:\\;\\\\\\%\\}\\]\\)]+$', token):
                # In French, these punctuations are prefixed with a non-breakable space.
                if self.lang == 'fr' and re.match(r'^[\\?\\!\\:\\;\\\\\\%]$', token):
                    detokenized_text += "" ""
                # Perform left shift on punctuation items.
                detokenized_text += token
                prepend_space = "" "" 
               
            elif (self.lang == 'en' and i > 0 
                  and re.match(u'^[\\'][{}]'.format(self.IsAlpha), token)
                  and re.match(u'[{}]'.format(self.IsAlnum), token)):
                # For English, left-shift the contraction.
                detokenized_text += token
                prepend_space = "" ""
                
            elif (self.lang == 'cs' and i > 1
                  and re.match(r'^[0-9]+$', tokens[-2]) # If the previous previous token is a number.
                  and re.match(r'^[.,]$', tokens[-1]) # If previous token is a dot.
                  and re.match(r'^[0-9]+$', token)): # If the current token is a number.
                # In Czech, left-shift floats that are decimal numbers.
                detokenized_text += token
                prepend_space = "" ""
            
            elif (self.lang in ['fr', 'it'] and i <= len(tokens)-2
                  and re.match(u'[{}][\\']$'.format(self.IsAlpha), token)
                  and re.match(u'^[{}]$'.format(self.IsAlpha), tokens[i+1])): # If the next token is alpha.
                # For French and Italian, right-shift the contraction.
                detokenized_text += prepend_space + token
                prepend_space = """"
            
            elif (self.lang == 'cs' and i <= len(tokens)-3
                  and re.match(u'[{}][\\']$'.format(self.IsAlpha), token)
                  and re.match(u'^[-–]$', tokens[i+1])
                  and re.match(u'^li$|^mail.*', tokens[i+2], re.IGNORECASE)): # In Perl, ($words[$i+2] =~ /^li$|^mail.*/i)
                # In Czech, right-shift ""-li"" and a few Czech dashed words (e.g. e-mail)
                detokenized_text += prepend_space + token + tokens[i+1]
                next(tokens, None) # Advance over the dash
                prepend_space = """"
                
            # Combine punctuation smartly.
            elif re.match(r'''^[\\'\\""„“`]+$''', token):
                normalized_quo = token
                if re.match(r'^[„“”]+$', token):
                    normalized_quo = '""'
                quote_counts.get(normalized_quo, 0)
                
                if self.lang == 'cs' and token == u""„"":
                    quote_counts[normalized_quo] = 0
                if self.lang == 'cs' and token == u""“"":
                    quote_counts[normalized_quo] = 1
            
            
                if quote_counts[normalized_quo] % 2 == 0:
                    if (self.lang == 'en' and token == u""'"" and i > 0 
                        and re.match(r'[s]$', tokens[i-1]) ):
                        # Left shift on single quote for possessives ending
                        # in ""s"", e.g. ""The Jones' house"" 
                        detokenized_text += token
                        prepend_space = "" ""
                    else:
                        # Right shift.
                        detokenized_text += prepend_space + token
                        prepend_space = """"
                        quote_counts[normalized_quo] += 1
                else:
                    # Left shift.
                    text += token
                    prepend_space = "" ""
                    quote_counts[normalized_quo] += 1
            
            elif (self.lang == 'fi' and re.match(r':$', tokens[i-1])
                  and re.match(self.FINNISH_REGEX, token)):
                # Finnish : without intervening space if followed by case suffix
                # EU:N EU:n EU:ssa EU:sta EU:hun EU:iin ...
                detokenized_text += prepend_space + token
                prepend_space = "" ""
            
            else:
                detokenized_text += prepend_space + token
                prepend_space = "" ""
                
        # Merge multiple spaces.
        regexp, subsitution = self.ONE_SPACE
        detokenized_text = re.sub(regexp, subsitution, detokenized_text)
        # Removes heading and trailing spaces.
        detokenized_text = detokenized_text.strip()
    
        return detokenized_text if return_str else detokenized_text.split()","1. Use `text_type()` to convert the input string to unicode.
2. Use `re.sub()` to replace multiple spaces with one space.
3. Use `strip()` to remove heading and trailing spaces."
"    def handles_nonbreaking_prefixes(self, text):
        # Splits the text into tokens to check for nonbreaking prefixes.
        tokens = text.split()
        num_tokens = len(tokens)
        for i, token in enumerate(tokens):
            # Checks if token ends with a fullstop.
            token_ends_with_period = re.match(r'^(\\S+)\\.$', text)
            if token_ends_with_period:
                prefix = token_ends_with_period.group(0)
                # Checks for 3 conditions if
                # i.   the prefix is a token made up of chars within the IsAlpha
                # ii.  the prefix is in the list of nonbreaking prefixes and
                #      does not contain #NUMERIC_ONLY#
                # iii. the token is not the last token and that the
                #      next token contains all lowercase.
                if ( (prefix and self.isalpha(prefix)) or
                     (prefix in self.NONBREAKING_PREFIXES and
                      prefix not in self.NUMERIC_ONLY_PREFIXES) or
                     (i != num_tokens-1 and self.islower(tokens[i+1])) ):
                    pass # No change to the token.
                # Checks if the prefix is in NUMERIC_ONLY_PREFIXES
                # and ensures that the next word is a digit.
                elif (prefix in self.NUMERIC_ONLY_PREFIXES and
                      re.match(r'^[0-9]+', token[i+1])):
                    pass # No change to the token.
                else: # Otherwise, adds a space after the tokens before a dot.
                    tokens[i] = prefix + ' .'
        return "" "".join(tokens) # Stitch the tokens back.","1. Use `re.compile()` to create a regular expression object once, and then use that object to match text instead of creating a new regular expression object each time.
2. Use `re.sub()` to replace substrings with a new string instead of manually splitting the string and rejoining it.
3. Use `isdigit()` to check if a string contains only digits instead of manually checking for a match against a regular expression."
"    def tokenize(self, tokens, return_str=False):
        """"""
        Python port of the Moses detokenizer.

        :param tokens: A list of strings, i.e. tokenized text.
        :type tokens: list(str)
        :return: str
        """"""
        # Convert the list of tokens into a string and pad it with spaces.
        text = u"" {} "".format("" "".join(tokens))
        # Converts input string into unicode.
        text = text_type(text)
        # Detokenize the agressive hyphen split.
        regexp, substitution = self.AGGRESSIVE_HYPHEN_SPLIT
        text = re.sub(regexp, substitution, text)
        # Unescape the XML symbols.
        text = self.unescape_xml(text)
        # Keep track of no. of quotation marks.
        quote_counts = {u""'"":0 , u'""':0, u""``"":0, u""`"":0, u""''"":0}

        # The *prepend_space* variable is used to control the ""effects"" of
        # detokenization as the function loops through the list of tokens and
        # changes the *prepend_space* accordingly as it sequentially checks
        # through the language specific and language independent conditions.
        prepend_space = "" ""
        detokenized_text = """"
        tokens = text.split()
        # Iterate through every token and apply language specific detokenization rule(s).
        for i, token in enumerate(iter(tokens)):
            # Check if the first char is CJK.
            if is_cjk(token[0]):
                # Perform left shift if this is a second consecutive CJK word.
                if i > 0 and is_cjk(token[-1]):
                    detokenized_text += token
                # But do nothing special if this is a CJK word that doesn't follow a CJK word
                else:
                    detokenized_text += prepend_space + token
                prepend_space = "" ""

            # If it's a currency symbol.
            elif token in self.IsSc:
                # Perform right shift on currency and other random punctuation items
                detokenized_text += prepend_space + token
                prepend_space = """"

            elif re.match(r'^[\\,\\.\\?\\!\\:\\;\\\\\\%\\}\\]\\)]+$', token):
                # In French, these punctuations are prefixed with a non-breakable space.
                if self.lang == 'fr' and re.match(r'^[\\?\\!\\:\\;\\\\\\%]$', token):
                    detokenized_text += "" ""
                # Perform left shift on punctuation items.
                detokenized_text += token
                prepend_space = "" ""

            elif (self.lang == 'en' and i > 0
                  and re.match(u'^[\\'][{}]'.format(self.IsAlpha), token)
                  and re.match(u'[{}]'.format(self.IsAlnum), token)):
                # For English, left-shift the contraction.
                detokenized_text += token
                prepend_space = "" ""

            elif (self.lang == 'cs' and i > 1
                  and re.match(r'^[0-9]+$', tokens[-2]) # If the previous previous token is a number.
                  and re.match(r'^[.,]$', tokens[-1]) # If previous token is a dot.
                  and re.match(r'^[0-9]+$', token)): # If the current token is a number.
                # In Czech, left-shift floats that are decimal numbers.
                detokenized_text += token
                prepend_space = "" ""

            elif (self.lang in ['fr', 'it'] and i <= len(tokens)-2
                  and re.match(u'[{}][\\']$'.format(self.IsAlpha), token)
                  and re.match(u'^[{}]$'.format(self.IsAlpha), tokens[i+1])): # If the next token is alpha.
                # For French and Italian, right-shift the contraction.
                detokenized_text += prepend_space + token
                prepend_space = """"

            elif (self.lang == 'cs' and i <= len(tokens)-3
                  and re.match(u'[{}][\\']$'.format(self.IsAlpha), token)
                  and re.match(u'^[-–]$', tokens[i+1])
                  and re.match(u'^li$|^mail.*', tokens[i+2], re.IGNORECASE)): # In Perl, ($words[$i+2] =~ /^li$|^mail.*/i)
                # In Czech, right-shift ""-li"" and a few Czech dashed words (e.g. e-mail)
                detokenized_text += prepend_space + token + tokens[i+1]
                next(tokens, None) # Advance over the dash
                prepend_space = """"

            # Combine punctuation smartly.
            elif re.match(r'''^[\\'\\""„“`]+$''', token):
                normalized_quo = token
                if re.match(r'^[„“”]+$', token):
                    normalized_quo = '""'
                quote_counts.get(normalized_quo, 0)

                if self.lang == 'cs' and token == u""„"":
                    quote_counts[normalized_quo] = 0
                if self.lang == 'cs' and token == u""“"":
                    quote_counts[normalized_quo] = 1


                if quote_counts[normalized_quo] % 2 == 0:
                    if (self.lang == 'en' and token == u""'"" and i > 0
                        and re.match(r'[s]$', tokens[i-1]) ):
                        # Left shift on single quote for possessives ending
                        # in ""s"", e.g. ""The Jones' house""
                        detokenized_text += token
                        prepend_space = "" ""
                    else:
                        # Right shift.
                        detokenized_text += prepend_space + token
                        prepend_space = """"
                        quote_counts[normalized_quo] += 1
                else:
                    # Left shift.
                    text += token
                    prepend_space = "" ""
                    quote_counts[normalized_quo] += 1

            elif (self.lang == 'fi' and re.match(r':$', tokens[i-1])
                  and re.match(self.FINNISH_REGEX, token)):
                # Finnish : without intervening space if followed by case suffix
                # EU:N EU:n EU:ssa EU:sta EU:hun EU:iin ...
                detokenized_text += prepend_space + token
                prepend_space = "" ""

            else:
                detokenized_text += prepend_space + token
                prepend_space = "" ""

        # Merge multiple spaces.
        regexp, substitution = self.ONE_SPACE
        detokenized_text = re.sub(regexp, substitution, detokenized_text)
        # Removes heading and trailing spaces.
        detokenized_text = detokenized_text.strip()

        return detokenized_text if return_str else detokenized_text.split()","1. Use `text_type()` to convert input string to unicode.
2. Use `re.sub()` to replace special characters with corresponding entities.
3. Use `strip()` to remove leading and trailing spaces."
"    def _update_index(self, url=None):
        """"""A helper function that ensures that self._index is
        up-to-date.  If the index is older than self.INDEX_TIMEOUT,
        then download it again.""""""
        # Check if the index is aleady up-to-date.  If so, do nothing.
        if not (self._index is None or url is not None or
                time.time()-self._index_timestamp > self.INDEX_TIMEOUT):
            return

        # If a URL was specified, then update our URL.
        self._url = url or self._url

        # Download the index file.
        self._index = nltk.internals.ElementWrapper(
            ElementTree.parse(compat.urlopen(self._url)).getroot())
        self._index_timestamp = time.time()

        # Build a dictionary of packages.
        packages = [Package.fromxml(p) for p in
                    self._index.findall('packages/package')]
        self._packages = dict((p.id, p) for p in packages)

        # Build a dictionary of collections.
        collections = [Collection.fromxml(c) for c in
                       self._index.findall('collections/collection')]
        self._collections = dict((c.id, c) for c in collections)

        # Replace identifiers with actual children in collection.children.
        for collection in self._collections.values():
            for i, child_id in enumerate(collection.children):
                if child_id in self._packages:
                    collection.children[i] = self._packages[child_id]
                if child_id in self._collections:
                    collection.children[i] = self._collections[child_id]

        # Fill in collection.packages for each collection.
        for collection in self._collections.values():
            packages = {}
            queue = [collection]
            for child in queue:
                if isinstance(child, Collection):
                    queue.extend(child.children)
                else:
                    packages[child.id] = child
            collection.packages = packages.values()

        # Flush the status cache
        self._status_cache.clear()","1. Use `urllib.request.urlopen` instead of `compat.urlopen` to avoid
    insecure SSL connections.
2. Use `ElementTree.parse` with `parser=xml.parsers.expat.ParserCreate()` to
    avoid XML external entity attacks.
3. Use `collections.defaultdict` instead of `dict` to avoid dictionary
    denial-of-service attacks."
"    def getattr_value(self, val):
        if isinstance(val, string_types):
            val = getattr(self, val)

        if isinstance(val, tt.TensorVariable):
            return val.tag.test_value

        if isinstance(val, tt.sharedvar.TensorSharedVariable):
            return val.get_value()

        if isinstance(val, theano_constant):
            return val.value

        return val","1. Use `getattr` instead of `eval` to get the value of a variable.
2. Use `theano.tensor.as_tensor_variable` to convert a Python object to a `TensorVariable`.
3. Use `theano.tensor.constant` to create a constant value."
"def sample(
    draws=1000,
    step=None,
    init=""auto"",
    n_init=200000,
    start=None,
    trace=None,
    chain_idx=0,
    chains=None,
    cores=None,
    tune=1000,
    progressbar=True,
    model=None,
    random_seed=None,
    discard_tuned_samples=True,
    compute_convergence_checks=True,
    callback=None,
    *,
    return_inferencedata=None,
    idata_kwargs: dict = None,
    mp_ctx=None,
    pickle_backend: str = ""pickle"",
    **kwargs,
):
    """"""Draw samples from the posterior using the given step methods.

    Multiple step methods are supported via compound step methods.

    Parameters
    ----------
    draws : int
        The number of samples to draw. Defaults to 1000. The number of tuned samples are discarded
        by default. See ``discard_tuned_samples``.
    init : str
        Initialization method to use for auto-assigned NUTS samplers.

        * auto: Choose a default initialization method automatically.
          Currently, this is ``jitter+adapt_diag``, but this can change in the future.
          If you depend on the exact behaviour, choose an initialization method explicitly.
        * adapt_diag: Start with a identity mass matrix and then adapt a diagonal based on the
          variance of the tuning samples. All chains use the test value (usually the prior mean)
          as starting point.
        * jitter+adapt_diag: Same as ``adapt_diag``, but add uniform jitter in [-1, 1] to the
          starting point in each chain.
        * advi+adapt_diag: Run ADVI and then adapt the resulting diagonal mass matrix based on the
          sample variance of the tuning samples.
        * advi+adapt_diag_grad: Run ADVI and then adapt the resulting diagonal mass matrix based
          on the variance of the gradients during tuning. This is **experimental** and might be
          removed in a future release.
        * advi: Run ADVI to estimate posterior mean and diagonal mass matrix.
        * advi_map: Initialize ADVI with MAP and use MAP as starting point.
        * map: Use the MAP as starting point. This is discouraged.
        * adapt_full: Adapt a dense mass matrix using the sample covariances

    step : function or iterable of functions
        A step function or collection of functions. If there are variables without step methods,
        step methods for those variables will be assigned automatically.  By default the NUTS step
        method will be used, if appropriate to the model; this is a good default for beginning
        users.
    n_init : int
        Number of iterations of initializer. Only works for 'ADVI' init methods.
    start : dict, or array of dict
        Starting point in parameter space (or partial point)
        Defaults to ``trace.point(-1))`` if there is a trace provided and model.test_point if not
        (defaults to empty dict). Initialization methods for NUTS (see ``init`` keyword) can
        overwrite the default.
    trace : backend, list, or MultiTrace
        This should be a backend instance, a list of variables to track, or a MultiTrace object
        with past values. If a MultiTrace object is given, it must contain samples for the chain
        number ``chain``. If None or a list of variables, the NDArray backend is used.
    chain_idx : int
        Chain number used to store sample in backend. If ``chains`` is greater than one, chain
        numbers will start here.
    chains : int
        The number of chains to sample. Running independent chains is important for some
        convergence statistics and can also reveal multiple modes in the posterior. If ``None``,
        then set to either ``cores`` or 2, whichever is larger.
    cores : int
        The number of chains to run in parallel. If ``None``, set to the number of CPUs in the
        system, but at most 4.
    tune : int
        Number of iterations to tune, defaults to 1000. Samplers adjust the step sizes, scalings or
        similar during tuning. Tuning samples will be drawn in addition to the number specified in
        the ``draws`` argument, and will be discarded unless ``discard_tuned_samples`` is set to
        False.
    progressbar : bool, optional default=True
        Whether or not to display a progress bar in the command line. The bar shows the percentage
        of completion, the sampling speed in samples per second (SPS), and the estimated remaining
        time until completion (""expected time of arrival""; ETA).
    model : Model (optional if in ``with`` context)
    random_seed : int or list of ints
        A list is accepted if ``cores`` is greater than one.
    discard_tuned_samples : bool
        Whether to discard posterior samples of the tune interval.
    compute_convergence_checks : bool, default=True
        Whether to compute sampler statistics like Gelman-Rubin and ``effective_n``.
    callback : function, default=None
        A function which gets called for every sample from the trace of a chain. The function is
        called with the trace and the current draw and will contain all samples for a single trace.
        the ``draw.chain`` argument can be used to determine which of the active chains the sample
        is drawn from.

        Sampling can be interrupted by throwing a ``KeyboardInterrupt`` in the callback.
    return_inferencedata : bool, default=False
        Whether to return the trace as an :class:`arviz:arviz.InferenceData` (True) object or a `MultiTrace` (False)
        Defaults to `False`, but we'll switch to `True` in an upcoming release.
    idata_kwargs : dict, optional
        Keyword arguments for :func:`arviz:arviz.from_pymc3`
    mp_ctx : multiprocessing.context.BaseContent
        A multiprocessing context for parallel sampling. See multiprocessing
        documentation for details.
    pickle_backend : str
        One of `'pickle'` or `'dill'`. The library used to pickle models
        in parallel sampling if the multiprocessing context is not of type
        `fork`.

    Returns
    -------
    trace : pymc3.backends.base.MultiTrace or arviz.InferenceData
        A ``MultiTrace`` or ArviZ ``InferenceData`` object that contains the samples.

    Notes
    -----
    Optional keyword arguments can be passed to ``sample`` to be delivered to the
    ``step_method``s used during sampling.

    If your model uses only one step method, you can address step method kwargs
    directly. In particular, the NUTS step method has several options including:

        * target_accept : float in [0, 1]. The step size is tuned such that we
          approximate this acceptance rate. Higher values like 0.9 or 0.95 often
          work better for problematic posteriors
        * max_treedepth : The maximum depth of the trajectory tree
        * step_scale : float, default 0.25
          The initial guess for the step size scaled down by :math:`1/n**(1/4)`

    If your model uses multiple step methods, aka a Compound Step, then you have
    two ways to address arguments to each step method:

        A: If you let ``sample()`` automatically assign the ``step_method``s,
         and you can correctly anticipate what they will be, then you can wrap
         step method kwargs in a dict and pass that to sample() with a kwarg set
         to the name of the step method.
         e.g. for a CompoundStep comprising NUTS and BinaryGibbsMetropolis,
         you could send:
            1. ``target_accept`` to NUTS: nuts={'target_accept':0.9}
            2. ``transit_p`` to BinaryGibbsMetropolis: binary_gibbs_metropolis={'transit_p':.7}

         Note that available names are:
            ``nuts``, ``hmc``, ``metropolis``, ``binary_metropolis``,
            ``binary_gibbs_metropolis``, ``categorical_gibbs_metropolis``,
            ``DEMetropolis``, ``DEMetropolisZ``, ``slice``

        B: If you manually declare the ``step_method``s, within the ``step``
         kwarg, then you can address the ``step_method`` kwargs directly.
         e.g. for a CompoundStep comprising NUTS and BinaryGibbsMetropolis,
         you could send:
            step=[pm.NUTS([freeRV1, freeRV2], target_accept=0.9),
                  pm.BinaryGibbsMetropolis([freeRV3], transit_p=.7)]

    You can find a full list of arguments in the docstring of the step methods.

    Examples
    --------
    .. code:: ipython

        >>> import pymc3 as pm
        ... n = 100
        ... h = 61
        ... alpha = 2
        ... beta = 2

    .. code:: ipython

        >>> with pm.Model() as model: # context management
        ...     p = pm.Beta('p', alpha=alpha, beta=beta)
        ...     y = pm.Binomial('y', n=n, p=p, observed=h)
        ...     trace = pm.sample()
        >>> pm.summary(trace)
               mean        sd  mc_error   hpd_2.5  hpd_97.5
        p  0.604625  0.047086   0.00078  0.510498  0.694774

    """"""
    model = modelcontext(model)
    if start is None:
        start = model.test_point
    else:
        if isinstance(start, dict):
            update_start_vals(start, model.test_point, model)
        else:
            for chain_start_vals in start:
                update_start_vals(chain_start_vals, model.test_point, model)

    check_start_vals(start, model)
    if cores is None:
        cores = min(4, _cpu_count())

    if chains is None:
        chains = max(2, cores)
    if isinstance(start, dict):
        start = [start] * chains
    if random_seed == -1:
        random_seed = None
    if chains == 1 and isinstance(random_seed, int):
        random_seed = [random_seed]
    if random_seed is None or isinstance(random_seed, int):
        if random_seed is not None:
            np.random.seed(random_seed)
        random_seed = [np.random.randint(2 ** 30) for _ in range(chains)]
    if not isinstance(random_seed, Iterable):
        raise TypeError(""Invalid value for `random_seed`. Must be tuple, list or int"")

    if not discard_tuned_samples and not return_inferencedata:
        warnings.warn(
            ""Tuning samples will be included in the returned `MultiTrace` object, which can lead to""
            "" complications in your downstream analysis. Please consider to switch to `InferenceData`:\\n""
            ""`pm.sample(..., return_inferencedata=True)`"",
            UserWarning,
        )

    if return_inferencedata is None:
        v = packaging.version.parse(pm.__version__)
        if v.release[0] > 3 or v.release[1] >= 10:  # type: ignore
            warnings.warn(
                ""In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. ""
                ""You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning."",
                FutureWarning,
            )
        # set the default
        return_inferencedata = False

    if start is not None:
        for start_vals in start:
            _check_start_shape(model, start_vals)

    # small trace warning
    if draws == 0:
        msg = ""Tuning was enabled throughout the whole trace.""
        _log.warning(msg)
    elif draws < 500:
        msg = ""Only %s samples in chain."" % draws
        _log.warning(msg)

    draws += tune

    if model.ndim == 0:
        raise ValueError(""The model does not contain any free variables."")

    if step is None and init is not None and all_continuous(model.vars):
        try:
            # By default, try to use NUTS
            _log.info(""Auto-assigning NUTS sampler..."")
            start_, step = init_nuts(
                init=init,
                chains=chains,
                n_init=n_init,
                model=model,
                random_seed=random_seed,
                progressbar=progressbar,
                **kwargs,
            )
            check_start_vals(start_, model)
            if start is None:
                start = start_
        except (AttributeError, NotImplementedError, tg.NullTypeGradError):
            # gradient computation failed
            _log.info(""Initializing NUTS failed. "" ""Falling back to elementwise auto-assignment."")
            _log.debug(""Exception in init nuts"", exec_info=True)
            step = assign_step_methods(model, step, step_kwargs=kwargs)
    else:
        step = assign_step_methods(model, step, step_kwargs=kwargs)

    if isinstance(step, list):
        step = CompoundStep(step)
    if start is None:
        start = {}
    if isinstance(start, dict):
        start = [start] * chains

    sample_args = {
        ""draws"": draws,
        ""step"": step,
        ""start"": start,
        ""trace"": trace,
        ""chain"": chain_idx,
        ""chains"": chains,
        ""tune"": tune,
        ""progressbar"": progressbar,
        ""model"": model,
        ""random_seed"": random_seed,
        ""cores"": cores,
        ""callback"": callback,
        ""discard_tuned_samples"": discard_tuned_samples,
    }
    parallel_args = {
        ""pickle_backend"": pickle_backend,
        ""mp_ctx"": mp_ctx,
    }

    sample_args.update(kwargs)

    has_population_samplers = np.any(
        [
            isinstance(m, arraystep.PopulationArrayStepShared)
            for m in (step.methods if isinstance(step, CompoundStep) else [step])
        ]
    )

    parallel = cores > 1 and chains > 1 and not has_population_samplers
    t_start = time.time()
    if parallel:
        _log.info(f""Multiprocess sampling ({chains} chains in {cores} jobs)"")
        _print_step_hierarchy(step)
        try:
            trace = _mp_sample(**sample_args, **parallel_args)
        except pickle.PickleError:
            _log.warning(""Could not pickle model, sampling singlethreaded."")
            _log.debug(""Pickling error:"", exec_info=True)
            parallel = False
        except AttributeError as e:
            if str(e).startswith(""AttributeError: Can't pickle""):
                _log.warning(""Could not pickle model, sampling singlethreaded."")
                _log.debug(""Pickling error:"", exec_info=True)
                parallel = False
            else:
                raise
    if not parallel:
        if has_population_samplers:
            has_demcmc = np.any(
                [
                    isinstance(m, DEMetropolis)
                    for m in (step.methods if isinstance(step, CompoundStep) else [step])
                ]
            )
            _log.info(f""Population sampling ({chains} chains)"")
            if has_demcmc and chains < 3:
                raise ValueError(
                    ""DEMetropolis requires at least 3 chains. ""
                    ""For this {}-dimensional model you should use ≥{} chains"".format(
                        model.ndim, model.ndim + 1
                    )
                )
            if has_demcmc and chains <= model.ndim:
                warnings.warn(
                    ""DEMetropolis should be used with more chains than dimensions! ""
                    ""(The model has {} dimensions.)"".format(model.ndim),
                    UserWarning,
                )
            _print_step_hierarchy(step)
            trace = _sample_population(parallelize=cores > 1, **sample_args)
        else:
            _log.info(f""Sequential sampling ({chains} chains in 1 job)"")
            _print_step_hierarchy(step)
            trace = _sample_many(**sample_args)

    t_sampling = time.time() - t_start
    # count the number of tune/draw iterations that happened
    # ideally via the ""tune"" statistic, but not all samplers record it!
    if ""tune"" in trace.stat_names:
        stat = trace.get_sampler_stats(""tune"", chains=0)
        # when CompoundStep is used, the stat is 2 dimensional!
        if len(stat.shape) == 2:
            stat = stat[:, 0]
        stat = tuple(stat)
        n_tune = stat.count(True)
        n_draws = stat.count(False)
    else:
        # these may be wrong when KeyboardInterrupt happened, but they're better than nothing
        n_tune = min(tune, len(trace))
        n_draws = max(0, len(trace) - n_tune)

    if discard_tuned_samples:
        trace = trace[n_tune:]

    # save metadata in SamplerReport
    trace.report._n_tune = n_tune
    trace.report._n_draws = n_draws
    trace.report._t_sampling = t_sampling

    if ""variable_inclusion"" in trace.stat_names:
        variable_inclusion = np.stack(trace.get_sampler_stats(""variable_inclusion"")).mean(0)
        trace.report.variable_importance = variable_inclusion / variable_inclusion.sum()

    n_chains = len(trace.chains)
    _log.info(
        f'Sampling {n_chains} chain{""s"" if n_chains > 1 else """"} for {n_tune:_d} tune and {n_draws:_d} draw iterations '
        f""({n_tune*n_chains:_d} + {n_draws*n_chains:_d} draws total) ""
        f""took {trace.report.t_sampling:.0f} seconds.""
    )

    idata = None
    if compute_convergence_checks or return_inferencedata:
        ikwargs = dict(model=model, save_warmup=not discard_tuned_samples)
        if idata_kwargs:
            ikwargs.update(idata_kwargs)
        idata = arviz.from_pymc3(trace, **ikwargs)

    if compute_convergence_checks:
        if draws - tune < 100:
            warnings.warn(""The number of samples is too small to check convergence reliably."")
        else:
            trace.report._run_convergence_checks(idata, model)
    trace.report._log_summary()

    if return_inferencedata:
        return idata
    else:
        return trace","1. Use `pickle_backend='dill'` to avoid pickling errors.
2. Use `parallelize=True` to speed up sampling.
3. Use `compute_convergence_checks=True` to check convergence of the samples."
"def init_nuts(
    init=""auto"",
    chains=1,
    n_init=500000,
    model=None,
    random_seed=None,
    progressbar=True,
    **kwargs,
):
    """"""Set up the mass matrix initialization for NUTS.

    NUTS convergence and sampling speed is extremely dependent on the
    choice of mass/scaling matrix. This function implements different
    methods for choosing or adapting the mass matrix.

    Parameters
    ----------
    init : str
        Initialization method to use.

        * auto: Choose a default initialization method automatically.
          Currently, this is `'jitter+adapt_diag'`, but this can change in the future. If you
          depend on the exact behaviour, choose an initialization method explicitly.
        * adapt_diag: Start with a identity mass matrix and then adapt a diagonal based on the
          variance of the tuning samples. All chains use the test value (usually the prior mean)
          as starting point.
        * jitter+adapt_diag: Same as ``adapt_diag``, but use test value plus a uniform jitter in
          [-1, 1] as starting point in each chain.
        * advi+adapt_diag: Run ADVI and then adapt the resulting diagonal mass matrix based on the
          sample variance of the tuning samples.
        * advi+adapt_diag_grad: Run ADVI and then adapt the resulting diagonal mass matrix based
          on the variance of the gradients during tuning. This is **experimental** and might be
          removed in a future release.
        * advi: Run ADVI to estimate posterior mean and diagonal mass matrix.
        * advi_map: Initialize ADVI with MAP and use MAP as starting point.
        * map: Use the MAP as starting point. This is discouraged.
        * adapt_full: Adapt a dense mass matrix using the sample covariances. All chains use the
          test value (usually the prior mean) as starting point.
        * jitter+adapt_full: Same as ``adapt_full`, but use test value plus a uniform jitter in
          [-1, 1] as starting point in each chain.

    chains : int
        Number of jobs to start.
    n_init : int
        Number of iterations of initializer. Only works for 'ADVI' init methods.
    model : Model (optional if in ``with`` context)
    progressbar : bool
        Whether or not to display a progressbar for advi sampling.
    **kwargs : keyword arguments
        Extra keyword arguments are forwarded to pymc3.NUTS.

    Returns
    -------
    start : ``pymc3.model.Point``
        Starting point for sampler
    nuts_sampler : ``pymc3.step_methods.NUTS``
        Instantiated and initialized NUTS sampler object
    """"""
    model = modelcontext(model)

    vars = kwargs.get(""vars"", model.vars)
    if set(vars) != set(model.vars):
        raise ValueError(""Must use init_nuts on all variables of a model."")
    if not all_continuous(vars):
        raise ValueError(""init_nuts can only be used for models with only "" ""continuous variables."")

    if not isinstance(init, str):
        raise TypeError(""init must be a string."")

    if init is not None:
        init = init.lower()

    if init == ""auto"":
        init = ""jitter+adapt_diag""

    _log.info(f""Initializing NUTS using {init}..."")

    if random_seed is not None:
        random_seed = int(np.atleast_1d(random_seed)[0])
        np.random.seed(random_seed)

    cb = [
        pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff=""absolute""),
        pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff=""relative""),
    ]

    if init == ""adapt_diag"":
        start = [model.test_point] * chains
        mean = np.mean([model.dict_to_array(vals) for vals in start], axis=0)
        var = np.ones_like(mean)
        potential = quadpotential.QuadPotentialDiagAdapt(model.ndim, mean, var, 10)
    elif init == ""jitter+adapt_diag"":
        start = []
        for _ in range(chains):
            mean = {var: val.copy() for var, val in model.test_point.items()}
            for val in mean.values():
                val[...] += 2 * np.random.rand(*val.shape) - 1
            start.append(mean)
        mean = np.mean([model.dict_to_array(vals) for vals in start], axis=0)
        var = np.ones_like(mean)
        potential = quadpotential.QuadPotentialDiagAdapt(model.ndim, mean, var, 10)
    elif init == ""advi+adapt_diag_grad"":
        approx = pm.fit(
            random_seed=random_seed,
            n=n_init,
            method=""advi"",
            model=model,
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window,
        )  # type: pm.MeanField
        start = approx.sample(draws=chains)
        start = list(start)
        stds = approx.bij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        mean = approx.bij.rmap(approx.mean.get_value())
        mean = model.dict_to_array(mean)
        weight = 50
        potential = quadpotential.QuadPotentialDiagAdaptGrad(model.ndim, mean, cov, weight)
    elif init == ""advi+adapt_diag"":
        approx = pm.fit(
            random_seed=random_seed,
            n=n_init,
            method=""advi"",
            model=model,
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window,
        )  # type: pm.MeanField
        start = approx.sample(draws=chains)
        start = list(start)
        stds = approx.bij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        mean = approx.bij.rmap(approx.mean.get_value())
        mean = model.dict_to_array(mean)
        weight = 50
        potential = quadpotential.QuadPotentialDiagAdapt(model.ndim, mean, cov, weight)
    elif init == ""advi"":
        approx = pm.fit(
            random_seed=random_seed,
            n=n_init,
            method=""advi"",
            model=model,
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window,
        )  # type: pm.MeanField
        start = approx.sample(draws=chains)
        start = list(start)
        stds = approx.bij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        potential = quadpotential.QuadPotentialDiag(cov)
    elif init == ""advi_map"":
        start = pm.find_MAP(include_transformed=True)
        approx = pm.MeanField(model=model, start=start)
        pm.fit(
            random_seed=random_seed,
            n=n_init,
            method=pm.KLqp(approx),
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window,
        )
        start = approx.sample(draws=chains)
        start = list(start)
        stds = approx.bij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        potential = quadpotential.QuadPotentialDiag(cov)
    elif init == ""map"":
        start = pm.find_MAP(include_transformed=True)
        cov = pm.find_hessian(point=start)
        start = [start] * chains
        potential = quadpotential.QuadPotentialFull(cov)
    elif init == ""adapt_full"":
        start = [model.test_point] * chains
        mean = np.mean([model.dict_to_array(vals) for vals in start], axis=0)
        cov = np.eye(model.ndim)
        potential = quadpotential.QuadPotentialFullAdapt(model.ndim, mean, cov, 10)
    elif init == ""jitter+adapt_full"":
        start = []
        for _ in range(chains):
            mean = {var: val.copy() for var, val in model.test_point.items()}
            for val in mean.values():
                val[...] += 2 * np.random.rand(*val.shape) - 1
            start.append(mean)
        mean = np.mean([model.dict_to_array(vals) for vals in start], axis=0)
        cov = np.eye(model.ndim)
        potential = quadpotential.QuadPotentialFullAdapt(model.ndim, mean, cov, 10)
    else:
        raise ValueError(f""Unknown initializer: {init}."")

    step = pm.NUTS(potential=potential, model=model, **kwargs)

    return start, step","1. Use `np.random.seed()` to set the random seed.
2. Use `pm.callbacks.CheckParametersConvergence()` to check for convergence.
3. Use `pm.callbacks.CheckParametersConvergence()` to check for divergence."
"    def __str__(self, **kwargs):
        return self._str_repr(formatting=""plain"", **kwargs)","1. Use `f-strings` instead of `str.format()` to avoid potential injection attacks.
2. Sanitize user input before using it in `str.format()`.
3. Use a secure default value for `formatting` instead of `""plain""`."
"    def _distr_parameters_for_repr(self):
        return [""a""]","1. **Use `__repr__` instead of `_distr_parameters_for_repr` to return the parameters for representation.** This will make the code more secure by preventing users from accessing sensitive information.
2. **Use `@property` to make the `a` attribute read-only.** This will prevent users from modifying the value of `a`, which could lead to security vulnerabilities.
3. **Use `assert` statements to validate the input to the `__init__` method.** This will help to ensure that the code is only used with valid inputs, which can help to prevent security vulnerabilities."
"    def __init__(self, w, comp_dists, *args, **kwargs):
        # comp_dists type checking
        if not (
            isinstance(comp_dists, Distribution)
            or (
                isinstance(comp_dists, Iterable)
                and all((isinstance(c, Distribution) for c in comp_dists))
            )
        ):
            raise TypeError(
                ""Supplied Mixture comp_dists must be a ""
                ""Distribution or an iterable of ""
                ""Distributions. Got {} instead."".format(
                    type(comp_dists)
                    if not isinstance(comp_dists, Iterable)
                    else [type(c) for c in comp_dists]
                )
            )
        shape = kwargs.pop('shape', ())

        self.w = w = tt.as_tensor_variable(w)
        self.comp_dists = comp_dists

        defaults = kwargs.pop('defaults', [])

        if all_discrete(comp_dists):
            default_dtype = _conversion_map[theano.config.floatX]
        else:
            default_dtype = theano.config.floatX

            try:
                self.mean = (w * self._comp_means()).sum(axis=-1)

                if 'mean' not in defaults:
                    defaults.append('mean')
            except AttributeError:
                pass
        dtype = kwargs.pop('dtype', default_dtype)

        try:
            comp_modes = self._comp_modes()
            comp_mode_logps = self.logp(comp_modes)
            self.mode = comp_modes[tt.argmax(w * comp_mode_logps, axis=-1)]

            if 'mode' not in defaults:
                defaults.append('mode')
        except (AttributeError, ValueError, IndexError):
            pass

        super().__init__(shape, dtype, defaults=defaults, *args, **kwargs)","1. Use `theano.config.floatX` instead of `float` to avoid precision loss.
2. Use `tt.as_tensor_variable` to cast the input to theano tensor.
3. Use `tt.argmax` to find the index of the maximum value."
"    def logp(self, value):
        """"""
        Calculate log-probability of defined Mixture distribution at specified value.

        Parameters
        ----------
        value: numeric
            Value(s) for which log-probability is calculated. If the log probabilities for multiple
            values are desired the values must be provided in a numpy array or theano tensor

        Returns
        -------
        TensorVariable
        """"""
        w = self.w

        return bound(logsumexp(tt.log(w) + self._comp_logp(value), axis=-1),
                     w >= 0, w <= 1, tt.allclose(w.sum(axis=-1), 1),
                     broadcast_conditions=False)","1. Use `tt.cast` to explicitly cast the data type of `value` to `float32`.
2. Use `tt.sum` instead of `tt.sum(axis=-1)` to calculate the sum of all elements in `w`.
3. Use `tt.allclose` to check if `w.sum(axis=-1)` is equal to 1."
"def logsumexp(x, axis=None):
    # Adapted from https://github.com/Theano/Theano/issues/1563
    x_max = tt.max(x, axis=axis, keepdims=True)
    return tt.log(tt.sum(tt.exp(x - x_max), axis=axis, keepdims=True)) + x_max","1. Use `tt.ndim` to check if `axis` is a valid axis.
2. Use `tt.is_scalar` to check if `x` is a scalar.
3. Use `tt.greater` to check if `x_max` is greater than all elements of `x`."
"def pandas_to_array(data):
    if hasattr(data, 'values'):  # pandas
        if data.isnull().any().any():  # missing values
            ret = np.ma.MaskedArray(data.values, data.isnull().values)
        else:
            ret = data.values
    elif hasattr(data, 'mask'):
        ret = data
    elif isinstance(data, theano.gof.graph.Variable):
        ret = data
    elif sps.issparse(data):
        ret = data
    elif isgenerator(data):
        ret = generator(data)
    else:
        ret = np.asarray(data)
    return pm.floatX(ret)","1. Use `np.asanyarray` instead of `np.asarray` to avoid casting errors.
2. Use `np.ma.masked_array` to handle missing values.
3. Use `np.floatX` to ensure that the data is of the correct type."
"    def random(self, point=None, size=None):
        """"""
        Draw random values from TruncatedNormal distribution.

        Parameters
        ----------
        point : dict, optional
            Dict of variable values on which random values are to be
            conditioned (uses default point if not specified).
        size : int, optional
            Desired size of random sample (returns one sample if not
            specified).

        Returns
        -------
        array
        """"""
        mu_v, std_v, a_v, b_v = draw_values(
            [self.mu, self.sigma, self.lower, self.upper], point=point, size=size)
        return generate_samples(stats.truncnorm.rvs,
                                a=(a_v - mu_v)/std_v,
                                b=(b_v - mu_v) / std_v,
                                loc=mu_v,
                                scale=std_v,
                                dist_shape=self.shape,
                                size=size,
                                )","1. Use `np.random.default_rng()` to generate random numbers instead of `np.random`. This will ensure that the random numbers are generated in a secure way.
2. Use `np.clip()` to clip the random numbers to the specified range. This will prevent the random numbers from going out of bounds.
3. Use `np.asarray()` to convert the random numbers to an array. This will make it easier to work with them in downstream code."
"    def random(self, point=None, size=None):
        """"""
        Draw random values from Triangular distribution.

        Parameters
        ----------
        point : dict, optional
            Dict of variable values on which random values are to be
            conditioned (uses default point if not specified).
        size : int, optional
            Desired size of random sample (returns one sample if not
            specified).

        Returns
        -------
        array
        """"""
        c, lower, upper = draw_values([self.c, self.lower, self.upper],
                                      point=point, size=size)
        scale = upper - lower
        c_ = (c - lower) / scale
        return generate_samples(stats.triang.rvs, c=c_, loc=lower, scale=scale,
                                size=size, dist_shape=self.shape)","1. Use `np.random.default_rng()` to generate random numbers instead of `np.random`. This will ensure that the random numbers are generated in a secure way.
2. Use `np.clip()` to clip the random numbers to the specified range. This will prevent the random numbers from going out of bounds.
3. Use `np.asarray()` to convert the random numbers to an array. This will make it easier to work with the random numbers."
"    def random(self, point=None, size=None):
        """"""
        Draw random values from Rice distribution.

        Parameters
        ----------
        point : dict, optional
            Dict of variable values on which random values are to be
            conditioned (uses default point if not specified).
        size : int, optional
            Desired size of random sample (returns one sample if not
            specified).

        Returns
        -------
        array
        """"""
        nu, sigma = draw_values([self.nu, self.sigma],
                             point=point, size=size)
        return generate_samples(stats.rice.rvs, b=nu / sigma, scale=sigma, loc=0,
                                dist_shape=self.shape, size=size)","1. Use `np.random.default_rng()` to generate random numbers instead of `np.random`.
2. Use `np.array_equal()` to compare arrays instead of `np.array()`.
3. Use `np.allclose()` to compare floating-point numbers instead of `np.equal()`."
"    def random(self, point=None, size=None):
        """"""
        Draw random values from ZeroInflatedNegativeBinomial distribution.

        Parameters
        ----------
        point : dict, optional
            Dict of variable values on which random values are to be
            conditioned (uses default point if not specified).
        size : int, optional
            Desired size of random sample (returns one sample if not
            specified).

        Returns
        -------
        array
        """"""
        mu, alpha, psi = draw_values(
            [self.mu, self.alpha, self.psi], point=point, size=size)
        g = generate_samples(stats.gamma.rvs, alpha, scale=mu / alpha,
                             dist_shape=self.shape,
                             size=size)
        g[g == 0] = np.finfo(float).eps  # Just in case
        g, psi = broadcast_distribution_samples([g, psi], size=size)
        return stats.poisson.rvs(g) * (np.random.random(g.shape) < psi)","1. Use `np.random.seed()` to set the random seed.
2. Use `np.finfo(float).eps` to avoid division by zero.
3. Use `broadcast_distribution_samples()` to broadcast distribution samples."
"    def _repr_cov_params(self, dist=None):
        if dist is None:
            dist = self
        if self._cov_type == 'chol':
            chol = get_variable_name(self.chol)
            return r'\\mathit{{chol}}={}'.format(chol)
        elif self._cov_type == 'cov':
            cov = get_variable_name(self.cov)
            return r'\\mathit{{cov}}={}'.format(cov)
        elif self._cov_type == 'tau':
            tau = get_variable_name(self.tau)
            return r'\\mathit{{tau}}={}'.format(tau)","1. Use `get_variable_name` to prevent information disclosure.
2. Use `repr` to format the output.
3. Use `self` to avoid confusion."
"    def __init__(self, mu=0, sigma=None, tau=None, lower=None, upper=None,
                 transform='auto', sd=None, *args, **kwargs):
        if sd is not None:
            sigma = sd
        tau, sigma = get_tau_sigma(tau=tau, sigma=sigma)
        self.sigma = self.sd = tt.as_tensor_variable(sigma)
        self.tau = tt.as_tensor_variable(tau)
        self.lower = tt.as_tensor_variable(floatX(lower)) if lower is not None else lower
        self.upper = tt.as_tensor_variable(floatX(upper)) if upper is not None else upper
        self.mu = tt.as_tensor_variable(floatX(mu))

        if self.lower is None and self.upper is None:
            self._defaultval = mu
        elif self.lower is None and self.upper is not None:
            self._defaultval = self.upper - 1.
        elif self.lower is not None and self.upper is None:
            self._defaultval = self.lower + 1.
        else:
            self._defaultval = (self.lower + self.upper) / 2

        assert_negative_support(sigma, 'sigma', 'TruncatedNormal')
        assert_negative_support(tau, 'tau', 'TruncatedNormal')

        super().__init__(defaults=('_defaultval',), transform=transform,
                         lower=lower, upper=upper, *args, **kwargs)","1. Use `tt.as_tensor_variable` to cast all inputs to `tt.Tensor`.
2. Use `assert_negative_support` to validate that `sigma` and `tau` are negative.
3. Use `super().__init__()` to call the parent class constructor."
"    def logp(self, value):
        """"""
        Calculate log-probability of TruncatedNormal distribution at specified value.

        Parameters
        ----------
        value : numeric
            Value(s) for which log-probability is calculated. If the log probabilities for multiple
            values are desired the values must be provided in a numpy array or theano tensor

        Returns
        -------
        TensorVariable
        """"""
        mu = self.mu
        sigma = self.sigma

        norm = self._normalization()
        logp = Normal.dist(mu=mu, sigma=sigma).logp(value) - norm

        bounds = [sigma > 0]
        if self.lower is not None:
            bounds.append(value >= self.lower)
        if self.upper is not None:
            bounds.append(value <= self.upper)
        return bound(logp, *bounds)","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `inspect.signature` to get the signature of the original function.
3. Use `functools.partial` to create a new function with the same signature as the original function."
"    def _normalization(self):
        mu, sigma = self.mu, self.sigma

        if self.lower is None and self.upper is None:
            return 0.

        if self.lower is not None and self.upper is not None:
            lcdf_a = normal_lcdf(mu, sigma, self.lower)
            lcdf_b = normal_lcdf(mu, sigma, self.upper)
            lsf_a = normal_lccdf(mu, sigma, self.lower)
            lsf_b = normal_lccdf(mu, sigma, self.upper)

            return tt.switch(
                self.lower > 0,
                logdiffexp(lsf_a, lsf_b),
                logdiffexp(lcdf_b, lcdf_a),
            )

        if self.lower is not None:
            return normal_lccdf(mu, sigma, self.lower)
        else:
            return normal_lcdf(mu, sigma, self.upper)","1. Use `torch.clamp` to ensure that `lower` and `upper` are within the valid range.
2. Use `torch.where` to check if `lower` is greater than 0, and return the appropriate value.
3. Use `torch.logaddexp` to compute the log-sum-exp of the two normal CDFs."
"    def __new__(cls, *args, **kwargs):
        # resolves the parent instance
        instance = super().__new__(cls)
        if cls.get_contexts():
            potential_parent = cls.get_contexts()[-1]
            # We have to make sure that the context is a _DrawValuesContext
            # and not a Model
            if isinstance(potential_parent, cls):
                instance._parent = potential_parent
            else:
                instance._parent = None
        else:
            instance._parent = None
        return instance","1. Use `isinstance()` to check if the potential parent is a `_DrawValuesContext` object.
2. Use `None` to initialize the `_parent` attribute if there is no parent.
3. Use `super().__new__(cls)` to call the parent class's constructor."
"    def __init__(self):
        if self.parent is not None:
            # All _DrawValuesContext instances that are in the context of
            # another _DrawValuesContext will share the reference to the
            # drawn_vars dictionary. This means that separate branches
            # in the nested _DrawValuesContext context tree will see the
            # same drawn values
            self.drawn_vars = self.parent.drawn_vars
        else:
            self.drawn_vars = dict()","1. Use `copy.deepcopy()` to create a new dictionary for each `_DrawValuesContext` instance.
2. Restrict access to the `drawn_vars` dictionary by making it private.
3. Use `f-strings` to format strings instead of concatenation."
"def draw_values(params, point=None, size=None):
    """"""
    Draw (fix) parameter values. Handles a number of cases:

        1) The parameter is a scalar
        2) The parameter is an *RV

            a) parameter can be fixed to the value in the point
            b) parameter can be fixed by sampling from the *RV
            c) parameter can be fixed using tag.test_value (last resort)

        3) The parameter is a tensor variable/constant. Can be evaluated using
        theano.function, but a variable may contain nodes which

            a) are named parameters in the point
            b) are *RVs with a random method

    """"""
    # Get fast drawable values (i.e. things in point or numbers, arrays,
    # constants or shares, or things that were already drawn in related
    # contexts)
    if point is None:
        point = {}
    with _DrawValuesContext() as context:
        params = dict(enumerate(params))
        drawn = context.drawn_vars
        evaluated = {}
        symbolic_params = []
        for i, p in params.items():
            # If the param is fast drawable, then draw the value immediately
            if is_fast_drawable(p):
                v = _draw_value(p, point=point, size=size)
                evaluated[i] = v
                continue

            name = getattr(p, 'name', None)
            if p in drawn:
                # param was drawn in related contexts
                v = drawn[p]
                evaluated[i] = v
            elif name is not None and name in point:
                # param.name is in point
                v = point[name]
                evaluated[i] = drawn[p] = v
            else:
                # param still needs to be drawn
                symbolic_params.append((i, p))

        if not symbolic_params:
            # We only need to enforce the correct order if there are symbolic
            # params that could be drawn in variable order
            return [evaluated[i] for i in params]

        # Distribution parameters may be nodes which have named node-inputs
        # specified in the point. Need to find the node-inputs, their
        # parents and children to replace them.
        leaf_nodes = {}
        named_nodes_parents = {}
        named_nodes_children = {}
        for _, param in symbolic_params:
            if hasattr(param, 'name'):
                # Get the named nodes under the `param` node
                nn, nnp, nnc = get_named_nodes_and_relations(param)
                leaf_nodes.update(nn)
                # Update the discovered parental relationships
                for k in nnp.keys():
                    if k not in named_nodes_parents.keys():
                        named_nodes_parents[k] = nnp[k]
                    else:
                        named_nodes_parents[k].update(nnp[k])
                # Update the discovered child relationships
                for k in nnc.keys():
                    if k not in named_nodes_children.keys():
                        named_nodes_children[k] = nnc[k]
                    else:
                        named_nodes_children[k].update(nnc[k])

        # Init givens and the stack of nodes to try to `_draw_value` from
        givens = {p.name: (p, v) for p, v in drawn.items()
                  if getattr(p, 'name', None) is not None}
        stack = list(leaf_nodes.values())  # A queue would be more appropriate
        while stack:
            next_ = stack.pop(0)
            if next_ in drawn:
                # If the node already has a givens value, skip it
                continue
            elif isinstance(next_, (tt.TensorConstant,
                                    tt.sharedvar.SharedVariable)):
                # If the node is a theano.tensor.TensorConstant or a
                # theano.tensor.sharedvar.SharedVariable, its value will be
                # available automatically in _compile_theano_function so
                # we can skip it. Furthermore, if this node was treated as a
                # TensorVariable that should be compiled by theano in
                # _compile_theano_function, it would raise a `TypeError:
                # ('Constants not allowed in param list', ...)` for
                # TensorConstant, and a `TypeError: Cannot use a shared
                # variable (...) as explicit input` for SharedVariable.
                continue
            else:
                # If the node does not have a givens value, try to draw it.
                # The named node's children givens values must also be taken
                # into account.
                children = named_nodes_children[next_]
                temp_givens = [givens[k] for k in givens if k in children]
                try:
                    # This may fail for autotransformed RVs, which don't
                    # have the random method
                    value = _draw_value(next_,
                                        point=point,
                                        givens=temp_givens,
                                        size=size)
                    givens[next_.name] = (next_, value)
                    drawn[next_] = value
                except theano.gof.fg.MissingInputError:
                    # The node failed, so we must add the node's parents to
                    # the stack of nodes to try to draw from. We exclude the
                    # nodes in the `params` list.
                    stack.extend([node for node in named_nodes_parents[next_]
                                  if node is not None and
                                  node.name not in drawn and
                                  node not in params])

        # the below makes sure the graph is evaluated in order
        # test_distributions_random::TestDrawValues::test_draw_order fails without it
        # The remaining params that must be drawn are all hashable
        to_eval = set()
        missing_inputs = set([j for j, p in symbolic_params])
        while to_eval or missing_inputs:
            if to_eval == missing_inputs:
                raise ValueError('Cannot resolve inputs for {}'.format([str(params[j]) for j in to_eval]))
            to_eval = set(missing_inputs)
            missing_inputs = set()
            for param_idx in to_eval:
                param = params[param_idx]
                if param in drawn:
                    evaluated[param_idx] = drawn[param]
                else:
                    try:  # might evaluate in a bad order,
                        value = _draw_value(param,
                                            point=point,
                                            givens=givens.values(),
                                            size=size)
                        evaluated[param_idx] = drawn[param] = value
                        givens[param.name] = (param, value)
                    except theano.gof.fg.MissingInputError:
                        missing_inputs.add(param_idx)

    return [evaluated[j] for j in params] # set the order back","1. Use `theano.compile.function` to compile theano function instead of `theano.function`.
2. Use `theano.tensor.TensorConstant` instead of `theano.tensor.TensorVariable` to avoid `TypeError`.
3. Use `theano.tensor.sharedvar.SharedVariable` to avoid `TypeError`."
"def _draw_value(param, point=None, givens=None, size=None):
    """"""Draw a random value from a distribution or return a constant.

    Parameters
    ----------
    param : number, array like, theano variable or pymc3 random variable
        The value or distribution. Constants or shared variables
        will be converted to an array and returned. Theano variables
        are evaluated. If `param` is a pymc3 random variables, draw
        a new value from it and return that, unless a value is specified
        in `point`.
    point : dict, optional
        A dictionary from pymc3 variable names to their values.
    givens : dict, optional
        A dictionary from theano variables to their values. These values
        are used to evaluate `param` if it is a theano variable.
    size : int, optional
        Number of samples
    """"""
    if isinstance(param, (numbers.Number, np.ndarray)):
        return param
    elif isinstance(param, tt.TensorConstant):
        return param.value
    elif isinstance(param, tt.sharedvar.SharedVariable):
        return param.get_value()
    elif isinstance(param, (tt.TensorVariable, MultiObservedRV)):
        if point and hasattr(param, 'model') and param.name in point:
            return point[param.name]
        elif hasattr(param, 'random') and param.random is not None:
            return param.random(point=point, size=size)
        elif (hasattr(param, 'distribution') and
                hasattr(param.distribution, 'random') and
                param.distribution.random is not None):
            if hasattr(param, 'observations'):
                # shape inspection for ObservedRV
                dist_tmp = param.distribution
                try:
                    distshape = param.observations.shape.eval()
                except AttributeError:
                    distshape = param.observations.shape

                dist_tmp.shape = distshape
                try:
                    dist_tmp.random(point=point, size=size)
                except (ValueError, TypeError):
                    # reset shape to account for shape changes
                    # with theano.shared inputs
                    dist_tmp.shape = np.array([])
                    val = np.atleast_1d(dist_tmp.random(point=point,
                                                        size=None))
                    # Sometimes point may change the size of val but not the
                    # distribution's shape
                    if point and size is not None:
                        temp_size = np.atleast_1d(size)
                        if all(val.shape[:len(temp_size)] == temp_size):
                            dist_tmp.shape = val.shape[len(temp_size):]
                        else:
                            dist_tmp.shape = val.shape
                return dist_tmp.random(point=point, size=size)
            else:
                return param.distribution.random(point=point, size=size)
        else:
            if givens:
                variables, values = list(zip(*givens))
            else:
                variables = values = []
            func = _compile_theano_function(param, variables)
            if size is not None:
                size = np.atleast_1d(size)
            dshaped_variables = all((hasattr(var, 'dshape')
                                     for var in variables))
            if (values and dshaped_variables and
                not all(var.dshape == getattr(val, 'shape', tuple())
                        for var, val in zip(variables, values))):
                output = np.array([func(*v) for v in zip(*values)])
            elif (size is not None and any((val.ndim > var.ndim)
                  for var, val in zip(variables, values))):
                output = np.array([func(*v) for v in zip(*values)])
            else:
                output = func(*values)
            return output
    raise ValueError('Unexpected type in draw_value: %s' % type(param))","1. Use `np.atleast_1d` to ensure that `val.shape` is at least 1D.
2. Use `func(*v) for v in zip(*values)` to handle the case when `values` is a list of tuples.
3. Use `func(*values)` to handle the case when `values` is a list of scalars."
"def to_tuple(shape):
    """"""Convert ints, arrays, and Nones to tuples""""""
    if shape is None:
        return tuple()
    return tuple(np.atleast_1d(shape))","1. **Validate input shape before converting to tuple.** This will help prevent malicious users from passing invalid input that could crash the program or lead to security vulnerabilities.
2. **Use a secure hash function to generate the tuple.** This will make it more difficult for attackers to reverse engineer the tuple and extract sensitive information.
3. **Encrypt the tuple before sending it over the network.** This will protect the tuple from being intercepted and read by unauthorized parties."
"    def _comp_samples(self, point=None, size=None):
        try:
            samples = self.comp_dists.random(point=point, size=size)
        except AttributeError:
            samples = np.column_stack([comp_dist.random(point=point, size=size)
                                       for comp_dist in self.comp_dists])

        return np.squeeze(samples)","1. Use `np.column_stack` instead of `np.concatenate` to avoid creating a copy of the data.
2. Use `np.squeeze` to remove any singleton dimensions from the output array.
3. Use `np.random.RandomState` to generate the random numbers instead of `np.random`."
"    def random(self, point=None, size=None):
        with _DrawValuesContext() as draw_context:
            w = draw_values([self.w], point=point)[0]
            comp_tmp = self._comp_samples(point=point, size=None)
        if np.asarray(self.shape).size == 0:
            distshape = np.asarray(np.broadcast(w, comp_tmp).shape)[..., :-1]
        else:
            distshape = np.asarray(self.shape)

        # Normalize inputs
        w /= w.sum(axis=-1, keepdims=True)

        w_samples = generate_samples(random_choice,
                                     p=w,
                                     broadcast_shape=w.shape[:-1] or (1,),
                                     dist_shape=distshape,
                                     size=size).squeeze()
        if (size is None) or (distshape.size == 0):
            with draw_context:
                comp_samples = self._comp_samples(point=point, size=size)
            if comp_samples.ndim > 1:
                samples = np.squeeze(comp_samples[np.arange(w_samples.size), ..., w_samples])
            else:
                samples = np.squeeze(comp_samples[w_samples])
        else:
            if w_samples.ndim == 1:
                w_samples = np.reshape(np.tile(w_samples, size), (size,) + w_samples.shape)
            samples = np.zeros((size,)+tuple(distshape))
            with draw_context:
                for i in range(size):
                    w_tmp = w_samples[i, :]
                    comp_tmp = self._comp_samples(point=point, size=None)
                    if comp_tmp.ndim > 1:
                        samples[i, :] = np.squeeze(comp_tmp[np.arange(w_tmp.size), ..., w_tmp])
                    else:
                        samples[i, :] = np.squeeze(comp_tmp[w_tmp])

        return samples","1. Use `np.random.default_rng()` to generate random numbers instead of `np.random.random()`. This will ensure that the random numbers are generated in a secure way.
2. Use `np.clip()` to clip the random numbers to the specified range. This will prevent the random numbers from being too large or too small.
3. Use `np.random.shuffle()` to shuffle the random numbers. This will make it more difficult for an attacker to guess the random numbers."
"    def random(self, point=None, size=None):
        if size is None:
            size = []
        else:
            try:
                size = list(size)
            except TypeError:
                size = [size]

        if self._cov_type == 'cov':
            mu, cov = draw_values([self.mu, self.cov], point=point, size=size)
            if mu.shape[-1] != cov.shape[-1]:
                raise ValueError(""Shapes for mu and cov don't match"")

            try:
                dist = stats.multivariate_normal(
                    mean=mu, cov=cov, allow_singular=True)
            except ValueError:
                size.append(mu.shape[-1])
                return np.nan * np.zeros(size)
            return dist.rvs(size)
        elif self._cov_type == 'chol':
            mu, chol = draw_values([self.mu, self.chol_cov], point=point, size=size)
            if mu.shape[-1] != chol[0].shape[-1]:
                raise ValueError(""Shapes for mu and chol don't match"")

            size.append(mu.shape[-1])
            standard_normal = np.random.standard_normal(size)
            return mu + np.dot(standard_normal, chol.T)
        else:
            mu, tau = draw_values([self.mu, self.tau], point=point, size=size)
            if mu.shape[-1] != tau[0].shape[-1]:
                raise ValueError(""Shapes for mu and tau don't match"")

            size.append(mu.shape[-1])
            try:
                chol = linalg.cholesky(tau, lower=True)
            except linalg.LinAlgError:
                return np.nan * np.zeros(size)

            standard_normal = np.random.standard_normal(size)
            transformed = linalg.solve_triangular(
                chol, standard_normal.T, lower=True)
            return mu + transformed.T","1. Use `np.nan` instead of `np.inf` to represent missing values.
2. Use `np.random.default_rng()` to generate random numbers instead of `np.random`.
3. Use `np.tril()` to get the lower triangular part of a matrix instead of manually computing it."
"    def __init__(self, eta, n, sd_dist, *args, **kwargs):
        self.n = n
        self.eta = eta

        if 'transform' in kwargs:
            raise ValueError('Invalid parameter: transform.')
        if 'shape' in kwargs:
            raise ValueError('Invalid parameter: shape.')

        shape = n * (n + 1) // 2

        if sd_dist.shape.ndim not in [0, 1]:
            raise ValueError('Invalid shape for sd_dist.')

        transform = transforms.CholeskyCovPacked(n)

        kwargs['shape'] = shape
        kwargs['transform'] = transform
        super().__init__(*args, **kwargs)

        self.sd_dist = sd_dist
        self.diag_idxs = transform.diag_idxs

        self.mode = floatX(np.zeros(shape))
        self.mode[self.diag_idxs] = 1","1. Use `np.full` instead of `floatX(np.zeros)` to initialize the mode array. This will prevent an attacker from injecting their own data into the array.
2. Check the shape of the `sd_dist` argument to ensure that it is a valid distribution. This will prevent an attacker from passing in a malicious distribution that could crash the code.
3. Use a more secure random number generator, such as `np.random.default_rng()`, to generate the random numbers used to initialize the distribution. This will prevent an attacker from predicting the values of the random numbers and exploiting the distribution."
"    def forward_val(self, y, point=None):
        y[self.diag_idxs] = np.log(y[self.diag_idxs])
        return y","1. Use `np.nan_to_num` to replace `np.log(0)` with `np.nan`.
2. Use `np.inf_to_num` to replace `np.log(-1)` with `np.inf`.
3. Use `np.clip` to bound the values of `y` to `[0, 1]`."
"def _get_named_nodes_and_relations(graph, parent, leaf_nodes,
                                        node_parents, node_children):
    if getattr(graph, 'owner', None) is None:  # Leaf node
        if graph.name is not None:  # Named leaf node
            leaf_nodes.update({graph.name: graph})
            if parent is not None:  # Is None for the root node
                try:
                    node_parents[graph].add(parent)
                except KeyError:
                    node_parents[graph] = {parent}
                node_children[parent].add(graph)
            # Flag that the leaf node has no children
            node_children[graph] = set()
    else:  # Intermediate node
        if graph.name is not None:  # Intermediate named node
            if parent is not None:  # Is only None for the root node
                try:
                    node_parents[graph].add(parent)
                except KeyError:
                    node_parents[graph] = {parent}
                node_children[parent].add(graph)
            # The current node will be set as the parent of the next
            # nodes only if it is a named node
            parent = graph
            # Init the nodes children to an empty set
            node_children[graph] = set()
        for i in graph.owner.inputs:
            temp_nodes, temp_inter, temp_tree = \\
                _get_named_nodes_and_relations(i, parent, leaf_nodes,
                                               node_parents, node_children)
            leaf_nodes.update(temp_nodes)
            node_parents.update(temp_inter)
            node_children.update(temp_tree)
    return leaf_nodes, node_parents, node_children","1. Use `getattr` to check if `graph.owner` exists before accessing it.
2. Use `try` and `except` to handle the case when `graph.name` is `None`.
3. Use `set()` to initialize `node_children` to an empty set."
"    def _random(self, n, p, size=None):
        original_dtype = p.dtype
        # Set float type to float64 for numpy. This change is related to numpy issue #8317 (https://github.com/numpy/numpy/issues/8317)
        p = p.astype('float64')
        # Now, re-normalize all of the values in float64 precision. This is done inside the conditionals
        if size == p.shape:
            size = None
        elif size[-len(p.shape):] == p.shape:
            size = size[:len(size) - len(p.shape)]

        n_dim = n.squeeze().ndim

        if (n_dim == 0) and (p.ndim == 1):
            p = p / p.sum()
            randnum = np.random.multinomial(n, p.squeeze(), size=size)
        elif (n_dim == 0) and (p.ndim > 1):
            p = p / p.sum(axis=1, keepdims=True)
            randnum = np.asarray([
                np.random.multinomial(n.squeeze(), pp, size=size)
                for pp in p
            ])
            randnum = np.moveaxis(randnum, 1, 0)
        elif (n_dim > 0) and (p.ndim == 1):
            p = p / p.sum()
            randnum = np.asarray([
                np.random.multinomial(nn, p.squeeze(), size=size)
                for nn in n
            ])
            randnum = np.moveaxis(randnum, 1, 0)
        else:
            p = p / p.sum(axis=1, keepdims=True)
            randnum = np.asarray([
                np.random.multinomial(nn, pp, size=size)
                for (nn, pp) in zip(n, p)
            ])
            randnum = np.moveaxis(randnum, 1, 0)
        return randnum.astype(original_dtype)","1. Use `np.random.dirichlet` instead of `np.random.multinomial` to generate samples from a Dirichlet distribution.
2. Use `np.random.choice` instead of `np.random.multinomial` to generate samples from a categorical distribution.
3. Use `np.random.randint` instead of `np.random.multinomial` to generate samples from a uniform distribution."
"    def __call__(self, *args, **kwargs):
        if 'observed' in kwargs:
            raise ValueError('Observed Bound distributions are not supported. '
                             'If you want to model truncated data '
                             'you can use a pm.Potential in combination '
                             'with the cumulative probability function. See '
                             'pymc3/examples/censored_data.py for an example.')
        first, args = args[0], args[1:]

        if issubclass(self.distribution, Continuous):
            return _ContinuousBounded(first, self.distribution,
                                      self.lower, self.upper, *args, **kwargs)
        elif issubclass(self.distribution, Discrete):
            return _DiscreteBounded(first, self.distribution,
                                    self.lower, self.upper, *args, **kwargs)
        else:
            raise ValueError('Distribution is neither continuous nor discrete.')","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `inspect.signature` to get the signature of the wrapped function.
3. Use `inspect.getfullargspec` to get the full argument spec of the wrapped function."
"    def _run_convergence_checks(self, trace, model):
        if trace.nchains == 1:
            msg = (""Only one chain was sampled, this makes it impossible to ""
                   ""run some convergence checks"")
            warn = SamplerWarning(WarningType.BAD_PARAMS, msg, 'info',
                                  None, None, None)
            self._add_warnings([warn])
            return

        from pymc3 import diagnostics

        valid_name = [rv.name for rv in model.free_RVs + model.deterministics]
        varnames = []
        for rv in model.free_RVs:
            rv_name = rv.name
            if is_transformed_name(rv_name):
                rv_name2 = get_untransformed_name(rv_name)
                rv_name = rv_name2 if rv_name2 in valid_name else rv_name
            varnames.append(rv_name)

        self._effective_n = effective_n = diagnostics.effective_n(trace, varnames)
        self._gelman_rubin = gelman_rubin = diagnostics.gelman_rubin(trace, varnames)

        warnings = []
        rhat_max = max(val.max() for val in gelman_rubin.values())
        if rhat_max > 1.4:
            msg = (""The gelman-rubin statistic is larger than 1.4 for some ""
                   ""parameters. The sampler did not converge."")
            warn = SamplerWarning(
                WarningType.CONVERGENCE, msg, 'error', None, None, gelman_rubin)
            warnings.append(warn)
        elif rhat_max > 1.2:
            msg = (""The gelman-rubin statistic is larger than 1.2 for some ""
                   ""parameters."")
            warn = SamplerWarning(
                WarningType.CONVERGENCE, msg, 'warn', None, None, gelman_rubin)
            warnings.append(warn)
        elif rhat_max > 1.05:
            msg = (""The gelman-rubin statistic is larger than 1.05 for some ""
                   ""parameters. This indicates slight problems during ""
                   ""sampling."")
            warn = SamplerWarning(
                WarningType.CONVERGENCE, msg, 'info', None, None, gelman_rubin)
            warnings.append(warn)

        eff_min = min(val.min() for val in effective_n.values())
        n_samples = len(trace) * trace.nchains
        if eff_min < 200 and n_samples >= 500:
            msg = (""The estimated number of effective samples is smaller than ""
                   ""200 for some parameters."")
            warn = SamplerWarning(
                WarningType.CONVERGENCE, msg, 'error', None, None, effective_n)
            warnings.append(warn)
        elif eff_min / n_samples < 0.1:
            msg = (""The number of effective samples is smaller than ""
                   ""10% for some parameters."")
            warn = SamplerWarning(
                WarningType.CONVERGENCE, msg, 'warn', None, None, effective_n)
            warnings.append(warn)
        elif eff_min / n_samples < 0.25:
            msg = (""The number of effective samples is smaller than ""
                   ""25% for some parameters."")
            warn = SamplerWarning(
                WarningType.CONVERGENCE, msg, 'info', None, None, effective_n)
            warnings.append(warn)

        self._add_warnings(warnings)","1. Use `warnings.filterwarnings` to ignore warnings that are not relevant to your application.
2. Use `warnings.simplefilter` to change the default behavior of warnings.
3. Use `warnings.showwarning` to get more information about a warning."
"    def __init__(self, distribution, lower, upper, transform='infer', *args, **kwargs):
        self.dist = distribution.dist(*args, **kwargs)

        self.__dict__.update(self.dist.__dict__)
        self.__dict__.update(locals())

        if hasattr(self.dist, 'mode'):
            self.mode = self.dist.mode

        if transform == 'infer':

            default = self.dist.default()

            if not np.isinf(lower) and not np.isinf(upper):
                self.transform = transforms.interval(lower, upper)
                if default <= lower or default >= upper:
                    self.testval = 0.5 * (upper + lower)

            if not np.isinf(lower) and np.isinf(upper):
                self.transform = transforms.lowerbound(lower)
                if default <= lower:
                    self.testval = lower + 1

            if np.isinf(lower) and not np.isinf(upper):
                self.transform = transforms.upperbound(upper)
                if default >= upper:
                    self.testval = upper - 1","1. Use `np.inf` instead of `float('inf')` to avoid accidental type conversion.
2. Use `np.isfinite` to check if a value is finite.
3. Use `np.clip` to clip a value to a specified range."
"    def __init__(self, distribution, lower=-np.inf, upper=np.inf):
        self.distribution = distribution
        self.lower = lower
        self.upper = upper","1. Use `np.random.default_rng()` instead of `np.random.RandomState()` to generate a new random number generator. This will prevent an attacker from using a previously generated seed to predict the output of the function.
2. Use `np.clip()` to ensure that the generated value is within the specified range. This will prevent an attacker from generating a value that is outside of the expected range.
3. Use `np.random.choice()` to select a random value from the specified distribution. This will prevent an attacker from generating a value that is not in the distribution."
"    def dist(self, *args, **kwargs):
        return Bounded.dist(self.distribution, self.lower, self.upper,
                            *args, **kwargs)","1. Use `functools.partial` to avoid exposing the `Bounded` class to users.
2. Use `validate_args=False` to disable argument validation.
3. Use `tf.debugging.assert_greater_equal` and `tf.debugging.assert_less_equal` to check that the input arguments are within the specified bounds."
"    def __init__(self, mu=0.0, sd=None, tau=None, alpha=1,  *args, **kwargs):
        super(SkewNormal, self).__init__(*args, **kwargs)
        self.mu = mu
        self.tau, self.sd = get_tau_sd(tau=tau, sd=sd)
        self.alpha = alpha
        self.mean = mu + self.sd * (2 / np.pi)**0.5 * alpha / (1 + alpha**2)**0.5
        self.variance = self.sd**2 * (1 - (2 * alpha**2) / ((1 + alpha**2) * np.pi))

        assert_negative_support(tau, 'tau', 'SkewNormal')
        assert_negative_support(sd, 'sd', 'SkewNormal')","1. Use `np.random.default_rng()` to generate a random number generator and use it throughout the code. This will ensure that the random numbers are generated in a secure way.
2. Use `np.clip()` to clip the values of `tau` and `sd` to be non-negative. This will prevent the code from generating negative values for these parameters, which could lead to errors.
3. Use `assert_negative_support()` to check that the values of `tau` and `sd` are non-negative. This will catch any errors that may have been missed by the previous two checks."
"def run(n=1000):
    if n == ""short"":
        n = 50
    with garch:
        tr = sample(n)","1. Use `f-strings` instead of string concatenation to prevent `format string vulnerabilities`.
2. Use `type hints` to make the code more explicit and prevent errors.
3. Use `proper error handling` to catch and handle errors gracefully."
"    def __init__(self, n, p, *args, **kwargs):
        super(Multinomial, self).__init__(*args, **kwargs)

        p = p / tt.sum(p, axis=-1, keepdims=True)

        lst = range(self.shape[-1])
        if len(self.shape) > 1:
            m = self.shape[-2]
            try:
                assert n.shape == (m,)
            except AttributeError:
                n *= tt.ones(m)
            self.n = tt.shape_padright(n)
            self.p = p if p.ndim > 1 else tt.shape_padleft(p)
            lst = list(lst for _ in range(m))
        else:
            # n is a scalar, p is a 1d array
            self.n = tt.as_tensor_variable(n)
            self.p = tt.as_tensor_variable(p)

        self.mean = self.n * self.p
        mode = tt.cast(tt.round(self.mean), 'int32')
        diff = self.n - tt.sum(mode, axis=-1, keepdims=True)
        inc_bool_arr = tt.as_tensor_variable(lst) < diff
        mode = tt.inc_subtensor(mode[inc_bool_arr.nonzero()], 1)
        dec_bool_arr = tt.as_tensor_variable(lst) < -diff
        mode = tt.inc_subtensor(mode[dec_bool_arr.nonzero()], -1)
        self.mode = mode","1. Use `tt.as_tensor_variable` to cast the input tensors to the correct type.
2. Use `tt.round` to round the mean to the nearest integer.
3. Use `tt.inc_subtensor` to increment or decrement the mode tensor."
"    def _random(self, n, p, size=None):
        original_dtype = p.dtype
        # Set float type to float64 for numpy. This change is related to numpy issue #8317 (https://github.com/numpy/numpy/issues/8317)
        p = p.astype('float64')
        # Now, re-normalize all of the values in float64 precision. This is done inside the conditionals
        if size == p.shape:
            size = None
        if p.ndim == 1:
            p = p / p.sum()
            randnum = np.random.multinomial(n, p.squeeze(), size=size)
        elif p.ndim == 2:
            p = p / p.sum(axis=1, keepdims=True)
            randnum = np.asarray([
                np.random.multinomial(nn, pp, size=size)
                for (nn, pp) in zip(n, p)
            ])
        else:
            raise ValueError('Outcome probabilities must be 1- or 2-dimensional '
                             '(supplied `p` has {} dimensions)'.format(p.ndim))
        return randnum.astype(original_dtype)","1. Use `np.random.dirichlet` instead of `np.random.multinomial` to generate samples from a multinomial distribution.
2. Sanitize the input `p` to ensure that it is a valid probability distribution.
3. Use a secure random number generator, such as `np.random.default_rng()`."
"def init_nuts(init='auto', njobs=1, n_init=500000, model=None,
              random_seed=-1, progressbar=True, **kwargs):
    """"""Set up the mass matrix initialization for NUTS.

    NUTS convergence and sampling speed is extremely dependent on the
    choice of mass/scaling matrix. This function implements different
    methods for choosing or adapting the mass matrix.

    Parameters
    ----------
    init : str
        Initialization method to use.

        * auto : Choose a default initialization method automatically.
          Currently, this is `'advi+adapt_diag'`, but this can change in
          the future. If you depend on the exact behaviour, choose an
          initialization method explicitly.
        * adapt_diag : Start with a identity mass matrix and then adapt
          a diagonal based on the variance of the tuning samples.
        * advi+adapt_diag : Run ADVI and then adapt the resulting diagonal
          mass matrix based on the sample variance of the tuning samples.
        * advi+adapt_diag_grad : Run ADVI and then adapt the resulting
          diagonal mass matrix based on the variance of the gradients
          during tuning. This is **experimental** and might be removed
          in a future release.
        * advi : Run ADVI to estimate posterior mean and diagonal mass
          matrix.
        * advi_map: Initialize ADVI with MAP and use MAP as starting point.
        * map : Use the MAP as starting point. This is discouraged.
        * nuts : Run NUTS and estimate posterior mean and mass matrix from
          the trace.
    njobs : int
        Number of parallel jobs to start.
    n_init : int
        Number of iterations of initializer
        If 'ADVI', number of iterations, if 'nuts', number of draws.
    model : Model (optional if in `with` context)
    progressbar : bool
        Whether or not to display a progressbar for advi sampling.
    **kwargs : keyword arguments
        Extra keyword arguments are forwarded to pymc3.NUTS.

    Returns
    -------
    start : pymc3.model.Point
        Starting point for sampler
    nuts_sampler : pymc3.step_methods.NUTS
        Instantiated and initialized NUTS sampler object
    """"""
    model = pm.modelcontext(model)

    vars = kwargs.get('vars', model.vars)
    if set(vars) != set(model.vars):
        raise ValueError('Must use init_nuts on all variables of a model.')
    if not pm.model.all_continuous(vars):
        raise ValueError('init_nuts can only be used for models with only '
                         'continuous variables.')

    if not isinstance(init, str):
        raise TypeError('init must be a string.')

    if init is not None:
        init = init.lower()

    if init == 'auto':
        init = 'advi+adapt_diag'

    pm._log.info('Initializing NUTS using {}...'.format(init))

    random_seed = int(np.atleast_1d(random_seed)[0])

    cb = [
        pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff='absolute'),
        pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff='relative'),
    ]

    if init == 'adapt_diag':
        start = []
        for _ in range(njobs):
            vals = distribution.draw_values(model.free_RVs)
            point = {var.name: vals[i] for i, var in enumerate(model.free_RVs)}
            start.append(point)
        mean = np.mean([model.dict_to_array(vals) for vals in start], axis=0)
        var = np.ones_like(mean)
        potential = quadpotential.QuadPotentialDiagAdapt(model.ndim, mean, var, 10)
        if njobs == 1:
            start = start[0]
    elif init == 'advi+adapt_diag_grad':
        approx = pm.fit(
            random_seed=random_seed,
            n=n_init, method='advi', model=model,
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window,
        )
        start = approx.sample(draws=njobs)
        start = list(start)
        stds = approx.gbij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        mean = approx.gbij.rmap(approx.mean.get_value())
        mean = model.dict_to_array(mean)
        weight = 50
        potential = quadpotential.QuadPotentialDiagAdaptGrad(
            model.ndim, mean, cov, weight)
        if njobs == 1:
            start = start[0]
    elif init == 'advi+adapt_diag':
        approx = pm.fit(
            random_seed=random_seed,
            n=n_init, method='advi', model=model,
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window,
        )
        start = approx.sample(draws=njobs)
        start = list(start)
        stds = approx.gbij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        mean = approx.gbij.rmap(approx.mean.get_value())
        mean = model.dict_to_array(mean)
        weight = 50
        potential = quadpotential.QuadPotentialDiagAdapt(
            model.ndim, mean, cov, weight)
        if njobs == 1:
            start = start[0]
    elif init == 'advi':
        approx = pm.fit(
            random_seed=random_seed,
            n=n_init, method='advi', model=model,
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window
        )  # type: pm.MeanField
        start = approx.sample(draws=njobs)
        start = list(start)
        stds = approx.gbij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        potential = quadpotential.QuadPotentialDiag(cov)
        if njobs == 1:
            start = start[0]
    elif init == 'advi_map':
        start = pm.find_MAP()
        approx = pm.MeanField(model=model, start=start)
        pm.fit(
            random_seed=random_seed,
            n=n_init, method=pm.ADVI.from_mean_field(approx),
            callbacks=cb,
            progressbar=progressbar,
            obj_optimizer=pm.adagrad_window
        )
        start = approx.sample(draws=njobs)
        start = list(start)
        stds = approx.gbij.rmap(approx.std.eval())
        cov = model.dict_to_array(stds) ** 2
        potential = quadpotential.QuadPotentialDiag(cov)
        if njobs == 1:
            start = start[0]
    elif init == 'map':
        start = pm.find_MAP()
        cov = pm.find_hessian(point=start)
        start = [start] * njobs
        potential = quadpotential.QuadPotentialFull(cov)
        if njobs == 1:
            start = start[0]
    elif init == 'nuts':
        init_trace = pm.sample(draws=n_init, step=pm.NUTS(),
                               tune=n_init // 2,
                               random_seed=random_seed)
        cov = np.atleast_1d(pm.trace_cov(init_trace))
        start = list(np.random.choice(init_trace, njobs))
        potential = quadpotential.QuadPotentialFull(cov)
        if njobs == 1:
            start = start[0]
    else:
        raise NotImplementedError('Initializer {} is not supported.'.format(init))

    step = pm.NUTS(potential=potential, **kwargs)

    return start, step","1. Use `pm.modelcontext()` to ensure that the model is properly initialized.
2. Use `pm.fit()` to fit the model and get the starting point for NUTS.
3. Use `pm.sample()` to sample from the posterior distribution and get the trace."
"    def __init__(self, n, initial_mean, initial_diag=None, initial_weight=0,
                 adaptation_window=100, dtype=None):
        """"""Set up a diagonal mass matrix.""""""
        if initial_diag is not None and initial_diag.ndim != 1:
            raise ValueError('Initial diagonal must be one-dimensional.')
        if initial_mean.ndim != 1:
            raise ValueError('Initial mean must be one-dimensional.')
        if initial_diag is not None and len(initial_diag) != n:
            raise ValueError('Wrong shape for initial_diag: expected %s got %s'
                             % (n, len(initial_diag)))
        if len(initial_mean) != n:
            raise ValueError('Wrong shape for initial_mean: expected %s got %s'
                             % (n, len(initial_mean)))

        if initial_diag is None:
            initial_diag = np.ones(n, dtype=theano.config.floatX)
            initial_weight = 1

        if dtype is None:
            dtype = theano.config.floatX
        self.dtype = dtype
        self._n = n
        self._var = np.array(initial_diag, dtype=self.dtype, copy=True)
        self._var_theano = theano.shared(self._var)
        self._stds = np.sqrt(initial_diag)
        self._inv_stds = floatX(1.) / self._stds
        self._foreground_var = _WeightedVariance(
            self._n, initial_mean, initial_diag, initial_weight, self.dtype)
        self._background_var = _WeightedVariance(self._n, dtype=self.dtype)
        self._n_samples = 0
        self.adaptation_window = adaptation_window","1. Use `np.zeros` instead of `np.ones` to initialize the `_var` array. This will prevent the `_var` array from being initialized to all ones, which could lead to security vulnerabilities.
2. Use `theano.shared(borrow=True)` instead of `theano.shared()` to initialize the `_var_theano` variable. This will prevent the `_var_theano` variable from being shared with other parts of the program, which could lead to security vulnerabilities.
3. Use `np.sqrt` instead of `math.sqrt` to calculate the square root of the `_var` array. This will prevent the `_var` array from being calculated incorrectly, which could lead to security vulnerabilities."
"    def random(self, point=None, size=None, repeat=None):
        def random_choice(*args, **kwargs):
            w = kwargs.pop('w')
            w /= w.sum(axis=-1, keepdims=True)
            k = w.shape[-1]

            if w.ndim > 1:
                return np.row_stack([np.random.choice(k, p=w_) for w_ in w])
            else:
                return np.random.choice(k, p=w, *args, **kwargs)

        w = draw_values([self.w], point=point)

        w_samples = generate_samples(random_choice,
                                     w=w,
                                     broadcast_shape=w.shape[:-1] or (1,),
                                     dist_shape=self.shape,
                                     size=size).squeeze()
        comp_samples = self._comp_samples(point=point, size=size, repeat=repeat)

        if comp_samples.ndim > 1:
            return np.squeeze(comp_samples[np.arange(w_samples.size), w_samples])
        else:
            return np.squeeze(comp_samples[w_samples])","1. Use `np.random.default_rng()` to generate random numbers instead of `np.random.choice()`.
2. Use `np.random.choice()` with `replace=False` to prevent duplicate samples.
3. Use `np.squeeze()` to remove any extra dimensions from the output array."
"    def __init__(self, dist, transform, *args, **kwargs):
        """"""
        Parameters
        ----------
        dist : Distribution
        transform : Transform
        args, kwargs
            arguments to Distribution""""""
        forward = transform.forward
        testval = forward(dist.default())

        self.dist = dist
        self.transform_used = transform
        v = forward(FreeRV(name='v', distribution=dist))
        self.type = v.type

        super(TransformedDistribution, self).__init__(
            v.shape.tag.test_value, v.dtype,
            testval, dist.defaults, *args, **kwargs)

        if transform.name == 'stickbreaking':
            b = np.hstack(((np.atleast_1d(self.shape) == 1)[:-1], False))
            # force the last dim not broadcastable
            self.type = tt.TensorType(v.dtype, b)","1. Use `np.atleast_1d` to check if the input array has at least 1 dimension.
2. Use `np.hstack` to concatenate the boolean array with False.
3. Use `tt.TensorType` to set the last dimension not broadcastable."
"    def forward(self, x):
        a = self.a
        r = tt.log(x - a)
        return r","1. Use `torch.clamp` to bound the input to the log function to prevent overflow.
2. Use `torch.autograd.gradcheck` to check for numerical errors.
3. Use `torch.jit.script` to compile the model to a more efficient representation."
"    def forward(self, x):
        b = self.b
        r = tt.log(b - x)
        return r","1. Use `torch.clamp` to ensure that `x` is always less than `b`.
2. Use `torch.fmod` to ensure that the output is always between 0 and `math.log(b)`.
3. Use `torch.where` to mask out any values of `x` that are equal to `b`."
"def _update_start_vals(a, b, model):
    """"""Update a with b, without overwriting existing keys. Values specified for
    transformed variables on the original scale are also transformed and inserted.
    """"""
    for name in a:
        for tname in b:
            if is_transformed_name(tname) and get_untransformed_name(tname) == name:
                transform_func = [d.transformation for d in model.deterministics if d.name == name]
                if transform_func:
                    b[tname] = transform_func[0].forward(a[name]).eval()

    a.update({k: v for k, v in b.items() if k not in a})","1. Use `model.deterministics` to get the transformation function for the variable.
2. Use `transform_func[0].forward(a[name]).eval()` to apply the transformation to the value.
3. Use `a.update({k: v for k, v in b.items() if k not in a})` to update the dictionary without overwriting existing keys."
"    def random(self, point=None, size=None, repeat=None):
        sd = draw_values([self.sd], point=point)
        return generate_samples(stats.halfnorm.rvs, loc=0., scale=sd,
                                dist_shape=self.shape,
                                size=size)","1. Use `np.random.default_rng()` to generate random numbers instead of `random()`.
2. Use `np.array_split()` to split the array into multiple parts instead of using `list()`.
3. Use `np.copy()` to create a copy of the array instead of using `list()`."
"    def random(self, point=None, size=None, repeat=None):
        lam = draw_values([self.lam], point=point)
        return generate_samples(np.random.exponential, scale=1. / lam,
                                dist_shape=self.shape,
                                size=size)","1. Use `np.random.default_rng()` to generate a secure random number generator.
2. Use `np.random.exponential()` with a `size` argument to generate multiple samples.
3. Use `np.random.exponential()` with a `scale` argument to control the distribution of the samples."
"    def random(self, point=None, size=None, repeat=None):
        beta = draw_values([self.beta], point=point)
        return generate_samples(self._random, beta,
                                dist_shape=self.shape,
                                size=size)","1. Use `secrets.choice()` instead of `random.choice()` to generate random numbers.
2. Use `secrets.token_urlsafe()` instead of `random.randint()` to generate random strings.
3. Use `os.urandom()` instead of `random.getrandbits()` to generate random bytes."
"    def random(self, point=None, size=None, repeat=None):
        p = draw_values([self.p], point=point)
        return generate_samples(stats.bernoulli.rvs, p,
                                dist_shape=self.shape,
                                size=size)","1. Use `np.random.choice()` instead of `stats.bernoulli.rvs()` to generate random numbers.
2. Sanitize the input `point` to prevent arbitrary code execution.
3. Use `np.array()` to create an array of the generated random numbers, instead of returning a list."
"    def random(self, point=None, size=None, repeat=None):
        mu = draw_values([self.mu], point=point)
        return generate_samples(stats.poisson.rvs, mu,
                                dist_shape=self.shape,
                                size=size)","1. Use `np.random.default_rng()` to generate random numbers instead of `stats.poisson.rvs`.
2. Sanitize user input to prevent injection attacks.
3. Use proper error handling to prevent unexpected errors from causing security breaches."
"    def random(self, point=None, size=None, repeat=None):
        p = draw_values([self.p], point=point)
        return generate_samples(np.random.geometric, p,
                                dist_shape=self.shape,
                                size=size)","1. Use `np.random.default_rng()` to get a secure random number generator.
2. Use `np.random.seed()` to set the seed of the random number generator.
3. Use `np.random.choice()` to generate random numbers."
"    def random(self, point=None, size=None, repeat=None):
        c = draw_values([self.c], point=point)
        dtype = np.array(c).dtype

        def _random(c, dtype=dtype, size=None):
            return np.full(size, fill_value=c, dtype=dtype)

        return generate_samples(_random, c=c, dist_shape=self.shape,
                                size=size).astype(dtype)","1. Use `np.random.default_rng()` to generate random numbers instead of `np.random.rand()`. This will ensure that the random numbers are generated in a secure way.
2. Use `np.full()` to create arrays of a specific size and dtype, instead of manually creating them. This will help to prevent buffer overflows.
3. Use `np.astype()` to cast arrays to the correct dtype, instead of manually casting them. This will help to prevent type errors."
"def draw_values(params, point=None):
    """"""
    Draw (fix) parameter values. Handles a number of cases:

        1) The parameter is a scalar
        2) The parameter is an *RV

            a) parameter can be fixed to the value in the point
            b) parameter can be fixed by sampling from the *RV
            c) parameter can be fixed using tag.test_value (last resort)

        3) The parameter is a tensor variable/constant. Can be evaluated using
        theano.function, but a variable may contain nodes which

            a) are named parameters in the point
            b) are *RVs with a random method

    """"""
    # Distribution parameters may be nodes which have named node-inputs
    # specified in the point. Need to find the node-inputs to replace them.
    givens = {}
    for param in params:
        if hasattr(param, 'name'):
            named_nodes = get_named_nodes(param)
            if param.name in named_nodes:
                named_nodes.pop(param.name)
            for name, node in named_nodes.items():
                if not isinstance(node, (tt.sharedvar.TensorSharedVariable,
                                         tt.TensorConstant)):
                    givens[name] = (node, draw_value(node, point=point))
    values = [None for _ in params]
    for i, param in enumerate(params):
        # ""Homogonise"" output
        values[i] = np.atleast_1d(draw_value(
            param, point=point, givens=givens.values()))
    if len(values) == 1:
        return values[0]
    else:
        return values","1. Use `theano.function` to evaluate theano variables.
2. Use `np.atleast_1d()` to ensure that the output is a 1-D array.
3. Use `draw_value()` to draw values from random variables."
"    def random(self, point=None, size=None):
        a = draw_values([self.a], point=point)

        def _random(a, size=None):
            return stats.dirichlet.rvs(a, None if size == a.shape else size)

        samples = generate_samples(_random, a,
                                   dist_shape=self.shape,
                                   size=size)
        return samples","1. Use `np.random.dirichlet` instead of `stats.dirichlet.rvs` to avoid leaking information about the random state.
2. Sanitize the input `point` to prevent it from being used to influence the random number generation.
3. Use a cryptographically secure random number generator, such as `np.random.default_rng()`."
"    def astep(self, q0, logp):
        """"""q0 : current state
        logp : log probability function
        """"""

        # Draw from the normal prior by multiplying the Cholesky decomposition
        # of the covariance with draws from a standard normal
        chol = draw_values([self.prior_chol])
        nu = np.dot(chol, nr.randn(chol.shape[0]))
        y = logp(q0) - nr.standard_exponential()

        # Draw initial proposal and propose a candidate point
        theta = nr.uniform(0, 2 * np.pi)
        theta_max = theta
        theta_min = theta - 2 * np.pi
        q_new = q0 * np.cos(theta) + nu * np.sin(theta)

        while logp(q_new) <= y:
            # Shrink the bracket and propose a new point
            if theta < 0:
                theta_min = theta
            else:
                theta_max = theta
            theta = nr.uniform(theta_min, theta_max)
            q_new = q0 * np.cos(theta) + nu * np.sin(theta)

        return q_new","1. Use a secure random number generator (RNG) to generate the proposal point.
2. Check that the proposal point is within the valid range before accepting it.
3. Sanitize the input to the log probability function to prevent attacks such as poisoning."
"    def _slice(self, idx):
        with self.activate_file:
            if idx.start is None:
                burn = 0
            else:
                burn = idx.start
            if idx.step is None:
                thin = 1
            else:
                thin = idx.step

            sliced = ndarray.NDArray(model=self.model, vars=self.vars)
            sliced.chain = self.chain
            sliced.samples = {v: self.get_values(v, burn=burn, thin=thin)
                              for v in self.varnames}
            return sliced","1. Use `with open(filename, 'rb') as f:` to open the file in binary mode.
2. Use `os.fchmod(f.fileno(), mode)` to change the file mode to `0o600`.
3. Use `os.fchown(f.fileno(), uid, gid)` to change the file owner and group to `uid` and `gid`."
"    def _slice(self, idx):
        # Slicing directly instead of using _slice_as_ndarray to
        # support stop value in slice (which is needed by
        # iter_sample).

        # Only the first `draw_idx` value are valid because of preallocation
        idx = slice(*idx.indices(len(self)))

        sliced = NDArray(model=self.model, vars=self.vars)
        sliced.chain = self.chain
        sliced.samples = {varname: values[idx]
                          for varname, values in self.samples.items()}
        sliced.sampler_vars = self.sampler_vars
        if self._stats is None:
            return sliced
        sliced._stats = []
        for vars in self._stats:
            var_sliced = {}
            sliced._stats.append(var_sliced)
            for key, vals in vars.items():
                var_sliced[key] = vals[idx]

        sliced.draw_idx = idx.stop - idx.start
        return sliced","1. Use `np.ndarray.copy()` instead of `slice()` to avoid modifying the original data.
2. Use `np.clip()` to sanitize the input values to prevent out-of-bounds access.
3. Use `np.inf` to represent missing values instead of `None` to avoid confusion."
"def _slice_as_ndarray(strace, idx):
    if idx.start is None:
        burn = 0
    else:
        burn = idx.start
    if idx.step is None:
        thin = 1
    else:
        thin = idx.step

    sliced = NDArray(model=strace.model, vars=strace.vars)
    sliced.chain = strace.chain
    sliced.samples = {v: strace.get_values(v, burn=burn, thin=thin)
                      for v in strace.varnames}
    return sliced","1. Use `torch.no_grad()` to disable gradient computation when getting values from the trace.
2. Sanitize the input `idx` to prevent malicious users from accessing invalid data.
3. Use `torch.tensor()` to cast the input `idx` to a `torch.LongTensor`."
"    def get_values(self, varname, burn=0, thin=1):
        """"""Get values from trace.

        Parameters
        ----------
        varname : str
        burn : int
        thin : int

        Returns
        -------
        A NumPy array
        """"""
        if burn < 0:
            burn = max(0, len(self) + burn)
        if thin < 1:
            raise ValueError('Only positive thin values are supported '
                             'in SQLite backend.')
        varname = str(varname)

        statement_args = {'chain': self.chain}
        if burn == 0 and thin == 1:
            action = 'select'
        elif thin == 1:
            action = 'select_burn'
            statement_args['burn'] = burn - 1
        elif burn == 0:
            action = 'select_thin'
            statement_args['thin'] = thin
        else:
            action = 'select_burn_thin'
            statement_args['burn'] = burn - 1
            statement_args['thin'] = thin

        self.db.connect()
        shape = (-1,) + self.var_shapes[varname]
        statement = TEMPLATES[action].format(table=varname)
        self.db.cursor.execute(statement, statement_args)
        values = _rows_to_ndarray(self.db.cursor)
        return values.reshape(shape)","1. Use prepared statements instead of concatenation to prevent SQL injection.
2. Use a secure password for the database.
3. Use access control to restrict who can access the database."
"    def __init__(self, lam, *args, **kwargs):
        super(Exponential, self).__init__(*args, **kwargs)
        self.lam = lam = tt.as_tensor_variable(lam)
        self.mean = 1. / self.lam
        self.median = self.mean * tt.log(2)
        self.mode = 0

        self.variance = self.lam**-2

        assert_negative_support(lam, 'lam', 'Exponential')","1. Use `tt.as_tensor` to cast the input argument to a tensor.
2. Use `assert_negative_support` to check that the input argument is negative.
3. Use `tt.log` to calculate the log of the input argument."
"def reshape_sampled(sampled, size, dist_shape):
    dist_shape = infer_shape(dist_shape)
    repeat_shape = infer_shape(size)
    return np.reshape(sampled, repeat_shape + dist_shape)","1. Sanitize user input to prevent injection attacks.
2. Use a secure random number generator to generate the random numbers.
3. Use proper error handling to prevent security vulnerabilities."
"def find_MAP(start=None, vars=None, fmin=None, return_raw=False,
             model=None, *args, **kwargs):
    """"""
    Sets state to the local maximum a posteriori point given a model.
    Current default of fmin_Hessian does not deal well with optimizing close
    to sharp edges, especially if they are the minimum.

    Parameters
    ----------
    start : `dict` of parameter values (Defaults to `model.test_point`)
    vars : list
        List of variables to set to MAP point (Defaults to all continuous).
    fmin : function
        Optimization algorithm (Defaults to `scipy.optimize.fmin_bfgs` unless
        discrete variables are specified in `vars`, then
        `scipy.optimize.fmin_powell` which will perform better).
    return_raw : Bool
        Whether to return extra value returned by fmin (Defaults to `False`)
    model : Model (optional if in `with` context)
    *args, **kwargs
        Extra args passed to fmin
    """"""
    model = modelcontext(model)
    if start is None:
        start = model.test_point

    if not set(start.keys()).issubset(model.named_vars.keys()):
        extra_keys = ', '.join(set(start.keys()) - set(model.named_vars.keys()))
        valid_keys = ', '.join(model.named_vars.keys())
        raise KeyError('Some start parameters do not appear in the model!\\n'
                       'Valid keys are: {}, but {} was supplied'.format(valid_keys, extra_keys))

    if vars is None:
        vars = model.cont_vars
    vars = inputvars(vars)

    disc_vars = list(typefilter(vars, discrete_types))

    if disc_vars:
        pm._log.warning(""Warning: vars contains discrete variables. MAP "" +
                        ""estimates may not be accurate for the default "" +
                        ""parameters. Defaulting to non-gradient minimization "" +
                        ""fmin_powell."")
        fmin = optimize.fmin_powell

    if fmin is None:
        if disc_vars:
            fmin = optimize.fmin_powell
        else:
            fmin = optimize.fmin_bfgs

    allinmodel(vars, model)

    start = Point(start, model=model)
    bij = DictToArrayBijection(ArrayOrdering(vars), start)

    logp = bij.mapf(model.fastlogp)
    dlogp = bij.mapf(model.fastdlogp(vars))

    def logp_o(point):
        return nan_to_high(-logp(point))

    def grad_logp_o(point):
        return nan_to_num(-dlogp(point))

    # Check to see if minimization function actually uses the gradient
    if 'fprime' in getargspec(fmin).args:
        r = fmin(logp_o, bij.map(
            start), fprime=grad_logp_o, *args, **kwargs)
    else:
        # Check to see if minimization function uses a starting value
        if 'x0' in getargspec(fmin).args:
            r = fmin(logp_o, bij.map(start), *args, **kwargs)
        else:
            r = fmin(logp_o, *args, **kwargs)

    if isinstance(r, tuple):
        mx0 = r[0]
    else:
        mx0 = r

    mx = bij.rmap(mx0)

    if (not allfinite(mx0) or
            not allfinite(model.logp(mx)) or
            not allfinite(model.dlogp()(mx))):

        messages = []
        for var in vars:

            vals = {
                ""value"": mx[var.name],
                ""logp"": var.logp(mx),
                ""dlogp"": var.dlogp()(mx)}

            def message(name, values):
                if np.size(values) < 10:
                    return name + "" bad: "" + str(values)
                else:
                    idx = np.nonzero(logical_not(isfinite(values)))
                    return name + "" bad at idx: "" + str(idx) + "" with values: "" + str(values[idx])

            messages += [
                message(var.name + ""."" + k, v)
                for k, v in vals.items()
                if not allfinite(v)]

        specific_errors = '\\n'.join(messages)
        raise ValueError(""Optimization error: max, logp or dlogp at "" +
                         ""max have non-finite values. Some values may be "" +
                         ""outside of distribution support. max: "" +
                         repr(mx) + "" logp: "" + repr(model.logp(mx)) +
                         "" dlogp: "" + repr(model.dlogp()(mx)) + ""Check that "" +
                         ""1) you don't have hierarchical parameters, "" +
                         ""these will lead to points with infinite "" +
                         ""density. 2) your distribution logp's are "" +
                         ""properly specified. Specific issues: \\n"" +
                         specific_errors)
    mx = {v.name: mx[v.name].astype(v.dtype) for v in model.vars}

    if return_raw:
        return mx, r
    else:
        return mx","1. Use `assert` statements to check for invalid inputs.
2. Use `type()` to check for the type of inputs.
3. Use `logging` to log errors and warnings."
"    def __getitem__(self, index_value):
        """"""
        Return copy NpTrace with sliced sample values if a slice is passed,
        or the array of samples if a varname is passed.
        """"""

        if isinstance(index_value, slice):

            sliced_trace = NpTrace(self.vars)
            sliced_trace.samples = dict((name, vals[index_value]) for (name, vals) in self.samples.items())

            return sliced_trace

        else:
            try:
                return self.point(index_value)
            except ValueError:
                pass
            except TypeError:
                pass

            return self.samples[str(index_value)].value","1. Use `np.array()` instead of `list()` to avoid type errors.
2. Use `try...except` to catch and handle errors.
3. Use `str()` to convert the index value to a string before accessing the dictionary."
"    async def purge_history(
        self, room_id: str, token: str, delete_local_events: bool
    ) -> Set[int]:
        """"""Deletes room history before a certain point

        Args:
            room_id:
            token: A topological token to delete events before
            delete_local_events:
                if True, we will delete local events as well as remote ones
                (instead of just marking them as outliers and deleting their
                state groups).

        Returns:
            The set of state groups that are referenced by deleted events.
        """"""

        parsed_token = await RoomStreamToken.parse(self, token)

        return await self.db_pool.runInteraction(
            ""purge_history"",
            self._purge_history_txn,
            room_id,
            parsed_token,
            delete_local_events,
        )","1. Use prepared statements to prevent SQL injection.
2. Sanitize user input to prevent code injection.
3. Use access control to restrict who can access the data."
"    def _purge_history_txn(self, txn, room_id, token, delete_local_events):
        # Tables that should be pruned:
        #     event_auth
        #     event_backward_extremities
        #     event_edges
        #     event_forward_extremities
        #     event_json
        #     event_push_actions
        #     event_reference_hashes
        #     event_relations
        #     event_search
        #     event_to_state_groups
        #     events
        #     rejections
        #     room_depth
        #     state_groups
        #     state_groups_state
        #     destination_rooms

        # we will build a temporary table listing the events so that we don't
        # have to keep shovelling the list back and forth across the
        # connection. Annoyingly the python sqlite driver commits the
        # transaction on CREATE, so let's do this first.
        #
        # furthermore, we might already have the table from a previous (failed)
        # purge attempt, so let's drop the table first.

        txn.execute(""DROP TABLE IF EXISTS events_to_purge"")

        txn.execute(
            ""CREATE TEMPORARY TABLE events_to_purge (""
            ""    event_id TEXT NOT NULL,""
            ""    should_delete BOOLEAN NOT NULL""
            "")""
        )

        # First ensure that we're not about to delete all the forward extremeties
        txn.execute(
            ""SELECT e.event_id, e.depth FROM events as e ""
            ""INNER JOIN event_forward_extremities as f ""
            ""ON e.event_id = f.event_id ""
            ""AND e.room_id = f.room_id ""
            ""WHERE f.room_id = ?"",
            (room_id,),
        )
        rows = txn.fetchall()
        max_depth = max(row[1] for row in rows)

        if max_depth < token.topological:
            # We need to ensure we don't delete all the events from the database
            # otherwise we wouldn't be able to send any events (due to not
            # having any backwards extremeties)
            raise SynapseError(
                400, ""topological_ordering is greater than forward extremeties""
            )

        logger.info(""[purge] looking for events to delete"")

        should_delete_expr = ""state_key IS NULL""
        should_delete_params = ()  # type: Tuple[Any, ...]
        if not delete_local_events:
            should_delete_expr += "" AND event_id NOT LIKE ?""

            # We include the parameter twice since we use the expression twice
            should_delete_params += (""%:"" + self.hs.hostname, ""%:"" + self.hs.hostname)

        should_delete_params += (room_id, token.topological)

        # Note that we insert events that are outliers and aren't going to be
        # deleted, as nothing will happen to them.
        txn.execute(
            ""INSERT INTO events_to_purge""
            "" SELECT event_id, %s""
            "" FROM events AS e LEFT JOIN state_events USING (event_id)""
            "" WHERE (NOT outlier OR (%s)) AND e.room_id = ? AND topological_ordering < ?""
            % (should_delete_expr, should_delete_expr),
            should_delete_params,
        )

        # We create the indices *after* insertion as that's a lot faster.

        # create an index on should_delete because later we'll be looking for
        # the should_delete / shouldn't_delete subsets
        txn.execute(
            ""CREATE INDEX events_to_purge_should_delete""
            "" ON events_to_purge(should_delete)""
        )

        # We do joins against events_to_purge for e.g. calculating state
        # groups to purge, etc., so lets make an index.
        txn.execute(""CREATE INDEX events_to_purge_id ON events_to_purge(event_id)"")

        txn.execute(""SELECT event_id, should_delete FROM events_to_purge"")
        event_rows = txn.fetchall()
        logger.info(
            ""[purge] found %i events before cutoff, of which %i can be deleted"",
            len(event_rows),
            sum(1 for e in event_rows if e[1]),
        )

        logger.info(""[purge] Finding new backward extremities"")

        # We calculate the new entries for the backward extremeties by finding
        # events to be purged that are pointed to by events we're not going to
        # purge.
        txn.execute(
            ""SELECT DISTINCT e.event_id FROM events_to_purge AS e""
            "" INNER JOIN event_edges AS ed ON e.event_id = ed.prev_event_id""
            "" LEFT JOIN events_to_purge AS ep2 ON ed.event_id = ep2.event_id""
            "" WHERE ep2.event_id IS NULL""
        )
        new_backwards_extrems = txn.fetchall()

        logger.info(""[purge] replacing backward extremities: %r"", new_backwards_extrems)

        txn.execute(
            ""DELETE FROM event_backward_extremities WHERE room_id = ?"", (room_id,)
        )

        # Update backward extremeties
        txn.execute_batch(
            ""INSERT INTO event_backward_extremities (room_id, event_id)""
            "" VALUES (?, ?)"",
            [(room_id, event_id) for event_id, in new_backwards_extrems],
        )

        logger.info(""[purge] finding state groups referenced by deleted events"")

        # Get all state groups that are referenced by events that are to be
        # deleted.
        txn.execute(
            """"""
            SELECT DISTINCT state_group FROM events_to_purge
            INNER JOIN event_to_state_groups USING (event_id)
        """"""
        )

        referenced_state_groups = {sg for sg, in txn}
        logger.info(
            ""[purge] found %i referenced state groups"", len(referenced_state_groups)
        )

        logger.info(""[purge] removing events from event_to_state_groups"")
        txn.execute(
            ""DELETE FROM event_to_state_groups ""
            ""WHERE event_id IN (SELECT event_id from events_to_purge)""
        )
        for event_id, _ in event_rows:
            txn.call_after(self._get_state_group_for_event.invalidate, (event_id,))

        # Delete all remote non-state events
        for table in (
            ""events"",
            ""event_json"",
            ""event_auth"",
            ""event_edges"",
            ""event_forward_extremities"",
            ""event_reference_hashes"",
            ""event_relations"",
            ""event_search"",
            ""rejections"",
        ):
            logger.info(""[purge] removing events from %s"", table)

            txn.execute(
                ""DELETE FROM %s WHERE event_id IN (""
                ""    SELECT event_id FROM events_to_purge WHERE should_delete""
                "")"" % (table,)
            )

        # event_push_actions lacks an index on event_id, and has one on
        # (room_id, event_id) instead.
        for table in (""event_push_actions"",):
            logger.info(""[purge] removing events from %s"", table)

            txn.execute(
                ""DELETE FROM %s WHERE room_id = ? AND event_id IN (""
                ""    SELECT event_id FROM events_to_purge WHERE should_delete""
                "")"" % (table,),
                (room_id,),
            )

        # Mark all state and own events as outliers
        logger.info(""[purge] marking remaining events as outliers"")
        txn.execute(
            ""UPDATE events SET outlier = ?""
            "" WHERE event_id IN (""
            ""    SELECT event_id FROM events_to_purge ""
            ""    WHERE NOT should_delete""
            "")"",
            (True,),
        )

        # synapse tries to take out an exclusive lock on room_depth whenever it
        # persists events (because upsert), and once we run this update, we
        # will block that for the rest of our transaction.
        #
        # So, let's stick it at the end so that we don't block event
        # persistence.
        #
        # We do this by calculating the minimum depth of the backwards
        # extremities. However, the events in event_backward_extremities
        # are ones we don't have yet so we need to look at the events that
        # point to it via event_edges table.
        txn.execute(
            """"""
            SELECT COALESCE(MIN(depth), 0)
            FROM event_backward_extremities AS eb
            INNER JOIN event_edges AS eg ON eg.prev_event_id = eb.event_id
            INNER JOIN events AS e ON e.event_id = eg.event_id
            WHERE eb.room_id = ?
        """""",
            (room_id,),
        )
        (min_depth,) = txn.fetchone()

        logger.info(""[purge] updating room_depth to %d"", min_depth)

        txn.execute(
            ""UPDATE room_depth SET min_depth = ? WHERE room_id = ?"",
            (min_depth, room_id),
        )

        # finally, drop the temp table. this will commit the txn in sqlite,
        # so make sure to keep this actually last.
        txn.execute(""DROP TABLE events_to_purge"")

        logger.info(""[purge] done"")

        return referenced_state_groups","1. Use prepared statements instead of building queries with string concatenation.
2. Use `CHECK` constraints to validate input data.
3. Use `PRAGMA foreign_keys = ON` to enable foreign key constraints."
"    def _purge_room_txn(self, txn, room_id):
        # First we fetch all the state groups that should be deleted, before
        # we delete that information.
        txn.execute(
            """"""
                SELECT DISTINCT state_group FROM events
                INNER JOIN event_to_state_groups USING(event_id)
                WHERE events.room_id = ?
            """""",
            (room_id,),
        )

        state_groups = [row[0] for row in txn]

        # Now we delete tables which lack an index on room_id but have one on event_id
        for table in (
            ""event_auth"",
            ""event_edges"",
            ""event_json"",
            ""event_push_actions_staging"",
            ""event_reference_hashes"",
            ""event_relations"",
            ""event_to_state_groups"",
            ""redactions"",
            ""rejections"",
            ""state_events"",
        ):
            logger.info(""[purge] removing %s from %s"", room_id, table)

            txn.execute(
                """"""
                DELETE FROM %s WHERE event_id IN (
                  SELECT event_id FROM events WHERE room_id=?
                )
                """"""
                % (table,),
                (room_id,),
            )

        # and finally, the tables with an index on room_id (or no useful index)
        for table in (
            ""current_state_events"",
            ""destination_rooms"",
            ""event_backward_extremities"",
            ""event_forward_extremities"",
            ""event_push_actions"",
            ""event_search"",
            ""events"",
            ""group_rooms"",
            ""public_room_list_stream"",
            ""receipts_graph"",
            ""receipts_linearized"",
            ""room_aliases"",
            ""room_depth"",
            ""room_memberships"",
            ""room_stats_state"",
            ""room_stats_current"",
            ""room_stats_historical"",
            ""room_stats_earliest_token"",
            ""rooms"",
            ""stream_ordering_to_exterm"",
            ""users_in_public_rooms"",
            ""users_who_share_private_rooms"",
            # no useful index, but let's clear them anyway
            ""appservice_room_list"",
            ""e2e_room_keys"",
            ""event_push_summary"",
            ""pusher_throttle"",
            ""group_summary_rooms"",
            ""room_account_data"",
            ""room_tags"",
            ""local_current_membership"",
        ):
            logger.info(""[purge] removing %s from %s"", room_id, table)
            txn.execute(""DELETE FROM %s WHERE room_id=?"" % (table,), (room_id,))

        # Other tables we do NOT need to clear out:
        #
        #  - blocked_rooms
        #    This is important, to make sure that we don't accidentally rejoin a blocked
        #    room after it was purged
        #
        #  - user_directory
        #    This has a room_id column, but it is unused
        #

        # Other tables that we might want to consider clearing out include:
        #
        #  - event_reports
        #       Given that these are intended for abuse management my initial
        #       inclination is to leave them in place.
        #
        #  - current_state_delta_stream
        #  - ex_outlier_stream
        #  - room_tags_revisions
        #       The problem with these is that they are largeish and there is no room_id
        #       index on them. In any case we should be clearing out 'stream' tables
        #       periodically anyway (#5888)

        # TODO: we could probably usefully do a bunch of cache invalidation here

        logger.info(""[purge] done"")

        return state_groups","1. Use prepared statements instead of building queries with string concatenation. This will prevent SQL injection attacks.
2. Use transactions to ensure that data is deleted in a consistent manner.
3. Use proper error handling to catch and log any errors that occur."
"    async def _find_unreferenced_groups(self, state_groups: Set[int]) -> Set[int]:
        """"""Used when purging history to figure out which state groups can be
        deleted.

        Args:
            state_groups: Set of state groups referenced by events
                that are going to be deleted.

        Returns:
            The set of state groups that can be deleted.
        """"""
        # Graph of state group -> previous group
        graph = {}

        # Set of events that we have found to be referenced by events
        referenced_groups = set()

        # Set of state groups we've already seen
        state_groups_seen = set(state_groups)

        # Set of state groups to handle next.
        next_to_search = set(state_groups)
        while next_to_search:
            # We bound size of groups we're looking up at once, to stop the
            # SQL query getting too big
            if len(next_to_search) < 100:
                current_search = next_to_search
                next_to_search = set()
            else:
                current_search = set(itertools.islice(next_to_search, 100))
                next_to_search -= current_search

            referenced = await self.stores.main.get_referenced_state_groups(
                current_search
            )
            referenced_groups |= referenced

            # We don't continue iterating up the state group graphs for state
            # groups that are referenced.
            current_search -= referenced

            edges = await self.stores.state.get_previous_state_groups(current_search)

            prevs = set(edges.values())
            # We don't bother re-handling groups we've already seen
            prevs -= state_groups_seen
            next_to_search |= prevs
            state_groups_seen |= prevs

            graph.update(edges)

        to_delete = state_groups_seen - referenced_groups

        return to_delete","1. Use prepared statements instead of concatenation to prevent SQL injection.
2. Use a library like `cryptography` to securely generate random numbers.
3. Sanitize user input to prevent cross-site scripting (XSS) attacks."
"    def __init__(self, database: DatabasePool, db_conn, hs):
        super().__init__(database, db_conn, hs)

        self.db_pool.updates.register_background_update_handler(
            self.EVENT_ORIGIN_SERVER_TS_NAME, self._background_reindex_origin_server_ts
        )
        self.db_pool.updates.register_background_update_handler(
            self.EVENT_FIELDS_SENDER_URL_UPDATE_NAME,
            self._background_reindex_fields_sender,
        )

        self.db_pool.updates.register_background_index_update(
            ""event_contains_url_index"",
            index_name=""event_contains_url_index"",
            table=""events"",
            columns=[""room_id"", ""topological_ordering"", ""stream_ordering""],
            where_clause=""contains_url = true AND outlier = false"",
        )

        # an event_id index on event_search is useful for the purge_history
        # api. Plus it means we get to enforce some integrity with a UNIQUE
        # clause
        self.db_pool.updates.register_background_index_update(
            ""event_search_event_id_idx"",
            index_name=""event_search_event_id_idx"",
            table=""event_search"",
            columns=[""event_id""],
            unique=True,
            psql_only=True,
        )

        self.db_pool.updates.register_background_update_handler(
            self.DELETE_SOFT_FAILED_EXTREMITIES, self._cleanup_extremities_bg_update
        )

        self.db_pool.updates.register_background_update_handler(
            ""redactions_received_ts"", self._redactions_received_ts
        )

        # This index gets deleted in `event_fix_redactions_bytes` update
        self.db_pool.updates.register_background_index_update(
            ""event_fix_redactions_bytes_create_index"",
            index_name=""redactions_censored_redacts"",
            table=""redactions"",
            columns=[""redacts""],
            where_clause=""have_censored"",
        )

        self.db_pool.updates.register_background_update_handler(
            ""event_fix_redactions_bytes"", self._event_fix_redactions_bytes
        )

        self.db_pool.updates.register_background_update_handler(
            ""event_store_labels"", self._event_store_labels
        )

        self.db_pool.updates.register_background_index_update(
            ""redactions_have_censored_ts_idx"",
            index_name=""redactions_have_censored_ts"",
            table=""redactions"",
            columns=[""received_ts""],
            where_clause=""NOT have_censored"",
        )

        self.db_pool.updates.register_background_index_update(
            ""users_have_local_media"",
            index_name=""users_have_local_media"",
            table=""local_media_repository"",
            columns=[""user_id"", ""created_ts""],
        )

        self.db_pool.updates.register_background_update_handler(
            ""rejected_events_metadata"",
            self._rejected_events_metadata,
        )

        self.db_pool.updates.register_background_update_handler(
            ""chain_cover"",
            self._chain_cover_index,
        )","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to avoid leaking sensitive information.
3. Use proper authorization checks to prevent unauthorized access."
"    def _purge_room_txn(self, txn, room_id: str) -> List[int]:
        # First we fetch all the state groups that should be deleted, before
        # we delete that information.
        txn.execute(
            """"""
                SELECT DISTINCT state_group FROM events
                INNER JOIN event_to_state_groups USING(event_id)
                WHERE events.room_id = ?
            """""",
            (room_id,),
        )

        state_groups = [row[0] for row in txn]

        # Get all the auth chains that are referenced by events that are to be
        # deleted.
        txn.execute(
            """"""
            SELECT chain_id, sequence_number FROM events
            LEFT JOIN event_auth_chains USING (event_id)
            WHERE room_id = ?
            """""",
            (room_id,),
        )
        referenced_chain_id_tuples = list(txn)

        logger.info(""[purge] removing events from event_auth_chain_links"")
        txn.executemany(
            """"""
            DELETE FROM event_auth_chain_links WHERE
            (origin_chain_id = ? AND origin_sequence_number = ?) OR
            (target_chain_id = ? AND target_sequence_number = ?)
            """""",
            (
                (chain_id, seq_num, chain_id, seq_num)
                for (chain_id, seq_num) in referenced_chain_id_tuples
            ),
        )

        # Now we delete tables which lack an index on room_id but have one on event_id
        for table in (
            ""event_auth"",
            ""event_edges"",
            ""event_json"",
            ""event_push_actions_staging"",
            ""event_reference_hashes"",
            ""event_relations"",
            ""event_to_state_groups"",
            ""event_auth_chains"",
            ""event_auth_chain_to_calculate"",
            ""redactions"",
            ""rejections"",
            ""state_events"",
        ):
            logger.info(""[purge] removing %s from %s"", room_id, table)

            txn.execute(
                """"""
                DELETE FROM %s WHERE event_id IN (
                  SELECT event_id FROM events WHERE room_id=?
                )
                """"""
                % (table,),
                (room_id,),
            )

        # and finally, the tables with an index on room_id (or no useful index)
        for table in (
            ""current_state_events"",
            ""destination_rooms"",
            ""event_backward_extremities"",
            ""event_forward_extremities"",
            ""event_push_actions"",
            ""event_search"",
            ""events"",
            ""group_rooms"",
            ""public_room_list_stream"",
            ""receipts_graph"",
            ""receipts_linearized"",
            ""room_aliases"",
            ""room_depth"",
            ""room_memberships"",
            ""room_stats_state"",
            ""room_stats_current"",
            ""room_stats_historical"",
            ""room_stats_earliest_token"",
            ""rooms"",
            ""stream_ordering_to_exterm"",
            ""users_in_public_rooms"",
            ""users_who_share_private_rooms"",
            # no useful index, but let's clear them anyway
            ""appservice_room_list"",
            ""e2e_room_keys"",
            ""event_push_summary"",
            ""pusher_throttle"",
            ""group_summary_rooms"",
            ""room_account_data"",
            ""room_tags"",
            ""local_current_membership"",
        ):
            logger.info(""[purge] removing %s from %s"", room_id, table)
            txn.execute(""DELETE FROM %s WHERE room_id=?"" % (table,), (room_id,))

        # Other tables we do NOT need to clear out:
        #
        #  - blocked_rooms
        #    This is important, to make sure that we don't accidentally rejoin a blocked
        #    room after it was purged
        #
        #  - user_directory
        #    This has a room_id column, but it is unused
        #

        # Other tables that we might want to consider clearing out include:
        #
        #  - event_reports
        #       Given that these are intended for abuse management my initial
        #       inclination is to leave them in place.
        #
        #  - current_state_delta_stream
        #  - ex_outlier_stream
        #  - room_tags_revisions
        #       The problem with these is that they are largeish and there is no room_id
        #       index on them. In any case we should be clearing out 'stream' tables
        #       periodically anyway (#5888)

        # TODO: we could probably usefully do a bunch of cache invalidation here

        logger.info(""[purge] done"")

        return state_groups","1. Use prepared statements instead of building queries with string concatenation.
2. Use transaction boundaries to avoid cascading deletes.
3. Use indexes to improve performance and avoid unnecessary scans."
"    def __init__(self, hs: ""HomeServer""):
        self.hs = hs
        self.auth = hs.get_auth()
        self.room_member_handler = hs.get_room_member_handler()
        self.admin_handler = hs.get_admin_handler()
        self.state_handler = hs.get_state_handler()","1. Use `@require_auth` decorator to protect endpoints that require authentication.
2. Use `@require_user_id` decorator to protect endpoints that require a user ID.
3. Use `@require_admin` decorator to protect endpoints that require an administrator."
"    async def on_POST(
        self, request: SynapseRequest, room_identifier: str
    ) -> Tuple[int, JsonDict]:
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)

        content = parse_json_object_from_request(request)

        assert_params_in_dict(content, [""user_id""])
        target_user = UserID.from_string(content[""user_id""])

        if not self.hs.is_mine(target_user):
            raise SynapseError(400, ""This endpoint can only be used with local users"")

        if not await self.admin_handler.get_user(target_user):
            raise NotFoundError(""User not found"")

        if RoomID.is_valid(room_identifier):
            room_id = room_identifier
            try:
                remote_room_hosts = [
                    x.decode(""ascii"") for x in request.args[b""server_name""]
                ]  # type: Optional[List[str]]
            except Exception:
                remote_room_hosts = None
        elif RoomAlias.is_valid(room_identifier):
            handler = self.room_member_handler
            room_alias = RoomAlias.from_string(room_identifier)
            room_id, remote_room_hosts = await handler.lookup_room_alias(room_alias)
        else:
            raise SynapseError(
                400, ""%s was not legal room ID or room alias"" % (room_identifier,)
            )

        fake_requester = create_requester(
            target_user, authenticated_entity=requester.authenticated_entity
        )

        # send invite if room has ""JoinRules.INVITE""
        room_state = await self.state_handler.get_current_state(room_id)
        join_rules_event = room_state.get((EventTypes.JoinRules, """"))
        if join_rules_event:
            if not (join_rules_event.content.get(""join_rule"") == JoinRules.PUBLIC):
                # update_membership with an action of ""invite"" can raise a
                # ShadowBanError. This is not handled since it is assumed that
                # an admin isn't going to call this API with a shadow-banned user.
                await self.room_member_handler.update_membership(
                    requester=requester,
                    target=fake_requester.user,
                    room_id=room_id,
                    action=""invite"",
                    remote_room_hosts=remote_room_hosts,
                    ratelimit=False,
                )

        await self.room_member_handler.update_membership(
            requester=fake_requester,
            target=fake_requester.user,
            room_id=room_id,
            action=""join"",
            remote_room_hosts=remote_room_hosts,
            ratelimit=False,
        )

        return 200, {""room_id"": room_id}","1. Sanitize all user input.
2. Validate the request parameters.
3. Handle errors gracefully."
"    def __init__(self, hs: ""HomeServer""):
        self.hs = hs
        self.auth = hs.get_auth()
        self.room_member_handler = hs.get_room_member_handler()
        self.event_creation_handler = hs.get_event_creation_handler()
        self.state_handler = hs.get_state_handler()
        self.is_mine_id = hs.is_mine_id","1. Use `@hs.wrap_function` to protect the function from unauthorized access.
2. Use `@hs.defer` to handle asynchronous operations.
3. Use `@hs.log_api` to log all API calls."
"    async def on_POST(self, request, room_identifier):
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)
        content = parse_json_object_from_request(request, allow_empty_body=True)

        # Resolve to a room ID, if necessary.
        if RoomID.is_valid(room_identifier):
            room_id = room_identifier
        elif RoomAlias.is_valid(room_identifier):
            room_alias = RoomAlias.from_string(room_identifier)
            room_id, _ = await self.room_member_handler.lookup_room_alias(room_alias)
            room_id = room_id.to_string()
        else:
            raise SynapseError(
                400, ""%s was not legal room ID or room alias"" % (room_identifier,)
            )

        # Which user to grant room admin rights to.
        user_to_add = content.get(""user_id"", requester.user.to_string())

        # Figure out which local users currently have power in the room, if any.
        room_state = await self.state_handler.get_current_state(room_id)
        if not room_state:
            raise SynapseError(400, ""Server not in room"")

        create_event = room_state[(EventTypes.Create, """")]
        power_levels = room_state.get((EventTypes.PowerLevels, """"))

        if power_levels is not None:
            # We pick the local user with the highest power.
            user_power = power_levels.content.get(""users"", {})
            admin_users = [
                user_id for user_id in user_power if self.is_mine_id(user_id)
            ]
            admin_users.sort(key=lambda user: user_power[user])

            if not admin_users:
                raise SynapseError(400, ""No local admin user in room"")

            admin_user_id = None

            for admin_user in reversed(admin_users):
                if room_state.get((EventTypes.Member, admin_user)):
                    admin_user_id = admin_user
                    break

            if not admin_user_id:
                raise SynapseError(
                    400,
                    ""No local admin user in room"",
                )

            pl_content = power_levels.content
        else:
            # If there is no power level events then the creator has rights.
            pl_content = {}
            admin_user_id = create_event.sender
            if not self.is_mine_id(admin_user_id):
                raise SynapseError(
                    400,
                    ""No local admin user in room"",
                )

        # Grant the user power equal to the room admin by attempting to send an
        # updated power level event.
        new_pl_content = dict(pl_content)
        new_pl_content[""users""] = dict(pl_content.get(""users"", {}))
        new_pl_content[""users""][user_to_add] = new_pl_content[""users""][admin_user_id]

        fake_requester = create_requester(
            admin_user_id,
            authenticated_entity=requester.authenticated_entity,
        )

        try:
            await self.event_creation_handler.create_and_send_nonmember_event(
                fake_requester,
                event_dict={
                    ""content"": new_pl_content,
                    ""sender"": admin_user_id,
                    ""type"": EventTypes.PowerLevels,
                    ""state_key"": """",
                    ""room_id"": room_id,
                },
            )
        except AuthError:
            # The admin user we found turned out not to have enough power.
            raise SynapseError(
                400, ""No local admin user in room with power to update power levels.""
            )

        # Now we check if the user we're granting admin rights to is already in
        # the room. If not and it's not a public room we invite them.
        member_event = room_state.get((EventTypes.Member, user_to_add))
        is_joined = False
        if member_event:
            is_joined = member_event.content[""membership""] in (
                Membership.JOIN,
                Membership.INVITE,
            )

        if is_joined:
            return 200, {}

        join_rules = room_state.get((EventTypes.JoinRules, """"))
        is_public = False
        if join_rules:
            is_public = join_rules.content.get(""join_rule"") == JoinRules.PUBLIC

        if is_public:
            return 200, {}

        await self.room_member_handler.update_membership(
            fake_requester,
            target=UserID.from_string(user_to_add),
            room_id=room_id,
            action=Membership.INVITE,
        )

        return 200, {}","1. Use `assert_user_is_admin` to verify that the user is an administrator before granting them room admin rights.
2. Use `create_requester` to create a fake requester with the permissions of the admin user.
3. Check if the user is already in the room before inviting them."
"    def __init__(self, hs: ""HomeServer""):
        self.hs = hs
        self.auth = hs.get_auth()
        self.room_member_handler = hs.get_room_member_handler()
        self.store = hs.get_datastore()","1. Use `@staticmethod` to make the constructor inaccessible from outside the class.
2. Use `@property` to make `auth`, `room_member_handler`, and `store` read-only attributes.
3. Use `hs.get_(attribute)` to get the attribute from the `HomeServer` instance instead of directly accessing it."
"    async def on_DELETE(self, request, room_identifier):
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)

        room_id = await self.resolve_room_id(room_identifier)

        deleted_count = await self.store.delete_forward_extremities_for_room(room_id)
        return 200, {""deleted"": deleted_count}","1. Use `cryptography` to generate a random secret key and use it to sign the request.
2. Use `JWT` to create a token with the user's identity and expiry time.
3. Use `HTTP basic authentication` to authenticate the user before they can access the endpoint."
"    async def on_GET(self, request, room_identifier):
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)

        room_id = await self.resolve_room_id(room_identifier)

        extremities = await self.store.get_forward_extremities_for_room(room_id)
        return 200, {""count"": len(extremities), ""results"": extremities}","1. Use `async with` to ensure that the database connection is closed after the request is processed.
2. Use `assert_user_is_authenticated` to verify that the user is logged in before accessing the data.
3. Use `assert_user_has_permission` to verify that the user has permission to access the data."
"    def __init__(self, hs):
        super().__init__()
        self.clock = hs.get_clock()
        self.room_context_handler = hs.get_room_context_handler()
        self._event_serializer = hs.get_event_client_serializer()
        self.auth = hs.get_auth()","1. Use `hs.get_event_auth_handler()` instead of `hs.get_auth()` to get the event auth handler.
2. Use `hs.get_event_client_serializer()` to get the event serializer.
3. Use `hs.get_room_context_handler()` to get the room context handler."
"    async def on_GET(self, request, room_id, event_id):
        requester = await self.auth.get_user_by_req(request, allow_guest=False)
        await assert_user_is_admin(self.auth, requester.user)

        limit = parse_integer(request, ""limit"", default=10)

        # picking the API shape for symmetry with /messages
        filter_str = parse_string(request, b""filter"", encoding=""utf-8"")
        if filter_str:
            filter_json = urlparse.unquote(filter_str)
            event_filter = Filter(
                json_decoder.decode(filter_json)
            )  # type: Optional[Filter]
        else:
            event_filter = None

        results = await self.room_context_handler.get_event_context(
            requester,
            room_id,
            event_id,
            limit,
            event_filter,
            use_admin_priviledge=True,
        )

        if not results:
            raise SynapseError(404, ""Event not found."", errcode=Codes.NOT_FOUND)

        time_now = self.clock.time_msec()
        results[""events_before""] = await self._event_serializer.serialize_events(
            results[""events_before""], time_now
        )
        results[""event""] = await self._event_serializer.serialize_event(
            results[""event""], time_now
        )
        results[""events_after""] = await self._event_serializer.serialize_events(
            results[""events_after""], time_now
        )
        results[""state""] = await self._event_serializer.serialize_events(
            results[""state""], time_now
        )

        return 200, results","1. Use `assert_user_is_admin` to verify that the user is an admin before granting access to the event context.
2. Sanitize the `filter_str` parameter to prevent XSS attacks.
3. Use `use_admin_priviledge=True` to ensure that the user has the necessary privileges to access the event context."
"    async def resolve_room_id(self, room_identifier: str) -> str:
        """"""Resolve to a room ID, if necessary.""""""
        if RoomID.is_valid(room_identifier):
            resolved_room_id = room_identifier
        elif RoomAlias.is_valid(room_identifier):
            room_alias = RoomAlias.from_string(room_identifier)
            room_id, _ = await self.room_member_handler.lookup_room_alias(room_alias)
            resolved_room_id = room_id.to_string()
        else:
            raise SynapseError(
                400, ""%s was not legal room ID or room alias"" % (room_identifier,)
            )
        if not resolved_room_id:
            raise SynapseError(
                400, ""Unknown room ID or room alias %s"" % room_identifier
            )
        return resolved_room_id","1. Use `RoomID.from_string()` to validate the room identifier before trying to resolve it.
2. Use `await self.room_member_handler.lookup_room_alias()` to lookup the room alias and get the room ID.
3. Raise a `SynapseError` if the room ID or room alias is not valid."
"    async def _unsafe_process(self) -> None:
        # If self.pos is None then means we haven't fetched it from DB
        if self.pos is None:
            self.pos = await self.store.get_user_directory_stream_pos()

        # Loop round handling deltas until we're up to date
        while True:
            with Measure(self.clock, ""user_dir_delta""):
                room_max_stream_ordering = self.store.get_room_max_stream_ordering()
                if self.pos == room_max_stream_ordering:
                    return

                logger.debug(
                    ""Processing user stats %s->%s"", self.pos, room_max_stream_ordering
                )
                max_pos, deltas = await self.store.get_current_state_deltas(
                    self.pos, room_max_stream_ordering
                )

                logger.debug(""Handling %d state deltas"", len(deltas))
                await self._handle_deltas(deltas)

                self.pos = max_pos

                # Expose current event processing position to prometheus
                synapse.metrics.event_processing_positions.labels(""user_dir"").set(
                    max_pos
                )

                await self.store.update_user_directory_stream_pos(max_pos)","1. Use `async with` to ensure that the database connection is closed properly.
2. Use `await` to wait for the database queries to complete before continuing.
3. Use `logger.info` to log information about the progress of the process."
"    async def get_user_directory_stream_pos(self) -> int:
        return await self.db_pool.simple_select_one_onecol(
            table=""user_directory_stream_pos"",
            keyvalues={},
            retcol=""stream_id"",
            desc=""get_user_directory_stream_pos"",
        )","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to avoid errors.
3. Sanitize user input to prevent XSS attacks."
"    async def clone_existing_room(
        self,
        requester: Requester,
        old_room_id: str,
        new_room_id: str,
        new_room_version: RoomVersion,
        tombstone_event_id: str,
    ) -> None:
        """"""Populate a new room based on an old room

        Args:
            requester: the user requesting the upgrade
            old_room_id : the id of the room to be replaced
            new_room_id: the id to give the new room (should already have been
                created with _gemerate_room_id())
            new_room_version: the new room version to use
            tombstone_event_id: the ID of the tombstone event in the old room.
        """"""
        user_id = requester.user.to_string()

        if not await self.spam_checker.user_may_create_room(user_id):
            raise SynapseError(403, ""You are not permitted to create rooms"")

        creation_content = {
            ""room_version"": new_room_version.identifier,
            ""predecessor"": {""room_id"": old_room_id, ""event_id"": tombstone_event_id},
        }  # type: JsonDict

        # Check if old room was non-federatable

        # Get old room's create event
        old_room_create_event = await self.store.get_create_event_for_room(old_room_id)

        # Check if the create event specified a non-federatable room
        if not old_room_create_event.content.get(""m.federate"", True):
            # If so, mark the new room as non-federatable as well
            creation_content[""m.federate""] = False

        initial_state = {}

        # Replicate relevant room events
        types_to_copy = (
            (EventTypes.JoinRules, """"),
            (EventTypes.Name, """"),
            (EventTypes.Topic, """"),
            (EventTypes.RoomHistoryVisibility, """"),
            (EventTypes.GuestAccess, """"),
            (EventTypes.RoomAvatar, """"),
            (EventTypes.RoomEncryption, """"),
            (EventTypes.ServerACL, """"),
            (EventTypes.RelatedGroups, """"),
            (EventTypes.PowerLevels, """"),
        )

        old_room_state_ids = await self.store.get_filtered_current_state_ids(
            old_room_id, StateFilter.from_types(types_to_copy)
        )
        # map from event_id to BaseEvent
        old_room_state_events = await self.store.get_events(old_room_state_ids.values())

        for k, old_event_id in old_room_state_ids.items():
            old_event = old_room_state_events.get(old_event_id)
            if old_event:
                initial_state[k] = old_event.content

        # deep-copy the power-levels event before we start modifying it
        # note that if frozen_dicts are enabled, `power_levels` will be a frozen
        # dict so we can't just copy.deepcopy it.
        initial_state[
            (EventTypes.PowerLevels, """")
        ] = power_levels = copy_power_levels_contents(
            initial_state[(EventTypes.PowerLevels, """")]
        )

        # Resolve the minimum power level required to send any state event
        # We will give the upgrading user this power level temporarily (if necessary) such that
        # they are able to copy all of the state events over, then revert them back to their
        # original power level afterwards in _update_upgraded_room_pls

        # Copy over user power levels now as this will not be possible with >100PL users once
        # the room has been created

        # Calculate the minimum power level needed to clone the room
        event_power_levels = power_levels.get(""events"", {})
        state_default = power_levels.get(""state_default"", 0)
        ban = power_levels.get(""ban"")
        needed_power_level = max(state_default, ban, max(event_power_levels.values()))

        # Raise the requester's power level in the new room if necessary
        current_power_level = power_levels[""users""][user_id]
        if current_power_level < needed_power_level:
            power_levels[""users""][user_id] = needed_power_level

        await self._send_events_for_new_room(
            requester,
            new_room_id,
            # we expect to override all the presets with initial_state, so this is
            # somewhat arbitrary.
            preset_config=RoomCreationPreset.PRIVATE_CHAT,
            invite_list=[],
            initial_state=initial_state,
            creation_content=creation_content,
            ratelimit=False,
        )

        # Transfer membership events
        old_room_member_state_ids = await self.store.get_filtered_current_state_ids(
            old_room_id, StateFilter.from_types([(EventTypes.Member, None)])
        )

        # map from event_id to BaseEvent
        old_room_member_state_events = await self.store.get_events(
            old_room_member_state_ids.values()
        )
        for old_event in old_room_member_state_events.values():
            # Only transfer ban events
            if (
                ""membership"" in old_event.content
                and old_event.content[""membership""] == ""ban""
            ):
                await self.room_member_handler.update_membership(
                    requester,
                    UserID.from_string(old_event[""state_key""]),
                    new_room_id,
                    ""ban"",
                    ratelimit=False,
                    content=old_event.content,
                )","1. Use `copy.deepcopy` to avoid modifying the original object.
2. Use `ratelimit` to prevent abuse.
3. Sanitize user input to prevent injection attacks."
"    async def delete_pusher_by_app_id_pushkey_user_id(
        self, app_id: str, pushkey: str, user_id: str
    ) -> None:
        def delete_pusher_txn(txn, stream_id):
            self._invalidate_cache_and_stream(  # type: ignore
                txn, self.get_if_user_has_pusher, (user_id,)
            )

            self.db_pool.simple_delete_one_txn(
                txn,
                ""pushers"",
                {""app_id"": app_id, ""pushkey"": pushkey, ""user_name"": user_id},
            )

            # it's possible for us to end up with duplicate rows for
            # (app_id, pushkey, user_id) at different stream_ids, but that
            # doesn't really matter.
            self.db_pool.simple_insert_txn(
                txn,
                table=""deleted_pushers"",
                values={
                    ""stream_id"": stream_id,
                    ""app_id"": app_id,
                    ""pushkey"": pushkey,
                    ""user_id"": user_id,
                },
            )

        async with self._pushers_id_gen.get_next() as stream_id:
            await self.db_pool.runInteraction(
                ""delete_pusher"", delete_pusher_txn, stream_id
            )","1. Use prepared statements to prevent SQL injection.
2. Use a salt when hashing passwords to make them more resistant to brute-force attacks.
3. Use proper error handling to prevent leaking sensitive information."
"        def delete_pusher_txn(txn, stream_id):
            self._invalidate_cache_and_stream(  # type: ignore
                txn, self.get_if_user_has_pusher, (user_id,)
            )

            self.db_pool.simple_delete_one_txn(
                txn,
                ""pushers"",
                {""app_id"": app_id, ""pushkey"": pushkey, ""user_name"": user_id},
            )

            # it's possible for us to end up with duplicate rows for
            # (app_id, pushkey, user_id) at different stream_ids, but that
            # doesn't really matter.
            self.db_pool.simple_insert_txn(
                txn,
                table=""deleted_pushers"",
                values={
                    ""stream_id"": stream_id,
                    ""app_id"": app_id,
                    ""pushkey"": pushkey,
                    ""user_id"": user_id,
                },
            )","1. Use prepared statements instead of building queries with string concatenation. This will prevent SQL injection attacks.
2. Use `user_id` instead of `user_name` as the primary key for the `pushers` table. This will prevent users from being able to delete other users' pushers.
3. Use `ON DELETE CASCADE` on the `pushers` foreign key to the `users` table. This will ensure that any pushers that are deleted are also deleted from the `users` table."
"def sorted_topologically(
    nodes: Iterable[T], graph: Mapping[T, Collection[T]],
) -> Generator[T, None, None]:
    """"""Given a set of nodes and a graph, yield the nodes in toplogical order.

    For example `sorted_topologically([1, 2], {1: [2]})` will yield `2, 1`.
    """"""

    # This is implemented by Kahn's algorithm.

    degree_map = {node: 0 for node in nodes}
    reverse_graph = {}  # type: Dict[T, Set[T]]

    for node, edges in graph.items():
        if node not in degree_map:
            continue

        for edge in edges:
            if edge in degree_map:
                degree_map[node] += 1

            reverse_graph.setdefault(edge, set()).add(node)
        reverse_graph.setdefault(node, set())

    zero_degree = [node for node, degree in degree_map.items() if degree == 0]
    heapq.heapify(zero_degree)

    while zero_degree:
        node = heapq.heappop(zero_degree)
        yield node

        for edge in reverse_graph.get(node, []):
            if edge in degree_map:
                degree_map[edge] -= 1
                if degree_map[edge] == 0:
                    heapq.heappush(zero_degree, edge)","1. Use a secure hashing algorithm, such as SHA-256, to hash passwords.
2. Do not store passwords in plaintext.
3. Use a salt when hashing passwords to make them more resistant to attacks."
"    async def get_file(
        self,
        url: str,
        output_stream: BinaryIO,
        max_size: Optional[int] = None,
        headers: Optional[RawHeaders] = None,
    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:
        """"""GETs a file from a given URL
        Args:
            url: The URL to GET
            output_stream: File to write the response body to.
            headers: A map from header name to a list of values for that header
        Returns:
            A tuple of the file length, dict of the response
            headers, absolute URI of the response and HTTP response code.

        Raises:
            RequestTimedOutError: if there is a timeout before the response headers
               are received. Note there is currently no timeout on reading the response
               body.

            SynapseError: if the response is not a 2xx, the remote file is too large, or
               another exception happens during the download.
        """"""

        actual_headers = {b""User-Agent"": [self.user_agent]}
        if headers:
            actual_headers.update(headers)  # type: ignore

        response = await self.request(""GET"", url, headers=Headers(actual_headers))

        resp_headers = dict(response.headers.getAllRawHeaders())

        if (
            b""Content-Length"" in resp_headers
            and max_size
            and int(resp_headers[b""Content-Length""][0]) > max_size
        ):
            logger.warning(""Requested URL is too large > %r bytes"" % (max_size,))
            raise SynapseError(
                502,
                ""Requested file is too large > %r bytes"" % (max_size,),
                Codes.TOO_LARGE,
            )

        if response.code > 299:
            logger.warning(""Got %d when downloading %s"" % (response.code, url))
            raise SynapseError(502, ""Got error %d"" % (response.code,), Codes.UNKNOWN)

        # TODO: if our Content-Type is HTML or something, just read the first
        # N bytes into RAM rather than saving it all to disk only to read it
        # straight back in again

        try:
            length = await make_deferred_yieldable(
                read_body_with_max_size(response, output_stream, max_size)
            )
        except BodyExceededMaxSize:
            SynapseError(
                502,
                ""Requested file is too large > %r bytes"" % (max_size,),
                Codes.TOO_LARGE,
            )
        except Exception as e:
            raise SynapseError(502, (""Failed to download remote body: %s"" % e)) from e

        return (
            length,
            resp_headers,
            response.request.absoluteURI.decode(""ascii""),
            response.code,
        )","1. Use `verify=False` when making requests to untrusted hosts.
2. Use `allow_redirects=False` to prevent the server from redirecting the client to a malicious site.
3. Use `timeout` to specify the maximum amount of time to wait for a response from the server."
"    async def get_file(
        self,
        destination: str,
        path: str,
        output_stream,
        args: Optional[QueryArgs] = None,
        retry_on_dns_fail: bool = True,
        max_size: Optional[int] = None,
        ignore_backoff: bool = False,
    ) -> Tuple[int, Dict[bytes, List[bytes]]]:
        """"""GETs a file from a given homeserver
        Args:
            destination: The remote server to send the HTTP request to.
            path: The HTTP path to GET.
            output_stream: File to write the response body to.
            args: Optional dictionary used to create the query string.
            ignore_backoff: true to ignore the historical backoff data
                and try the request anyway.

        Returns:
            Resolves with an (int,dict) tuple of
            the file length and a dict of the response headers.

        Raises:
            HttpResponseException: If we get an HTTP response code >= 300
                (except 429).
            NotRetryingDestination: If we are not yet ready to retry this
                server.
            FederationDeniedError: If this destination  is not on our
                federation whitelist
            RequestSendFailed: If there were problems connecting to the
                remote, due to e.g. DNS failures, connection timeouts etc.
        """"""
        request = MatrixFederationRequest(
            method=""GET"", destination=destination, path=path, query=args
        )

        response = await self._send_request(
            request, retry_on_dns_fail=retry_on_dns_fail, ignore_backoff=ignore_backoff
        )

        headers = dict(response.headers.getAllRawHeaders())

        try:
            d = read_body_with_max_size(response, output_stream, max_size)
            d.addTimeout(self.default_timeout, self.reactor)
            length = await make_deferred_yieldable(d)
        except BodyExceededMaxSize:
            msg = ""Requested file is too large > %r bytes"" % (max_size,)
            logger.warning(
                ""{%s} [%s] %s"", request.txn_id, request.destination, msg,
            )
            SynapseError(502, msg, Codes.TOO_LARGE)
        except Exception as e:
            logger.warning(
                ""{%s} [%s] Error reading response: %s"",
                request.txn_id,
                request.destination,
                e,
            )
            raise
        logger.info(
            ""{%s} [%s] Completed: %d %s [%d bytes] %s %s"",
            request.txn_id,
            request.destination,
            response.code,
            response.phrase.decode(""ascii"", errors=""replace""),
            length,
            request.method,
            request.uri.decode(""ascii""),
        )
        return (length, headers)","1. Use a secure protocol (HTTPS) to transfer files.
2. Encrypt the file contents with a strong encryption algorithm.
3. Use a secure authentication method to verify the identity of the remote server."
"    async def on_PUT(self, request, user_id):
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)

        target_user = UserID.from_string(user_id)
        body = parse_json_object_from_request(request)

        if not self.hs.is_mine(target_user):
            raise SynapseError(400, ""This endpoint can only be used with local users"")

        user = await self.admin_handler.get_user(target_user)
        user_id = target_user.to_string()

        if user:  # modify user
            if ""displayname"" in body:
                await self.profile_handler.set_displayname(
                    target_user, requester, body[""displayname""], True
                )

            if ""threepids"" in body:
                # check for required parameters for each threepid
                for threepid in body[""threepids""]:
                    assert_params_in_dict(threepid, [""medium"", ""address""])

                # remove old threepids from user
                threepids = await self.store.user_get_threepids(user_id)
                for threepid in threepids:
                    try:
                        await self.auth_handler.delete_threepid(
                            user_id, threepid[""medium""], threepid[""address""], None
                        )
                    except Exception:
                        logger.exception(""Failed to remove threepids"")
                        raise SynapseError(500, ""Failed to remove threepids"")

                # add new threepids to user
                current_time = self.hs.get_clock().time_msec()
                for threepid in body[""threepids""]:
                    await self.auth_handler.add_threepid(
                        user_id, threepid[""medium""], threepid[""address""], current_time
                    )

            if ""avatar_url"" in body and type(body[""avatar_url""]) == str:
                await self.profile_handler.set_avatar_url(
                    target_user, requester, body[""avatar_url""], True
                )

            if ""admin"" in body:
                set_admin_to = bool(body[""admin""])
                if set_admin_to != user[""admin""]:
                    auth_user = requester.user
                    if target_user == auth_user and not set_admin_to:
                        raise SynapseError(400, ""You may not demote yourself."")

                    await self.store.set_server_admin(target_user, set_admin_to)

            if ""password"" in body:
                if not isinstance(body[""password""], str) or len(body[""password""]) > 512:
                    raise SynapseError(400, ""Invalid password"")
                else:
                    new_password = body[""password""]
                    logout_devices = True

                    new_password_hash = await self.auth_handler.hash(new_password)

                    await self.set_password_handler.set_password(
                        target_user.to_string(),
                        new_password_hash,
                        logout_devices,
                        requester,
                    )

            if ""deactivated"" in body:
                deactivate = body[""deactivated""]
                if not isinstance(deactivate, bool):
                    raise SynapseError(
                        400, ""'deactivated' parameter is not of type boolean""
                    )

                if deactivate and not user[""deactivated""]:
                    await self.deactivate_account_handler.deactivate_account(
                        target_user.to_string(), False
                    )
                elif not deactivate and user[""deactivated""]:
                    if ""password"" not in body:
                        raise SynapseError(
                            400, ""Must provide a password to re-activate an account.""
                        )

                    await self.deactivate_account_handler.activate_account(
                        target_user.to_string()
                    )

            user = await self.admin_handler.get_user(target_user)
            return 200, user

        else:  # create user
            password = body.get(""password"")
            password_hash = None
            if password is not None:
                if not isinstance(password, str) or len(password) > 512:
                    raise SynapseError(400, ""Invalid password"")
                password_hash = await self.auth_handler.hash(password)

            admin = body.get(""admin"", None)
            user_type = body.get(""user_type"", None)
            displayname = body.get(""displayname"", None)

            if user_type is not None and user_type not in UserTypes.ALL_USER_TYPES:
                raise SynapseError(400, ""Invalid user type"")

            user_id = await self.registration_handler.register_user(
                localpart=target_user.localpart,
                password_hash=password_hash,
                admin=bool(admin),
                default_display_name=displayname,
                user_type=user_type,
                by_admin=True,
            )

            if ""threepids"" in body:
                # check for required parameters for each threepid
                for threepid in body[""threepids""]:
                    assert_params_in_dict(threepid, [""medium"", ""address""])

                current_time = self.hs.get_clock().time_msec()
                for threepid in body[""threepids""]:
                    await self.auth_handler.add_threepid(
                        user_id, threepid[""medium""], threepid[""address""], current_time
                    )
                    if (
                        self.hs.config.email_enable_notifs
                        and self.hs.config.email_notif_for_new_users
                    ):
                        await self.pusher_pool.add_pusher(
                            user_id=user_id,
                            access_token=None,
                            kind=""email"",
                            app_id=""m.email"",
                            app_display_name=""Email Notifications"",
                            device_display_name=threepid[""address""],
                            pushkey=threepid[""address""],
                            lang=None,  # We don't know a user's language here
                            data={},
                        )

            if ""avatar_url"" in body and type(body[""avatar_url""]) == str:
                await self.profile_handler.set_avatar_url(
                    user_id, requester, body[""avatar_url""], True
                )

            ret = await self.admin_handler.get_user(target_user)

            return 201, ret","1. Use `assert_user_is_admin` to check if the user is an admin before updating the user's information.
2. Sanitize the input data to prevent XSS attacks.
3. Use `hash` to securely store the user's password."
"def start(hs: ""synapse.server.HomeServer"", listeners: Iterable[ListenerConfig]):
    """"""
    Start a Synapse server or worker.

    Should be called once the reactor is running and (if we're using ACME) the
    TLS certificates are in place.

    Will start the main HTTP listeners and do some other startup tasks, and then
    notify systemd.

    Args:
        hs: homeserver instance
        listeners: Listener configuration ('listeners' in homeserver.yaml)
    """"""
    try:
        # Set up the SIGHUP machinery.
        if hasattr(signal, ""SIGHUP""):

            def handle_sighup(*args, **kwargs):
                # Tell systemd our state, if we're using it. This will silently fail if
                # we're not using systemd.
                sdnotify(b""RELOADING=1"")

                for i, args, kwargs in _sighup_callbacks:
                    i(*args, **kwargs)

                sdnotify(b""READY=1"")

            signal.signal(signal.SIGHUP, handle_sighup)

            register_sighup(refresh_certificate, hs)

        # Load the certificate from disk.
        refresh_certificate(hs)

        # Start the tracer
        synapse.logging.opentracing.init_tracer(  # type: ignore[attr-defined] # noqa
            hs
        )

        # It is now safe to start your Synapse.
        hs.start_listening(listeners)
        hs.get_datastore().db_pool.start_profiling()
        hs.get_pusherpool().start()

        # Log when we start the shut down process.
        hs.get_reactor().addSystemEventTrigger(
            ""before"", ""shutdown"", logger.info, ""Shutting down...""
        )

        setup_sentry(hs)
        setup_sdnotify(hs)

        # If background tasks are running on the main process, start collecting the
        # phone home stats.
        if hs.config.run_background_tasks:
            start_phone_stats_home(hs)

        # We now freeze all allocated objects in the hopes that (almost)
        # everything currently allocated are things that will be used for the
        # rest of time. Doing so means less work each GC (hopefully).
        #
        # This only works on Python 3.7
        if sys.version_info >= (3, 7):
            gc.collect()
            gc.freeze()
    except Exception:
        traceback.print_exc(file=sys.stderr)
        reactor = hs.get_reactor()
        if reactor.running:
            reactor.stop()
        sys.exit(1)","1. Use `os.path.join` to concatenate paths instead of string concatenation.
2. Use `pwd.getpwuid` to get the username from uid instead of string comparison.
3. Use `json.dumps` to serialize objects instead of `str`."
"    async def get_profile(self, user_id: str) -> JsonDict:
        target_user = UserID.from_string(user_id)

        if self.hs.is_mine(target_user):
            try:
                displayname = await self.store.get_profile_displayname(
                    target_user.localpart
                )
                avatar_url = await self.store.get_profile_avatar_url(
                    target_user.localpart
                )
            except StoreError as e:
                if e.code == 404:
                    raise SynapseError(404, ""Profile was not found"", Codes.NOT_FOUND)
                raise

            return {""displayname"": displayname, ""avatar_url"": avatar_url}
        else:
            try:
                result = await self.federation.make_query(
                    destination=target_user.domain,
                    query_type=""profile"",
                    args={""user_id"": user_id},
                    ignore_backoff=True,
                )
                return result
            except RequestSendFailed as e:
                raise SynapseError(502, ""Failed to fetch profile"") from e
            except HttpResponseException as e:
                raise e.to_synapse_error()","1. Use `check_user_id_in_domain` to check if the user ID is owned by the current server.
2. Use `make_request` to make requests to remote servers.
3. Handle errors properly by catching `StoreError`, `RequestSendFailed`, and `HttpResponseException`."
"def respond_with_json_bytes(
    request: Request, code: int, json_bytes: bytes, send_cors: bool = False,
):
    """"""Sends encoded JSON in response to the given request.

    Args:
        request: The http request to respond to.
        code: The HTTP response code.
        json_bytes: The json bytes to use as the response body.
        send_cors: Whether to send Cross-Origin Resource Sharing headers
            https://fetch.spec.whatwg.org/#http-cors-protocol

    Returns:
        twisted.web.server.NOT_DONE_YET if the request is still active.
    """"""

    request.setResponseCode(code)
    request.setHeader(b""Content-Type"", b""application/json"")
    request.setHeader(b""Content-Length"", b""%d"" % (len(json_bytes),))
    request.setHeader(b""Cache-Control"", b""no-cache, no-store, must-revalidate"")

    if send_cors:
        set_cors_headers(request)

    # note that this is zero-copy (the bytesio shares a copy-on-write buffer with
    # the original `bytes`).
    bytes_io = BytesIO(json_bytes)

    producer = NoRangeStaticProducer(request, bytes_io)
    producer.start()
    return NOT_DONE_YET","1. Use `json.dumps` to encode the JSON data instead of `bytes()`.
2. Set the `Access-Control-Allow-Origin` header to a specific domain or wildcard value.
3. Use `corsheaders` to automatically set the CORS headers."
"async def respond_with_responder(
    request, responder, media_type, file_size, upload_name=None
):
    """"""Responds to the request with given responder. If responder is None then
    returns 404.

    Args:
        request (twisted.web.http.Request)
        responder (Responder|None)
        media_type (str): The media/content type.
        file_size (int|None): Size in bytes of the media. If not known it should be None
        upload_name (str|None): The name of the requested file, if any.
    """"""
    if not responder:
        respond_404(request)
        return

    logger.debug(""Responding to media request with responder %s"", responder)
    add_file_headers(request, media_type, file_size, upload_name)
    try:
        with responder:
            await responder.write_to_consumer(request)
    except Exception as e:
        # The majority of the time this will be due to the client having gone
        # away. Unfortunately, Twisted simply throws a generic exception at us
        # in that case.
        logger.warning(""Failed to write to consumer: %s %s"", type(e), e)

        # Unregister the producer, if it has one, so Twisted doesn't complain
        if request.producer:
            request.unregisterProducer()

    finish_request(request)","1. Use `isinstance` to check if `responder` is a `Responder` object before calling `responder.write_to_consumer()`.
2. Use `request.setHeader()` to set the `Content-Type` and `Content-Length` headers.
3. Close the `responder` object after writing to the consumer."
"def check_redaction(
    room_version_obj: RoomVersion, event: EventBase, auth_events: StateMap[EventBase],
) -> bool:
    """"""Check whether the event sender is allowed to redact the target event.

    Returns:
        True if the the sender is allowed to redact the target event if the
        target event was created by them.
        False if the sender is allowed to redact the target event with no
        further checks.

    Raises:
        AuthError if the event sender is definitely not allowed to redact
        the target event.
    """"""
    user_level = get_user_power_level(event.user_id, auth_events)

    redact_level = _get_named_level(auth_events, ""redact"", 50)

    if user_level >= redact_level:
        return False

    if room_version_obj.event_format == EventFormatVersions.V1:
        redacter_domain = get_domain_from_id(event.event_id)
        redactee_domain = get_domain_from_id(event.redacts)
        if redacter_domain == redactee_domain:
            return True
    else:
        event.internal_metadata.recheck_redaction = True
        return True

    raise AuthError(403, ""You don't have permission to redact events"")","1. Use event format version 2 or later.
2. Check the user's power level before allowing them to redact an event.
3. Raise an AuthError if the user is not allowed to redact the event."
"    async def copy_room_tags_and_direct_to_room(
        self, old_room_id, new_room_id, user_id
    ) -> None:
        """"""Copies the tags and direct room state from one room to another.

        Args:
            old_room_id: The room ID of the old room.
            new_room_id: The room ID of the new room.
            user_id: The user's ID.
        """"""
        # Retrieve user account data for predecessor room
        user_account_data, _ = await self.store.get_account_data_for_user(user_id)

        # Copy direct message state if applicable
        direct_rooms = user_account_data.get(""m.direct"", {})

        # Check which key this room is under
        if isinstance(direct_rooms, dict):
            for key, room_id_list in direct_rooms.items():
                if old_room_id in room_id_list and new_room_id not in room_id_list:
                    # Add new room_id to this key
                    direct_rooms[key].append(new_room_id)

                    # Save back to user's m.direct account data
                    await self.store.add_account_data_for_user(
                        user_id, ""m.direct"", direct_rooms
                    )
                    break

        # Copy room tags if applicable
        room_tags = await self.store.get_tags_for_room(user_id, old_room_id)

        # Copy each room tag to the new room
        for tag, tag_content in room_tags.items():
            await self.store.add_tag_to_room(user_id, new_room_id, tag, tag_content)","1. Use `get_account_data_for_user` with `user_id` as the key to get the user's account data.
2. Use `get_tags_for_room` with `user_id` and `room_id` as the keys to get the room tags.
3. Use `add_tag_to_room` with `user_id`, `room_id`, `tag`, and `tag_content` as the keys to add the room tag."
"    async def _generate_sync_entry_for_rooms(
        self,
        sync_result_builder: ""SyncResultBuilder"",
        account_data_by_room: Dict[str, Dict[str, JsonDict]],
    ) -> Tuple[Set[str], Set[str], Set[str], Set[str]]:
        """"""Generates the rooms portion of the sync response. Populates the
        `sync_result_builder` with the result.

        Args:
            sync_result_builder
            account_data_by_room: Dictionary of per room account data

        Returns:
            Returns a 4-tuple of
            `(newly_joined_rooms, newly_joined_or_invited_users,
            newly_left_rooms, newly_left_users)`
        """"""
        user_id = sync_result_builder.sync_config.user.to_string()
        block_all_room_ephemeral = (
            sync_result_builder.since_token is None
            and sync_result_builder.sync_config.filter_collection.blocks_all_room_ephemeral()
        )

        if block_all_room_ephemeral:
            ephemeral_by_room = {}  # type: Dict[str, List[JsonDict]]
        else:
            now_token, ephemeral_by_room = await self.ephemeral_by_room(
                sync_result_builder,
                now_token=sync_result_builder.now_token,
                since_token=sync_result_builder.since_token,
            )
            sync_result_builder.now_token = now_token

        # We check up front if anything has changed, if it hasn't then there is
        # no point in going further.
        since_token = sync_result_builder.since_token
        if not sync_result_builder.full_state:
            if since_token and not ephemeral_by_room and not account_data_by_room:
                have_changed = await self._have_rooms_changed(sync_result_builder)
                if not have_changed:
                    tags_by_room = await self.store.get_updated_tags(
                        user_id, since_token.account_data_key
                    )
                    if not tags_by_room:
                        logger.debug(""no-oping sync"")
                        return set(), set(), set(), set()

        ignored_account_data = await self.store.get_global_account_data_by_type_for_user(
            ""m.ignored_user_list"", user_id=user_id
        )

        if ignored_account_data:
            ignored_users = ignored_account_data.get(""ignored_users"", {}).keys()
        else:
            ignored_users = frozenset()

        if since_token:
            room_changes = await self._get_rooms_changed(
                sync_result_builder, ignored_users
            )
            tags_by_room = await self.store.get_updated_tags(
                user_id, since_token.account_data_key
            )
        else:
            room_changes = await self._get_all_rooms(sync_result_builder, ignored_users)

            tags_by_room = await self.store.get_tags_for_user(user_id)

        room_entries = room_changes.room_entries
        invited = room_changes.invited
        newly_joined_rooms = room_changes.newly_joined_rooms
        newly_left_rooms = room_changes.newly_left_rooms

        async def handle_room_entries(room_entry):
            logger.debug(""Generating room entry for %s"", room_entry.room_id)
            res = await self._generate_room_entry(
                sync_result_builder,
                ignored_users,
                room_entry,
                ephemeral=ephemeral_by_room.get(room_entry.room_id, []),
                tags=tags_by_room.get(room_entry.room_id),
                account_data=account_data_by_room.get(room_entry.room_id, {}),
                always_include=sync_result_builder.full_state,
            )
            logger.debug(""Generated room entry for %s"", room_entry.room_id)
            return res

        await concurrently_execute(handle_room_entries, room_entries, 10)

        sync_result_builder.invited.extend(invited)

        # Now we want to get any newly joined or invited users
        newly_joined_or_invited_users = set()
        newly_left_users = set()
        if since_token:
            for joined_sync in sync_result_builder.joined:
                it = itertools.chain(
                    joined_sync.timeline.events, joined_sync.state.values()
                )
                for event in it:
                    if event.type == EventTypes.Member:
                        if (
                            event.membership == Membership.JOIN
                            or event.membership == Membership.INVITE
                        ):
                            newly_joined_or_invited_users.add(event.state_key)
                        else:
                            prev_content = event.unsigned.get(""prev_content"", {})
                            prev_membership = prev_content.get(""membership"", None)
                            if prev_membership == Membership.JOIN:
                                newly_left_users.add(event.state_key)

        newly_left_users -= newly_joined_or_invited_users

        return (
            set(newly_joined_rooms),
            newly_joined_or_invited_users,
            set(newly_left_rooms),
            newly_left_users,
        )","1. Use `await` instead of `asyncio.ensure_future` to avoid `gather`.
2. Use `contextlib.closing` to ensure that the database connection is closed after use.
3. Use `typing` to annotate the function parameters and return values."
"    async def _get_rooms_changed(
        self, sync_result_builder: ""SyncResultBuilder"", ignored_users: Set[str]
    ) -> _RoomChanges:
        """"""Gets the the changes that have happened since the last sync.
        """"""
        user_id = sync_result_builder.sync_config.user.to_string()
        since_token = sync_result_builder.since_token
        now_token = sync_result_builder.now_token
        sync_config = sync_result_builder.sync_config

        assert since_token

        # Get a list of membership change events that have happened.
        rooms_changed = await self.store.get_membership_changes_for_user(
            user_id, since_token.room_key, now_token.room_key
        )

        mem_change_events_by_room_id = {}  # type: Dict[str, List[EventBase]]
        for event in rooms_changed:
            mem_change_events_by_room_id.setdefault(event.room_id, []).append(event)

        newly_joined_rooms = []
        newly_left_rooms = []
        room_entries = []
        invited = []
        for room_id, events in mem_change_events_by_room_id.items():
            logger.debug(
                ""Membership changes in %s: [%s]"",
                room_id,
                "", "".join((""%s (%s)"" % (e.event_id, e.membership) for e in events)),
            )

            non_joins = [e for e in events if e.membership != Membership.JOIN]
            has_join = len(non_joins) != len(events)

            # We want to figure out if we joined the room at some point since
            # the last sync (even if we have since left). This is to make sure
            # we do send down the room, and with full state, where necessary

            old_state_ids = None
            if room_id in sync_result_builder.joined_room_ids and non_joins:
                # Always include if the user (re)joined the room, especially
                # important so that device list changes are calculated correctly.
                # If there are non-join member events, but we are still in the room,
                # then the user must have left and joined
                newly_joined_rooms.append(room_id)

                # User is in the room so we don't need to do the invite/leave checks
                continue

            if room_id in sync_result_builder.joined_room_ids or has_join:
                old_state_ids = await self.get_state_at(room_id, since_token)
                old_mem_ev_id = old_state_ids.get((EventTypes.Member, user_id), None)
                old_mem_ev = None
                if old_mem_ev_id:
                    old_mem_ev = await self.store.get_event(
                        old_mem_ev_id, allow_none=True
                    )

                # debug for #4422
                if has_join:
                    prev_membership = None
                    if old_mem_ev:
                        prev_membership = old_mem_ev.membership
                    issue4422_logger.debug(
                        ""Previous membership for room %s with join: %s (event %s)"",
                        room_id,
                        prev_membership,
                        old_mem_ev_id,
                    )

                if not old_mem_ev or old_mem_ev.membership != Membership.JOIN:
                    newly_joined_rooms.append(room_id)

            # If user is in the room then we don't need to do the invite/leave checks
            if room_id in sync_result_builder.joined_room_ids:
                continue

            if not non_joins:
                continue

            # Check if we have left the room. This can either be because we were
            # joined before *or* that we since joined and then left.
            if events[-1].membership != Membership.JOIN:
                if has_join:
                    newly_left_rooms.append(room_id)
                else:
                    if not old_state_ids:
                        old_state_ids = await self.get_state_at(room_id, since_token)
                        old_mem_ev_id = old_state_ids.get(
                            (EventTypes.Member, user_id), None
                        )
                        old_mem_ev = None
                        if old_mem_ev_id:
                            old_mem_ev = await self.store.get_event(
                                old_mem_ev_id, allow_none=True
                            )
                    if old_mem_ev and old_mem_ev.membership == Membership.JOIN:
                        newly_left_rooms.append(room_id)

            # Only bother if we're still currently invited
            should_invite = non_joins[-1].membership == Membership.INVITE
            if should_invite:
                if event.sender not in ignored_users:
                    room_sync = InvitedSyncResult(room_id, invite=non_joins[-1])
                    if room_sync:
                        invited.append(room_sync)

            # Always include leave/ban events. Just take the last one.
            # TODO: How do we handle ban -> leave in same batch?
            leave_events = [
                e
                for e in non_joins
                if e.membership in (Membership.LEAVE, Membership.BAN)
            ]

            if leave_events:
                leave_event = leave_events[-1]
                leave_position = await self.store.get_position_for_event(
                    leave_event.event_id
                )

                # If the leave event happened before the since token then we
                # bail.
                if since_token and not leave_position.persisted_after(
                    since_token.room_key
                ):
                    continue

                # We can safely convert the position of the leave event into a
                # stream token as it'll only be used in the context of this
                # room. (c.f. the docstring of `to_room_stream_token`).
                leave_token = since_token.copy_and_replace(
                    ""room_key"", leave_position.to_room_stream_token()
                )

                # If this is an out of band message, like a remote invite
                # rejection, we include it in the recents batch. Otherwise, we
                # let _load_filtered_recents handle fetching the correct
                # batches.
                #
                # This is all screaming out for a refactor, as the logic here is
                # subtle and the moving parts numerous.
                if leave_event.internal_metadata.is_out_of_band_membership():
                    batch_events = [leave_event]  # type: Optional[List[EventBase]]
                else:
                    batch_events = None

                room_entries.append(
                    RoomSyncResultBuilder(
                        room_id=room_id,
                        rtype=""archived"",
                        events=batch_events,
                        newly_joined=room_id in newly_joined_rooms,
                        full_state=False,
                        since_token=since_token,
                        upto_token=leave_token,
                    )
                )

        timeline_limit = sync_config.filter_collection.timeline_limit()

        # Get all events for rooms we're currently joined to.
        room_to_events = await self.store.get_room_events_stream_for_rooms(
            room_ids=sync_result_builder.joined_room_ids,
            from_key=since_token.room_key,
            to_key=now_token.room_key,
            limit=timeline_limit + 1,
        )

        # We loop through all room ids, even if there are no new events, in case
        # there are non room events that we need to notify about.
        for room_id in sync_result_builder.joined_room_ids:
            room_entry = room_to_events.get(room_id, None)

            newly_joined = room_id in newly_joined_rooms
            if room_entry:
                events, start_key = room_entry

                prev_batch_token = now_token.copy_and_replace(""room_key"", start_key)

                entry = RoomSyncResultBuilder(
                    room_id=room_id,
                    rtype=""joined"",
                    events=events,
                    newly_joined=newly_joined,
                    full_state=False,
                    since_token=None if newly_joined else since_token,
                    upto_token=prev_batch_token,
                )
            else:
                entry = RoomSyncResultBuilder(
                    room_id=room_id,
                    rtype=""joined"",
                    events=[],
                    newly_joined=newly_joined,
                    full_state=False,
                    since_token=since_token,
                    upto_token=since_token,
                )

            if newly_joined:
                # debugging for https://github.com/matrix-org/synapse/issues/4422
                issue4422_logger.debug(
                    ""RoomSyncResultBuilder events for newly joined room %s: %r"",
                    room_id,
                    entry.events,
                )
            room_entries.append(entry)

        return _RoomChanges(room_entries, invited, newly_joined_rooms, newly_left_rooms)","1. Use `allow_none=True` when getting events from the store, to avoid errors when the event does not exist.
2. Use `copy_and_replace()` to create a new token with the updated room key, to avoid accidentally using the old token.
3. Use `to_room_stream_token()` to convert the position of the leave event into a stream token, to ensure that it is valid."
"    async def _get_all_rooms(
        self, sync_result_builder: ""SyncResultBuilder"", ignored_users: Set[str]
    ) -> _RoomChanges:
        """"""Returns entries for all rooms for the user.

        Args:
            sync_result_builder
            ignored_users: Set of users ignored by user.

        """"""

        user_id = sync_result_builder.sync_config.user.to_string()
        since_token = sync_result_builder.since_token
        now_token = sync_result_builder.now_token
        sync_config = sync_result_builder.sync_config

        membership_list = (
            Membership.INVITE,
            Membership.JOIN,
            Membership.LEAVE,
            Membership.BAN,
        )

        room_list = await self.store.get_rooms_for_local_user_where_membership_is(
            user_id=user_id, membership_list=membership_list
        )

        room_entries = []
        invited = []

        for event in room_list:
            if event.membership == Membership.JOIN:
                room_entries.append(
                    RoomSyncResultBuilder(
                        room_id=event.room_id,
                        rtype=""joined"",
                        events=None,
                        newly_joined=False,
                        full_state=True,
                        since_token=since_token,
                        upto_token=now_token,
                    )
                )
            elif event.membership == Membership.INVITE:
                if event.sender in ignored_users:
                    continue
                invite = await self.store.get_event(event.event_id)
                invited.append(InvitedSyncResult(room_id=event.room_id, invite=invite))
            elif event.membership in (Membership.LEAVE, Membership.BAN):
                # Always send down rooms we were banned or kicked from.
                if not sync_config.filter_collection.include_leave:
                    if event.membership == Membership.LEAVE:
                        if user_id == event.sender:
                            continue

                leave_token = now_token.copy_and_replace(
                    ""room_key"", RoomStreamToken(None, event.stream_ordering)
                )
                room_entries.append(
                    RoomSyncResultBuilder(
                        room_id=event.room_id,
                        rtype=""archived"",
                        events=None,
                        newly_joined=False,
                        full_state=True,
                        since_token=since_token,
                        upto_token=leave_token,
                    )
                )

        return _RoomChanges(room_entries, invited, [], [])","1. Use `user_id` instead of `user.to_string()` to avoid leaking user information.
2. Use `store.get_rooms_for_local_user_where_membership_is()` to get rooms for the user, instead of hard-coding the list of memberships.
3. Use `InvitedSyncResult` to represent invited rooms, instead of using a raw `event` object."
"    async def _generate_room_entry(
        self,
        sync_result_builder: ""SyncResultBuilder"",
        ignored_users: Set[str],
        room_builder: ""RoomSyncResultBuilder"",
        ephemeral: List[JsonDict],
        tags: Optional[Dict[str, Dict[str, Any]]],
        account_data: Dict[str, JsonDict],
        always_include: bool = False,
    ):
        """"""Populates the `joined` and `archived` section of `sync_result_builder`
        based on the `room_builder`.

        Args:
            sync_result_builder
            ignored_users: Set of users ignored by user.
            room_builder
            ephemeral: List of new ephemeral events for room
            tags: List of *all* tags for room, or None if there has been
                no change.
            account_data: List of new account data for room
            always_include: Always include this room in the sync response,
                even if empty.
        """"""
        newly_joined = room_builder.newly_joined
        full_state = (
            room_builder.full_state or newly_joined or sync_result_builder.full_state
        )
        events = room_builder.events

        # We want to shortcut out as early as possible.
        if not (always_include or account_data or ephemeral or full_state):
            if events == [] and tags is None:
                return

        now_token = sync_result_builder.now_token
        sync_config = sync_result_builder.sync_config

        room_id = room_builder.room_id
        since_token = room_builder.since_token
        upto_token = room_builder.upto_token

        batch = await self._load_filtered_recents(
            room_id,
            sync_config,
            now_token=upto_token,
            since_token=since_token,
            potential_recents=events,
            newly_joined_room=newly_joined,
        )

        # Note: `batch` can be both empty and limited here in the case where
        # `_load_filtered_recents` can't find any events the user should see
        # (e.g. due to having ignored the sender of the last 50 events).

        if newly_joined:
            # debug for https://github.com/matrix-org/synapse/issues/4422
            issue4422_logger.debug(
                ""Timeline events after filtering in newly-joined room %s: %r"",
                room_id,
                batch,
            )

        # When we join the room (or the client requests full_state), we should
        # send down any existing tags. Usually the user won't have tags in a
        # newly joined room, unless either a) they've joined before or b) the
        # tag was added by synapse e.g. for server notice rooms.
        if full_state:
            user_id = sync_result_builder.sync_config.user.to_string()
            tags = await self.store.get_tags_for_room(user_id, room_id)

            # If there aren't any tags, don't send the empty tags list down
            # sync
            if not tags:
                tags = None

        account_data_events = []
        if tags is not None:
            account_data_events.append({""type"": ""m.tag"", ""content"": {""tags"": tags}})

        for account_data_type, content in account_data.items():
            account_data_events.append({""type"": account_data_type, ""content"": content})

        account_data_events = sync_config.filter_collection.filter_room_account_data(
            account_data_events
        )

        ephemeral = sync_config.filter_collection.filter_room_ephemeral(ephemeral)

        if not (
            always_include or batch or account_data_events or ephemeral or full_state
        ):
            return

        state = await self.compute_state_delta(
            room_id, batch, sync_config, since_token, now_token, full_state=full_state
        )

        summary = {}  # type: Optional[JsonDict]

        # we include a summary in room responses when we're lazy loading
        # members (as the client otherwise doesn't have enough info to form
        # the name itself).
        if sync_config.filter_collection.lazy_load_members() and (
            # we recalulate the summary:
            #   if there are membership changes in the timeline, or
            #   if membership has changed during a gappy sync, or
            #   if this is an initial sync.
            any(ev.type == EventTypes.Member for ev in batch.events)
            or (
                # XXX: this may include false positives in the form of LL
                # members which have snuck into state
                batch.limited
                and any(t == EventTypes.Member for (t, k) in state)
            )
            or since_token is None
        ):
            summary = await self.compute_summary(
                room_id, sync_config, batch, state, now_token
            )

        if room_builder.rtype == ""joined"":
            unread_notifications = {}  # type: Dict[str, int]
            room_sync = JoinedSyncResult(
                room_id=room_id,
                timeline=batch,
                state=state,
                ephemeral=ephemeral,
                account_data=account_data_events,
                unread_notifications=unread_notifications,
                summary=summary,
                unread_count=0,
            )

            if room_sync or always_include:
                notifs = await self.unread_notifs_for_room_id(room_id, sync_config)

                unread_notifications[""notification_count""] = notifs[""notify_count""]
                unread_notifications[""highlight_count""] = notifs[""highlight_count""]

                room_sync.unread_count = notifs[""unread_count""]

                sync_result_builder.joined.append(room_sync)

            if batch.limited and since_token:
                user_id = sync_result_builder.sync_config.user.to_string()
                logger.debug(
                    ""Incremental gappy sync of %s for user %s with %d state events""
                    % (room_id, user_id, len(state))
                )
        elif room_builder.rtype == ""archived"":
            archived_room_sync = ArchivedSyncResult(
                room_id=room_id,
                timeline=batch,
                state=state,
                account_data=account_data_events,
            )
            if archived_room_sync or always_include:
                sync_result_builder.archived.append(archived_room_sync)
        else:
            raise Exception(""Unrecognized rtype: %r"", room_builder.rtype)","1. Use `sync_config.filter_collection.filter_room_account_data()` to filter room account data.
2. Use `sync_config.filter_collection.filter_room_ephemeral()` to filter room ephemeral.
3. Use `sync_config.filter_collection.lazy_load_members()` to lazy load members."
"    async def is_ignored_by(
        self, ignored_user_id: str, ignorer_user_id: str, cache_context: _CacheContext
    ) -> bool:
        ignored_account_data = await self.get_global_account_data_by_type_for_user(
            ""m.ignored_user_list"",
            ignorer_user_id,
            on_invalidate=cache_context.invalidate,
        )
        if not ignored_account_data:
            return False

        return ignored_user_id in ignored_account_data.get(""ignored_users"", {})","1. Use `check_user_id` to verify that the user IDs are valid.
2. Use `get_global_account_data_by_type_for_user` to get the ignored user list for the ignorer user.
3. Check if the ignored user ID is in the ignored user list."
"async def filter_events_for_client(
    storage: Storage,
    user_id,
    events,
    is_peeking=False,
    always_include_ids=frozenset(),
    filter_send_to_client=True,
):
    """"""
    Check which events a user is allowed to see. If the user can see the event but its
    sender asked for their data to be erased, prune the content of the event.

    Args:
        storage
        user_id(str): user id to be checked
        events(list[synapse.events.EventBase]): sequence of events to be checked
        is_peeking(bool): should be True if:
          * the user is not currently a member of the room, and:
          * the user has not been a member of the room since the given
            events
        always_include_ids (set(event_id)): set of event ids to specifically
            include (unless sender is ignored)
        filter_send_to_client (bool): Whether we're checking an event that's going to be
            sent to a client. This might not always be the case since this function can
            also be called to check whether a user can see the state at a given point.

    Returns:
        list[synapse.events.EventBase]
    """"""
    # Filter out events that have been soft failed so that we don't relay them
    # to clients.
    events = [e for e in events if not e.internal_metadata.is_soft_failed()]

    types = ((EventTypes.RoomHistoryVisibility, """"), (EventTypes.Member, user_id))
    event_id_to_state = await storage.state.get_state_for_events(
        frozenset(e.event_id for e in events),
        state_filter=StateFilter.from_types(types),
    )

    ignore_dict_content = await storage.main.get_global_account_data_by_type_for_user(
        ""m.ignored_user_list"", user_id
    )

    # FIXME: This will explode if people upload something incorrect.
    ignore_list = frozenset(
        ignore_dict_content.get(""ignored_users"", {}).keys()
        if ignore_dict_content
        else []
    )

    erased_senders = await storage.main.are_users_erased((e.sender for e in events))

    if filter_send_to_client:
        room_ids = {e.room_id for e in events}
        retention_policies = {}

        for room_id in room_ids:
            retention_policies[
                room_id
            ] = await storage.main.get_retention_policy_for_room(room_id)

    def allowed(event):
        """"""
        Args:
            event (synapse.events.EventBase): event to check

        Returns:
            None|EventBase:
               None if the user cannot see this event at all

               a redacted copy of the event if they can only see a redacted
               version

               the original event if they can see it as normal.
        """"""
        # Only run some checks if these events aren't about to be sent to clients. This is
        # because, if this is not the case, we're probably only checking if the users can
        # see events in the room at that point in the DAG, and that shouldn't be decided
        # on those checks.
        if filter_send_to_client:
            if event.type == ""org.matrix.dummy_event"":
                return None

            if not event.is_state() and event.sender in ignore_list:
                return None

            # Until MSC2261 has landed we can't redact malicious alias events, so for
            # now we temporarily filter out m.room.aliases entirely to mitigate
            # abuse, while we spec a better solution to advertising aliases
            # on rooms.
            if event.type == EventTypes.Aliases:
                return None

            # Don't try to apply the room's retention policy if the event is a state
            # event, as MSC1763 states that retention is only considered for non-state
            # events.
            if not event.is_state():
                retention_policy = retention_policies[event.room_id]
                max_lifetime = retention_policy.get(""max_lifetime"")

                if max_lifetime is not None:
                    oldest_allowed_ts = storage.main.clock.time_msec() - max_lifetime

                    if event.origin_server_ts < oldest_allowed_ts:
                        return None

        if event.event_id in always_include_ids:
            return event

        state = event_id_to_state[event.event_id]

        # get the room_visibility at the time of the event.
        visibility_event = state.get((EventTypes.RoomHistoryVisibility, """"), None)
        if visibility_event:
            visibility = visibility_event.content.get(""history_visibility"", ""shared"")
        else:
            visibility = ""shared""

        if visibility not in VISIBILITY_PRIORITY:
            visibility = ""shared""

        # Always allow history visibility events on boundaries. This is done
        # by setting the effective visibility to the least restrictive
        # of the old vs new.
        if event.type == EventTypes.RoomHistoryVisibility:
            prev_content = event.unsigned.get(""prev_content"", {})
            prev_visibility = prev_content.get(""history_visibility"", None)

            if prev_visibility not in VISIBILITY_PRIORITY:
                prev_visibility = ""shared""

            new_priority = VISIBILITY_PRIORITY.index(visibility)
            old_priority = VISIBILITY_PRIORITY.index(prev_visibility)
            if old_priority < new_priority:
                visibility = prev_visibility

        # likewise, if the event is the user's own membership event, use
        # the 'most joined' membership
        membership = None
        if event.type == EventTypes.Member and event.state_key == user_id:
            membership = event.content.get(""membership"", None)
            if membership not in MEMBERSHIP_PRIORITY:
                membership = ""leave""

            prev_content = event.unsigned.get(""prev_content"", {})
            prev_membership = prev_content.get(""membership"", None)
            if prev_membership not in MEMBERSHIP_PRIORITY:
                prev_membership = ""leave""

            # Always allow the user to see their own leave events, otherwise
            # they won't see the room disappear if they reject the invite
            if membership == ""leave"" and (
                prev_membership == ""join"" or prev_membership == ""invite""
            ):
                return event

            new_priority = MEMBERSHIP_PRIORITY.index(membership)
            old_priority = MEMBERSHIP_PRIORITY.index(prev_membership)
            if old_priority < new_priority:
                membership = prev_membership

        # otherwise, get the user's membership at the time of the event.
        if membership is None:
            membership_event = state.get((EventTypes.Member, user_id), None)
            if membership_event:
                membership = membership_event.membership

        # if the user was a member of the room at the time of the event,
        # they can see it.
        if membership == Membership.JOIN:
            return event

        # otherwise, it depends on the room visibility.

        if visibility == ""joined"":
            # we weren't a member at the time of the event, so we can't
            # see this event.
            return None

        elif visibility == ""invited"":
            # user can also see the event if they were *invited* at the time
            # of the event.
            return event if membership == Membership.INVITE else None

        elif visibility == ""shared"" and is_peeking:
            # if the visibility is shared, users cannot see the event unless
            # they have *subequently* joined the room (or were members at the
            # time, of course)
            #
            # XXX: if the user has subsequently joined and then left again,
            # ideally we would share history up to the point they left. But
            # we don't know when they left. We just treat it as though they
            # never joined, and restrict access.
            return None

        # the visibility is either shared or world_readable, and the user was
        # not a member at the time. We allow it, provided the original sender
        # has not requested their data to be erased, in which case, we return
        # a redacted version.
        if erased_senders[event.sender]:
            return prune_event(event)

        return event

    # check each event: gives an iterable[None|EventBase]
    filtered_events = map(allowed, events)

    # remove the None entries
    filtered_events = filter(operator.truth, filtered_events)

    # we turn it into a list before returning it.
    return list(filtered_events)","1. Use `isinstance` to check the type of the event before casting it.
2. Use `get` to get the value of a key from a dictionary, and check if the key exists before getting its value.
3. Use `filter` and `map` to iterate over a collection and apply a function to each element."
"    def _store_room_members_txn(self, txn, events, backfilled):
        """"""Store a room member in the database.
        """"""
        self.db_pool.simple_insert_many_txn(
            txn,
            table=""room_memberships"",
            values=[
                {
                    ""event_id"": event.event_id,
                    ""user_id"": event.state_key,
                    ""sender"": event.user_id,
                    ""room_id"": event.room_id,
                    ""membership"": event.membership,
                    ""display_name"": event.content.get(""displayname"", None),
                    ""avatar_url"": event.content.get(""avatar_url"", None),
                }
                for event in events
            ],
        )

        for event in events:
            txn.call_after(
                self.store._membership_stream_cache.entity_has_changed,
                event.state_key,
                event.internal_metadata.stream_ordering,
            )
            txn.call_after(
                self.store.get_invited_rooms_for_local_user.invalidate,
                (event.state_key,),
            )

            # We update the local_current_membership table only if the event is
            # ""current"", i.e., its something that has just happened.
            #
            # This will usually get updated by the `current_state_events` handling,
            # unless its an outlier, and an outlier is only ""current"" if it's an ""out of
            # band membership"", like a remote invite or a rejection of a remote invite.
            if (
                self.is_mine_id(event.state_key)
                and not backfilled
                and event.internal_metadata.is_outlier()
                and event.internal_metadata.is_out_of_band_membership()
            ):
                self.db_pool.simple_upsert_txn(
                    txn,
                    table=""local_current_membership"",
                    keyvalues={""room_id"": event.room_id, ""user_id"": event.state_key},
                    values={
                        ""event_id"": event.event_id,
                        ""membership"": event.membership,
                    },
                )","1. Use prepared statements to avoid SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use strong passwords and security measures to protect the database."
"    async def on_POST(self, request):
        body = parse_json_object_from_request(request)

        client_addr = request.getClientIP()

        self.ratelimiter.ratelimit(client_addr, update=False)

        kind = b""user""
        if b""kind"" in request.args:
            kind = request.args[b""kind""][0]

        if kind == b""guest"":
            ret = await self._do_guest_registration(body, address=client_addr)
            return ret
        elif kind != b""user"":
            raise UnrecognizedRequestError(
                ""Do not understand membership kind: %s"" % (kind.decode(""utf8""),)
            )

        # Pull out the provided username and do basic sanity checks early since
        # the auth layer will store these in sessions.
        desired_username = None
        if ""username"" in body:
            if not isinstance(body[""username""], str) or len(body[""username""]) > 512:
                raise SynapseError(400, ""Invalid username"")
            desired_username = body[""username""]

        appservice = None
        if self.auth.has_access_token(request):
            appservice = self.auth.get_appservice_by_req(request)

        # fork off as soon as possible for ASes which have completely
        # different registration flows to normal users

        # == Application Service Registration ==
        if appservice:
            # Set the desired user according to the AS API (which uses the
            # 'user' key not 'username'). Since this is a new addition, we'll
            # fallback to 'username' if they gave one.
            desired_username = body.get(""user"", desired_username)

            # XXX we should check that desired_username is valid. Currently
            # we give appservices carte blanche for any insanity in mxids,
            # because the IRC bridges rely on being able to register stupid
            # IDs.

            access_token = self.auth.get_access_token_from_request(request)

            if isinstance(desired_username, str):
                result = await self._do_appservice_registration(
                    desired_username, access_token, body
                )
            return 200, result  # we throw for non 200 responses

        # == Normal User Registration == (everyone else)
        if not self._registration_enabled:
            raise SynapseError(403, ""Registration has been disabled"")

        # For regular registration, convert the provided username to lowercase
        # before attempting to register it. This should mean that people who try
        # to register with upper-case in their usernames don't get a nasty surprise.
        #
        # Note that we treat usernames case-insensitively in login, so they are
        # free to carry on imagining that their username is CrAzYh4cKeR if that
        # keeps them happy.
        if desired_username is not None:
            desired_username = desired_username.lower()

        # Check if this account is upgrading from a guest account.
        guest_access_token = body.get(""guest_access_token"", None)

        # Pull out the provided password and do basic sanity checks early.
        #
        # Note that we remove the password from the body since the auth layer
        # will store the body in the session and we don't want a plaintext
        # password store there.
        password = body.pop(""password"", None)
        if password is not None:
            if not isinstance(password, str) or len(password) > 512:
                raise SynapseError(400, ""Invalid password"")
            self.password_policy_handler.validate_password(password)

        if ""initial_device_display_name"" in body and password is None:
            # ignore 'initial_device_display_name' if sent without
            # a password to work around a client bug where it sent
            # the 'initial_device_display_name' param alone, wiping out
            # the original registration params
            logger.warning(""Ignoring initial_device_display_name without password"")
            del body[""initial_device_display_name""]

        session_id = self.auth_handler.get_session_id(body)
        registered_user_id = None
        password_hash = None
        if session_id:
            # if we get a registered user id out of here, it means we previously
            # registered a user for this session, so we could just return the
            # user here. We carry on and go through the auth checks though,
            # for paranoia.
            registered_user_id = await self.auth_handler.get_session_data(
                session_id, ""registered_user_id"", None
            )
            # Extract the previously-hashed password from the session.
            password_hash = await self.auth_handler.get_session_data(
                session_id, ""password_hash"", None
            )

        # Ensure that the username is valid.
        if desired_username is not None:
            await self.registration_handler.check_username(
                desired_username,
                guest_access_token=guest_access_token,
                assigned_user_id=registered_user_id,
            )

        # Check if the user-interactive authentication flows are complete, if
        # not this will raise a user-interactive auth error.
        try:
            auth_result, params, session_id = await self.auth_handler.check_ui_auth(
                self._registration_flows,
                request,
                body,
                self.hs.get_ip_from_request(request),
                ""register a new account"",
            )
        except InteractiveAuthIncompleteError as e:
            # The user needs to provide more steps to complete auth.
            #
            # Hash the password and store it with the session since the client
            # is not required to provide the password again.
            #
            # If a password hash was previously stored we will not attempt to
            # re-hash and store it for efficiency. This assumes the password
            # does not change throughout the authentication flow, but this
            # should be fine since the data is meant to be consistent.
            if not password_hash and password:
                password_hash = await self.auth_handler.hash(password)
                await self.auth_handler.set_session_data(
                    e.session_id, ""password_hash"", password_hash
                )
            raise

        # Check that we're not trying to register a denied 3pid.
        #
        # the user-facing checks will probably already have happened in
        # /register/email/requestToken when we requested a 3pid, but that's not
        # guaranteed.
        if auth_result:
            for login_type in [LoginType.EMAIL_IDENTITY, LoginType.MSISDN]:
                if login_type in auth_result:
                    medium = auth_result[login_type][""medium""]
                    address = auth_result[login_type][""address""]

                    if not check_3pid_allowed(self.hs, medium, address):
                        raise SynapseError(
                            403,
                            ""Third party identifiers (email/phone numbers)""
                            + "" are not authorized on this server"",
                            Codes.THREEPID_DENIED,
                        )

        if registered_user_id is not None:
            logger.info(
                ""Already registered user ID %r for this session"", registered_user_id
            )
            # don't re-register the threepids
            registered = False
        else:
            # If we have a password in this request, prefer it. Otherwise, there
            # might be a password hash from an earlier request.
            if password:
                password_hash = await self.auth_handler.hash(password)
            if not password_hash:
                raise SynapseError(400, ""Missing params: password"", Codes.MISSING_PARAM)

            desired_username = params.get(""username"", None)
            guest_access_token = params.get(""guest_access_token"", None)

            if desired_username is not None:
                desired_username = desired_username.lower()

            threepid = None
            if auth_result:
                threepid = auth_result.get(LoginType.EMAIL_IDENTITY)

                # Also check that we're not trying to register a 3pid that's already
                # been registered.
                #
                # This has probably happened in /register/email/requestToken as well,
                # but if a user hits this endpoint twice then clicks on each link from
                # the two activation emails, they would register the same 3pid twice.
                for login_type in [LoginType.EMAIL_IDENTITY, LoginType.MSISDN]:
                    if login_type in auth_result:
                        medium = auth_result[login_type][""medium""]
                        address = auth_result[login_type][""address""]
                        # For emails, canonicalise the address.
                        # We store all email addresses canonicalised in the DB.
                        # (See on_POST in EmailThreepidRequestTokenRestServlet
                        # in synapse/rest/client/v2_alpha/account.py)
                        if medium == ""email"":
                            try:
                                address = canonicalise_email(address)
                            except ValueError as e:
                                raise SynapseError(400, str(e))

                        existing_user_id = await self.store.get_user_id_by_threepid(
                            medium, address
                        )

                        if existing_user_id is not None:
                            raise SynapseError(
                                400,
                                ""%s is already in use"" % medium,
                                Codes.THREEPID_IN_USE,
                            )

            entries = await self.store.get_user_agents_ips_to_ui_auth_session(
                session_id
            )

            registered_user_id = await self.registration_handler.register_user(
                localpart=desired_username,
                password_hash=password_hash,
                guest_access_token=guest_access_token,
                threepid=threepid,
                address=client_addr,
                user_agent_ips=entries,
            )
            # Necessary due to auth checks prior to the threepid being
            # written to the db
            if threepid:
                if is_threepid_reserved(
                    self.hs.config.mau_limits_reserved_threepids, threepid
                ):
                    await self.store.upsert_monthly_active_user(registered_user_id)

            # Remember that the user account has been registered (and the user
            # ID it was registered with, since it might not have been specified).
            await self.auth_handler.set_session_data(
                session_id, ""registered_user_id"", registered_user_id
            )

            registered = True

        return_dict = await self._create_registration_details(
            registered_user_id, params
        )

        if registered:
            await self.registration_handler.post_registration_actions(
                user_id=registered_user_id,
                auth_result=auth_result,
                access_token=return_dict.get(""access_token""),
            )

        return 200, return_dict","1. Use `synapse.crypto.generate_hash()` to generate a hash of the password instead of `auth_handler.hash()`.
2. Check that the user-interactive authentication flows are complete before registering the user.
3. Check that the provided username is not already in use."
"def _setup_stdlib_logging(config, log_config, logBeginner: LogBeginner):
    """"""
    Set up Python stdlib logging.
    """"""
    if log_config is None:
        log_format = (
            ""%(asctime)s - %(name)s - %(lineno)d - %(levelname)s - %(request)s""
            "" - %(message)s""
        )

        logger = logging.getLogger("""")
        logger.setLevel(logging.INFO)
        logging.getLogger(""synapse.storage.SQL"").setLevel(logging.INFO)

        formatter = logging.Formatter(log_format)

        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    else:
        logging.config.dictConfig(log_config)

    # We add a log record factory that runs all messages through the
    # LoggingContextFilter so that we get the context *at the time we log*
    # rather than when we write to a handler. This can be done in config using
    # filter options, but care must when using e.g. MemoryHandler to buffer
    # writes.

    log_filter = LoggingContextFilter(request="""")
    old_factory = logging.getLogRecordFactory()

    def factory(*args, **kwargs):
        record = old_factory(*args, **kwargs)
        log_filter.filter(record)
        return record

    logging.setLogRecordFactory(factory)

    # Route Twisted's native logging through to the standard library logging
    # system.
    observer = STDLibLogObserver()

    def _log(event):

        if ""log_text"" in event:
            if event[""log_text""].startswith(""DNSDatagramProtocol starting on ""):
                return

            if event[""log_text""].startswith(""(UDP Port ""):
                return

            if event[""log_text""].startswith(""Timing out client""):
                return

        return observer(event)

    logBeginner.beginLoggingTo([_log], redirectStandardIO=not config.no_redirect_stdio)
    if not config.no_redirect_stdio:
        print(""Redirected stdout/stderr to logs"")

    return observer","1. Use a more secure logging format, such as JSON.
2. Use a secure logging library, such as `raven` or `sentry`.
3. Encrypt sensitive data, such as passwords, before logging them."
"    def _log(event):

        if ""log_text"" in event:
            if event[""log_text""].startswith(""DNSDatagramProtocol starting on ""):
                return

            if event[""log_text""].startswith(""(UDP Port ""):
                return

            if event[""log_text""].startswith(""Timing out client""):
                return

        return observer(event)","1. **Use strong cryptographic hashing functions**. The code currently uses the MD5 hash function, which is considered insecure. Use a stronger hash function, such as SHA-256 or SHA-512.
2. **Encrypt sensitive data**. The code currently stores passwords in plaintext. This is a security risk, as anyone who can access the code can read the passwords. Encrypt passwords using a strong encryption algorithm, such as AES-256.
3. **Use secure communication channels**. The code currently uses UDP, which is a connectionless protocol. This means that data can be lost or corrupted, and it is not possible to guarantee that data will be delivered in the same order that it was sent. Use a connection-oriented protocol, such as TCP, instead."
"    def _purge_history_txn(self, txn, room_id, token_str, delete_local_events):
        token = RoomStreamToken.parse(token_str)

        # Tables that should be pruned:
        #     event_auth
        #     event_backward_extremities
        #     event_edges
        #     event_forward_extremities
        #     event_json
        #     event_push_actions
        #     event_reference_hashes
        #     event_search
        #     event_to_state_groups
        #     events
        #     rejections
        #     room_depth
        #     state_groups
        #     state_groups_state

        # we will build a temporary table listing the events so that we don't
        # have to keep shovelling the list back and forth across the
        # connection. Annoyingly the python sqlite driver commits the
        # transaction on CREATE, so let's do this first.
        #
        # furthermore, we might already have the table from a previous (failed)
        # purge attempt, so let's drop the table first.

        txn.execute(""DROP TABLE IF EXISTS events_to_purge"")

        txn.execute(
            ""CREATE TEMPORARY TABLE events_to_purge (""
            ""    event_id TEXT NOT NULL,""
            ""    should_delete BOOLEAN NOT NULL""
            "")""
        )

        # First ensure that we're not about to delete all the forward extremeties
        txn.execute(
            ""SELECT e.event_id, e.depth FROM events as e ""
            ""INNER JOIN event_forward_extremities as f ""
            ""ON e.event_id = f.event_id ""
            ""AND e.room_id = f.room_id ""
            ""WHERE f.room_id = ?"",
            (room_id,),
        )
        rows = txn.fetchall()
        max_depth = max(row[1] for row in rows)

        if max_depth < token.topological:
            # We need to ensure we don't delete all the events from the database
            # otherwise we wouldn't be able to send any events (due to not
            # having any backwards extremeties)
            raise SynapseError(
                400, ""topological_ordering is greater than forward extremeties""
            )

        logger.info(""[purge] looking for events to delete"")

        should_delete_expr = ""state_key IS NULL""
        should_delete_params = ()  # type: Tuple[Any, ...]
        if not delete_local_events:
            should_delete_expr += "" AND event_id NOT LIKE ?""

            # We include the parameter twice since we use the expression twice
            should_delete_params += (""%:"" + self.hs.hostname, ""%:"" + self.hs.hostname)

        should_delete_params += (room_id, token.topological)

        # Note that we insert events that are outliers and aren't going to be
        # deleted, as nothing will happen to them.
        txn.execute(
            ""INSERT INTO events_to_purge""
            "" SELECT event_id, %s""
            "" FROM events AS e LEFT JOIN state_events USING (event_id)""
            "" WHERE (NOT outlier OR (%s)) AND e.room_id = ? AND topological_ordering < ?""
            % (should_delete_expr, should_delete_expr),
            should_delete_params,
        )

        # We create the indices *after* insertion as that's a lot faster.

        # create an index on should_delete because later we'll be looking for
        # the should_delete / shouldn't_delete subsets
        txn.execute(
            ""CREATE INDEX events_to_purge_should_delete""
            "" ON events_to_purge(should_delete)""
        )

        # We do joins against events_to_purge for e.g. calculating state
        # groups to purge, etc., so lets make an index.
        txn.execute(""CREATE INDEX events_to_purge_id ON events_to_purge(event_id)"")

        txn.execute(""SELECT event_id, should_delete FROM events_to_purge"")
        event_rows = txn.fetchall()
        logger.info(
            ""[purge] found %i events before cutoff, of which %i can be deleted"",
            len(event_rows),
            sum(1 for e in event_rows if e[1]),
        )

        logger.info(""[purge] Finding new backward extremities"")

        # We calculate the new entries for the backward extremeties by finding
        # events to be purged that are pointed to by events we're not going to
        # purge.
        txn.execute(
            ""SELECT DISTINCT e.event_id FROM events_to_purge AS e""
            "" INNER JOIN event_edges AS ed ON e.event_id = ed.prev_event_id""
            "" LEFT JOIN events_to_purge AS ep2 ON ed.event_id = ep2.event_id""
            "" WHERE ep2.event_id IS NULL""
        )
        new_backwards_extrems = txn.fetchall()

        logger.info(""[purge] replacing backward extremities: %r"", new_backwards_extrems)

        txn.execute(
            ""DELETE FROM event_backward_extremities WHERE room_id = ?"", (room_id,)
        )

        # Update backward extremeties
        txn.executemany(
            ""INSERT INTO event_backward_extremities (room_id, event_id)""
            "" VALUES (?, ?)"",
            [(room_id, event_id) for event_id, in new_backwards_extrems],
        )

        logger.info(""[purge] finding state groups referenced by deleted events"")

        # Get all state groups that are referenced by events that are to be
        # deleted.
        txn.execute(
            """"""
            SELECT DISTINCT state_group FROM events_to_purge
            INNER JOIN event_to_state_groups USING (event_id)
        """"""
        )

        referenced_state_groups = {sg for sg, in txn}
        logger.info(
            ""[purge] found %i referenced state groups"", len(referenced_state_groups)
        )

        logger.info(""[purge] removing events from event_to_state_groups"")
        txn.execute(
            ""DELETE FROM event_to_state_groups ""
            ""WHERE event_id IN (SELECT event_id from events_to_purge)""
        )
        for event_id, _ in event_rows:
            txn.call_after(self._get_state_group_for_event.invalidate, (event_id,))

        # Delete all remote non-state events
        for table in (
            ""events"",
            ""event_json"",
            ""event_auth"",
            ""event_edges"",
            ""event_forward_extremities"",
            ""event_reference_hashes"",
            ""event_search"",
            ""rejections"",
        ):
            logger.info(""[purge] removing events from %s"", table)

            txn.execute(
                ""DELETE FROM %s WHERE event_id IN (""
                ""    SELECT event_id FROM events_to_purge WHERE should_delete""
                "")"" % (table,)
            )

        # event_push_actions lacks an index on event_id, and has one on
        # (room_id, event_id) instead.
        for table in (""event_push_actions"",):
            logger.info(""[purge] removing events from %s"", table)

            txn.execute(
                ""DELETE FROM %s WHERE room_id = ? AND event_id IN (""
                ""    SELECT event_id FROM events_to_purge WHERE should_delete""
                "")"" % (table,),
                (room_id,),
            )

        # Mark all state and own events as outliers
        logger.info(""[purge] marking remaining events as outliers"")
        txn.execute(
            ""UPDATE events SET outlier = ?""
            "" WHERE event_id IN (""
            ""    SELECT event_id FROM events_to_purge ""
            ""    WHERE NOT should_delete""
            "")"",
            (True,),
        )

        # synapse tries to take out an exclusive lock on room_depth whenever it
        # persists events (because upsert), and once we run this update, we
        # will block that for the rest of our transaction.
        #
        # So, let's stick it at the end so that we don't block event
        # persistence.
        #
        # We do this by calculating the minimum depth of the backwards
        # extremities. However, the events in event_backward_extremities
        # are ones we don't have yet so we need to look at the events that
        # point to it via event_edges table.
        txn.execute(
            """"""
            SELECT COALESCE(MIN(depth), 0)
            FROM event_backward_extremities AS eb
            INNER JOIN event_edges AS eg ON eg.prev_event_id = eb.event_id
            INNER JOIN events AS e ON e.event_id = eg.event_id
            WHERE eb.room_id = ?
        """""",
            (room_id,),
        )
        (min_depth,) = txn.fetchone()

        logger.info(""[purge] updating room_depth to %d"", min_depth)

        txn.execute(
            ""UPDATE room_depth SET min_depth = ? WHERE room_id = ?"",
            (min_depth, room_id),
        )

        # finally, drop the temp table. this will commit the txn in sqlite,
        # so make sure to keep this actually last.
        txn.execute(""DROP TABLE events_to_purge"")

        logger.info(""[purge] done"")

        return referenced_state_groups","1. Use prepared statements instead of building queries dynamically to avoid SQL injection.
2. Use transactions to ensure that the database is in a consistent state even if an error occurs.
3. Use proper error handling to catch and log errors, and return meaningful error messages to the user."
"    async def notify_device_update(self, user_id, device_ids):
        """"""Notify that a user's device(s) has changed. Pokes the notifier, and
        remote servers if the user is local.
        """"""
        users_who_share_room = await self.store.get_users_who_share_room_with_user(
            user_id
        )

        hosts = set()
        if self.hs.is_mine_id(user_id):
            hosts.update(get_domain_from_id(u) for u in users_who_share_room)
            hosts.discard(self.server_name)

        set_tag(""target_hosts"", hosts)

        position = await self.store.add_device_change_to_streams(
            user_id, device_ids, list(hosts)
        )

        for device_id in device_ids:
            logger.debug(
                ""Notifying about update %r/%r, ID: %r"", user_id, device_id, position
            )

        room_ids = await self.store.get_rooms_for_user(user_id)

        # specify the user ID too since the user should always get their own device list
        # updates, even if they aren't in any rooms.
        self.notifier.on_new_event(
            ""device_list_key"", position, users=[user_id], rooms=room_ids
        )

        if hosts:
            logger.info(
                ""Sending device list update notif for %r to: %r"", user_id, hosts
            )
            for host in hosts:
                self.federation_sender.send_device_messages(host)
                log_kv({""message"": ""sent device update to host"", ""host"": host})","1. Use `get_domain_from_id` to get the domain name of the user.
2. Use `set_tag` to set the `target_hosts` tag.
3. Use `add_device_change_to_streams` to add the device change to streams."
"    def _background_insert_retention(self, progress, batch_size):
        """"""Retrieves a list of all rooms within a range and inserts an entry for each of
        them into the room_retention table.
        NULLs the property's columns if missing from the retention event in the room's
        state (or NULLs all of them if there's no retention event in the room's state),
        so that we fall back to the server's retention policy.
        """"""

        last_room = progress.get(""room_id"", """")

        def _background_insert_retention_txn(txn):
            txn.execute(
                """"""
                SELECT state.room_id, state.event_id, events.json
                FROM current_state_events as state
                LEFT JOIN event_json AS events ON (state.event_id = events.event_id)
                WHERE state.room_id > ? AND state.type = '%s'
                ORDER BY state.room_id ASC
                LIMIT ?;
                """"""
                % EventTypes.Retention,
                (last_room, batch_size),
            )

            rows = self.db.cursor_to_dict(txn)

            if not rows:
                return True

            for row in rows:
                if not row[""json""]:
                    retention_policy = {}
                else:
                    ev = json.loads(row[""json""])
                    retention_policy = json.dumps(ev[""content""])

                self.db.simple_insert_txn(
                    txn=txn,
                    table=""room_retention"",
                    values={
                        ""room_id"": row[""room_id""],
                        ""event_id"": row[""event_id""],
                        ""min_lifetime"": retention_policy.get(""min_lifetime""),
                        ""max_lifetime"": retention_policy.get(""max_lifetime""),
                    },
                )

            logger.info(""Inserted %d rows into room_retention"", len(rows))

            self.db.updates._background_update_progress_txn(
                txn, ""insert_room_retention"", {""room_id"": rows[-1][""room_id""]}
            )

            if batch_size > len(rows):
                return True
            else:
                return False

        end = yield self.db.runInteraction(
            ""insert_room_retention"", _background_insert_retention_txn,
        )

        if end:
            yield self.db.updates._end_background_update(""insert_room_retention"")

        defer.returnValue(batch_size)","1. Use prepared statements instead of building queries dynamically. This will prevent SQL injection attacks.
2. Use parameterized queries instead of passing values directly into the query string. This will prevent SQL injection attacks.
3. Use the `json.dumps()` function to escape any special characters in JSON strings. This will prevent JSON injection attacks."
"        def _background_insert_retention_txn(txn):
            txn.execute(
                """"""
                SELECT state.room_id, state.event_id, events.json
                FROM current_state_events as state
                LEFT JOIN event_json AS events ON (state.event_id = events.event_id)
                WHERE state.room_id > ? AND state.type = '%s'
                ORDER BY state.room_id ASC
                LIMIT ?;
                """"""
                % EventTypes.Retention,
                (last_room, batch_size),
            )

            rows = self.db.cursor_to_dict(txn)

            if not rows:
                return True

            for row in rows:
                if not row[""json""]:
                    retention_policy = {}
                else:
                    ev = json.loads(row[""json""])
                    retention_policy = json.dumps(ev[""content""])

                self.db.simple_insert_txn(
                    txn=txn,
                    table=""room_retention"",
                    values={
                        ""room_id"": row[""room_id""],
                        ""event_id"": row[""event_id""],
                        ""min_lifetime"": retention_policy.get(""min_lifetime""),
                        ""max_lifetime"": retention_policy.get(""max_lifetime""),
                    },
                )

            logger.info(""Inserted %d rows into room_retention"", len(rows))

            self.db.updates._background_update_progress_txn(
                txn, ""insert_room_retention"", {""room_id"": rows[-1][""room_id""]}
            )

            if batch_size > len(rows):
                return True
            else:
                return False","1. Use prepared statements to prevent SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use strong encryption to protect sensitive data."
"    def _invalidate_get_users_with_receipts_in_room(
        self, room_id, receipt_type, user_id
    ):
        if receipt_type != ""m.read"":
            return

        # Returns either an ObservableDeferred or the raw result
        res = self.get_users_with_read_receipts_in_room.cache.get(
            room_id, None, update_metrics=False
        )

        # first handle the Deferred case
        if isinstance(res, defer.Deferred):
            if res.called:
                res = res.result
            else:
                res = None

        if res and user_id in res:
            # We'd only be adding to the set, so no point invalidating if the
            # user is already there
            return

        self.get_users_with_read_receipts_in_room.invalidate((room_id,))","1. Use `get_users_with_read_receipts_in_room.cache.get(room_id, None, update_metrics=False)` instead of `self.get_users_with_read_receipts_in_room.cache.get(room_id, None)` to avoid caching the result if the user is not already in the set.
2. Use `defer.Deferred` instead of `defer.inlineCallbacks` to avoid creating a new Deferred every time the function is called.
3. Use `user_id in res` instead of `user_id in self.get_users_with_read_receipts_in_room.cache.get(room_id, None)` to avoid checking the cache if the user is not already in the set."
"    def observe(self) -> defer.Deferred:
        """"""Observe the underlying deferred.

        This returns a brand new deferred that is resolved when the underlying
        deferred is resolved. Interacting with the returned deferred does not
        effect the underdlying deferred.
        """"""
        if not self._result:
            d = defer.Deferred()

            def remove(r):
                self._observers.discard(d)
                return r

            d.addBoth(remove)

            self._observers.add(d)
            return d
        else:
            success, res = self._result
            return defer.succeed(res) if success else defer.fail(res)","1. Use `defer.Deferred.addCallback` instead of `addBoth` to avoid
    creating a new deferred that is not needed.
2. Use `defer.Deferred.callback` or `defer.Deferred.errback` instead of
    returning the deferred directly to avoid creating a new deferred that is not
    needed.
3. Check the `success` flag of the deferred result before calling `succeed` or
    `fail` to avoid calling the wrong function."
"    def _invalidate_get_users_with_receipts_in_room(self, room_id, receipt_type,
                                                    user_id):
        if receipt_type != ""m.read"":
            return

        # Returns an ObservableDeferred
        res = self.get_users_with_read_receipts_in_room.cache.get(
            room_id, None, update_metrics=False,
        )

        if res:
            if isinstance(res, defer.Deferred) and res.called:
                res = res.result
            if user_id in res:
                # We'd only be adding to the set, so no point invalidating if the
                # user is already there
                return

        self.get_users_with_read_receipts_in_room.invalidate((room_id,))","1. Use `get_users_with_read_receipts_in_room.cache.get(room_id, None, update_metrics=False)` to check if the user is already in the set.
2. If the user is already in the set, there is no need to invalidate the cache.
3. Use `defer.Deferred` to handle asynchronous operations."
"    def register_user(self, localpart, displayname=None, emails=[]):
        """"""Registers a new user with given localpart and optional displayname, emails.

        Args:
            localpart (str): The localpart of the new user.
            displayname (str|None): The displayname of the new user.
            emails (List[str]): Emails to bind to the new user.

        Raises:
            SynapseError if there is an error performing the registration. Check the
                'errcode' property for more information on the reason for failure

        Returns:
            Deferred[str]: user_id
        """"""
        return defer.ensureDeferred(
            self._hs.get_registration_handler().register_user(
                localpart=localpart,
                default_display_name=displayname,
                bind_emails=emails,
            )
        )","1. Sanitize user input to prevent SQL injection attacks.
2. Use strong hashing algorithms and salt values to protect passwords.
3. Implement rate limiting to prevent brute force attacks."
"    def register_device(self, user_id, device_id=None, initial_display_name=None):
        """"""Register a device for a user and generate an access token.

        Args:
            user_id (str): full canonical @user:id
            device_id (str|None): The device ID to check, or None to generate
                a new one.
            initial_display_name (str|None): An optional display name for the
                device.

        Returns:
            defer.Deferred[tuple[str, str]]: Tuple of device ID and access token
        """"""
        return self._hs.get_registration_handler().register_device(
            user_id=user_id,
            device_id=device_id,
            initial_display_name=initial_display_name,
        )","1. Use HTTPS to secure the communication between the client and the server.
2. Use strong cryptography to protect the data.
3. Implement proper access control to restrict who can access the data."
"    def _event_match(self, condition: dict, user_id: str) -> bool:
        pattern = condition.get(""pattern"", None)

        if not pattern:
            pattern_type = condition.get(""pattern_type"", None)
            if pattern_type == ""user_id"":
                pattern = user_id
            elif pattern_type == ""user_localpart"":
                pattern = UserID.from_string(user_id).localpart

        if not pattern:
            logger.warning(""event_match condition with no pattern"")
            return False

        # XXX: optimisation: cache our pattern regexps
        if condition[""key""] == ""content.body"":
            body = self._event.content.get(""body"", None)
            if not body:
                return False

            return _glob_matches(pattern, body, word_boundary=True)
        else:
            haystack = self._get_value(condition[""key""])
            if haystack is None:
                return False

            return _glob_matches(pattern, haystack)","1. Use `re.compile()` to cache the regular expression pattern instead of creating a new one each time. This will improve performance.
2. Use `UserID.from_string()` to parse the user ID string into a `UserID` object. This will prevent invalid user IDs from being used.
3. Use `_get_value()` to get the value of the specified key from the event object. This will prevent access to invalid keys."
"    def _contains_display_name(self, display_name: str) -> bool:
        if not display_name:
            return False

        body = self._event.content.get(""body"", None)
        if not body:
            return False

        # Similar to _glob_matches, but do not treat display_name as a glob.
        r = regex_cache.get((display_name, False, True), None)
        if not r:
            r = re.escape(display_name)
            r = _re_word_boundary(r)
            r = re.compile(r, flags=re.IGNORECASE)
            regex_cache[(display_name, False, True)] = r

        return r.search(body)","1. Use `re.compile` with the `DOTALL` flag to match newline characters.
2. Use `re.IGNORECASE` to match the display name case-insensitively.
3. Use `re.MULTILINE` to match the display name across multiple lines."
"def add_resizable_cache(cache_name: str, cache_resize_callback: Callable):
    """"""Register a cache that's size can dynamically change

    Args:
        cache_name: A reference to the cache
        cache_resize_callback: A callback function that will be ran whenever
            the cache needs to be resized
    """"""
    # Some caches have '*' in them which we strip out.
    cache_name = _canonicalise_cache_name(cache_name)

    _CACHES[cache_name] = cache_resize_callback

    # Ensure all loaded caches are sized appropriately
    #
    # This method should only run once the config has been read,
    # as it uses values read from it
    if properties.resize_all_caches_func:
        properties.resize_all_caches_func()","1. Use `functools.wraps` to preserve the original function metadata.
2. Sanitize the input `cache_name` to prevent malicious code injection.
3. Use `functools.lru_cache` to cache the results of the callback function."
"    def reset():
        """"""Resets the caches to their defaults. Used for tests.""""""
        properties.default_factor_size = float(
            os.environ.get(_CACHE_PREFIX, _DEFAULT_FACTOR_SIZE)
        )
        properties.resize_all_caches_func = None
        _CACHES.clear()","1. Use `os.getenv` to get the environment variable instead of hard-coding it.
2. Use `os.environ.get` to check if the environment variable exists, and return a default value if it doesn't.
3. Use `contextlib.suppress` to suppress the `KeyError` exception when accessing the `_CACHES` dictionary."
"    def resize_all_caches(self):
        """"""Ensure all cache sizes are up to date

        For each cache, run the mapped callback function with either
        a specific cache factor or the default, global one.
        """"""
        for cache_name, callback in _CACHES.items():
            new_factor = self.cache_factors.get(cache_name, self.global_factor)
            callback(new_factor)","1. Use `functools.lru_cache` instead of a custom cache.
2. Use `os.getenv` to get the cache size from an environment variable.
3. Use `hashlib.sha256` to generate a secure hash of the cache key."
"    def __init__(self, hs):
        super(GenericWorkerReplicationHandler, self).__init__(hs)

        self.store = hs.get_datastore()
        self.typing_handler = hs.get_typing_handler()
        self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence
        self.notifier = hs.get_notifier()

        self.notify_pushers = hs.config.start_pushers
        self.pusher_pool = hs.get_pusherpool()

        if hs.config.send_federation:
            self.send_handler = FederationSenderHandler(hs, self)
        else:
            self.send_handler = None","1. Use `functools.lru_cache` to cache expensive function calls.
2. Use `os.makedirs` to create directories if they don't exist.
3. Use `contextlib.closing` to ensure that file objects are closed after use."
"    async def _process_and_notify(self, stream_name, instance_name, token, rows):
        try:
            if self.send_handler:
                await self.send_handler.process_replication_rows(
                    stream_name, token, rows
                )

            if stream_name == PushRulesStream.NAME:
                self.notifier.on_new_event(
                    ""push_rules_key"", token, users=[row.user_id for row in rows]
                )
            elif stream_name in (AccountDataStream.NAME, TagAccountDataStream.NAME):
                self.notifier.on_new_event(
                    ""account_data_key"", token, users=[row.user_id for row in rows]
                )
            elif stream_name == ReceiptsStream.NAME:
                self.notifier.on_new_event(
                    ""receipt_key"", token, rooms=[row.room_id for row in rows]
                )
                await self.pusher_pool.on_new_receipts(
                    token, token, {row.room_id for row in rows}
                )
            elif stream_name == TypingStream.NAME:
                self.typing_handler.process_replication_rows(token, rows)
                self.notifier.on_new_event(
                    ""typing_key"", token, rooms=[row.room_id for row in rows]
                )
            elif stream_name == ToDeviceStream.NAME:
                entities = [row.entity for row in rows if row.entity.startswith(""@"")]
                if entities:
                    self.notifier.on_new_event(""to_device_key"", token, users=entities)
            elif stream_name == DeviceListsStream.NAME:
                all_room_ids = set()
                for row in rows:
                    if row.entity.startswith(""@""):
                        room_ids = await self.store.get_rooms_for_user(row.entity)
                        all_room_ids.update(room_ids)
                self.notifier.on_new_event(""device_list_key"", token, rooms=all_room_ids)
            elif stream_name == PresenceStream.NAME:
                await self.presence_handler.process_replication_rows(token, rows)
            elif stream_name == GroupServerStream.NAME:
                self.notifier.on_new_event(
                    ""groups_key"", token, users=[row.user_id for row in rows]
                )
            elif stream_name == PushersStream.NAME:
                for row in rows:
                    if row.deleted:
                        self.stop_pusher(row.user_id, row.app_id, row.pushkey)
                    else:
                        await self.start_pusher(row.user_id, row.app_id, row.pushkey)
        except Exception:
            logger.exception(""Error processing replication"")","1. Use proper error handling to catch and log exceptions.
2. Validate input parameters to prevent injection attacks.
3. Use secure communication channels to protect sensitive data."
"    def __init__(self, hs: GenericWorkerServer, replication_client):
        self.store = hs.get_datastore()
        self._is_mine_id = hs.is_mine_id
        self.federation_sender = hs.get_federation_sender()
        self.replication_client = replication_client

        self.federation_position = self.store.federation_out_pos_startup
        self._fed_position_linearizer = Linearizer(name=""_fed_position_linearizer"")

        self._last_ack = self.federation_position

        self._room_serials = {}
        self._room_typing = {}","1. Use `functools.lru_cache` to cache the result of expensive function calls.
2. Use `typing` to annotate the function parameters and return types.
3. Use `black` to format the code consistently."
"    async def update_token(self, token):
        try:
            self.federation_position = token

            # We linearize here to ensure we don't have races updating the token
            with (await self._fed_position_linearizer.queue(None)):
                if self._last_ack < self.federation_position:
                    await self.store.update_federation_out_pos(
                        ""federation"", self.federation_position
                    )

                    # We ACK this token over replication so that the master can drop
                    # its in memory queues
                    self.replication_client.send_federation_ack(
                        self.federation_position
                    )
                    self._last_ack = self.federation_position
        except Exception:
            logger.exception(""Error updating federation stream position"")","1. Use `async with` instead of `try/finally` to ensure that the linearizer is always closed.
2. Use `await` instead of `blocking` calls to `store.update_federation_out_pos` and `replication_client.send_federation_ack` to avoid blocking the event loop.
3. Use `logger.exception` instead of `logger.error` to log the stack trace of any exceptions."
"    def __init__(self, hs):
        self.store = hs.get_datastore()
        self.federation = hs.get_federation_client()
        self.device_handler = hs.get_device_handler()
        self.is_mine = hs.is_mine
        self.clock = hs.get_clock()

        self._edu_updater = SigningKeyEduUpdater(hs, self)

        self._is_master = hs.config.worker_app is None
        if not self._is_master:
            self._user_device_resync_client = ReplicationUserDevicesResyncRestServlet.make_client(
                hs
            )

        federation_registry = hs.get_federation_registry()

        # FIXME: switch to m.signing_key_update when MSC1756 is merged into the spec
        federation_registry.register_edu_handler(
            ""org.matrix.signing_key_update"",
            self._edu_updater.incoming_signing_key_update,
        )
        # doesn't really work as part of the generic query API, because the
        # query request requires an object POST, but we abuse the
        # ""query handler"" interface.
        federation_registry.register_query_handler(
            ""client_keys"", self.on_federation_query_client_keys
        )","1. Use strong hashing algorithms and salt values.
2. Use strong access control to restrict who can access the data.
3. Implement security measures to protect against common attacks, such as DDoS attacks and SQL injection attacks."
"    def _get_public_room_list(
        self,
        limit=None,
        since_token=None,
        search_filter=None,
        network_tuple=EMPTY_THIRD_PARTY_ID,
        from_federation=False,
    ):
        """"""Generate a public room list.
        Args:
            limit (int|None): Maximum amount of rooms to return.
            since_token (str|None)
            search_filter (dict|None): Dictionary to filter rooms by.
            network_tuple (ThirdPartyInstanceID): Which public list to use.
                This can be (None, None) to indicate the main list, or a particular
                appservice and network id to use an appservice specific one.
                Setting to None returns all public rooms across all lists.
            from_federation (bool): Whether this request originated from a
                federating server or a client. Used for room filtering.
        """"""

        # Pagination tokens work by storing the room ID sent in the last batch,
        # plus the direction (forwards or backwards). Next batch tokens always
        # go forwards, prev batch tokens always go backwards.

        if since_token:
            batch_token = RoomListNextBatch.from_token(since_token)

            bounds = (batch_token.last_joined_members, batch_token.last_room_id)
            forwards = batch_token.direction_is_forward
        else:
            batch_token = None
            bounds = None

            forwards = True

        # we request one more than wanted to see if there are more pages to come
        probing_limit = limit + 1 if limit is not None else None

        results = yield self.store.get_largest_public_rooms(
            network_tuple,
            search_filter,
            probing_limit,
            bounds=bounds,
            forwards=forwards,
            ignore_non_federatable=from_federation,
        )

        def build_room_entry(room):
            entry = {
                ""room_id"": room[""room_id""],
                ""name"": room[""name""],
                ""topic"": room[""topic""],
                ""canonical_alias"": room[""canonical_alias""],
                ""num_joined_members"": room[""joined_members""],
                ""avatar_url"": room[""avatar""],
                ""world_readable"": room[""history_visibility""] == ""world_readable"",
                ""guest_can_join"": room[""guest_access""] == ""can_join"",
            }

            # Filter out Nones – rather omit the field altogether
            return {k: v for k, v in entry.items() if v is not None}

        results = [build_room_entry(r) for r in results]

        response = {}
        num_results = len(results)
        if limit is not None:
            more_to_come = num_results == probing_limit

            # Depending on direction we trim either the front or back.
            if forwards:
                results = results[:limit]
            else:
                results = results[-limit:]
        else:
            more_to_come = False

        if num_results > 0:
            final_entry = results[-1]
            initial_entry = results[0]

            if forwards:
                if batch_token:
                    # If there was a token given then we assume that there
                    # must be previous results.
                    response[""prev_batch""] = RoomListNextBatch(
                        last_joined_members=initial_entry[""num_joined_members""],
                        last_room_id=initial_entry[""room_id""],
                        direction_is_forward=False,
                    ).to_token()

                if more_to_come:
                    response[""next_batch""] = RoomListNextBatch(
                        last_joined_members=final_entry[""num_joined_members""],
                        last_room_id=final_entry[""room_id""],
                        direction_is_forward=True,
                    ).to_token()
            else:
                if batch_token:
                    response[""next_batch""] = RoomListNextBatch(
                        last_joined_members=final_entry[""num_joined_members""],
                        last_room_id=final_entry[""room_id""],
                        direction_is_forward=True,
                    ).to_token()

                if more_to_come:
                    response[""prev_batch""] = RoomListNextBatch(
                        last_joined_members=initial_entry[""num_joined_members""],
                        last_room_id=initial_entry[""room_id""],
                        direction_is_forward=False,
                    ).to_token()

        response[""chunk""] = results

        response[""total_room_count_estimate""] = yield self.store.count_public_rooms(
            network_tuple, ignore_non_federatable=from_federation
        )

        return response","1. Use `yield` instead of `return` to avoid synchronous code execution.
2. Use `build_room_entry()` to sanitize the room data before returning it.
3. Use `RoomListNextBatch()` to create pagination tokens."
"    async def on_GET(self, request):
        server = parse_string(request, ""server"", default=None)

        try:
            await self.auth.get_user_by_req(request, allow_guest=True)
        except InvalidClientCredentialsError as e:
            # Option to allow servers to require auth when accessing
            # /publicRooms via CS API. This is especially helpful in private
            # federations.
            if not self.hs.config.allow_public_rooms_without_auth:
                raise

            # We allow people to not be authed if they're just looking at our
            # room list, but require auth when we proxy the request.
            # In both cases we call the auth function, as that has the side
            # effect of logging who issued this request if an access token was
            # provided.
            if server:
                raise e
            else:
                pass

        limit = parse_integer(request, ""limit"", 0)
        since_token = parse_string(request, ""since"", None)

        if limit == 0:
            # zero is a special value which corresponds to no limit.
            limit = None

        handler = self.hs.get_room_list_handler()
        if server:
            data = await handler.get_remote_public_room_list(
                server, limit=limit, since_token=since_token
            )
        else:
            data = await handler.get_local_public_room_list(
                limit=limit, since_token=since_token
            )

        return 200, data","1. Use `require_auth=True` to require authentication for all requests.
2. Validate the `server` parameter to prevent unauthorized access to remote servers.
3. Sanitize the `since_token` parameter to prevent injection attacks."
"    async def on_POST(self, request):
        await self.auth.get_user_by_req(request, allow_guest=True)

        server = parse_string(request, ""server"", default=None)
        content = parse_json_object_from_request(request)

        limit = int(content.get(""limit"", 100))  # type: Optional[int]
        since_token = content.get(""since"", None)
        search_filter = content.get(""filter"", None)

        include_all_networks = content.get(""include_all_networks"", False)
        third_party_instance_id = content.get(""third_party_instance_id"", None)

        if include_all_networks:
            network_tuple = None
            if third_party_instance_id is not None:
                raise SynapseError(
                    400, ""Can't use include_all_networks with an explicit network""
                )
        elif third_party_instance_id is None:
            network_tuple = ThirdPartyInstanceID(None, None)
        else:
            network_tuple = ThirdPartyInstanceID.from_string(third_party_instance_id)

        if limit == 0:
            # zero is a special value which corresponds to no limit.
            limit = None

        handler = self.hs.get_room_list_handler()
        if server:
            data = await handler.get_remote_public_room_list(
                server,
                limit=limit,
                since_token=since_token,
                search_filter=search_filter,
                include_all_networks=include_all_networks,
                third_party_instance_id=third_party_instance_id,
            )
        else:
            data = await handler.get_local_public_room_list(
                limit=limit,
                since_token=since_token,
                search_filter=search_filter,
                network_tuple=network_tuple,
            )

        return 200, data","1. Use `parse_json_object_from_request` to parse JSON data from the request.
2. Validate the `server` parameter to prevent server-spoofing attacks.
3. Use `ThirdPartyInstanceID.from_string` to parse the `third_party_instance_id` parameter."
"    def upload_room_keys(self, user_id, version, room_keys):
        """"""Bulk upload a list of room keys into a given backup version, asserting
        that the given version is the current backup version.  room_keys are merged
        into the current backup as described in RoomKeysServlet.on_PUT().

        Args:
            user_id(str): the user whose backup we're setting
            version(str): the version ID of the backup we're updating
            room_keys(dict): a nested dict describing the room_keys we're setting:

        {
            ""rooms"": {
                ""!abc:matrix.org"": {
                    ""sessions"": {
                        ""c0ff33"": {
                            ""first_message_index"": 1,
                            ""forwarded_count"": 1,
                            ""is_verified"": false,
                            ""session_data"": ""SSBBTSBBIEZJU0gK""
                        }
                    }
                }
            }
        }

        Returns:
            A dict containing the count and etag for the backup version

        Raises:
            NotFoundError: if there are no versions defined
            RoomKeysVersionError: if the uploaded version is not the current version
        """"""

        # TODO: Validate the JSON to make sure it has the right keys.

        # XXX: perhaps we should use a finer grained lock here?
        with (yield self._upload_linearizer.queue(user_id)):

            # Check that the version we're trying to upload is the current version
            try:
                version_info = yield self.store.get_e2e_room_keys_version_info(user_id)
            except StoreError as e:
                if e.code == 404:
                    raise NotFoundError(""Version '%s' not found"" % (version,))
                else:
                    raise

            if version_info[""version""] != version:
                # Check that the version we're trying to upload actually exists
                try:
                    version_info = yield self.store.get_e2e_room_keys_version_info(
                        user_id, version
                    )
                    # if we get this far, the version must exist
                    raise RoomKeysVersionError(current_version=version_info[""version""])
                except StoreError as e:
                    if e.code == 404:
                        raise NotFoundError(""Version '%s' not found"" % (version,))
                    else:
                        raise

            # Fetch any existing room keys for the sessions that have been
            # submitted.  Then compare them with the submitted keys.  If the
            # key is new, insert it; if the key should be updated, then update
            # it; otherwise, drop it.
            existing_keys = yield self.store.get_e2e_room_keys_multi(
                user_id, version, room_keys[""rooms""]
            )
            to_insert = []  # batch the inserts together
            changed = False  # if anything has changed, we need to update the etag
            for room_id, room in iteritems(room_keys[""rooms""]):
                for session_id, room_key in iteritems(room[""sessions""]):
                    log_kv(
                        {
                            ""message"": ""Trying to upload room key"",
                            ""room_id"": room_id,
                            ""session_id"": session_id,
                            ""user_id"": user_id,
                        }
                    )
                    current_room_key = existing_keys.get(room_id, {}).get(session_id)
                    if current_room_key:
                        if self._should_replace_room_key(current_room_key, room_key):
                            log_kv({""message"": ""Replacing room key.""})
                            # updates are done one at a time in the DB, so send
                            # updates right away rather than batching them up,
                            # like we do with the inserts
                            yield self.store.update_e2e_room_key(
                                user_id, version, room_id, session_id, room_key
                            )
                            changed = True
                        else:
                            log_kv({""message"": ""Not replacing room_key.""})
                    else:
                        log_kv(
                            {
                                ""message"": ""Room key not found."",
                                ""room_id"": room_id,
                                ""user_id"": user_id,
                            }
                        )
                        log_kv({""message"": ""Replacing room key.""})
                        to_insert.append((room_id, session_id, room_key))
                        changed = True

            if len(to_insert):
                yield self.store.add_e2e_room_keys(user_id, version, to_insert)

            version_etag = version_info[""etag""]
            if changed:
                version_etag = version_etag + 1
                yield self.store.update_e2e_room_keys_version(
                    user_id, version, None, version_etag
                )

            count = yield self.store.count_e2e_room_keys(user_id, version)
            return {""etag"": str(version_etag), ""count"": count}","1. Use a fine-grained lock instead of a coarse-grained lock.
2. Validate the JSON to make sure it has the right keys.
3. Check that the version we're trying to upload actually exists."
"    async def on_PUT(self, request, user_id):
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)

        target_user = UserID.from_string(user_id)
        body = parse_json_object_from_request(request)

        if not self.hs.is_mine(target_user):
            raise SynapseError(400, ""This endpoint can only be used with local users"")

        user = await self.admin_handler.get_user(target_user)
        user_id = target_user.to_string()

        if user:  # modify user
            if ""displayname"" in body:
                await self.profile_handler.set_displayname(
                    target_user, requester, body[""displayname""], True
                )

            if ""threepids"" in body:
                # check for required parameters for each threepid
                for threepid in body[""threepids""]:
                    assert_params_in_dict(threepid, [""medium"", ""address""])

                # remove old threepids from user
                threepids = await self.store.user_get_threepids(user_id)
                for threepid in threepids:
                    try:
                        await self.auth_handler.delete_threepid(
                            user_id, threepid[""medium""], threepid[""address""], None
                        )
                    except Exception:
                        logger.exception(""Failed to remove threepids"")
                        raise SynapseError(500, ""Failed to remove threepids"")

                # add new threepids to user
                current_time = self.hs.get_clock().time_msec()
                for threepid in body[""threepids""]:
                    await self.auth_handler.add_threepid(
                        user_id, threepid[""medium""], threepid[""address""], current_time
                    )

            if ""avatar_url"" in body:
                await self.profile_handler.set_avatar_url(
                    target_user, requester, body[""avatar_url""], True
                )

            if ""admin"" in body:
                set_admin_to = bool(body[""admin""])
                if set_admin_to != user[""admin""]:
                    auth_user = requester.user
                    if target_user == auth_user and not set_admin_to:
                        raise SynapseError(400, ""You may not demote yourself."")

                    await self.admin_handler.set_user_server_admin(
                        target_user, set_admin_to
                    )

            if ""password"" in body:
                if (
                    not isinstance(body[""password""], text_type)
                    or len(body[""password""]) > 512
                ):
                    raise SynapseError(400, ""Invalid password"")
                else:
                    new_password = body[""password""]
                    await self.set_password_handler.set_password(
                        target_user.to_string(), new_password, requester
                    )

            if ""deactivated"" in body:
                deactivate = body[""deactivated""]
                if not isinstance(deactivate, bool):
                    raise SynapseError(
                        400, ""'deactivated' parameter is not of type boolean""
                    )

                if deactivate and not user[""deactivated""]:
                    await self.deactivate_account_handler.deactivate_account(
                        target_user.to_string(), False
                    )

            user = await self.admin_handler.get_user(target_user)
            return 200, user

        else:  # create user
            password = body.get(""password"")
            if password is not None and (
                not isinstance(body[""password""], text_type)
                or len(body[""password""]) > 512
            ):
                raise SynapseError(400, ""Invalid password"")

            admin = body.get(""admin"", None)
            user_type = body.get(""user_type"", None)
            displayname = body.get(""displayname"", None)
            threepids = body.get(""threepids"", None)

            if user_type is not None and user_type not in UserTypes.ALL_USER_TYPES:
                raise SynapseError(400, ""Invalid user type"")

            user_id = await self.registration_handler.register_user(
                localpart=target_user.localpart,
                password=password,
                admin=bool(admin),
                default_display_name=displayname,
                user_type=user_type,
            )

            if ""threepids"" in body:
                # check for required parameters for each threepid
                for threepid in body[""threepids""]:
                    assert_params_in_dict(threepid, [""medium"", ""address""])

                current_time = self.hs.get_clock().time_msec()
                for threepid in body[""threepids""]:
                    await self.auth_handler.add_threepid(
                        user_id, threepid[""medium""], threepid[""address""], current_time
                    )

            if ""avatar_url"" in body:
                await self.profile_handler.set_avatar_url(
                    user_id, requester, body[""avatar_url""], True
                )

            ret = await self.admin_handler.get_user(target_user)

            return 201, ret","1. Use `assert_user_is_admin` to check if the user is an admin before modifying or creating a user.
2. Sanitize user input to prevent against injection attacks.
3. Use strong passwords for users and never store passwords in plaintext."
"    async def on_PUT(self, request, user_id):
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)
        auth_user = requester.user

        target_user = UserID.from_string(user_id)

        body = parse_json_object_from_request(request)

        assert_params_in_dict(body, [""admin""])

        if not self.hs.is_mine(target_user):
            raise SynapseError(400, ""Only local users can be admins of this homeserver"")

        set_admin_to = bool(body[""admin""])

        if target_user == auth_user and not set_admin_to:
            raise SynapseError(400, ""You may not demote yourself."")

        await self.store.set_user_server_admin(target_user, set_admin_to)

        return 200, {}","1. Use `assert_user_is_server_admin` instead of `assert_user_is_admin` to verify that the user is an admin of the homeserver that the user is trying to modify.
2. Check that the target user is local to the homeserver before setting their admin status.
3. Raise an error if the user is trying to demote themselves."
"    def set_server_admin(self, user, admin):
        """"""Sets whether a user is an admin of this homeserver.

        Args:
            user (UserID): user ID of the user to test
            admin (bool): true iff the user is to be a server admin,
                false otherwise.
        """"""
        return self.db.simple_update_one(
            table=""users"",
            keyvalues={""name"": user.to_string()},
            updatevalues={""admin"": 1 if admin else 0},
            desc=""set_server_admin"",
        )","1. Use prepared statements to prevent SQL injection.
2. Use the `user.to_dict()` method to avoid exposing the user ID in the query string.
3. Use the `acl.is_admin()` method to check if the user is an admin, rather than relying on the `admin` field."
"    async def on_PUT(self, request, user_id):
        requester = await self.auth.get_user_by_req(request)
        await assert_user_is_admin(self.auth, requester.user)

        target_user = UserID.from_string(user_id)
        body = parse_json_object_from_request(request)

        if not self.hs.is_mine(target_user):
            raise SynapseError(400, ""This endpoint can only be used with local users"")

        user = await self.admin_handler.get_user(target_user)
        user_id = target_user.to_string()

        if user:  # modify user
            if ""displayname"" in body:
                await self.profile_handler.set_displayname(
                    target_user, requester, body[""displayname""], True
                )

            if ""threepids"" in body:
                # check for required parameters for each threepid
                for threepid in body[""threepids""]:
                    assert_params_in_dict(threepid, [""medium"", ""address""])

                # remove old threepids from user
                threepids = await self.store.user_get_threepids(user_id)
                for threepid in threepids:
                    try:
                        await self.auth_handler.delete_threepid(
                            user_id, threepid[""medium""], threepid[""address""], None
                        )
                    except Exception:
                        logger.exception(""Failed to remove threepids"")
                        raise SynapseError(500, ""Failed to remove threepids"")

                # add new threepids to user
                current_time = self.hs.get_clock().time_msec()
                for threepid in body[""threepids""]:
                    await self.auth_handler.add_threepid(
                        user_id, threepid[""medium""], threepid[""address""], current_time
                    )

            if ""avatar_url"" in body:
                await self.profile_handler.set_avatar_url(
                    target_user, requester, body[""avatar_url""], True
                )

            if ""admin"" in body:
                set_admin_to = bool(body[""admin""])
                if set_admin_to != user[""admin""]:
                    auth_user = requester.user
                    if target_user == auth_user and not set_admin_to:
                        raise SynapseError(400, ""You may not demote yourself."")

                    await self.admin_handler.set_user_server_admin(
                        target_user, set_admin_to
                    )

            if ""password"" in body:
                if (
                    not isinstance(body[""password""], text_type)
                    or len(body[""password""]) > 512
                ):
                    raise SynapseError(400, ""Invalid password"")
                else:
                    new_password = body[""password""]
                    await self.set_password_handler.set_password(
                        target_user.to_string(), new_password, requester
                    )

            if ""deactivated"" in body:
                deactivate = bool(body[""deactivated""])
                if deactivate and not user[""deactivated""]:
                    result = await self.deactivate_account_handler.deactivate_account(
                        target_user.to_string(), False
                    )
                    if not result:
                        raise SynapseError(500, ""Could not deactivate user"")

            user = await self.admin_handler.get_user(target_user)
            return 200, user

        else:  # create user
            password = body.get(""password"")
            if password is not None and (
                not isinstance(body[""password""], text_type)
                or len(body[""password""]) > 512
            ):
                raise SynapseError(400, ""Invalid password"")

            admin = body.get(""admin"", None)
            user_type = body.get(""user_type"", None)
            displayname = body.get(""displayname"", None)
            threepids = body.get(""threepids"", None)

            if user_type is not None and user_type not in UserTypes.ALL_USER_TYPES:
                raise SynapseError(400, ""Invalid user type"")

            user_id = await self.registration_handler.register_user(
                localpart=target_user.localpart,
                password=password,
                admin=bool(admin),
                default_display_name=displayname,
                user_type=user_type,
            )

            if ""threepids"" in body:
                # check for required parameters for each threepid
                for threepid in body[""threepids""]:
                    assert_params_in_dict(threepid, [""medium"", ""address""])

                current_time = self.hs.get_clock().time_msec()
                for threepid in body[""threepids""]:
                    await self.auth_handler.add_threepid(
                        user_id, threepid[""medium""], threepid[""address""], current_time
                    )

            if ""avatar_url"" in body:
                await self.profile_handler.set_avatar_url(
                    user_id, requester, body[""avatar_url""], True
                )

            ret = await self.admin_handler.get_user(target_user)

            return 201, ret","1. Use `assert_params_in_dict` to check for required parameters for each threepid.
2. Use `synapse.util.stringprep.to_ascii` to sanitize the input before storing it in the database.
3. Use `synapse.util.async_to_sync` to convert the async functions to sync functions so that they can be called from a non-async context."
"    def _check_sigs_and_hash_and_fetch(
        self, origin, pdus, room_version, outlier=False, include_none=False
    ):
        """"""Takes a list of PDUs and checks the signatures and hashs of each
        one. If a PDU fails its signature check then we check if we have it in
        the database and if not then request if from the originating server of
        that PDU.

        If a PDU fails its content hash check then it is redacted.

        The given list of PDUs are not modified, instead the function returns
        a new list.

        Args:
            origin (str)
            pdu (list)
            room_version (str)
            outlier (bool): Whether the events are outliers or not
            include_none (str): Whether to include None in the returned list
                for events that have failed their checks

        Returns:
            Deferred : A list of PDUs that have valid signatures and hashes.
        """"""
        deferreds = self._check_sigs_and_hashes(room_version, pdus)

        @defer.inlineCallbacks
        def handle_check_result(pdu, deferred):
            try:
                res = yield make_deferred_yieldable(deferred)
            except SynapseError:
                res = None

            if not res:
                # Check local db.
                res = yield self.store.get_event(
                    pdu.event_id, allow_rejected=True, allow_none=True
                )

            if not res and pdu.origin != origin:
                try:
                    res = yield defer.ensureDeferred(
                        self.get_pdu(
                            destinations=[pdu.origin],
                            event_id=pdu.event_id,
                            room_version=room_version,
                            outlier=outlier,
                            timeout=10000,
                        )
                    )
                except SynapseError:
                    pass

            if not res:
                logger.warning(
                    ""Failed to find copy of %s with valid signature"", pdu.event_id
                )

            return res

        handle = preserve_fn(handle_check_result)
        deferreds2 = [handle(pdu, deferred) for pdu, deferred in zip(pdus, deferreds)]

        valid_pdus = yield make_deferred_yieldable(
            defer.gatherResults(deferreds2, consumeErrors=True)
        ).addErrback(unwrapFirstError)

        if include_none:
            return valid_pdus
        else:
            return [p for p in valid_pdus if p]","1. Use `make_deferred_yieldable` to ensure that the deferred is run in a coroutine.
2. Use `unwrapFirstError` to handle errors from the deferred.
3. Use `preserve_fn` to preserve the function's signature when wrapping it in a coroutine."
"        def handle_check_result(pdu, deferred):
            try:
                res = yield make_deferred_yieldable(deferred)
            except SynapseError:
                res = None

            if not res:
                # Check local db.
                res = yield self.store.get_event(
                    pdu.event_id, allow_rejected=True, allow_none=True
                )

            if not res and pdu.origin != origin:
                try:
                    res = yield defer.ensureDeferred(
                        self.get_pdu(
                            destinations=[pdu.origin],
                            event_id=pdu.event_id,
                            room_version=room_version,
                            outlier=outlier,
                            timeout=10000,
                        )
                    )
                except SynapseError:
                    pass

            if not res:
                logger.warning(
                    ""Failed to find copy of %s with valid signature"", pdu.event_id
                )

            return res","1. Use `make_deferred_yieldable` to handle errors from `deferred`.
2. Check `allow_rejected` and `allow_none` when getting event from `store`.
3. Use `outlier` to filter events from untrusted sources."
"    def _check_sigs_and_hash(self, room_version, pdu):
        return make_deferred_yieldable(
            self._check_sigs_and_hashes(room_version, [pdu])[0]
        )","1. Use a cryptographically secure hash function, such as SHA-256 or SHA-512.
2. Use a salt to prevent rainbow table attacks.
3. Use a long, random key."
"    def _check_sigs_and_hashes(self, room_version, pdus):
        """"""Checks that each of the received events is correctly signed by the
        sending server.

        Args:
            room_version (str): The room version of the PDUs
            pdus (list[FrozenEvent]): the events to be checked

        Returns:
            list[Deferred]: for each input event, a deferred which:
              * returns the original event if the checks pass
              * returns a redacted version of the event (if the signature
                matched but the hash did not)
              * throws a SynapseError if the signature check failed.
            The deferreds run their callbacks in the sentinel
        """"""
        deferreds = _check_sigs_on_pdus(self.keyring, room_version, pdus)

        ctx = LoggingContext.current_context()

        def callback(_, pdu):
            with PreserveLoggingContext(ctx):
                if not check_event_content_hash(pdu):
                    # let's try to distinguish between failures because the event was
                    # redacted (which are somewhat expected) vs actual ball-tampering
                    # incidents.
                    #
                    # This is just a heuristic, so we just assume that if the keys are
                    # about the same between the redacted and received events, then the
                    # received event was probably a redacted copy (but we then use our
                    # *actual* redacted copy to be on the safe side.)
                    redacted_event = prune_event(pdu)
                    if set(redacted_event.keys()) == set(pdu.keys()) and set(
                        six.iterkeys(redacted_event.content)
                    ) == set(six.iterkeys(pdu.content)):
                        logger.info(
                            ""Event %s seems to have been redacted; using our redacted ""
                            ""copy"",
                            pdu.event_id,
                        )
                    else:
                        logger.warning(
                            ""Event %s content has been tampered, redacting"",
                            pdu.event_id,
                        )
                    return redacted_event

                if self.spam_checker.check_event_for_spam(pdu):
                    logger.warning(
                        ""Event contains spam, redacting %s: %s"",
                        pdu.event_id,
                        pdu.get_pdu_json(),
                    )
                    return prune_event(pdu)

                return pdu

        def errback(failure, pdu):
            failure.trap(SynapseError)
            with PreserveLoggingContext(ctx):
                logger.warning(
                    ""Signature check failed for %s: %s"",
                    pdu.event_id,
                    failure.getErrorMessage(),
                )
            return failure

        for deferred, pdu in zip(deferreds, pdus):
            deferred.addCallbacks(
                callback, errback, callbackArgs=[pdu], errbackArgs=[pdu]
            )

        return deferreds","1. Use a cryptographically secure hash function to check the event content hash.
2. Check the event for spam before accepting it.
3. Trap SynapseError and log the error message."
"        def callback(_, pdu):
            with PreserveLoggingContext(ctx):
                if not check_event_content_hash(pdu):
                    # let's try to distinguish between failures because the event was
                    # redacted (which are somewhat expected) vs actual ball-tampering
                    # incidents.
                    #
                    # This is just a heuristic, so we just assume that if the keys are
                    # about the same between the redacted and received events, then the
                    # received event was probably a redacted copy (but we then use our
                    # *actual* redacted copy to be on the safe side.)
                    redacted_event = prune_event(pdu)
                    if set(redacted_event.keys()) == set(pdu.keys()) and set(
                        six.iterkeys(redacted_event.content)
                    ) == set(six.iterkeys(pdu.content)):
                        logger.info(
                            ""Event %s seems to have been redacted; using our redacted ""
                            ""copy"",
                            pdu.event_id,
                        )
                    else:
                        logger.warning(
                            ""Event %s content has been tampered, redacting"",
                            pdu.event_id,
                        )
                    return redacted_event

                if self.spam_checker.check_event_for_spam(pdu):
                    logger.warning(
                        ""Event contains spam, redacting %s: %s"",
                        pdu.event_id,
                        pdu.get_pdu_json(),
                    )
                    return prune_event(pdu)

                return pdu","1. Use `six.iterkeys()` instead of `dict.keys()` to avoid a potential security vulnerability.
2. Use `logging.warning()` instead of `logging.info()` to log warnings.
3. Use `return prune_event(pdu)` instead of `return redacted_event` to avoid returning a redacted event when the event is not actually redacted."
"        def errback(failure, pdu):
            failure.trap(SynapseError)
            with PreserveLoggingContext(ctx):
                logger.warning(
                    ""Signature check failed for %s: %s"",
                    pdu.event_id,
                    failure.getErrorMessage(),
                )
            return failure","1. Use a more specific exception type than `SynapseError`.
2. Log the failure message with the event ID.
3. Return the failure instead of swallowing it."
"def _check_sigs_on_pdus(keyring, room_version, pdus):
    """"""Check that the given events are correctly signed

    Args:
        keyring (synapse.crypto.Keyring): keyring object to do the checks
        room_version (str): the room version of the PDUs
        pdus (Collection[EventBase]): the events to be checked

    Returns:
        List[Deferred]: a Deferred for each event in pdus, which will either succeed if
           the signatures are valid, or fail (with a SynapseError) if not.
    """"""

    # we want to check that the event is signed by:
    #
    # (a) the sender's server
    #
    #     - except in the case of invites created from a 3pid invite, which are exempt
    #     from this check, because the sender has to match that of the original 3pid
    #     invite, but the event may come from a different HS, for reasons that I don't
    #     entirely grok (why do the senders have to match? and if they do, why doesn't the
    #     joining server ask the inviting server to do the switcheroo with
    #     exchange_third_party_invite?).
    #
    #     That's pretty awful, since redacting such an invite will render it invalid
    #     (because it will then look like a regular invite without a valid signature),
    #     and signatures are *supposed* to be valid whether or not an event has been
    #     redacted. But this isn't the worst of the ways that 3pid invites are broken.
    #
    # (b) for V1 and V2 rooms, the server which created the event_id
    #
    # let's start by getting the domain for each pdu, and flattening the event back
    # to JSON.

    pdus_to_check = [
        PduToCheckSig(
            pdu=p,
            redacted_pdu_json=prune_event(p).get_pdu_json(),
            sender_domain=get_domain_from_id(p.sender),
            deferreds=[],
        )
        for p in pdus
    ]

    v = KNOWN_ROOM_VERSIONS.get(room_version)
    if not v:
        raise RuntimeError(""Unrecognized room version %s"" % (room_version,))

    # First we check that the sender event is signed by the sender's domain
    # (except if its a 3pid invite, in which case it may be sent by any server)
    pdus_to_check_sender = [p for p in pdus_to_check if not _is_invite_via_3pid(p.pdu)]

    more_deferreds = keyring.verify_json_objects_for_server(
        [
            (
                p.sender_domain,
                p.redacted_pdu_json,
                p.pdu.origin_server_ts if v.enforce_key_validity else 0,
                p.pdu.event_id,
            )
            for p in pdus_to_check_sender
        ]
    )

    def sender_err(e, pdu_to_check):
        errmsg = ""event id %s: unable to verify signature for sender %s: %s"" % (
            pdu_to_check.pdu.event_id,
            pdu_to_check.sender_domain,
            e.getErrorMessage(),
        )
        raise SynapseError(403, errmsg, Codes.FORBIDDEN)

    for p, d in zip(pdus_to_check_sender, more_deferreds):
        d.addErrback(sender_err, p)
        p.deferreds.append(d)

    # now let's look for events where the sender's domain is different to the
    # event id's domain (normally only the case for joins/leaves), and add additional
    # checks. Only do this if the room version has a concept of event ID domain
    # (ie, the room version uses old-style non-hash event IDs).
    if v.event_format == EventFormatVersions.V1:
        pdus_to_check_event_id = [
            p
            for p in pdus_to_check
            if p.sender_domain != get_domain_from_id(p.pdu.event_id)
        ]

        more_deferreds = keyring.verify_json_objects_for_server(
            [
                (
                    get_domain_from_id(p.pdu.event_id),
                    p.redacted_pdu_json,
                    p.pdu.origin_server_ts if v.enforce_key_validity else 0,
                    p.pdu.event_id,
                )
                for p in pdus_to_check_event_id
            ]
        )

        def event_err(e, pdu_to_check):
            errmsg = (
                ""event id %s: unable to verify signature for event id domain: %s""
                % (pdu_to_check.pdu.event_id, e.getErrorMessage())
            )
            raise SynapseError(403, errmsg, Codes.FORBIDDEN)

        for p, d in zip(pdus_to_check_event_id, more_deferreds):
            d.addErrback(event_err, p)
            p.deferreds.append(d)

    # replace lists of deferreds with single Deferreds
    return [_flatten_deferred_list(p.deferreds) for p in pdus_to_check]","1. Use a constant time comparison to prevent timing attacks.
2. Use cryptographically secure random number generator to generate the salt.
3. Use a secure hashing algorithm to hash the password."
"def _flatten_deferred_list(deferreds):
    """"""Given a list of deferreds, either return the single deferred,
    combine into a DeferredList, or return an already resolved deferred.
    """"""
    if len(deferreds) > 1:
        return DeferredList(deferreds, fireOnOneErrback=True, consumeErrors=True)
    elif len(deferreds) == 1:
        return deferreds[0]
    else:
        return defer.succeed(None)","1. Use `defer.ensureDeferred` to ensure that all deferreds are resolved before returning a value.
2. Use `defer.Deferred.addErrback` to handle errors that may occur during processing.
3. Use `defer.Deferred.addCallback` to execute code after a deferred has been resolved."
"def _is_invite_via_3pid(event):
    return (
        event.type == EventTypes.Member
        and event.membership == Membership.INVITE
        and ""third_party_invite"" in event.content
    )","1. Use `event.member` instead of `event.content['third_party_invite']` to get the third-party invite data. This will prevent an attacker from injecting arbitrary data into the event.
2. Use `event.timestamp` to check if the invite is still valid. This will prevent an attacker from using an old invite to join the team.
3. Use `event.actor` to check if the invite was sent by a trusted user. This will prevent an attacker from sending a malicious invite to a user."
"    async def backfill(
        self, dest: str, room_id: str, limit: int, extremities: Iterable[str]
    ) -> List[EventBase]:
        """"""Requests some more historic PDUs for the given room from the
        given destination server.

        Args:
            dest (str): The remote homeserver to ask.
            room_id (str): The room_id to backfill.
            limit (int): The maximum number of events to return.
            extremities (list): our current backwards extremities, to backfill from
        """"""
        logger.debug(""backfill extrem=%s"", extremities)

        # If there are no extremeties then we've (probably) reached the start.
        if not extremities:
            return

        transaction_data = await self.transport_layer.backfill(
            dest, room_id, extremities, limit
        )

        logger.debug(""backfill transaction_data=%r"", transaction_data)

        room_version = await self.store.get_room_version(room_id)

        pdus = [
            event_from_pdu_json(p, room_version, outlier=False)
            for p in transaction_data[""pdus""]
        ]

        # FIXME: We should handle signature failures more gracefully.
        pdus[:] = await make_deferred_yieldable(
            defer.gatherResults(
                self._check_sigs_and_hashes(room_version.identifier, pdus),
                consumeErrors=True,
            ).addErrback(unwrapFirstError)
        )

        return pdus","1. Use a more secure algorithm for signatures.
2. Handle signature failures gracefully.
3. Check the hashes of the PDUs."
"    async def get_pdu(
        self,
        destinations: Iterable[str],
        event_id: str,
        room_version: RoomVersion,
        outlier: bool = False,
        timeout: Optional[int] = None,
    ) -> Optional[EventBase]:
        """"""Requests the PDU with given origin and ID from the remote home
        servers.

        Will attempt to get the PDU from each destination in the list until
        one succeeds.

        Args:
            destinations: Which homeservers to query
            event_id: event to fetch
            room_version: version of the room
            outlier: Indicates whether the PDU is an `outlier`, i.e. if
                it's from an arbitary point in the context as opposed to part
                of the current block of PDUs. Defaults to `False`
            timeout: How long to try (in ms) each destination for before
                moving to the next destination. None indicates no timeout.

        Returns:
            The requested PDU, or None if we were unable to find it.
        """"""

        # TODO: Rate limit the number of times we try and get the same event.

        ev = self._get_pdu_cache.get(event_id)
        if ev:
            return ev

        pdu_attempts = self.pdu_destination_tried.setdefault(event_id, {})

        signed_pdu = None
        for destination in destinations:
            now = self._clock.time_msec()
            last_attempt = pdu_attempts.get(destination, 0)
            if last_attempt + PDU_RETRY_TIME_MS > now:
                continue

            try:
                transaction_data = await self.transport_layer.get_event(
                    destination, event_id, timeout=timeout
                )

                logger.debug(
                    ""retrieved event id %s from %s: %r"",
                    event_id,
                    destination,
                    transaction_data,
                )

                pdu_list = [
                    event_from_pdu_json(p, room_version, outlier=outlier)
                    for p in transaction_data[""pdus""]
                ]

                if pdu_list and pdu_list[0]:
                    pdu = pdu_list[0]

                    # Check signatures are correct.
                    signed_pdu = await self._check_sigs_and_hash(
                        room_version.identifier, pdu
                    )

                    break

                pdu_attempts[destination] = now

            except SynapseError as e:
                logger.info(
                    ""Failed to get PDU %s from %s because %s"", event_id, destination, e
                )
                continue
            except NotRetryingDestination as e:
                logger.info(str(e))
                continue
            except FederationDeniedError as e:
                logger.info(str(e))
                continue
            except Exception as e:
                pdu_attempts[destination] = now

                logger.info(
                    ""Failed to get PDU %s from %s because %s"", event_id, destination, e
                )
                continue

        if signed_pdu:
            self._get_pdu_cache[event_id] = signed_pdu

        return signed_pdu","1. Use rate limiting to prevent DDoS attacks.
2. Check signatures and hashes to verify the authenticity of the PDU.
3. Handle errors gracefully and log all exceptions."
"    async def send_join(
        self, destinations: Iterable[str], pdu: EventBase, room_version: RoomVersion
    ) -> Dict[str, Any]:
        """"""Sends a join event to one of a list of homeservers.

        Doing so will cause the remote server to add the event to the graph,
        and send the event out to the rest of the federation.

        Args:
            destinations: Candidate homeservers which are probably
                participating in the room.
            pdu: event to be sent
            room_version: the version of the room (according to the server that
                did the make_join)

        Returns:
            a dict with members ``origin`` (a string
            giving the server the event was sent to, ``state`` (?) and
            ``auth_chain``.

        Raises:
            SynapseError: if the chosen remote server returns a 300/400 code.

            RuntimeError: if no servers were reachable.
        """"""

        async def send_request(destination) -> Dict[str, Any]:
            content = await self._do_send_join(destination, pdu)

            logger.debug(""Got content: %s"", content)

            state = [
                event_from_pdu_json(p, room_version, outlier=True)
                for p in content.get(""state"", [])
            ]

            auth_chain = [
                event_from_pdu_json(p, room_version, outlier=True)
                for p in content.get(""auth_chain"", [])
            ]

            pdus = {p.event_id: p for p in itertools.chain(state, auth_chain)}

            create_event = None
            for e in state:
                if (e.type, e.state_key) == (EventTypes.Create, """"):
                    create_event = e
                    break

            if create_event is None:
                # If the state doesn't have a create event then the room is
                # invalid, and it would fail auth checks anyway.
                raise SynapseError(400, ""No create event in state"")

            # the room version should be sane.
            create_room_version = create_event.content.get(
                ""room_version"", RoomVersions.V1.identifier
            )
            if create_room_version != room_version.identifier:
                # either the server that fulfilled the make_join, or the server that is
                # handling the send_join, is lying.
                raise InvalidResponseError(
                    ""Unexpected room version %s in create event""
                    % (create_room_version,)
                )

            valid_pdus = await self._check_sigs_and_hash_and_fetch(
                destination,
                list(pdus.values()),
                outlier=True,
                room_version=room_version.identifier,
            )

            valid_pdus_map = {p.event_id: p for p in valid_pdus}

            # NB: We *need* to copy to ensure that we don't have multiple
            # references being passed on, as that causes... issues.
            signed_state = [
                copy.copy(valid_pdus_map[p.event_id])
                for p in state
                if p.event_id in valid_pdus_map
            ]

            signed_auth = [
                valid_pdus_map[p.event_id]
                for p in auth_chain
                if p.event_id in valid_pdus_map
            ]

            # NB: We *need* to copy to ensure that we don't have multiple
            # references being passed on, as that causes... issues.
            for s in signed_state:
                s.internal_metadata = copy.deepcopy(s.internal_metadata)

            # double-check that the same create event has ended up in the auth chain
            auth_chain_create_events = [
                e.event_id
                for e in signed_auth
                if (e.type, e.state_key) == (EventTypes.Create, """")
            ]
            if auth_chain_create_events != [create_event.event_id]:
                raise InvalidResponseError(
                    ""Unexpected create event(s) in auth chain""
                    % (auth_chain_create_events,)
                )

            return {
                ""state"": signed_state,
                ""auth_chain"": signed_auth,
                ""origin"": destination,
            }

        return await self._try_destination_list(""send_join"", destinations, send_request)","1. Use `event_from_pdu_json` to deserialize the events.
2. Check the room version of the create event and the response event.
3. Copy the events to avoid multiple references."
"        async def send_request(destination) -> Dict[str, Any]:
            content = await self._do_send_join(destination, pdu)

            logger.debug(""Got content: %s"", content)

            state = [
                event_from_pdu_json(p, room_version, outlier=True)
                for p in content.get(""state"", [])
            ]

            auth_chain = [
                event_from_pdu_json(p, room_version, outlier=True)
                for p in content.get(""auth_chain"", [])
            ]

            pdus = {p.event_id: p for p in itertools.chain(state, auth_chain)}

            create_event = None
            for e in state:
                if (e.type, e.state_key) == (EventTypes.Create, """"):
                    create_event = e
                    break

            if create_event is None:
                # If the state doesn't have a create event then the room is
                # invalid, and it would fail auth checks anyway.
                raise SynapseError(400, ""No create event in state"")

            # the room version should be sane.
            create_room_version = create_event.content.get(
                ""room_version"", RoomVersions.V1.identifier
            )
            if create_room_version != room_version.identifier:
                # either the server that fulfilled the make_join, or the server that is
                # handling the send_join, is lying.
                raise InvalidResponseError(
                    ""Unexpected room version %s in create event""
                    % (create_room_version,)
                )

            valid_pdus = await self._check_sigs_and_hash_and_fetch(
                destination,
                list(pdus.values()),
                outlier=True,
                room_version=room_version.identifier,
            )

            valid_pdus_map = {p.event_id: p for p in valid_pdus}

            # NB: We *need* to copy to ensure that we don't have multiple
            # references being passed on, as that causes... issues.
            signed_state = [
                copy.copy(valid_pdus_map[p.event_id])
                for p in state
                if p.event_id in valid_pdus_map
            ]

            signed_auth = [
                valid_pdus_map[p.event_id]
                for p in auth_chain
                if p.event_id in valid_pdus_map
            ]

            # NB: We *need* to copy to ensure that we don't have multiple
            # references being passed on, as that causes... issues.
            for s in signed_state:
                s.internal_metadata = copy.deepcopy(s.internal_metadata)

            # double-check that the same create event has ended up in the auth chain
            auth_chain_create_events = [
                e.event_id
                for e in signed_auth
                if (e.type, e.state_key) == (EventTypes.Create, """")
            ]
            if auth_chain_create_events != [create_event.event_id]:
                raise InvalidResponseError(
                    ""Unexpected create event(s) in auth chain""
                    % (auth_chain_create_events,)
                )

            return {
                ""state"": signed_state,
                ""auth_chain"": signed_auth,
                ""origin"": destination,
            }","1. Use `event_from_pdu_json` to parse the events.
2. Check the room version of the create event and the auth chain.
3. Make a copy of the events to avoid multiple references."
"    def read_config(self, config: dict, config_dir_path: str, **kwargs):

        acme_config = config.get(""acme"", None)
        if acme_config is None:
            acme_config = {}

        self.acme_enabled = acme_config.get(""enabled"", False)

        # hyperlink complains on py2 if this is not a Unicode
        self.acme_url = six.text_type(
            acme_config.get(""url"", ""https://acme-v01.api.letsencrypt.org/directory"")
        )
        self.acme_port = acme_config.get(""port"", 80)
        self.acme_bind_addresses = acme_config.get(""bind_addresses"", [""::"", ""0.0.0.0""])
        self.acme_reprovision_threshold = acme_config.get(""reprovision_threshold"", 30)
        self.acme_domain = acme_config.get(""domain"", config.get(""server_name""))

        self.acme_account_key_file = self.abspath(
            acme_config.get(""account_key_file"", config_dir_path + ""/client.key"")
        )

        self.tls_certificate_file = self.abspath(config.get(""tls_certificate_path""))
        self.tls_private_key_file = self.abspath(config.get(""tls_private_key_path""))

        if self.root.server.has_tls_listener():
            if not self.tls_certificate_file:
                raise ConfigError(
                    ""tls_certificate_path must be specified if TLS-enabled listeners are ""
                    ""configured.""
                )
            if not self.tls_private_key_file:
                raise ConfigError(
                    ""tls_private_key_path must be specified if TLS-enabled listeners are ""
                    ""configured.""
                )

        self._original_tls_fingerprints = config.get(""tls_fingerprints"", [])

        if self._original_tls_fingerprints is None:
            self._original_tls_fingerprints = []

        self.tls_fingerprints = list(self._original_tls_fingerprints)

        # Whether to verify certificates on outbound federation traffic
        self.federation_verify_certificates = config.get(
            ""federation_verify_certificates"", True
        )

        # Minimum TLS version to use for outbound federation traffic
        self.federation_client_minimum_tls_version = str(
            config.get(""federation_client_minimum_tls_version"", 1)
        )

        if self.federation_client_minimum_tls_version not in [""1"", ""1.1"", ""1.2"", ""1.3""]:
            raise ConfigError(
                ""federation_client_minimum_tls_version must be one of: 1, 1.1, 1.2, 1.3""
            )

        # Prevent people shooting themselves in the foot here by setting it to
        # the biggest number blindly
        if self.federation_client_minimum_tls_version == ""1.3"":
            if getattr(SSL, ""OP_NO_TLSv1_3"", None) is None:
                raise ConfigError(
                    (
                        ""federation_client_minimum_tls_version cannot be 1.3, ""
                        ""your OpenSSL does not support it""
                    )
                )

        # Whitelist of domains to not verify certificates for
        fed_whitelist_entries = config.get(
            ""federation_certificate_verification_whitelist"", []
        )

        # Support globs (*) in whitelist values
        self.federation_certificate_verification_whitelist = []  # type: List[str]
        for entry in fed_whitelist_entries:
            try:
                entry_regex = glob_to_regex(entry.encode(""ascii"").decode(""ascii""))
            except UnicodeEncodeError:
                raise ConfigError(
                    ""IDNA domain names are not allowed in the ""
                    ""federation_certificate_verification_whitelist: %s"" % (entry,)
                )

            # Convert globs to regex
            self.federation_certificate_verification_whitelist.append(entry_regex)

        # List of custom certificate authorities for federation traffic validation
        custom_ca_list = config.get(""federation_custom_ca_list"", None)

        # Read in and parse custom CA certificates
        self.federation_ca_trust_root = None
        if custom_ca_list is not None:
            if len(custom_ca_list) == 0:
                # A trustroot cannot be generated without any CA certificates.
                # Raise an error if this option has been specified without any
                # corresponding certificates.
                raise ConfigError(
                    ""federation_custom_ca_list specified without ""
                    ""any certificate files""
                )

            certs = []
            for ca_file in custom_ca_list:
                logger.debug(""Reading custom CA certificate file: %s"", ca_file)
                content = self.read_file(ca_file, ""federation_custom_ca_list"")

                # Parse the CA certificates
                try:
                    cert_base = Certificate.loadPEM(content)
                    certs.append(cert_base)
                except Exception as e:
                    raise ConfigError(
                        ""Error parsing custom CA certificate file %s: %s"" % (ca_file, e)
                    )

            self.federation_ca_trust_root = trustRootFromCertificates(certs)

        # This config option applies to non-federation HTTP clients
        # (e.g. for talking to recaptcha, identity servers, and such)
        # It should never be used in production, and is intended for
        # use only when running tests.
        self.use_insecure_ssl_client_just_for_testing_do_not_use = config.get(
            ""use_insecure_ssl_client_just_for_testing_do_not_use""
        )

        self.tls_certificate = None
        self.tls_private_key = None","1. Use a secure TLS certificate and private key.
2. Verify certificates on outbound federation traffic.
3. Use a minimum TLS version of 1.2 or higher."
"    def __init__(self, database: Database, db_conn, hs):
        super(MonthlyActiveUsersStore, self).__init__(database, db_conn, hs)
        self._clock = hs.get_clock()
        self.hs = hs
        # Do not add more reserved users than the total allowable number
        self.db.new_transaction(
            db_conn,
            ""initialise_mau_threepids"",
            [],
            [],
            self._initialise_reserved_users,
            hs.config.mau_limits_reserved_threepids[: self.hs.config.max_mau_value],
        )","1. Use prepared statements instead of building queries manually to avoid SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use strong encryption for passwords and other sensitive data."
"    def reap_monthly_active_users(self):
        """"""Cleans out monthly active user table to ensure that no stale
        entries exist.

        Returns:
            Deferred[]
        """"""

        def _reap_users(txn, reserved_users):
            """"""
            Args:
                reserved_users (tuple): reserved users to preserve
            """"""

            thirty_days_ago = int(self._clock.time_msec()) - (1000 * 60 * 60 * 24 * 30)
            query_args = [thirty_days_ago]
            base_sql = ""DELETE FROM monthly_active_users WHERE timestamp < ?""

            # Need if/else since 'AND user_id NOT IN ({})' fails on Postgres
            # when len(reserved_users) == 0. Works fine on sqlite.
            if len(reserved_users) > 0:
                # questionmarks is a hack to overcome sqlite not supporting
                # tuples in 'WHERE IN %s'
                question_marks = "","".join(""?"" * len(reserved_users))

                query_args.extend(reserved_users)
                sql = base_sql + "" AND user_id NOT IN ({})"".format(question_marks)
            else:
                sql = base_sql

            txn.execute(sql, query_args)

            max_mau_value = self.hs.config.max_mau_value
            if self.hs.config.limit_usage_by_mau:
                # If MAU user count still exceeds the MAU threshold, then delete on
                # a least recently active basis.
                # Note it is not possible to write this query using OFFSET due to
                # incompatibilities in how sqlite and postgres support the feature.
                # sqlite requires 'LIMIT -1 OFFSET ?', the LIMIT must be present
                # While Postgres does not require 'LIMIT', but also does not support
                # negative LIMIT values. So there is no way to write it that both can
                # support
                if len(reserved_users) == 0:
                    sql = """"""
                        DELETE FROM monthly_active_users
                        WHERE user_id NOT IN (
                            SELECT user_id FROM monthly_active_users
                            ORDER BY timestamp DESC
                            LIMIT ?
                        )
                        """"""
                    txn.execute(sql, (max_mau_value,))
                # Need if/else since 'AND user_id NOT IN ({})' fails on Postgres
                # when len(reserved_users) == 0. Works fine on sqlite.
                else:
                    # Must be >= 0 for postgres
                    num_of_non_reserved_users_to_remove = max(
                        max_mau_value - len(reserved_users), 0
                    )

                    # It is important to filter reserved users twice to guard
                    # against the case where the reserved user is present in the
                    # SELECT, meaning that a legitmate mau is deleted.
                    sql = """"""
                        DELETE FROM monthly_active_users
                        WHERE user_id NOT IN (
                            SELECT user_id FROM monthly_active_users
                            WHERE user_id NOT IN ({})
                            ORDER BY timestamp DESC
                            LIMIT ?
                        )
                        AND user_id NOT IN ({})
                    """""".format(
                        question_marks, question_marks
                    )

                    query_args = [
                        *reserved_users,
                        num_of_non_reserved_users_to_remove,
                        *reserved_users,
                    ]

                    txn.execute(sql, query_args)

        reserved_users = yield self.get_registered_reserved_users()
        yield self.db.runInteraction(
            ""reap_monthly_active_users"", _reap_users, reserved_users
        )
        # It seems poor to invalidate the whole cache, Postgres supports
        # 'Returning' which would allow me to invalidate only the
        # specific users, but sqlite has no way to do this and instead
        # I would need to SELECT and the DELETE which without locking
        # is racy.
        # Have resolved to invalidate the whole cache for now and do
        # something about it if and when the perf becomes significant
        self.user_last_seen_monthly_active.invalidate_all()
        self.get_monthly_active_count.invalidate_all()","1. Use prepared statements instead of building queries with string concatenation.
2. Use LIMIT and OFFSET to delete rows in a specific order.
3. Use transactions to ensure that multiple operations are performed atomically."
"        def _reap_users(txn, reserved_users):
            """"""
            Args:
                reserved_users (tuple): reserved users to preserve
            """"""

            thirty_days_ago = int(self._clock.time_msec()) - (1000 * 60 * 60 * 24 * 30)
            query_args = [thirty_days_ago]
            base_sql = ""DELETE FROM monthly_active_users WHERE timestamp < ?""

            # Need if/else since 'AND user_id NOT IN ({})' fails on Postgres
            # when len(reserved_users) == 0. Works fine on sqlite.
            if len(reserved_users) > 0:
                # questionmarks is a hack to overcome sqlite not supporting
                # tuples in 'WHERE IN %s'
                question_marks = "","".join(""?"" * len(reserved_users))

                query_args.extend(reserved_users)
                sql = base_sql + "" AND user_id NOT IN ({})"".format(question_marks)
            else:
                sql = base_sql

            txn.execute(sql, query_args)

            max_mau_value = self.hs.config.max_mau_value
            if self.hs.config.limit_usage_by_mau:
                # If MAU user count still exceeds the MAU threshold, then delete on
                # a least recently active basis.
                # Note it is not possible to write this query using OFFSET due to
                # incompatibilities in how sqlite and postgres support the feature.
                # sqlite requires 'LIMIT -1 OFFSET ?', the LIMIT must be present
                # While Postgres does not require 'LIMIT', but also does not support
                # negative LIMIT values. So there is no way to write it that both can
                # support
                if len(reserved_users) == 0:
                    sql = """"""
                        DELETE FROM monthly_active_users
                        WHERE user_id NOT IN (
                            SELECT user_id FROM monthly_active_users
                            ORDER BY timestamp DESC
                            LIMIT ?
                        )
                        """"""
                    txn.execute(sql, (max_mau_value,))
                # Need if/else since 'AND user_id NOT IN ({})' fails on Postgres
                # when len(reserved_users) == 0. Works fine on sqlite.
                else:
                    # Must be >= 0 for postgres
                    num_of_non_reserved_users_to_remove = max(
                        max_mau_value - len(reserved_users), 0
                    )

                    # It is important to filter reserved users twice to guard
                    # against the case where the reserved user is present in the
                    # SELECT, meaning that a legitmate mau is deleted.
                    sql = """"""
                        DELETE FROM monthly_active_users
                        WHERE user_id NOT IN (
                            SELECT user_id FROM monthly_active_users
                            WHERE user_id NOT IN ({})
                            ORDER BY timestamp DESC
                            LIMIT ?
                        )
                        AND user_id NOT IN ({})
                    """""".format(
                        question_marks, question_marks
                    )

                    query_args = [
                        *reserved_users,
                        num_of_non_reserved_users_to_remove,
                        *reserved_users,
                    ]

                    txn.execute(sql, query_args)","1. Use prepared statements instead of building queries with string concatenation.
2. Use LIMIT OFFSET to truncate the results set instead of deleting rows in a loop.
3. Use DISTINCT to avoid deleting the same user multiple times."
"    def upsert_monthly_active_user(self, user_id):
        """"""Updates or inserts the user into the monthly active user table, which
        is used to track the current MAU usage of the server

        Args:
            user_id (str): user to add/update
        """"""
        # Support user never to be included in MAU stats. Note I can't easily call this
        # from upsert_monthly_active_user_txn because then I need a _txn form of
        # is_support_user which is complicated because I want to cache the result.
        # Therefore I call it here and ignore the case where
        # upsert_monthly_active_user_txn is called directly from
        # _initialise_reserved_users reasoning that it would be very strange to
        #  include a support user in this context.

        is_support = yield self.is_support_user(user_id)
        if is_support:
            return

        yield self.db.runInteraction(
            ""upsert_monthly_active_user"", self.upsert_monthly_active_user_txn, user_id
        )

        user_in_mau = self.user_last_seen_monthly_active.cache.get(
            (user_id,), None, update_metrics=False
        )
        if user_in_mau is None:
            self.get_monthly_active_count.invalidate(())

        self.user_last_seen_monthly_active.invalidate((user_id,))","1. Use `@require_user_id` decorator to protect the function from unauthorized access.
2. Use `check_user_access` function to check if the user has permission to access the data.
3. Use `user_last_seen_monthly_active.cache.set` to cache the result instead of `user_last_seen_monthly_active.cache.get` to avoid race conditions."
"    def upsert_monthly_active_user_txn(self, txn, user_id):
        """"""Updates or inserts monthly active user member

        Note that, after calling this method, it will generally be necessary
        to invalidate the caches on user_last_seen_monthly_active and
        get_monthly_active_count. We can't do that here, because we are running
        in a database thread rather than the main thread, and we can't call
        txn.call_after because txn may not be a LoggingTransaction.

        We consciously do not call is_support_txn from this method because it
        is not possible to cache the response. is_support_txn will be false in
        almost all cases, so it seems reasonable to call it only for
        upsert_monthly_active_user and to call is_support_txn manually
        for cases where upsert_monthly_active_user_txn is called directly,
        like _initialise_reserved_users

        In short, don't call this method with support users. (Support users
        should not appear in the MAU stats).

        Args:
            txn (cursor):
            user_id (str): user to add/update

        Returns:
            bool: True if a new entry was created, False if an
            existing one was updated.
        """"""

        # Am consciously deciding to lock the table on the basis that is ought
        # never be a big table and alternative approaches (batching multiple
        # upserts into a single txn) introduced a lot of extra complexity.
        # See https://github.com/matrix-org/synapse/issues/3854 for more
        is_insert = self.db.simple_upsert_txn(
            txn,
            table=""monthly_active_users"",
            keyvalues={""user_id"": user_id},
            values={""timestamp"": int(self._clock.time_msec())},
        )

        return is_insert","1. Use prepared statements instead of building queries manually to avoid SQL injection attacks.
2. Use transactions to ensure that multiple updates are atomic.
3. Validate user input to prevent malicious users from injecting invalid data into the database."
"    def read_config(self, config, config_dir_path, **kwargs):

        acme_config = config.get(""acme"", None)
        if acme_config is None:
            acme_config = {}

        self.acme_enabled = acme_config.get(""enabled"", False)

        # hyperlink complains on py2 if this is not a Unicode
        self.acme_url = six.text_type(
            acme_config.get(""url"", ""https://acme-v01.api.letsencrypt.org/directory"")
        )
        self.acme_port = acme_config.get(""port"", 80)
        self.acme_bind_addresses = acme_config.get(""bind_addresses"", [""::"", ""0.0.0.0""])
        self.acme_reprovision_threshold = acme_config.get(""reprovision_threshold"", 30)
        self.acme_domain = acme_config.get(""domain"", config.get(""server_name""))

        self.acme_account_key_file = self.abspath(
            acme_config.get(""account_key_file"", config_dir_path + ""/client.key"")
        )

        self.tls_certificate_file = self.abspath(config.get(""tls_certificate_path""))
        self.tls_private_key_file = self.abspath(config.get(""tls_private_key_path""))

        if self.has_tls_listener():
            if not self.tls_certificate_file:
                raise ConfigError(
                    ""tls_certificate_path must be specified if TLS-enabled listeners are ""
                    ""configured.""
                )
            if not self.tls_private_key_file:
                raise ConfigError(
                    ""tls_private_key_path must be specified if TLS-enabled listeners are ""
                    ""configured.""
                )

        self._original_tls_fingerprints = config.get(""tls_fingerprints"", [])

        if self._original_tls_fingerprints is None:
            self._original_tls_fingerprints = []

        self.tls_fingerprints = list(self._original_tls_fingerprints)

        # Whether to verify certificates on outbound federation traffic
        self.federation_verify_certificates = config.get(
            ""federation_verify_certificates"", True
        )

        # Minimum TLS version to use for outbound federation traffic
        self.federation_client_minimum_tls_version = str(
            config.get(""federation_client_minimum_tls_version"", 1)
        )

        if self.federation_client_minimum_tls_version not in [""1"", ""1.1"", ""1.2"", ""1.3""]:
            raise ConfigError(
                ""federation_client_minimum_tls_version must be one of: 1, 1.1, 1.2, 1.3""
            )

        # Prevent people shooting themselves in the foot here by setting it to
        # the biggest number blindly
        if self.federation_client_minimum_tls_version == ""1.3"":
            if getattr(SSL, ""OP_NO_TLSv1_3"", None) is None:
                raise ConfigError(
                    (
                        ""federation_client_minimum_tls_version cannot be 1.3, ""
                        ""your OpenSSL does not support it""
                    )
                )

        # Whitelist of domains to not verify certificates for
        fed_whitelist_entries = config.get(
            ""federation_certificate_verification_whitelist"", []
        )

        # Support globs (*) in whitelist values
        self.federation_certificate_verification_whitelist = []
        for entry in fed_whitelist_entries:
            # Convert globs to regex
            entry_regex = glob_to_regex(entry)
            self.federation_certificate_verification_whitelist.append(entry_regex)

        # List of custom certificate authorities for federation traffic validation
        custom_ca_list = config.get(""federation_custom_ca_list"", None)

        # Read in and parse custom CA certificates
        self.federation_ca_trust_root = None
        if custom_ca_list is not None:
            if len(custom_ca_list) == 0:
                # A trustroot cannot be generated without any CA certificates.
                # Raise an error if this option has been specified without any
                # corresponding certificates.
                raise ConfigError(
                    ""federation_custom_ca_list specified without ""
                    ""any certificate files""
                )

            certs = []
            for ca_file in custom_ca_list:
                logger.debug(""Reading custom CA certificate file: %s"", ca_file)
                content = self.read_file(ca_file, ""federation_custom_ca_list"")

                # Parse the CA certificates
                try:
                    cert_base = Certificate.loadPEM(content)
                    certs.append(cert_base)
                except Exception as e:
                    raise ConfigError(
                        ""Error parsing custom CA certificate file %s: %s"" % (ca_file, e)
                    )

            self.federation_ca_trust_root = trustRootFromCertificates(certs)

        # This config option applies to non-federation HTTP clients
        # (e.g. for talking to recaptcha, identity servers, and such)
        # It should never be used in production, and is intended for
        # use only when running tests.
        self.use_insecure_ssl_client_just_for_testing_do_not_use = config.get(
            ""use_insecure_ssl_client_just_for_testing_do_not_use""
        )

        self.tls_certificate = None
        self.tls_private_key = None","1. Use a secure TLS certificate and private key.
2. Verify certificates on outbound federation traffic.
3. Use a minimum TLS version of 1.2 or higher."
"    def get_options(self, host):
        # Check if certificate verification has been enabled
        should_verify = self._config.federation_verify_certificates

        # Check if we've disabled certificate verification for this host
        if should_verify:
            for regex in self._config.federation_certificate_verification_whitelist:
                if regex.match(host):
                    should_verify = False
                    break

        ssl_context = (
            self._verify_ssl_context if should_verify else self._no_verify_ssl_context
        )

        return SSLClientConnectionCreator(host, ssl_context, should_verify)","1. Use a secure default for certificate verification.
2. Use a whitelist for hosts that are allowed to bypass certificate verification.
3. Use a separate context for each host to avoid connection leaks."
"    def __init__(self, hostname, ctx, verify_certs):
        self._ctx = ctx
        self._verifier = ConnectionVerifier(hostname, verify_certs)","1. Use a secure random number generator to generate the secret key.
2. Use TLS 1.2 or higher with strong ciphers.
3. Verify the server's certificate against a trusted CA."
"    def __init__(self, hostname, verify_certs):
        self._verify_certs = verify_certs

        if isIPAddress(hostname) or isIPv6Address(hostname):
            self._hostnameBytes = hostname.encode(""ascii"")
            self._is_ip_address = True
        else:
            # twisted's ClientTLSOptions falls back to the stdlib impl here if
            # idna is not installed, but points out that lacks support for
            # IDNA2008 (http://bugs.python.org/issue17305).
            #
            # We can rely on having idna.
            self._hostnameBytes = idna.encode(hostname)
            self._is_ip_address = False

        self._hostnameASCII = self._hostnameBytes.decode(""ascii"")","1. Use `hostname.encode(""idna"")` instead of `idna.encode(hostname)` to avoid falling back to the stdlib implementation which lacks support for IDNA2008.
2. Use `hostname.decode(""ascii"")` instead of `_hostnameBytes.decode(""ascii"")` to avoid decoding a non-ASCII string.
3. Use `is_ip_address(hostname)` to check if the hostname is an IP address, and use `hostname.encode(""ascii"")` if it is."
"    def __init__(self, reactor, tls_client_options_factory, srv_resolver, parsed_uri):
        self._reactor = reactor

        self._parsed_uri = parsed_uri

        # set up the TLS connection params
        #
        # XXX disabling TLS is really only supported here for the benefit of the
        # unit tests. We should make the UTs cope with TLS rather than having to make
        # the code support the unit tests.

        if tls_client_options_factory is None:
            self._tls_options = None
        else:
            self._tls_options = tls_client_options_factory.get_options(
                self._parsed_uri.host.decode(""ascii"")
            )

        self._srv_resolver = srv_resolver","1. Use a secure TLS connection by setting `tls_client_options_factory`.
2. Use a valid certificate by passing the host name to `tls_client_options_factory.get_options()`.
3. Use a secure DNS resolver by passing a `srv_resolver` to the constructor."
"    def compute_state_delta(
        self, room_id, batch, sync_config, since_token, now_token, full_state
    ):
        """""" Works out the difference in state between the start of the timeline
        and the previous sync.

        Args:
            room_id(str):
            batch(synapse.handlers.sync.TimelineBatch): The timeline batch for
                the room that will be sent to the user.
            sync_config(synapse.handlers.sync.SyncConfig):
            since_token(str|None): Token of the end of the previous batch. May
                be None.
            now_token(str): Token of the end of the current batch.
            full_state(bool): Whether to force returning the full state.

        Returns:
             A deferred dict of (type, state_key) -> Event
        """"""
        # TODO(mjark) Check if the state events were received by the server
        # after the previous sync, since we need to include those state
        # updates even if they occured logically before the previous event.
        # TODO(mjark) Check for new redactions in the state events.

        with Measure(self.clock, ""compute_state_delta""):

            members_to_fetch = None

            lazy_load_members = sync_config.filter_collection.lazy_load_members()
            include_redundant_members = (
                sync_config.filter_collection.include_redundant_members()
            )

            if lazy_load_members:
                # We only request state for the members needed to display the
                # timeline:

                members_to_fetch = set(
                    event.sender  # FIXME: we also care about invite targets etc.
                    for event in batch.events
                )

                if full_state:
                    # always make sure we LL ourselves so we know we're in the room
                    # (if we are) to fix https://github.com/vector-im/riot-web/issues/7209
                    # We only need apply this on full state syncs given we disabled
                    # LL for incr syncs in #3840.
                    members_to_fetch.add(sync_config.user.to_string())

                state_filter = StateFilter.from_lazy_load_member_list(members_to_fetch)
            else:
                state_filter = StateFilter.all()

            timeline_state = {
                (event.type, event.state_key): event.event_id
                for event in batch.events
                if event.is_state()
            }

            if full_state:
                if batch:
                    current_state_ids = yield self.store.get_state_ids_for_event(
                        batch.events[-1].event_id, state_filter=state_filter
                    )

                    state_ids = yield self.store.get_state_ids_for_event(
                        batch.events[0].event_id, state_filter=state_filter
                    )

                else:
                    current_state_ids = yield self.get_state_at(
                        room_id, stream_position=now_token, state_filter=state_filter
                    )

                    state_ids = current_state_ids

                state_ids = _calculate_state(
                    timeline_contains=timeline_state,
                    timeline_start=state_ids,
                    previous={},
                    current=current_state_ids,
                    lazy_load_members=lazy_load_members,
                )
            elif batch.limited:
                state_at_timeline_start = yield self.store.get_state_ids_for_event(
                    batch.events[0].event_id, state_filter=state_filter
                )

                # for now, we disable LL for gappy syncs - see
                # https://github.com/vector-im/riot-web/issues/7211#issuecomment-419976346
                # N.B. this slows down incr syncs as we are now processing way
                # more state in the server than if we were LLing.
                #
                # We still have to filter timeline_start to LL entries (above) in order
                # for _calculate_state's LL logic to work, as we have to include LL
                # members for timeline senders in case they weren't loaded in the initial
                # sync.  We do this by (counterintuitively) by filtering timeline_start
                # members to just be ones which were timeline senders, which then ensures
                # all of the rest get included in the state block (if we need to know
                # about them).
                state_filter = StateFilter.all()

                state_at_previous_sync = yield self.get_state_at(
                    room_id, stream_position=since_token, state_filter=state_filter
                )

                current_state_ids = yield self.store.get_state_ids_for_event(
                    batch.events[-1].event_id, state_filter=state_filter
                )

                state_ids = _calculate_state(
                    timeline_contains=timeline_state,
                    timeline_start=state_at_timeline_start,
                    previous=state_at_previous_sync,
                    current=current_state_ids,
                    # we have to include LL members in case LL initial sync missed them
                    lazy_load_members=lazy_load_members,
                )
            else:
                state_ids = {}
                if lazy_load_members:
                    if members_to_fetch and batch.events:
                        # We're returning an incremental sync, with no
                        # ""gap"" since the previous sync, so normally there would be
                        # no state to return.
                        # But we're lazy-loading, so the client might need some more
                        # member events to understand the events in this timeline.
                        # So we fish out all the member events corresponding to the
                        # timeline here, and then dedupe any redundant ones below.

                        state_ids = yield self.store.get_state_ids_for_event(
                            batch.events[0].event_id,
                            # we only want members!
                            state_filter=StateFilter.from_types(
                                (EventTypes.Member, member)
                                for member in members_to_fetch
                            ),
                        )

            if lazy_load_members and not include_redundant_members:
                cache_key = (sync_config.user.to_string(), sync_config.device_id)
                cache = self.get_lazy_loaded_members_cache(cache_key)

                # if it's a new sync sequence, then assume the client has had
                # amnesia and doesn't want any recent lazy-loaded members
                # de-duplicated.
                if since_token is None:
                    logger.debug(""clearing LruCache for %r"", cache_key)
                    cache.clear()
                else:
                    # only send members which aren't in our LruCache (either
                    # because they're new to this client or have been pushed out
                    # of the cache)
                    logger.debug(""filtering state from %r..."", state_ids)
                    state_ids = {
                        t: event_id
                        for t, event_id in iteritems(state_ids)
                        if cache.get(t[1]) != event_id
                    }
                    logger.debug(""...to %r"", state_ids)

                # add any member IDs we are about to send into our LruCache
                for t, event_id in itertools.chain(
                    state_ids.items(), timeline_state.items()
                ):
                    if t[0] == EventTypes.Member:
                        cache.set(t[1], event_id)

        state = {}
        if state_ids:
            state = yield self.store.get_events(list(state_ids.values()))

        return {
            (e.type, e.state_key): e
            for e in sync_config.filter_collection.filter_room_state(
                list(state.values())
            )
        }","1. Use a constant time comparison operator for checking if two strings are equal. This will prevent timing attacks.
2. Use proper escaping when including user input in SQL queries. This will prevent SQL injection attacks.
3. Use a secure random number generator to generate tokens. This will prevent attackers from guessing or predicting token values."
"    def _generate_room_entry(
        self,
        sync_result_builder,
        ignored_users,
        room_builder,
        ephemeral,
        tags,
        account_data,
        always_include=False,
    ):
        """"""Populates the `joined` and `archived` section of `sync_result_builder`
        based on the `room_builder`.

        Args:
            sync_result_builder(SyncResultBuilder)
            ignored_users(set(str)): Set of users ignored by user.
            room_builder(RoomSyncResultBuilder)
            ephemeral(list): List of new ephemeral events for room
            tags(list): List of *all* tags for room, or None if there has been
                no change.
            account_data(list): List of new account data for room
            always_include(bool): Always include this room in the sync response,
                even if empty.
        """"""
        newly_joined = room_builder.newly_joined
        full_state = (
            room_builder.full_state or newly_joined or sync_result_builder.full_state
        )
        events = room_builder.events

        # We want to shortcut out as early as possible.
        if not (always_include or account_data or ephemeral or full_state):
            if events == [] and tags is None:
                return

        now_token = sync_result_builder.now_token
        sync_config = sync_result_builder.sync_config

        room_id = room_builder.room_id
        since_token = room_builder.since_token
        upto_token = room_builder.upto_token

        batch = yield self._load_filtered_recents(
            room_id,
            sync_config,
            now_token=upto_token,
            since_token=since_token,
            recents=events,
            newly_joined_room=newly_joined,
        )

        if newly_joined:
            # debug for https://github.com/matrix-org/synapse/issues/4422
            issue4422_logger.debug(
                ""Timeline events after filtering in newly-joined room %s: %r"",
                room_id,
                batch,
            )

        # When we join the room (or the client requests full_state), we should
        # send down any existing tags. Usually the user won't have tags in a
        # newly joined room, unless either a) they've joined before or b) the
        # tag was added by synapse e.g. for server notice rooms.
        if full_state:
            user_id = sync_result_builder.sync_config.user.to_string()
            tags = yield self.store.get_tags_for_room(user_id, room_id)

            # If there aren't any tags, don't send the empty tags list down
            # sync
            if not tags:
                tags = None

        account_data_events = []
        if tags is not None:
            account_data_events.append({""type"": ""m.tag"", ""content"": {""tags"": tags}})

        for account_data_type, content in account_data.items():
            account_data_events.append({""type"": account_data_type, ""content"": content})

        account_data_events = sync_config.filter_collection.filter_room_account_data(
            account_data_events
        )

        ephemeral = sync_config.filter_collection.filter_room_ephemeral(ephemeral)

        if not (
            always_include or batch or account_data_events or ephemeral or full_state
        ):
            return

        state = yield self.compute_state_delta(
            room_id, batch, sync_config, since_token, now_token, full_state=full_state
        )

        summary = {}

        # we include a summary in room responses when we're lazy loading
        # members (as the client otherwise doesn't have enough info to form
        # the name itself).
        if sync_config.filter_collection.lazy_load_members() and (
            # we recalulate the summary:
            #   if there are membership changes in the timeline, or
            #   if membership has changed during a gappy sync, or
            #   if this is an initial sync.
            any(ev.type == EventTypes.Member for ev in batch.events)
            or (
                # XXX: this may include false positives in the form of LL
                # members which have snuck into state
                batch.limited
                and any(t == EventTypes.Member for (t, k) in state)
            )
            or since_token is None
        ):
            summary = yield self.compute_summary(
                room_id, sync_config, batch, state, now_token
            )

        if room_builder.rtype == ""joined"":
            unread_notifications = {}
            room_sync = JoinedSyncResult(
                room_id=room_id,
                timeline=batch,
                state=state,
                ephemeral=ephemeral,
                account_data=account_data_events,
                unread_notifications=unread_notifications,
                summary=summary,
            )

            if room_sync or always_include:
                notifs = yield self.unread_notifs_for_room_id(room_id, sync_config)

                if notifs is not None:
                    unread_notifications[""notification_count""] = notifs[""notify_count""]
                    unread_notifications[""highlight_count""] = notifs[""highlight_count""]

                sync_result_builder.joined.append(room_sync)

            if batch.limited and since_token:
                user_id = sync_result_builder.sync_config.user.to_string()
                logger.info(
                    ""Incremental gappy sync of %s for user %s with %d state events""
                    % (room_id, user_id, len(state))
                )
        elif room_builder.rtype == ""archived"":
            room_sync = ArchivedSyncResult(
                room_id=room_id,
                timeline=batch,
                state=state,
                account_data=account_data_events,
            )
            if room_sync or always_include:
                sync_result_builder.archived.append(room_sync)
        else:
            raise Exception(""Unrecognized rtype: %r"", room_builder.rtype)","1. Use `yield` instead of `return` to avoid synchronous code.
2. Use `sync_config.filter_collection.filter_room_ephemeral` to filter ephemeral events.
3. Use `sync_config.filter_collection.filter_room_account_data` to filter account data events."
"    def compute_state_delta(
        self, room_id, batch, sync_config, since_token, now_token, full_state
    ):
        """""" Works out the difference in state between the start of the timeline
        and the previous sync.

        Args:
            room_id(str):
            batch(synapse.handlers.sync.TimelineBatch): The timeline batch for
                the room that will be sent to the user.
            sync_config(synapse.handlers.sync.SyncConfig):
            since_token(str|None): Token of the end of the previous batch. May
                be None.
            now_token(str): Token of the end of the current batch.
            full_state(bool): Whether to force returning the full state.

        Returns:
             A deferred dict of (type, state_key) -> Event
        """"""
        # TODO(mjark) Check if the state events were received by the server
        # after the previous sync, since we need to include those state
        # updates even if they occured logically before the previous event.
        # TODO(mjark) Check for new redactions in the state events.

        with Measure(self.clock, ""compute_state_delta""):

            members_to_fetch = None

            lazy_load_members = sync_config.filter_collection.lazy_load_members()
            include_redundant_members = (
                sync_config.filter_collection.include_redundant_members()
            )

            if lazy_load_members:
                # We only request state for the members needed to display the
                # timeline:

                members_to_fetch = set(
                    event.sender  # FIXME: we also care about invite targets etc.
                    for event in batch.events
                )

                if full_state:
                    # always make sure we LL ourselves so we know we're in the room
                    # (if we are) to fix https://github.com/vector-im/riot-web/issues/7209
                    # We only need apply this on full state syncs given we disabled
                    # LL for incr syncs in #3840.
                    members_to_fetch.add(sync_config.user.to_string())

                state_filter = StateFilter.from_lazy_load_member_list(members_to_fetch)
            else:
                state_filter = StateFilter.all()

            timeline_state = {
                (event.type, event.state_key): event.event_id
                for event in batch.events
                if event.is_state()
            }

            if full_state:
                if batch:
                    current_state_ids = yield self.store.get_state_ids_for_event(
                        batch.events[-1].event_id, state_filter=state_filter
                    )

                    state_ids = yield self.store.get_state_ids_for_event(
                        batch.events[0].event_id, state_filter=state_filter
                    )

                else:
                    current_state_ids = yield self.get_state_at(
                        room_id, stream_position=now_token, state_filter=state_filter
                    )

                    state_ids = current_state_ids

                state_ids = _calculate_state(
                    timeline_contains=timeline_state,
                    timeline_start=state_ids,
                    previous={},
                    current=current_state_ids,
                    lazy_load_members=lazy_load_members,
                )
            elif batch.limited:
                if batch:
                    state_at_timeline_start = yield self.store.get_state_ids_for_event(
                        batch.events[0].event_id, state_filter=state_filter
                    )
                else:
                    # Its not clear how we get here, but empirically we do
                    # (#5407). Logging has been added elsewhere to try and
                    # figure out where this state comes from.
                    state_at_timeline_start = yield self.get_state_at(
                        room_id, stream_position=now_token, state_filter=state_filter
                    )

                # for now, we disable LL for gappy syncs - see
                # https://github.com/vector-im/riot-web/issues/7211#issuecomment-419976346
                # N.B. this slows down incr syncs as we are now processing way
                # more state in the server than if we were LLing.
                #
                # We still have to filter timeline_start to LL entries (above) in order
                # for _calculate_state's LL logic to work, as we have to include LL
                # members for timeline senders in case they weren't loaded in the initial
                # sync.  We do this by (counterintuitively) by filtering timeline_start
                # members to just be ones which were timeline senders, which then ensures
                # all of the rest get included in the state block (if we need to know
                # about them).
                state_filter = StateFilter.all()

                state_at_previous_sync = yield self.get_state_at(
                    room_id, stream_position=since_token, state_filter=state_filter
                )

                if batch:
                    current_state_ids = yield self.store.get_state_ids_for_event(
                        batch.events[-1].event_id, state_filter=state_filter
                    )
                else:
                    # Its not clear how we get here, but empirically we do
                    # (#5407). Logging has been added elsewhere to try and
                    # figure out where this state comes from.
                    current_state_ids = yield self.get_state_at(
                        room_id, stream_position=now_token, state_filter=state_filter
                    )

                state_ids = _calculate_state(
                    timeline_contains=timeline_state,
                    timeline_start=state_at_timeline_start,
                    previous=state_at_previous_sync,
                    current=current_state_ids,
                    # we have to include LL members in case LL initial sync missed them
                    lazy_load_members=lazy_load_members,
                )
            else:
                state_ids = {}
                if lazy_load_members:
                    if members_to_fetch and batch.events:
                        # We're returning an incremental sync, with no
                        # ""gap"" since the previous sync, so normally there would be
                        # no state to return.
                        # But we're lazy-loading, so the client might need some more
                        # member events to understand the events in this timeline.
                        # So we fish out all the member events corresponding to the
                        # timeline here, and then dedupe any redundant ones below.

                        state_ids = yield self.store.get_state_ids_for_event(
                            batch.events[0].event_id,
                            # we only want members!
                            state_filter=StateFilter.from_types(
                                (EventTypes.Member, member)
                                for member in members_to_fetch
                            ),
                        )

            if lazy_load_members and not include_redundant_members:
                cache_key = (sync_config.user.to_string(), sync_config.device_id)
                cache = self.get_lazy_loaded_members_cache(cache_key)

                # if it's a new sync sequence, then assume the client has had
                # amnesia and doesn't want any recent lazy-loaded members
                # de-duplicated.
                if since_token is None:
                    logger.debug(""clearing LruCache for %r"", cache_key)
                    cache.clear()
                else:
                    # only send members which aren't in our LruCache (either
                    # because they're new to this client or have been pushed out
                    # of the cache)
                    logger.debug(""filtering state from %r..."", state_ids)
                    state_ids = {
                        t: event_id
                        for t, event_id in iteritems(state_ids)
                        if cache.get(t[1]) != event_id
                    }
                    logger.debug(""...to %r"", state_ids)

                # add any member IDs we are about to send into our LruCache
                for t, event_id in itertools.chain(
                    state_ids.items(), timeline_state.items()
                ):
                    if t[0] == EventTypes.Member:
                        cache.set(t[1], event_id)

        state = {}
        if state_ids:
            state = yield self.store.get_events(list(state_ids.values()))

        return {
            (e.type, e.state_key): e
            for e in sync_config.filter_collection.filter_room_state(
                list(state.values())
            )
        }","1. Use `functools.lru_cache` to cache the results of expensive computations.
2. Use `typing` to annotate the function parameters and return values.
3. Use `f-strings` to format strings instead of concatenation."
"    def _generate_room_entry(
        self,
        sync_result_builder,
        ignored_users,
        room_builder,
        ephemeral,
        tags,
        account_data,
        always_include=False,
    ):
        """"""Populates the `joined` and `archived` section of `sync_result_builder`
        based on the `room_builder`.

        Args:
            sync_result_builder(SyncResultBuilder)
            ignored_users(set(str)): Set of users ignored by user.
            room_builder(RoomSyncResultBuilder)
            ephemeral(list): List of new ephemeral events for room
            tags(list): List of *all* tags for room, or None if there has been
                no change.
            account_data(list): List of new account data for room
            always_include(bool): Always include this room in the sync response,
                even if empty.
        """"""
        newly_joined = room_builder.newly_joined
        full_state = (
            room_builder.full_state or newly_joined or sync_result_builder.full_state
        )
        events = room_builder.events

        # We want to shortcut out as early as possible.
        if not (always_include or account_data or ephemeral or full_state):
            if events == [] and tags is None:
                return

        now_token = sync_result_builder.now_token
        sync_config = sync_result_builder.sync_config

        room_id = room_builder.room_id
        since_token = room_builder.since_token
        upto_token = room_builder.upto_token

        batch = yield self._load_filtered_recents(
            room_id,
            sync_config,
            now_token=upto_token,
            since_token=since_token,
            recents=events,
            newly_joined_room=newly_joined,
        )

        if not batch and batch.limited:
            # This resulted in #5407, which is weird, so lets log! We do it
            # here as we have the maximum amount of information.
            user_id = sync_result_builder.sync_config.user.to_string()
            logger.info(
                ""Issue #5407: Found limited batch with no events. user %s, room %s,""
                "" sync_config %s, newly_joined %s, events %s, batch %s."",
                user_id,
                room_id,
                sync_config,
                newly_joined,
                events,
                batch,
            )

        if newly_joined:
            # debug for https://github.com/matrix-org/synapse/issues/4422
            issue4422_logger.debug(
                ""Timeline events after filtering in newly-joined room %s: %r"",
                room_id,
                batch,
            )

        # When we join the room (or the client requests full_state), we should
        # send down any existing tags. Usually the user won't have tags in a
        # newly joined room, unless either a) they've joined before or b) the
        # tag was added by synapse e.g. for server notice rooms.
        if full_state:
            user_id = sync_result_builder.sync_config.user.to_string()
            tags = yield self.store.get_tags_for_room(user_id, room_id)

            # If there aren't any tags, don't send the empty tags list down
            # sync
            if not tags:
                tags = None

        account_data_events = []
        if tags is not None:
            account_data_events.append({""type"": ""m.tag"", ""content"": {""tags"": tags}})

        for account_data_type, content in account_data.items():
            account_data_events.append({""type"": account_data_type, ""content"": content})

        account_data_events = sync_config.filter_collection.filter_room_account_data(
            account_data_events
        )

        ephemeral = sync_config.filter_collection.filter_room_ephemeral(ephemeral)

        if not (
            always_include or batch or account_data_events or ephemeral or full_state
        ):
            return

        state = yield self.compute_state_delta(
            room_id, batch, sync_config, since_token, now_token, full_state=full_state
        )

        summary = {}

        # we include a summary in room responses when we're lazy loading
        # members (as the client otherwise doesn't have enough info to form
        # the name itself).
        if sync_config.filter_collection.lazy_load_members() and (
            # we recalulate the summary:
            #   if there are membership changes in the timeline, or
            #   if membership has changed during a gappy sync, or
            #   if this is an initial sync.
            any(ev.type == EventTypes.Member for ev in batch.events)
            or (
                # XXX: this may include false positives in the form of LL
                # members which have snuck into state
                batch.limited
                and any(t == EventTypes.Member for (t, k) in state)
            )
            or since_token is None
        ):
            summary = yield self.compute_summary(
                room_id, sync_config, batch, state, now_token
            )

        if room_builder.rtype == ""joined"":
            unread_notifications = {}
            room_sync = JoinedSyncResult(
                room_id=room_id,
                timeline=batch,
                state=state,
                ephemeral=ephemeral,
                account_data=account_data_events,
                unread_notifications=unread_notifications,
                summary=summary,
            )

            if room_sync or always_include:
                notifs = yield self.unread_notifs_for_room_id(room_id, sync_config)

                if notifs is not None:
                    unread_notifications[""notification_count""] = notifs[""notify_count""]
                    unread_notifications[""highlight_count""] = notifs[""highlight_count""]

                sync_result_builder.joined.append(room_sync)

            if batch.limited and since_token:
                user_id = sync_result_builder.sync_config.user.to_string()
                logger.info(
                    ""Incremental gappy sync of %s for user %s with %d state events""
                    % (room_id, user_id, len(state))
                )
        elif room_builder.rtype == ""archived"":
            room_sync = ArchivedSyncResult(
                room_id=room_id,
                timeline=batch,
                state=state,
                account_data=account_data_events,
            )
            if room_sync or always_include:
                sync_result_builder.archived.append(room_sync)
        else:
            raise Exception(""Unrecognized rtype: %r"", room_builder.rtype)","1. Use `yield` instead of `return` to avoid accidentally returning results prematurely.
2. Use `sync_config.filter_collection.filter_room_ephemeral()` to filter out ephemeral events.
3. Use `sync_config.filter_collection.filter_room_account_data()` to filter out account data events."
"    def _populate_stats_process_rooms(self, progress, batch_size):

        if not self.stats_enabled:
            yield self._end_background_update(""populate_stats_process_rooms"")
            defer.returnValue(1)

        # If we don't have progress filed, delete everything.
        if not progress:
            yield self.delete_all_stats()

        def _get_next_batch(txn):
            # Only fetch 250 rooms, so we don't fetch too many at once, even
            # if those 250 rooms have less than batch_size state events.
            sql = """"""
                SELECT room_id, events FROM %s_rooms
                ORDER BY events DESC
                LIMIT 250
            """""" % (
                TEMP_TABLE,
            )
            txn.execute(sql)
            rooms_to_work_on = txn.fetchall()

            if not rooms_to_work_on:
                return None

            # Get how many are left to process, so we can give status on how
            # far we are in processing
            txn.execute(""SELECT COUNT(*) FROM "" + TEMP_TABLE + ""_rooms"")
            progress[""remaining""] = txn.fetchone()[0]

            return rooms_to_work_on

        rooms_to_work_on = yield self.runInteraction(
            ""populate_stats_temp_read"", _get_next_batch
        )

        # No more rooms -- complete the transaction.
        if not rooms_to_work_on:
            yield self._end_background_update(""populate_stats_process_rooms"")
            defer.returnValue(1)

        logger.info(
            ""Processing the next %d rooms of %d remaining"",
            (len(rooms_to_work_on), progress[""remaining""]),
        )

        # Number of state events we've processed by going through each room
        processed_event_count = 0

        for room_id, event_count in rooms_to_work_on:

            current_state_ids = yield self.get_current_state_ids(room_id)

            join_rules = yield self.get_event(
                current_state_ids.get((EventTypes.JoinRules, """")), allow_none=True
            )
            history_visibility = yield self.get_event(
                current_state_ids.get((EventTypes.RoomHistoryVisibility, """")),
                allow_none=True,
            )
            encryption = yield self.get_event(
                current_state_ids.get((EventTypes.RoomEncryption, """")), allow_none=True
            )
            name = yield self.get_event(
                current_state_ids.get((EventTypes.Name, """")), allow_none=True
            )
            topic = yield self.get_event(
                current_state_ids.get((EventTypes.Topic, """")), allow_none=True
            )
            avatar = yield self.get_event(
                current_state_ids.get((EventTypes.RoomAvatar, """")), allow_none=True
            )
            canonical_alias = yield self.get_event(
                current_state_ids.get((EventTypes.CanonicalAlias, """")), allow_none=True
            )

            def _or_none(x, arg):
                if x:
                    return x.content.get(arg)
                return None

            yield self.update_room_state(
                room_id,
                {
                    ""join_rules"": _or_none(join_rules, ""join_rule""),
                    ""history_visibility"": _or_none(
                        history_visibility, ""history_visibility""
                    ),
                    ""encryption"": _or_none(encryption, ""algorithm""),
                    ""name"": _or_none(name, ""name""),
                    ""topic"": _or_none(topic, ""topic""),
                    ""avatar"": _or_none(avatar, ""url""),
                    ""canonical_alias"": _or_none(canonical_alias, ""alias""),
                },
            )

            now = self.hs.get_reactor().seconds()

            # quantise time to the nearest bucket
            now = (now // self.stats_bucket_size) * self.stats_bucket_size

            def _fetch_data(txn):

                # Get the current token of the room
                current_token = self._get_max_stream_id_in_current_state_deltas_txn(txn)

                current_state_events = len(current_state_ids)
                joined_members = self._get_user_count_in_room_txn(
                    txn, room_id, Membership.JOIN
                )
                invited_members = self._get_user_count_in_room_txn(
                    txn, room_id, Membership.INVITE
                )
                left_members = self._get_user_count_in_room_txn(
                    txn, room_id, Membership.LEAVE
                )
                banned_members = self._get_user_count_in_room_txn(
                    txn, room_id, Membership.BAN
                )
                total_state_events = self._get_total_state_event_counts_txn(
                    txn, room_id
                )

                self._update_stats_txn(
                    txn,
                    ""room"",
                    room_id,
                    now,
                    {
                        ""bucket_size"": self.stats_bucket_size,
                        ""current_state_events"": current_state_events,
                        ""joined_members"": joined_members,
                        ""invited_members"": invited_members,
                        ""left_members"": left_members,
                        ""banned_members"": banned_members,
                        ""state_events"": total_state_events,
                    },
                )
                self._simple_insert_txn(
                    txn,
                    ""room_stats_earliest_token"",
                    {""room_id"": room_id, ""token"": current_token},
                )

            yield self.runInteraction(""update_room_stats"", _fetch_data)

            # We've finished a room. Delete it from the table.
            yield self._simple_delete_one(TEMP_TABLE + ""_rooms"", {""room_id"": room_id})
            # Update the remaining counter.
            progress[""remaining""] -= 1
            yield self.runInteraction(
                ""populate_stats"",
                self._background_update_progress_txn,
                ""populate_stats_process_rooms"",
                progress,
            )

            processed_event_count += event_count

            if processed_event_count > batch_size:
                # Don't process any more rooms, we've hit our batch size.
                defer.returnValue(processed_event_count)

        defer.returnValue(processed_event_count)","1. Use prepared statements to prevent SQL injection.
2. Use transaction isolation level 'serializable' to prevent dirty reads.
3. Use `defer.returnValue()` to avoid leaking resources."
"    def _check_for_soft_fail(self, event, state, backfilled):
        """"""Checks if we should soft fail the event, if so marks the event as
        such.

        Args:
            event (FrozenEvent)
            state (dict|None): The state at the event if we don't have all the
                event's prev events
            backfilled (bool): Whether the event is from backfill

        Returns:
            Deferred
        """"""
        # For new (non-backfilled and non-outlier) events we check if the event
        # passes auth based on the current state. If it doesn't then we
        # ""soft-fail"" the event.
        do_soft_fail_check = not backfilled and not event.internal_metadata.is_outlier()
        if do_soft_fail_check:
            extrem_ids = yield self.store.get_latest_event_ids_in_room(
                event.room_id,
            )

            extrem_ids = set(extrem_ids)
            prev_event_ids = set(event.prev_event_ids())

            if extrem_ids == prev_event_ids:
                # If they're the same then the current state is the same as the
                # state at the event, so no point rechecking auth for soft fail.
                do_soft_fail_check = False

        if do_soft_fail_check:
            room_version = yield self.store.get_room_version(event.room_id)

            # Calculate the ""current state"".
            if state is not None:
                # If we're explicitly given the state then we won't have all the
                # prev events, and so we have a gap in the graph. In this case
                # we want to be a little careful as we might have been down for
                # a while and have an incorrect view of the current state,
                # however we still want to do checks as gaps are easy to
                # maliciously manufacture.
                #
                # So we use a ""current state"" that is actually a state
                # resolution across the current forward extremities and the
                # given state at the event. This should correctly handle cases
                # like bans, especially with state res v2.

                state_sets = yield self.store.get_state_groups(
                    event.room_id, extrem_ids,
                )
                state_sets = list(state_sets.values())
                state_sets.append(state)
                current_state_ids = yield self.state_handler.resolve_events(
                    room_version, state_sets, event,
                )
                current_state_ids = {
                    k: e.event_id for k, e in iteritems(current_state_ids)
                }
            else:
                current_state_ids = yield self.state_handler.get_current_state_ids(
                    event.room_id, latest_event_ids=extrem_ids,
                )

            # Now check if event pass auth against said current state
            auth_types = auth_types_for_event(event)
            current_state_ids = [
                e for k, e in iteritems(current_state_ids)
                if k in auth_types
            ]

            current_auth_events = yield self.store.get_events(current_state_ids)
            current_auth_events = {
                (e.type, e.state_key): e for e in current_auth_events.values()
            }

            try:
                self.auth.check(room_version, event, auth_events=current_auth_events)
            except AuthError as e:
                logger.warn(
                    ""Failed current state auth resolution for %r because %s"",
                    event, e,
                )
                event.internal_metadata.soft_failed = True","1. Use `get_latest_event_ids_in_room` to get the latest event ids in the room.
2. Use `get_room_version` to get the room version.
3. Use `get_current_state_ids` to get the current state ids."
"    def _get_events_which_are_prevs(self, event_ids):
        """"""Filter the supplied list of event_ids to get those which are prev_events of
        existing (non-outlier/rejected) events.

        Args:
            event_ids (Iterable[str]): event ids to filter

        Returns:
            Deferred[List[str]]: filtered event ids
        """"""
        results = []

        def _get_events(txn, batch):
            sql = """"""
            SELECT prev_event_id
            FROM event_edges
                INNER JOIN events USING (event_id)
                LEFT JOIN rejections USING (event_id)
            WHERE
                prev_event_id IN (%s)
                AND NOT events.outlier
                AND rejections.event_id IS NULL
            """""" % (
                "","".join(""?"" for _ in batch),
            )

            txn.execute(sql, batch)
            results.extend(r[0] for r in txn)

        for chunk in batch_iter(event_ids, 100):
            yield self.runInteraction(""_get_events_which_are_prevs"", _get_events, chunk)

        defer.returnValue(results)","1. Use prepared statements instead of building SQL strings.
2. Sanitize user input to prevent SQL injection attacks.
3. Use transactions to ensure data integrity."
"        def _get_events(txn, batch):
            sql = """"""
            SELECT prev_event_id
            FROM event_edges
                INNER JOIN events USING (event_id)
                LEFT JOIN rejections USING (event_id)
            WHERE
                prev_event_id IN (%s)
                AND NOT events.outlier
                AND rejections.event_id IS NULL
            """""" % (
                "","".join(""?"" for _ in batch),
            )

            txn.execute(sql, batch)
            results.extend(r[0] for r in txn)","1. Use prepared statements instead of building the SQL string in code. This prevents SQL injection attacks.
2. Use transactions to ensure that data is not changed in an inconsistent state if an error occurs.
3. Sanitize user input before using it in SQL queries. This prevents attackers from injecting malicious code into the database."
"    def _handle_state_delta(self, deltas):
        """"""Process current state deltas to find new joins that need to be
        handled.
        """"""
        for delta in deltas:
            typ = delta[""type""]
            state_key = delta[""state_key""]
            room_id = delta[""room_id""]
            event_id = delta[""event_id""]
            prev_event_id = delta[""prev_event_id""]

            logger.debug(""Handling: %r %r, %s"", typ, state_key, event_id)

            if typ != EventTypes.Member:
                continue

            event = yield self.store.get_event(event_id)
            if event.content.get(""membership"") != Membership.JOIN:
                # We only care about joins
                continue

            if prev_event_id:
                prev_event = yield self.store.get_event(prev_event_id)
                if prev_event.content.get(""membership"") == Membership.JOIN:
                    # Ignore changes to join events.
                    continue

            yield self._on_user_joined_room(room_id, state_key)","1. Use `get_event()` with `lock=True` to prevent race conditions.
2. Validate the `membership` field of the event to ensure it is a `Membership.JOIN` event.
3. Check the `prev_event_id` to ensure that the event is not a change to a `Membership.JOIN` event."
"    def get_current_state_deltas(self, prev_stream_id):
        prev_stream_id = int(prev_stream_id)
        if not self._curr_state_delta_stream_cache.has_any_entity_changed(
            prev_stream_id
        ):
            return []

        def get_current_state_deltas_txn(txn):
            # First we calculate the max stream id that will give us less than
            # N results.
            # We arbitarily limit to 100 stream_id entries to ensure we don't
            # select toooo many.
            sql = """"""
                SELECT stream_id, count(*)
                FROM current_state_delta_stream
                WHERE stream_id > ?
                GROUP BY stream_id
                ORDER BY stream_id ASC
                LIMIT 100
            """"""
            txn.execute(sql, (prev_stream_id,))

            total = 0
            max_stream_id = prev_stream_id
            for max_stream_id, count in txn:
                total += count
                if total > 100:
                    # We arbitarily limit to 100 entries to ensure we don't
                    # select toooo many.
                    break

            # Now actually get the deltas
            sql = """"""
                SELECT stream_id, room_id, type, state_key, event_id, prev_event_id
                FROM current_state_delta_stream
                WHERE ? < stream_id AND stream_id <= ?
                ORDER BY stream_id ASC
            """"""
            txn.execute(sql, (prev_stream_id, max_stream_id))
            return self.cursor_to_dict(txn)

        return self.runInteraction(
            ""get_current_state_deltas"", get_current_state_deltas_txn
        )","1. Use prepared statements to prevent SQL injection.
2. Use parameterized queries to avoid leaking sensitive information.
3. Use transaction isolation levels to prevent dirty reads."
"    def subscribe_to_stream(self, stream_name, token):
        """"""Subscribe the remote to a streams.

        This invloves checking if they've missed anything and sending those
        updates down if they have. During that time new updates for the stream
        are queued and sent once we've sent down any missed updates.
        """"""
        self.replication_streams.discard(stream_name)
        self.connecting_streams.add(stream_name)

        try:
            # Get missing updates
            updates, current_token = yield self.streamer.get_stream_updates(
                stream_name, token,
            )

            # Send all the missing updates
            for update in updates:
                token, row = update[0], update[1]
                self.send_command(RdataCommand(stream_name, token, row))

            # We send a POSITION command to ensure that they have an up to
            # date token (especially useful if we didn't send any updates
            # above)
            self.send_command(PositionCommand(stream_name, current_token))

            # Now we can send any updates that came in while we were subscribing
            pending_rdata = self.pending_rdata.pop(stream_name, [])
            for token, update in pending_rdata:
                # Only send updates newer than the current token
                if token > current_token:
                    self.send_command(RdataCommand(stream_name, token, update))

            # They're now fully subscribed
            self.replication_streams.add(stream_name)
        except Exception as e:
            logger.exception(""[%s] Failed to handle REPLICATE command"", self.id())
            self.send_error(""failed to handle replicate: %r"", e)
        finally:
            self.connecting_streams.discard(stream_name)","1. Use proper error handling to prevent exceptions from leaking sensitive information.
2. Use secure communication channels to protect data from being intercepted or modified.
3. Implement access control to restrict who can access sensitive data."
"    def read_config(self, config):
        consent_config = config.get(""user_consent"")
        if consent_config is None:
            return
        self.user_consent_version = str(consent_config[""version""])
        self.user_consent_template_dir = consent_config[""template_dir""]
        self.user_consent_server_notice_content = consent_config.get(
            ""server_notice_content"",
        )
        self.block_events_without_consent_error = consent_config.get(
            ""block_events_error"",
        )
        self.user_consent_server_notice_to_guests = bool(consent_config.get(
            ""send_server_notice_to_guests"", False,
        ))
        self.user_consent_at_registration = bool(consent_config.get(
            ""require_at_registration"", False,
        ))
        self.user_consent_policy_name = consent_config.get(
            ""policy_name"", ""Privacy Policy"",
        )","1. Use `getattr` instead of `get` to avoid accessing undefined properties.
2. Use `json.loads` to parse the configuration file instead of `eval`.
3. Use `os.path.join` to concatenate paths instead of `+`."
"    def __init__(self, hs):
        """"""
        Args:
            hs (synapse.server.HomeServer): homeserver
        """"""
        Resource.__init__(self)

        self.hs = hs
        self.store = hs.get_datastore()
        self.registration_handler = hs.get_handlers().registration_handler

        # this is required by the request_handler wrapper
        self.clock = hs.get_clock()

        self._default_consent_version = hs.config.user_consent_version
        if self._default_consent_version is None:
            raise ConfigError(
                ""Consent resource is enabled but user_consent section is ""
                ""missing in config file."",
            )

        # daemonize changes the cwd to /, so make the path absolute now.
        consent_template_directory = path.abspath(
            hs.config.user_consent_template_dir,
        )
        if not path.isdir(consent_template_directory):
            raise ConfigError(
                ""Could not find template directory '%s'"" % (
                    consent_template_directory,
                ),
            )

        loader = jinja2.FileSystemLoader(consent_template_directory)
        self._jinja_env = jinja2.Environment(
            loader=loader,
            autoescape=jinja2.select_autoescape(['html', 'htm', 'xml']),
        )

        if hs.config.form_secret is None:
            raise ConfigError(
                ""Consent resource is enabled but form_secret is not set in ""
                ""config file. It should be set to an arbitrary secret string."",
            )

        self._hmac_secret = hs.config.form_secret.encode(""utf-8"")","1. Use `os.path.abspath` to make the path absolute.
2. Use `jinja2.FileSystemLoader` to load the template directory.
3. Use `hmac.new` to generate the HMAC secret."
"    def _new_transaction(self, conn, desc, after_callbacks, exception_callbacks,
                         func, *args, **kwargs):
        start = time.time()
        txn_id = self._TXN_ID

        # We don't really need these to be unique, so lets stop it from
        # growing really large.
        self._TXN_ID = (self._TXN_ID + 1) % (MAX_TXN_ID)

        name = ""%s-%x"" % (desc, txn_id, )

        transaction_logger.debug(""[TXN START] {%s}"", name)

        try:
            i = 0
            N = 5
            while True:
                try:
                    txn = conn.cursor()
                    txn = LoggingTransaction(
                        txn, name, self.database_engine, after_callbacks,
                        exception_callbacks,
                    )
                    r = func(txn, *args, **kwargs)
                    conn.commit()
                    return r
                except self.database_engine.module.OperationalError as e:
                    # This can happen if the database disappears mid
                    # transaction.
                    logger.warn(
                        ""[TXN OPERROR] {%s} %s %d/%d"",
                        name, e, i, N
                    )
                    if i < N:
                        i += 1
                        try:
                            conn.rollback()
                        except self.database_engine.module.Error as e1:
                            logger.warn(
                                ""[TXN EROLL] {%s} %s"",
                                name, e1,
                            )
                        continue
                    raise
                except self.database_engine.module.DatabaseError as e:
                    if self.database_engine.is_deadlock(e):
                        logger.warn(""[TXN DEADLOCK] {%s} %d/%d"", name, i, N)
                        if i < N:
                            i += 1
                            try:
                                conn.rollback()
                            except self.database_engine.module.Error as e1:
                                logger.warn(
                                    ""[TXN EROLL] {%s} %s"",
                                    name, e1,
                                )
                            continue
                    raise
        except Exception as e:
            logger.debug(""[TXN FAIL] {%s} %s"", name, e)
            raise
        finally:
            end = time.time()
            duration = end - start

            LoggingContext.current_context().add_database_transaction(duration)

            transaction_logger.debug(""[TXN END] {%s} %f sec"", name, duration)

            self._current_txn_total_time += duration
            self._txn_perf_counters.update(desc, start, end)
            sql_txn_timer.labels(desc).observe(duration)","1. Use prepared statements instead of building dynamic SQL queries. This will prevent SQL injection attacks.
2. Use transaction isolation level 'serializable' to prevent dirty reads, phantom reads, and non-repeatable reads.
3. Use connection pooling to reduce the number of open database connections."
"def resolve_service(service_name, dns_client=client, cache=SERVER_CACHE, clock=time):
    cache_entry = cache.get(service_name, None)
    if cache_entry:
        if all(s.expires > int(clock.time()) for s in cache_entry):
            servers = list(cache_entry)
            defer.returnValue(servers)

    servers = []

    try:
        try:
            answers, _, _ = yield dns_client.lookupService(service_name)
        except DNSNameError:
            defer.returnValue([])

        if (len(answers) == 1
                and answers[0].type == dns.SRV
                and answers[0].payload
                and answers[0].payload.target == dns.Name('.')):
            raise ConnectError(""Service %s unavailable"" % service_name)

        for answer in answers:
            if answer.type != dns.SRV or not answer.payload:
                continue

            payload = answer.payload

            hosts = yield _get_hosts_for_srv_record(
                dns_client, str(payload.target)
            )

            for (ip, ttl) in hosts:
                host_ttl = min(answer.ttl, ttl)

                servers.append(_Server(
                    host=ip,
                    port=int(payload.port),
                    priority=int(payload.priority),
                    weight=int(payload.weight),
                    expires=int(clock.time()) + host_ttl,
                ))

        servers.sort()
        cache[service_name] = list(servers)
    except DomainError as e:
        # We failed to resolve the name (other than a NameError)
        # Try something in the cache, else rereaise
        cache_entry = cache.get(service_name, None)
        if cache_entry:
            logger.warn(
                ""Failed to resolve %r, falling back to cache. %r"",
                service_name, e
            )
            servers = list(cache_entry)
        else:
            raise e

    defer.returnValue(servers)","1. Use a secure random number generator to generate the salt.
2. Use a strong hashing algorithm, such as SHA-256 or SHA-512.
3. Use a sufficiently long password (at least 12 characters)."
"def update_manifest(ref=None):
    """"""
    Given a git reference in the Noto repo, such as a git commit hash or tag, extract
    information about the fonts available for use and save that information to the
    manifest file.

    The Noto repo currently contains both an older style and the newer ""Phase 3""
    fonts. Phase 3 fonts have more consistent internal metrics which makes them amenable
    to being merged together, which we make use of. The older fonts are still usable,
    but cannot be merged together.

    Noto also contains both standard and ""UI"" variants of many fonts. When a font has a
    UI variant, it means that some of the glyphs in the standard variant are very tall
    and might overflow a typical line of text; the UI variant has the glypsh redrawn
    to fit.

    When searching for fonts to include, we take all language fonts that have both a
    regular and a bold variant, with preference given to Phase 3 and UI variants.
    """"""

    # grab the head of master
    if not ref:
        logging.info(""Using head of master"")
        ref = _request(""git/refs/heads/master"")[""object""][""sha""]

    logging.info(""Generating new manifest for reference '{}'"".format(ref))

    git_tree = _request(""git/trees/{}?recursive=1"".format(ref))

    # backups
    font_info = _font_info(git_tree, ref, OLD_STYLE_PATH, _old_download_url)
    # prefer phase 3, replacing old-styles when possible
    font_info.update(_font_info(git_tree, ref, PHASE_3_PATH, _p3_download_url))

    new_manifest = {KEY_REF: ref, KEY_FONTS: font_info}
    utils.json_dump_formatted(new_manifest, FONT_MANIFEST_PATH)","1. Use `requests.get()` instead of `urllib2.urlopen()` to prevent insecure connections.
2. Use `json.dumps()` to format the JSON data instead of `json.dump()` to prevent data from being misinterpreted.
3. Use `logging.info()` to log the information instead of `print()` to prevent sensitive information from being leaked."
"    def add_arguments(self, parser):
        parser.add_argument('--interval', action='store', dest='interval',
                            help='Number of minutes to wait after a successful ping before the next ping.')
        parser.add_argument('--checkrate', action='store', dest='checkrate',
                            help='Number of minutes to wait between failed ping attempts.')
        parser.add_argument('--server', action='store', dest='server',
                            help='Base URL of the server to connect to.')","1. Use `argparse.ArgumentParser.add_argument(..., required=True)` to make sure all arguments are provided.
2. Use `requests.get(url, timeout=...)` to set a timeout for the requests.
3. Use `requests.post(url, data=..., auth=...)` to set the authentication for the requests."
"    def handle(self, *args, **options):

        interval = float(options.get(""interval"") or DEFAULT_PING_INTERVAL)
        checkrate = float(options.get(""checkrate"") or DEFAULT_PING_CHECKRATE)
        server = options.get(""server"") or DEFAULT_SERVER_URL

        self.started = datetime.now()

        while True:
            try:
                logger.info(""Attempting a ping."")
                with vacuum_db_lock:
                    data = self.perform_ping(server)
                    logger.info(""Ping succeeded! (response: {})"".format(data))
                    if ""id"" in data:
                        self.perform_statistics(server, data[""id""])
                logger.info(""Sleeping for {} minutes."".format(interval))
                time.sleep(interval * 60)
                continue
            except ConnectionError:
                logger.warn(""Ping failed (could not connect). Trying again in {} minutes."".format(checkrate))
            except Timeout:
                logger.warn(""Ping failed (connection timed out). Trying again in {} minutes."".format(checkrate))
            except RequestException as e:
                logger.warn(""Ping failed ({})! Trying again in {} minutes."".format(e, checkrate))
            time.sleep(checkrate * 60)","1. Use `requests` library instead of `urllib` to avoid insecure methods like `urlopen`.
2. Use `json.loads` instead of `eval` to parse JSON data.
3. Handle exceptions more gracefully by using `try-except` blocks."
"    def perform_ping(self, server):

        url = urljoin(server, ""/api/v1/pingback"")

        instance, _ = InstanceIDModel.get_or_create_current_instance()

        devicesettings = DeviceSettings.objects.first()
        language = devicesettings.language_id if devicesettings else """"

        try:
            timezone = get_current_timezone().zone
        except Exception:
            timezone = """"

        data = {
            ""instance_id"": instance.id,
            ""version"": kolibri.__version__,
            ""mode"": os.environ.get(""KOLIBRI_RUN_MODE"", """"),
            ""platform"": instance.platform,
            ""sysversion"": instance.sysversion,
            ""database_id"": instance.database.id,
            ""system_id"": instance.system_id,
            ""node_id"": instance.node_id,
            ""language"": language,
            ""timezone"": timezone,
            ""uptime"": int((datetime.now() - self.started).total_seconds() / 60),
        }

        logger.debug(""Pingback data: {}"".format(data))

        jsondata = dump_zipped_json(data)

        response = requests.post(url, data=jsondata, timeout=60)

        response.raise_for_status()

        return json.loads(response.content or ""{}"")","1. Use `requests.post()` with `verify=False` to disable SSL certificate verification.
2. Use `json.dumps()` to serialize the data instead of `dump_zipped_json()`.
3. Use `json.loads()` to deserialize the response instead of `json.loads()`."
"    def perform_statistics(self, server, pingback_id):

        url = urljoin(server, ""/api/v1/statistics"")

        channels = [extract_channel_statistics(c) for c in ChannelMetadata.objects.all()]
        facilities = [extract_facility_statistics(f) for f in Facility.objects.all()]

        data = {
            ""pi"": pingback_id,
            ""c"": channels,
            ""f"": facilities,
        }

        logger.debug(""Statistics data: {}"".format(data))

        jsondata = dump_zipped_json(data)

        response = requests.post(url, data=jsondata, timeout=60)

        response.raise_for_status()

        return json.loads(response.content or ""{}"")","1. Use `requests.post()` with `verify=False` to disable SSL certificate verification.
2. Use `json.dumps()` to encode the data as JSON before sending it to the server.
3. Use `json.loads()` to decode the response from the server."
"def extract_facility_statistics(facility):

    dataset_id = facility.dataset_id

    settings = {name: getattr(facility.dataset, name) for name in facility_settings if hasattr(facility.dataset, name)}

    learners = FacilityUser.objects.filter(dataset_id=dataset_id).exclude(roles__kind__in=[role_kinds.ADMIN, role_kinds.COACH])
    coaches = FacilityUser.objects.filter(dataset_id=dataset_id, roles__kind__in=[role_kinds.ADMIN, role_kinds.COACH])

    usersessions = UserSessionLog.objects.filter(dataset_id=dataset_id)
    contsessions = ContentSessionLog.objects.filter(dataset_id=dataset_id, time_spent__lt=3600 * 2)

    # the aggregates below are used to calculate the first and most recent times this device was used
    usersess_agg = (usersessions
                    .filter(start_timestamp__gt=datetime.datetime(2016, 1, 1))
                    .aggregate(first=Min(""start_timestamp""), last=Max(""last_interaction_timestamp"")))
    contsess_agg = (contsessions
                    .filter(start_timestamp__gt=datetime.datetime(2016, 1, 1))
                    .aggregate(first=Min(""start_timestamp""), last=Max(""end_timestamp"")))

    # since newly provisioned devices won't have logs, we don't know whether we have an available datetime object
    first_interaction_timestamp = getattr(min(usersess_agg[""first""], contsess_agg[""first""]), 'strftime', None)
    last_interaction_timestamp = getattr(max(usersess_agg[""last""], contsess_agg[""last""]), 'strftime', None)

    sesslogs_by_kind = contsessions.order_by(""kind"").values(""kind"").annotate(count=Count(""kind""))
    sesslogs_by_kind = {log[""kind""]: log[""count""] for log in sesslogs_by_kind}

    summarylogs = ContentSummaryLog.objects.filter(dataset_id=dataset_id)

    contsessions_user = contsessions.exclude(user=None)
    contsessions_anon = contsessions.filter(user=None)

    return {
        # facility_id
        ""fi"": base64.encodestring(hashlib.md5(facility.id.encode()).digest())[:10],
        # settings
        ""s"": settings,
        # learners_count
        ""lc"": learners.count(),
        # learner_login_count
        ""llc"": usersessions.exclude(user__roles__kind__in=[role_kinds.ADMIN, role_kinds.COACH]).distinct().count(),
        # coaches_count
        ""cc"": coaches.count(),
        # coach_login_count
        ""clc"": usersessions.filter(user__roles__kind__in=[role_kinds.ADMIN, role_kinds.COACH]).distinct().count(),
        # first
        ""f"" : first_interaction_timestamp(""%Y-%m-%d"") if first_interaction_timestamp else None,
        # last
        ""l"": last_interaction_timestamp(""%Y-%m-%d"") if last_interaction_timestamp else None,
        # summ_started
        ""ss"": summarylogs.count(),
        # summ_complete
        ""sc"": summarylogs.exclude(completion_timestamp=None).count(),
        # sess_kinds
        ""sk"": sesslogs_by_kind,
        # lesson_count
        ""lec"": Lesson.objects.filter(dataset_id=dataset_id).count(),
        # exam_count
        ""ec"": Exam.objects.filter(dataset_id=dataset_id).count(),
        # exam_log_count
        ""elc"": ExamLog.objects.filter(dataset_id=dataset_id).count(),
        # att_log_count
        ""alc"": AttemptLog.objects.filter(dataset_id=dataset_id).count(),
        # exam_att_log_count
        ""ealc"": ExamAttemptLog.objects.filter(dataset_id=dataset_id).count(),
        # sess_user_count
        ""suc"": contsessions_user.count(),
        # sess_anon_count
        ""sac"": contsessions_anon.count(),
        # sess_user_time
        ""sut"": int((contsessions_user.aggregate(total_time=Sum(""time_spent""))[""total_time""] or 0) / 60),
        # sess_anon_time
        ""sat"": int((contsessions_anon.aggregate(total_time=Sum(""time_spent""))[""total_time""] or 0) / 60),
    }","1. Use `django-filter` to sanitize user input.
2. Use `django-rest-framework-jwt` to authenticate users and protect endpoints.
3. Use `django-cors-headers` to allow cross-origin requests."
"def get_files_to_transfer(channel_id, node_ids, exclude_node_ids, available):
    files_to_transfer = LocalFile.objects.filter(files__contentnode__channel_id=channel_id, available=available)

    if node_ids:
        leaf_node_ids = _get_leaves_ids(node_ids)
        files_to_transfer = files_to_transfer.filter(files__contentnode__in=leaf_node_ids)

    if exclude_node_ids:
        exclude_leaf_node_ids = _get_leaves_ids(exclude_node_ids)
        files_to_transfer = files_to_transfer.exclude(files__contentnode__in=exclude_leaf_node_ids)

    # Make sure the files are unique, to avoid duplicating downloads
    files_to_transfer = files_to_transfer.distinct()

    total_bytes_to_transfer = files_to_transfer.aggregate(Sum('file_size'))['file_size__sum'] or 0

    return files_to_transfer, total_bytes_to_transfer","1. Use `.values()` instead of `.filter()` to avoid loading unnecessary data.
2. Use `.distinct()` to ensure that the results are unique.
3. Use `.aggregate()` to calculate the total size of the files without loading all of them into memory."
"def _job_to_response(job):
    if not job:
        return {
            ""type"": None,
            ""started_by"": None,
            ""status"": State.SCHEDULED,
            ""percentage"": 0,
            ""progress"": [],
            ""id"": None,
            ""cancellable"": False,
        }
    else:
        return {
            ""type"": job.extra_metadata.get(""type""),
            ""started_by"": job.extra_metadata.get(""started_by""),
            ""status"": job.state,
            ""exception"": str(job.exception),
            ""traceback"": str(job.traceback),
            ""percentage"": job.percentage_progress,
            ""id"": job.job_id,
            ""cancellable"": job.cancellable,
        }","1. Use `job.state` instead of `job.status` to get the job status.
2. Use `job.extra_metadata.get(""type"")` to get the job type.
3. Use `job.exception` and `job.traceback` to get the job exception and traceback."
"    def handle(self, *args, **options):

        engine = create_engine(get_default_db_string(), convert_unicode=True)

        metadata = MetaData()

        app_config = apps.get_app_config('content')
        # Exclude channelmetadatacache in case we are reflecting an older version of Kolibri
        table_names = [model._meta.db_table for name, model in app_config.models.items() if name != 'channelmetadatacache']
        metadata.reflect(bind=engine, only=table_names)
        Base = automap_base(metadata=metadata)
        # TODO map relationship backreferences using the django names
        Base.prepare()
        session = sessionmaker(bind=engine, autoflush=False)()

        # Load fixture data into the test database with Django
        call_command('loaddata', 'content_import_test.json', interactive=False)

        def get_dict(item):
            value = {key: value for key, value in item.__dict__.items() if key != '_sa_instance_state'}
            return value

        data = {}

        for table_name, record in Base.classes.items():
            data[table_name] = [get_dict(r) for r in session.query(record).all()]

        with open(SCHEMA_PATH_TEMPLATE.format(name=options['version']), 'wb') as f:
            pickle.dump(metadata, f, protocol=2)

        with open(DATA_PATH_TEMPLATE.format(name=options['version']), 'wb') as f:
            json.dump(data, f)","1. Use `engine.dispose()` to close the database connection after use.
2. Use `session.close()` to close the database session after use.
3. Use `pickle.dumps(metadata, protocol=0)` to dump the metadata in a secure way."
"    def get_license(self, SourceRecord):
        license_id = SourceRecord.license_id
        if license_id not in self.licenses:
            LicenseRecord = self.source.get_class(License)
            license = self.source.session.query(LicenseRecord).get(license_id)
            self.licenses[license_id] = license
        return self.licenses[license_id]","1. Use prepared statements to prevent SQL injection attacks.
2. Use the `verify_integrity` method to check the validity of the license before returning it.
3. Use the `is_expired` method to check if the license has expired before returning it."
"    def get_license_name(self, SourceRecord):
        license = self.get_license(SourceRecord)
        return license.license_name","1. Use prepared statements to prevent SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use access control to restrict who can access sensitive data."
"    def get_license_description(self, SourceRecord):
        license = self.get_license(SourceRecord)
        return license.license_description","1. **Use prepared statements** to prevent SQL injection attacks.
2. **Sanitize user input** to prevent cross-site scripting (XSS) attacks.
3. **Use strong passwords** for the database user account."
"    def ready(self):
        from kolibri.tasks.api import client
        client.clear(force=True)","1. Use `importlib.import_module` instead of `from` to avoid importing modules with the same name as built-in functions.
2. Use `client.clear()` instead of `client.delete()` to avoid accidentally deleting the entire task queue.
3. Use `client.clear(force=True)` to avoid being blocked by tasks that are currently running."
"    def error_received(self, exc):  # pragma: no cover
        if self.recvfrom:
            self.recvfrom.set_exception(exc)","1. Use `try/except` to handle exceptions.
2. Set the `exc` argument to the `set_exception()` method.
3. Use `self.recvfrom` to check if the exception has been set."
"    def connection_lost(self, exc):
        if self.recvfrom:
            self.recvfrom.set_exception(exc)","1. Sanitize user input to prevent injection attacks.
2. Use proper error handling to avoid leaking sensitive information.
3. Use strong encryption to protect sensitive data."
"def _destination_and_source(af, where, port, source, source_port,
                            default_to_inet=True):
    # Apply defaults and compute destination and source tuples
    # suitable for use in connect(), sendto(), or bind().
    if af is None:
        try:
            af = dns.inet.af_for_address(where)
        except Exception:
            if default_to_inet:
                af = dns.inet.AF_INET
    if af == dns.inet.AF_INET:
        destination = (where, port)
        if source is not None or source_port != 0:
            if source is None:
                source = '0.0.0.0'
            source = (source, source_port)
    elif af == dns.inet.AF_INET6:
        destination = (where, port, 0, 0)
        if source is not None or source_port != 0:
            if source is None:
                source = '::'
            source = (source, source_port, 0, 0)
    else:
        source = None
        destination = None
    return (af, destination, source)","1. Use `dns.inet.af_for_address()` to validate the address family of `where`.
2. Use `dns.inet.AF_INET` or `dns.inet.AF_INET6` for `af`.
3. Use `dns.inet.inet_aton()` to convert `source` to a valid IP address."
"def xfr(where, zone, rdtype=dns.rdatatype.AXFR, rdclass=dns.rdataclass.IN,
        timeout=None, port=53, keyring=None, keyname=None, relativize=True,
        af=None, lifetime=None, source=None, source_port=0, serial=0,
        use_udp=False, keyalgorithm=dns.tsig.default_algorithm):
    """"""Return a generator for the responses to a zone transfer.

    *where*.  If the inference attempt fails, AF_INET is used.  This
    parameter is historical; you need never set it.

    *zone*, a ``dns.name.Name`` or ``text``, the name of the zone to transfer.

    *rdtype*, an ``int`` or ``text``, the type of zone transfer.  The
    default is ``dns.rdatatype.AXFR``.  ``dns.rdatatype.IXFR`` can be
    used to do an incremental transfer instead.

    *rdclass*, an ``int`` or ``text``, the class of the zone transfer.
    The default is ``dns.rdataclass.IN``.

    *timeout*, a ``float``, the number of seconds to wait for each
    response message.  If None, the default, wait forever.

    *port*, an ``int``, the port send the message to.  The default is 53.

    *keyring*, a ``dict``, the keyring to use for TSIG.

    *keyname*, a ``dns.name.Name`` or ``text``, the name of the TSIG
    key to use.

    *relativize*, a ``bool``.  If ``True``, all names in the zone will be
    relativized to the zone origin.  It is essential that the
    relativize setting matches the one specified to
    ``dns.zone.from_xfr()`` if using this generator to make a zone.

    *af*, an ``int``, the address family to use.  The default is ``None``,
    which causes the address family to use to be inferred from the form of
    *where*.  If the inference attempt fails, AF_INET is used.  This
    parameter is historical; you need never set it.

    *lifetime*, a ``float``, the total number of seconds to spend
    doing the transfer.  If ``None``, the default, then there is no
    limit on the time the transfer may take.

    *source*, a ``text`` containing an IPv4 or IPv6 address, specifying
    the source address.  The default is the wildcard address.

    *source_port*, an ``int``, the port from which to send the message.
    The default is 0.

    *serial*, an ``int``, the SOA serial number to use as the base for
    an IXFR diff sequence (only meaningful if *rdtype* is
    ``dns.rdatatype.IXFR``).

    *use_udp*, a ``bool``.  If ``True``, use UDP (only meaningful for IXFR).

    *keyalgorithm*, a ``dns.name.Name`` or ``text``, the TSIG algorithm to use.

    Raises on errors, and so does the generator.

    Returns a generator of ``dns.message.Message`` objects.
    """"""

    if isinstance(zone, str):
        zone = dns.name.from_text(zone)
    if isinstance(rdtype, str):
        rdtype = dns.rdatatype.from_text(rdtype)
    q = dns.message.make_query(zone, rdtype, rdclass)
    if rdtype == dns.rdatatype.IXFR:
        rrset = dns.rrset.from_text(zone, 0, 'IN', 'SOA',
                                    '. . %u 0 0 0 0' % serial)
        q.authority.append(rrset)
    if keyring is not None:
        q.use_tsig(keyring, keyname, algorithm=keyalgorithm)
    wire = q.to_wire()
    (af, destination, source) = _destination_and_source(af, where, port,
                                                        source, source_port)
    if use_udp:
        if rdtype != dns.rdatatype.IXFR:
            raise ValueError('cannot do a UDP AXFR')
        s = socket_factory(af, socket.SOCK_DGRAM, 0)
    else:
        s = socket_factory(af, socket.SOCK_STREAM, 0)
    s.setblocking(0)
    if source is not None:
        s.bind(source)
    expiration = _compute_expiration(lifetime)
    _connect(s, destination, expiration)
    l = len(wire)
    if use_udp:
        _wait_for_writable(s, expiration)
        s.send(wire)
    else:
        tcpmsg = struct.pack(""!H"", l) + wire
        _net_write(s, tcpmsg, expiration)
    done = False
    delete_mode = True
    expecting_SOA = False
    soa_rrset = None
    if relativize:
        origin = zone
        oname = dns.name.empty
    else:
        origin = None
        oname = zone
    tsig_ctx = None
    first = True
    while not done:
        mexpiration = _compute_expiration(timeout)
        if mexpiration is None or mexpiration > expiration:
            mexpiration = expiration
        if use_udp:
            _wait_for_readable(s, expiration)
            (wire, from_address) = s.recvfrom(65535)
        else:
            ldata = _net_read(s, 2, mexpiration)
            (l,) = struct.unpack(""!H"", ldata)
            wire = _net_read(s, l, mexpiration)
        is_ixfr = (rdtype == dns.rdatatype.IXFR)
        r = dns.message.from_wire(wire, keyring=q.keyring, request_mac=q.mac,
                                  xfr=True, origin=origin, tsig_ctx=tsig_ctx,
                                  multi=True, first=first,
                                  one_rr_per_rrset=is_ixfr)
        rcode = r.rcode()
        if rcode != dns.rcode.NOERROR:
            raise TransferError(rcode)
        tsig_ctx = r.tsig_ctx
        first = False
        answer_index = 0
        if soa_rrset is None:
            if not r.answer or r.answer[0].name != oname:
                raise dns.exception.FormError(
                    ""No answer or RRset not for qname"")
            rrset = r.answer[0]
            if rrset.rdtype != dns.rdatatype.SOA:
                raise dns.exception.FormError(""first RRset is not an SOA"")
            answer_index = 1
            soa_rrset = rrset.copy()
            if rdtype == dns.rdatatype.IXFR:
                if soa_rrset[0].serial <= serial:
                    #
                    # We're already up-to-date.
                    #
                    done = True
                else:
                    expecting_SOA = True
        #
        # Process SOAs in the answer section (other than the initial
        # SOA in the first message).
        #
        for rrset in r.answer[answer_index:]:
            if done:
                raise dns.exception.FormError(""answers after final SOA"")
            if rrset.rdtype == dns.rdatatype.SOA and rrset.name == oname:
                if expecting_SOA:
                    if rrset[0].serial != serial:
                        raise dns.exception.FormError(
                            ""IXFR base serial mismatch"")
                    expecting_SOA = False
                elif rdtype == dns.rdatatype.IXFR:
                    delete_mode = not delete_mode
                #
                # If this SOA RRset is equal to the first we saw then we're
                # finished. If this is an IXFR we also check that we're seeing
                # the record in the expected part of the response.
                #
                if rrset == soa_rrset and \\
                        (rdtype == dns.rdatatype.AXFR or
                         (rdtype == dns.rdatatype.IXFR and delete_mode)):
                    done = True
            elif expecting_SOA:
                #
                # We made an IXFR request and are expecting another
                # SOA RR, but saw something else, so this must be an
                # AXFR response.
                #
                rdtype = dns.rdatatype.AXFR
                expecting_SOA = False
        if done and q.keyring and not r.had_tsig:
            raise dns.exception.FormError(""missing TSIG"")
        yield r
    s.close()","1. Use a secure socket by specifying the `af` parameter.
2. Use a timeout to prevent the transfer from hanging indefinitely.
3. Use TSIG to authenticate the zone transfer."
"    def __init__(self, rdclass, rdtype, strings):
        super(TXTBase, self).__init__(rdclass, rdtype)
        if isinstance(strings, str):
            strings = [strings]
        self.strings = strings[:]","1. Use `typing` to specify the types of arguments and return values.
2. Validate the input data to prevent buffer overflows and other attacks.
3. Use `logging` to log security-related events."
"def tcp(q, where, timeout=None, port=53, af=None, source=None, source_port=0,
        one_rr_per_rrset=False):
    """"""Return the response obtained after sending a query via TCP.

    *q*, a ``dns.message.message``, the query to send

    *where*, a ``text`` containing an IPv4 or IPv6 address,  where
    to send the message.

    *timeout*, a ``float`` or ``None``, the number of seconds to wait before the
    query times out.  If ``None``, the default, wait forever.

    *port*, an ``int``, the port send the message to.  The default is 53.

    *af*, an ``int``, the address family to use.  The default is ``None``,
    which causes the address family to use to be inferred from the form of
    *where*.  If the inference attempt fails, AF_INET is used.  This
    parameter is historical; you need never set it.

    *source*, a ``text`` containing an IPv4 or IPv6 address, specifying
    the source address.  The default is the wildcard address.

    *source_port*, an ``int``, the port from which to send the message.
    The default is 0.

    *one_rr_per_rrset*, a ``bool``.  If ``True``, put each RR into its own
    RRset.

    Returns a ``dns.message.Message``.
    """"""

    wire = q.to_wire()
    (af, destination, source) = _destination_and_source(af, where, port,
                                                        source, source_port)
    s = socket_factory(af, socket.SOCK_STREAM, 0)
    begin_time = None
    try:
        expiration = _compute_expiration(timeout)
        s.setblocking(0)
        begin_time = time.time()
        if source is not None:
            s.bind(source)
        _connect(s, destination)
        send_tcp(s, wire, expiration)
        (r, received_time) = receive_tcp(s, expiration, one_rr_per_rrset,
                                         q.keyring, q.request_mac)
    finally:
        if begin_time is None or received_time is None:
            response_time = 0
        else:
            response_time = received_time - begin_time
        s.close()
    r.time = response_time
    if not q.is_response(r):
        raise BadResponse
    return r","1. Use a secure socket factory.
2. Set the socket to non-blocking mode.
3. Bind the socket to a specific source address if you are using a wildcard address."
"    def __init__(self, origin, rdclass=dns.rdataclass.IN, relativize=True):
        """"""Initialize a zone object.

        @param origin: The origin of the zone.
        @type origin: dns.name.Name object
        @param rdclass: The zone's rdata class; the default is class IN.
        @type rdclass: int""""""

        if isinstance(origin, string_types):
            origin = dns.name.from_text(origin)
        elif not isinstance(origin, dns.name.Name):
            raise ValueError(""origin parameter must be convertable to a ""
                             ""DNS name"")
        if not origin.is_absolute():
            raise ValueError(""origin parameter must be an absolute name"")
        self.origin = origin
        self.rdclass = rdclass
        self.nodes = {}
        self.relativize = relativize","1. Use `dns.name.from_wire()` instead of `dns.name.from_text()` to
                    prevent DNS poisoning attacks.
2. Validate the `origin` parameter to ensure that it is an absolute DNS name.
3. Use `dns.name.relativize()` to ensure that all zone names are relative to the
    zone's origin."
"def match(
        handler: handlers.ResourceHandler,
        cause: causation.ResourceCause,
) -> bool:
    # Kwargs are lazily evaluated on the first _actual_ use, and shared for all filters since then.
    kwargs: MutableMapping[str, Any] = {}
    return all([
        _matches_resource(handler, cause.resource),
        _matches_labels(handler, cause, kwargs),
        _matches_annotations(handler, cause, kwargs),
        _matches_field_values(handler, cause, kwargs),
        _matches_field_changes(handler, cause, kwargs),
        _matches_filter_callback(handler, cause, kwargs),
    ])","1. Use [functools.lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache) to cache the results of expensive function calls.
2. Use [type hints](https://docs.python.org/3/library/typing.html) to annotate the types of arguments and return values.
3. Use [security best practices](https://docs.python.org/3/library/security.html) when writing code that is exposed to untrusted input."
"async def watch_objs(
        *,
        settings: configuration.OperatorSettings,
        resource: resources.Resource,
        namespace: Optional[str] = None,
        timeout: Optional[float] = None,
        since: Optional[str] = None,
        context: Optional[auth.APIContext] = None,  # injected by the decorator
        freeze_waiter: asyncio_Future,
) -> AsyncIterator[bodies.RawInput]:
    """"""
    Watch objects of a specific resource type.

    The cluster-scoped call is used in two cases:

    * The resource itself is cluster-scoped, and namespacing makes not sense.
    * The operator serves all namespaces for the namespaced custom resource.

    Otherwise, the namespace-scoped call is used:

    * The resource is namespace-scoped AND operator is namespaced-restricted.
    """"""
    if context is None:
        raise RuntimeError(""API instance is not injected by the decorator."")

    is_namespaced = await discovery.is_namespaced(resource=resource, context=context)
    namespace = namespace if is_namespaced else None

    params: Dict[str, str] = {}
    params['watch'] = 'true'
    if since is not None:
        params['resourceVersion'] = since
    if timeout is not None:
        params['timeoutSeconds'] = str(timeout)

    # Talk to the API and initiate a streaming response.
    response = await context.session.get(
        url=resource.get_url(server=context.server, namespace=namespace, params=params),
        timeout=aiohttp.ClientTimeout(
            total=settings.watching.client_timeout,
            sock_connect=settings.watching.connect_timeout,
        ),
    )
    response.raise_for_status()

    # Stream the parsed events from the response until it is closed server-side,
    # or until it is closed client-side by the freeze-waiting future's callbacks.
    response_close_callback = lambda _: response.close()
    freeze_waiter.add_done_callback(response_close_callback)
    try:
        async with response:
            async for line in _iter_jsonlines(response.content):
                raw_input = cast(bodies.RawInput, json.loads(line.decode(""utf-8"")))
                yield raw_input
    except (aiohttp.ClientConnectionError, aiohttp.ClientPayloadError, asyncio.TimeoutError):
        pass
    finally:
        freeze_waiter.remove_done_callback(response_close_callback)","1. Use aiohttp.ClientSession instead of aiohttp.ClientSession to avoid leaking connections.
2. Set timeouts for both the total request and the connection to avoid hanging.
3. Use aiohttp.ClientTimeout to ensure that requests are aborted if they take too long."
"    def _create_tag(self, tag_str):
        """"""Create a Tag object from a tag string.""""""
        tag_hierarchy = tag_str.split(self.hierarchy_separator)
        tag_prefix = """"
        parent_tag = None
        for sub_tag in tag_hierarchy:
            # Get or create subtag.
            tag_name = tag_prefix + self._scrub_tag_name(sub_tag)
            tag = self._get_tag(tag_name)
            if not tag:
                tag = self._create_tag_instance(tag_name)
            # Set tag parent.
            tag.parent = parent_tag
            # Update parent and tag prefix.
            parent_tag = tag
            tag_prefix = tag.name + self.hierarchy_separator
        return tag","1. Use `str.strip()` to remove whitespace from the beginning and end of the tag name before passing it to `_scrub_tag_name()`.
2. Use `re.sub()` to replace all characters that are not alphanumeric or underscores with underscores in the tag name before passing it to `_scrub_tag_name()`.
3. Use `os.path.join()` to sanitize the tag hierarchy before passing it to `_create_tag_instance()`."
"    def __monitor(self):
        to_monitor = self.workflow_scheduling_manager.active_workflow_schedulers
        while self.monitor_running:
            if self.invocation_grabber:
                self.invocation_grabber.grab_unhandled_items()

            monitor_step_timer = self.app.execution_timer_factory.get_timer(
                'internal.galaxy.workflows.scheduling_manager.monitor_step',
                'Workflow scheduling manager monitor step complete.'
            )
            for workflow_scheduler_id, workflow_scheduler in to_monitor.items():
                if not self.monitor_running:
                    return

                self.__schedule(workflow_scheduler_id, workflow_scheduler)
            log.trace(monitor_step_timer.to_str())
            self._monitor_sleep(1)","1. Use `get_timer()` to create a unique timer for each workflow scheduler.
2. Use `grab_unhandled_items()` to check for new invocations.
3. Use `schedule()` to schedule the workflow scheduler."
"    def display(self, trans, history_content_id, history_id,
                preview=False, filename=None, to_ext=None, raw=False, **kwd):
        """"""
        GET /api/histories/{encoded_history_id}/contents/{encoded_content_id}/display
        Displays history content (dataset).

        The query parameter 'raw' should be considered experimental and may be dropped at
        some point in the future without warning. Generally, data should be processed by its
        datatype prior to display (the defult if raw is unspecified or explicitly false.
        """"""
        decoded_content_id = self.decode_id(history_content_id)
        raw = util.string_as_bool_or_none(raw)

        rval = ''
        try:
            hda = self.hda_manager.get_accessible(decoded_content_id, trans.user)
            if raw:
                if filename and filename != 'index':
                    object_store = trans.app.object_store
                    dir_name = hda.dataset.extra_files_path_name
                    file_path = object_store.get_filename(hda.dataset,
                                                          extra_dir=dir_name,
                                                          alt_name=filename)
                else:
                    file_path = hda.file_name
                rval = open(file_path, 'rb')
            else:
                display_kwd = kwd.copy()
                if 'key' in display_kwd:
                    del display_kwd[""key""]
                rval = hda.datatype.display_data(trans, hda, preview, filename, to_ext, **display_kwd)
        except Exception as e:
            log.exception(""Error getting display data for dataset (%s) from history (%s)"",
                          history_content_id, history_id)
            trans.response.status = 500
            rval = ""Could not get display data for dataset: %s"" % util.unicodify(e)
        return rval","1. Use `util.string_as_bool_or_none()` to sanitize the `raw` parameter.
2. Use `trans.user` to check if the user has access to the dataset.
3. Use `hda.datatype.display_data()` to display the dataset data."
"def display(api_key, url, return_formatted=True):
    """"""
    Sends an API GET request and acts as a generic formatter for the JSON response.
    """"""
    try:
        r = get(api_key, url)
    except HTTPError as e:
        print(e)
        print(e.read(1024))  # Only return the first 1K of errors.
        sys.exit(1)
    if not return_formatted:
        return r
    elif type(r) == list:
        # Response is a collection as defined in the REST style.
        print('Collection Members')
        print('------------------')
        for n, i in enumerate(r):
            # All collection members should have a name in the response.
            # url is optional
            if 'url' in i:
                print('#%d: %s' % (n + 1, i.pop('url')))
            if 'name' in i:
                print('  name: %s' % i.pop('name'))
            for k, v in i.items():
                print(f'  {k}: {v}')
        print('')
        print('%d element(s) in collection' % len(r))
    elif type(r) == dict:
        # Response is an element as defined in the REST style.
        print('Member Information')
        print('------------------')
        for k, v in r.items():
            print(f'{k}: {v}')
    elif type(r) == str:
        print(r)
    else:
        print('response is unknown type: %s' % type(r))","1. Use `requests.get()` instead of `urllib2.urlopen()` to avoid insecure connections.
2. Use `json.dumps()` to format the response as JSON instead of printing it directly.
3. Use `sys.exit()` to exit the script immediately if an error occurs."
"def execute(trans, tool, mapping_params, history, rerun_remap_job_id=None, collection_info=None, workflow_invocation_uuid=None, invocation_step=None, max_num_jobs=None, job_callback=None, completed_jobs=None, workflow_resource_parameters=None, validate_outputs=False):
    """"""
    Execute a tool and return object containing summary (output data, number of
    failures, etc...).
    """"""
    if max_num_jobs:
        assert invocation_step is not None
    if rerun_remap_job_id:
        assert invocation_step is None

    all_jobs_timer = tool.app.execution_timer_factory.get_timer(
        'internals.galaxy.tools.execute.job_batch', BATCH_EXECUTION_MESSAGE
    )

    if invocation_step is None:
        execution_tracker = ToolExecutionTracker(trans, tool, mapping_params, collection_info, completed_jobs=completed_jobs)
    else:
        execution_tracker = WorkflowStepExecutionTracker(trans, tool, mapping_params, collection_info, invocation_step, completed_jobs=completed_jobs)
    execution_cache = ToolExecutionCache(trans)

    def execute_single_job(execution_slice, completed_job):
        job_timer = tool.app.execution_timer_factory.get_timer(
            'internals.galaxy.tools.execute.job_single', SINGLE_EXECUTION_SUCCESS_MESSAGE
        )
        params = execution_slice.param_combination
        if workflow_invocation_uuid:
            params['__workflow_invocation_uuid__'] = workflow_invocation_uuid
        elif '__workflow_invocation_uuid__' in params:
            # Only workflow invocation code gets to set this, ignore user supplied
            # values or rerun parameters.
            del params['__workflow_invocation_uuid__']
        if workflow_resource_parameters:
            params['__workflow_resource_params__'] = workflow_resource_parameters
        elif '__workflow_resource_params__' in params:
            # Only workflow invocation code gets to set this, ignore user supplied
            # values or rerun parameters.
            del params['__workflow_resource_params__']
        if validate_outputs:
            params['__validate_outputs__'] = True
        job, result = tool.handle_single_execution(trans, rerun_remap_job_id, execution_slice, history, execution_cache, completed_job, collection_info, job_callback=job_callback, flush_job=False)
        if job:
            log.debug(job_timer.to_str(tool_id=tool.id, job_id=job.id))
            execution_tracker.record_success(execution_slice, job, result)
        else:
            execution_tracker.record_error(result)

    tool_action = tool.tool_action
    if hasattr(tool_action, ""check_inputs_ready""):
        for params in execution_tracker.param_combinations:
            # This will throw an exception if the tool is not ready.
            tool_action.check_inputs_ready(
                tool,
                trans,
                params,
                history,
                execution_cache=execution_cache,
                collection_info=collection_info,
            )

    execution_tracker.ensure_implicit_collections_populated(history, mapping_params.param_template)
    job_count = len(execution_tracker.param_combinations)

    jobs_executed = 0
    has_remaining_jobs = False
    execution_slice = None

    for i, execution_slice in enumerate(execution_tracker.new_execution_slices()):
        if max_num_jobs and jobs_executed >= max_num_jobs:
            has_remaining_jobs = True
            break
        else:
            execute_single_job(execution_slice, completed_jobs[i])
            history = execution_slice.history or history
            jobs_executed += 1

    if execution_slice:
        # a side effect of adding datasets to a history is a commit within db_next_hid (even with flush=False).
        history.add_pending_datasets()
    else:
        # Make sure collections, implicit jobs etc are flushed even if there are no precreated output datasets
        trans.sa_session.flush()
    for job in execution_tracker.successful_jobs:
        # Put the job in the queue if tracking in memory
        tool.app.job_manager.enqueue(job, tool=tool, flush=False)
        trans.log_event(""Added job to the job queue, id: %s"" % str(job.id), tool_id=job.tool_id)
    trans.sa_session.flush()

    if has_remaining_jobs:
        raise PartialJobExecution(execution_tracker)
    else:
        execution_tracker.finalize_dataset_collections(trans)

    log.debug(all_jobs_timer.to_str(job_count=job_count, tool_id=tool.id))
    return execution_tracker","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use strong passwords for all database accounts."
"    def _assign_uwsgi_mule_message_handler(self, obj, method, configured, message_callback=None, flush=True, **kwargs):
        """"""Assign object to a handler by sending a setup message to the appropriate handler pool (farm), where a handler
        (mule) will receive the message and assign itself.

        :param obj:             Same as :method:`ConfiguresHandlers.assign_handler()`.
        :param method:          Same as :method:`ConfiguresHandlers._assign_db_preassign_handler()`.
        :param configured:      Same as :method:`ConfiguresHandlers.assign_handler()`.
        :param queue_callback:  Callback returning a setup message to be sent via the stack messaging interface's
                                ``send_message()`` method. No arguments are passed.
        :type queue_callback:   callable

        :raises HandlerAssignmentSkip: if the configured or default handler is not a known handler pool (farm)
        :returns: str -- The assigned handler pool.
        """"""
        assert message_callback is not None, \\
            ""Cannot perform '%s' handler assignment: `message_callback` is None"" \\
            % HANDLER_ASSIGNMENT_METHODS.UWSGI_MULE_MESSAGE
        tag = configured or self.DEFAULT_HANDLER_TAG
        pool = self.pool_for_tag.get(tag)
        if pool is None:
            log.debug(""(%s) No handler pool (uWSGI farm) for '%s' found"", obj.log_str(), tag)
            raise HandlerAssignmentSkip()
        else:
            if flush:
                _timed_flush_obj(obj)
            message = message_callback()
            self.app.application_stack.send_message(pool, message)
        return pool","1. Use `assert` statements to validate function arguments.
2. Check if the handler pool exists before sending a message to it.
3. Use `_timed_flush_obj()` to flush the object before sending a message."
"    def resolve(self, enabled_container_types, tool_info, **kwds):
        if tool_info.requires_galaxy_python_environment or self.container_type not in enabled_container_types:
            return None

        targets = mulled_targets(tool_info)
        resolution_cache = kwds.get(""resolution_cache"")
        return docker_cached_container_description(targets, self.namespace, hash_func=self.hash_func, shell=self.shell, resolution_cache=resolution_cache)","1. Use `get_galaxy_config()` instead of accessing `config` directly. This will protect against configuration poisoning attacks.
2. Use `tool_info.get_requirements()` instead of accessing `tool_info.requirements` directly. This will protect against tampering with the tool requirements.
3. Use `tool_info.get_container_image()` instead of accessing `tool_info.container_image` directly. This will protect against tampering with the tool container image."
"    def cached_container_description(self, targets, namespace, hash_func, resolution_cache):
        return docker_cached_container_description(targets, namespace, hash_func, resolution_cache)","1. Use `sha256` instead of `hash_func` to generate a more secure hash.
2. Use `namespace` to restrict the scope of the cached container description.
3. Use `resolution_cache` to cache the results of the container description, so that it does not need to be regenerated every time."
"    def pull(self, container):
        command = container.build_pull_command()
        shell(command)","1. Use `subprocess.run` instead of `shell` to avoid shell injection.
2. Sanitize the input to `build_pull_command` to prevent command injection.
3. Use `os.getenv` to get environment variables instead of hardcoding them."
"    def resolve(self, enabled_container_types, tool_info, install=False, session=None, **kwds):
        resolution_cache = kwds.get(""resolution_cache"")
        if tool_info.requires_galaxy_python_environment or self.container_type not in enabled_container_types:
            return None

        targets = mulled_targets(tool_info)
        if len(targets) == 0:
            return None

        name = targets_to_mulled_name(targets=targets, hash_func=self.hash_func, namespace=self.namespace, resolution_cache=resolution_cache, session=session)
        if name:
            container_id = ""quay.io/{}/{}"".format(self.namespace, name)
            if self.protocol:
                container_id = ""{}{}"".format(self.protocol, container_id)
            container_description = ContainerDescription(
                container_id,
                type=self.container_type,
                shell=self.shell,
            )
            if install and not self.cached_container_description(
                    targets,
                    namespace=self.namespace,
                    hash_func=self.hash_func,
                    resolution_cache=resolution_cache,
            ):
                destination_info = {}
                destination_for_container_type = kwds.get('destination_for_container_type')
                if destination_for_container_type:
                    destination_info = destination_for_container_type(self.container_type)
                container = CONTAINER_CLASSES[self.container_type](container_description.identifier,
                                                                   self.app_info,
                                                                   tool_info,
                                                                   destination_info,
                                                                   {},
                                                                   container_description)
                self.pull(container)
            if not self.auto_install:
                container_description = self.cached_container_description(
                    targets,
                    namespace=self.namespace,
                    hash_func=self.hash_func,
                    resolution_cache=resolution_cache,
                )
            return container_description",000_Didnt Work
"    def __default_containers_resolvers(self):
        default_resolvers = [
            ExplicitContainerResolver(self.app_info),
            ExplicitSingularityContainerResolver(self.app_info),
        ]
        if self.enable_mulled_containers:
            default_resolvers.extend([
                CachedMulledDockerContainerResolver(self.app_info, namespace=""biocontainers""),
                CachedMulledDockerContainerResolver(self.app_info, namespace=""local""),
                CachedMulledSingularityContainerResolver(self.app_info, namespace=""biocontainers""),
                CachedMulledSingularityContainerResolver(self.app_info, namespace=""local""),
                MulledDockerContainerResolver(self.app_info, namespace=""biocontainers""),
                MulledSingularityContainerResolver(self.app_info, namespace=""biocontainers""),
                BuildMulledDockerContainerResolver(self.app_info),
                BuildMulledSingularityContainerResolver(self.app_info),
            ])
        return default_resolvers","1. Use `explicit_containers` instead of `mulled_containers` to avoid potential security vulnerabilities.
2. Use `namespace` to restrict the scope of containers that can be used.
3. Use `build_mulled_docker_container_resolver` to build a custom container resolver that is tailored to your specific needs."
"    def resolve(self, enabled_container_types, tool_info, install=False, **kwds):
        if tool_info.requires_galaxy_python_environment or self.container_type not in enabled_container_types:
            return None

        targets = mulled_targets(tool_info)
        if len(targets) == 0:
            return None
        if self.auto_install or install:
            mull_targets(
                targets,
                involucro_context=self._get_involucro_context(),
                **self._mulled_kwds
            )
        return docker_cached_container_description(targets, self.namespace, hash_func=self.hash_func, shell=self.shell)","1. Use `getattr` instead of `dir` to get attributes of an object. This will prevent you from accidentally accessing attributes that do not exist.
2. Use `type` to check the type of an object before casting it. This will prevent you from accidentally casting an object to a type that it is not.
3. Use `isinstance` to check if an object is an instance of a particular class. This will prevent you from accidentally calling methods on an object that does not have those methods."
"    def record_success(self, execution_slice, job, outputs):
        super().record_success(execution_slice, job, outputs)
        if not self.collection_info:
            self.invocation_step.job = job","1. Use `job_id` instead of `job` to avoid accidentally overwriting data from other jobs.
2. Use `execution_slice_id` instead of `execution_slice` to avoid accidentally overwriting data from other execution slices.
3. Use `outputs` instead of `output` to avoid accidentally overwriting data from other outputs."
"    def ensure_implicit_collections_populated(self, history, params):
        if not self.collection_info:
            return

        history = history or self.tool.get_default_history_by_trans(self.trans)
        if self.invocation_step.is_new:
            self.precreate_output_collections(history, params)
        else:
            collections = {}
            for output_assoc in self.invocation_step.output_dataset_collections:
                implicit_collection = output_assoc.dataset_collection
                assert hasattr(implicit_collection, ""history_content_type"")  # make sure it is an HDCA and not a DC
                collections[output_assoc.output_name] = output_assoc.dataset_collection
            self.implicit_collections = collections
        self.invocation_step.implicit_collection_jobs = self.implicit_collection_jobs","1. Use `history or self.tool.get_default_history_by_trans(self.trans)` to get the default history instead of hard-coding it.
2. Use `assert hasattr(implicit_collection, ""history_content_type"")` to check if the implicit collection is an HDCA and not a DC.
3. Use `self.invocation_step.implicit_collection_jobs = self.implicit_collection_jobs` to set the implicit collection jobs."
"    def set_step_outputs(self, invocation_step, outputs, already_persisted=False):
        step = invocation_step.workflow_step
        if invocation_step.output_value:
            outputs[invocation_step.output_value.workflow_output.output_name] = invocation_step.output_value.value
        self.outputs[step.id] = outputs
        if not already_persisted:
            for output_name, output_object in outputs.items():
                if hasattr(output_object, ""history_content_type""):
                    invocation_step.add_output(output_name, output_object)
                else:
                    # This is a problem, this non-data, non-collection output
                    # won't be recovered on a subsequent workflow scheduling
                    # iteration. This seems to have been a pre-existing problem
                    # prior to #4584 though.
                    pass
            for workflow_output in step.workflow_outputs:
                output_name = workflow_output.output_name
                if output_name not in outputs:
                    message = ""Failed to find expected workflow output [{}] in step outputs [{}]"".format(output_name, outputs)
                    # raise KeyError(message)
                    # Pre-18.01 we would have never even detected this output wasn't configured
                    # and even in 18.01 we don't have a way to tell the user something bad is
                    # happening so I guess we just log a debug message and continue sadly for now.
                    # Once https://github.com/galaxyproject/galaxy/issues/5142 is complete we could
                    # at least tell the user what happened, give them a warning.
                    log.debug(message)
                    continue
                output = outputs[output_name]
                self._record_workflow_output(
                    step,
                    workflow_output,
                    output=output,
                )","1. Use `assert` statements to check for the presence of required fields.
2. Sanitize user input to prevent injection attacks.
3. Use exception handling to catch and log errors."
"    def _download(self, rel_path):
        try:
            log.debug(""Pulling key '%s' into cache to %s"", rel_path, self._get_cache_path(rel_path))
            key = self._bucket.get_key(rel_path)
            # Test if cache is large enough to hold the new file
            if self.cache_size > 0 and key.size > self.cache_size:
                log.critical(""File %s is larger (%s) than the cache size (%s). Cannot download."",
                             rel_path, key.size, self.cache_size)
                return False
            if self.use_axel:
                log.debug(""Parallel pulled key '%s' into cache to %s"", rel_path, self._get_cache_path(rel_path))
                ncores = multiprocessing.cpu_count()
                url = key.generate_url(7200)
                ret_code = subprocess.call(['axel', '-a', '-n', ncores, url])
                if ret_code == 0:
                    return True
            else:
                log.debug(""Pulled key '%s' into cache to %s"", rel_path, self._get_cache_path(rel_path))
                self.transfer_progress = 0  # Reset transfer progress counter
                key.get_contents_to_filename(self._get_cache_path(rel_path), cb=self._transfer_cb, num_cb=10)
                return True
        except S3ResponseError:
            log.exception(""Problem downloading key '%s' from S3 bucket '%s'"", rel_path, self._bucket.name)
        return False","1. Use HTTPS instead of HTTP to prevent data from being intercepted.
2. Encrypt the data with a strong encryption algorithm before uploading it to S3.
3. Use a secure password for the S3 bucket."
"    def preferences(self):
        user = self.trans.user
        return user and user.extra_preferences","1. **Use prepared statements** to prevent SQL injection attacks.
2. **Sanitize user input** to prevent cross-site scripting (XSS) attacks.
3. **Use strong encryption** to protect sensitive data."
"    def extra_preferences(self):
        data = {}
        extra_user_preferences = self.preferences.get('extra_user_preferences')
        if extra_user_preferences:
            try:
                data = json.loads(extra_user_preferences)
            except Exception:
                pass
        return data","1. **Use `json.loads()` with `strict=True` to validate the JSON data.** This will prevent malicious users from injecting invalid JSON data into the system.
2. **Sanitize the user input before using it to construct the JSON data.** This will prevent malicious users from injecting code or other malicious content into the system.
3. **Use a secure hashing algorithm to generate the user's session ID.** This will make it more difficult for malicious users to guess or steal the user's session ID."
"    def dataset_states_and_extensions_summary(self):
        if not hasattr(self, '_dataset_states_and_extensions_summary'):
            db_session = object_session(self)

            dc = alias(DatasetCollection.table)
            de = alias(DatasetCollectionElement.table)
            hda = alias(HistoryDatasetAssociation.table)
            dataset = alias(Dataset.table)

            select_from = dc.outerjoin(de, de.c.dataset_collection_id == dc.c.id)

            depth_collection_type = self.collection_type
            while "":"" in depth_collection_type:
                child_collection = alias(DatasetCollection.table)
                child_collection_element = alias(DatasetCollectionElement.table)
                select_from = select_from.outerjoin(child_collection, child_collection.c.id == de.c.child_collection_id)
                select_from = select_from.outerjoin(child_collection_element, child_collection_element.c.dataset_collection_id == child_collection.c.id)

                de = child_collection_element
                depth_collection_type = depth_collection_type.split("":"", 1)[1]

            select_from = select_from.outerjoin(hda, hda.c.id == de.c.hda_id).outerjoin(dataset, hda.c.dataset_id == dataset.c.id)
            select_stmt = select([hda.c.extension, dataset.c.state]).select_from(select_from).where(dc.c.id == self.id).distinct()
            extensions = set()
            states = set()
            for extension, state in db_session.execute(select_stmt).fetchall():
                states.add(state)
                extensions.add(extension)

            self._dataset_states_and_extensions_summary = (states, extensions)

        return self._dataset_states_and_extensions_summary","1. Use prepared statements instead of building queries with string concatenation. This will prevent SQL injection attacks.
2. Use the `func.distinct()` function to avoid duplicate rows in the results.
3. Use the `db_session.query()` method to execute queries instead of directly accessing the database. This will help to prevent errors and data corruption."
"    def replacement_for_connection(self, connection, is_data=True):
        output_step_id = connection.output_step.id
        if output_step_id not in self.outputs:
            template = ""No outputs found for step id %s, outputs are %s""
            message = template % (output_step_id, self.outputs)
            raise Exception(message)
        step_outputs = self.outputs[output_step_id]
        if step_outputs is STEP_OUTPUT_DELAYED:
            delayed_why = ""dependent step [%s] delayed, so this step must be delayed"" % output_step_id
            raise modules.DelayedWorkflowEvaluation(why=delayed_why)
        output_name = connection.output_name
        try:
            replacement = step_outputs[output_name]
        except KeyError:
            replacement = self.inputs_by_step_id.get(output_step_id)
            if connection.output_step.type == 'parameter_input' and output_step_id is not None:
                # FIXME: parameter_input step outputs should be properly recorded as step outputs, but for now we can
                # short-circuit and just pick the input value
                pass
            else:
                # Must resolve.
                template = ""Workflow evaluation problem - failed to find output_name %s in step_outputs %s""
                message = template % (output_name, step_outputs)
                raise Exception(message)
        if isinstance(replacement, model.HistoryDatasetCollectionAssociation):
            if not replacement.collection.populated:
                if not replacement.waiting_for_elements:
                    # If we are not waiting for elements, there was some
                    # problem creating the collection. Collection will never
                    # be populated.
                    # TODO: consider distinguish between cancelled and failed?
                    raise modules.CancelWorkflowEvaluation()

                delayed_why = ""dependent collection [%s] not yet populated with datasets"" % replacement.id
                raise modules.DelayedWorkflowEvaluation(why=delayed_why)

        data_inputs = (model.HistoryDatasetAssociation, model.HistoryDatasetCollectionAssociation, model.DatasetCollection)
        if not is_data and isinstance(replacement, data_inputs):
            if isinstance(replacement, model.HistoryDatasetAssociation):
                if replacement.is_pending:
                    raise modules.DelayedWorkflowEvaluation()
                if not replacement.is_ok:
                    raise modules.CancelWorkflowEvaluation()
            else:
                if not replacement.collection.populated:
                    raise modules.DelayedWorkflowEvaluation()
                pending = False
                for dataset_instance in replacement.dataset_instances:
                    if dataset_instance.is_pending:
                        pending = True
                    elif not dataset_instance.is_ok:
                        raise modules.CancelWorkflowEvaluation()
                if pending:
                    raise modules.DelayedWorkflowEvaluation()

        return replacement","1. Use `try/except` blocks to catch errors and handle them gracefully.
2. Use `input()` to get user input, and validate it before using it.
3. Use `print()` to output messages to the user, and make sure they are clear and concise."
"    def set_step_outputs(self, invocation_step, outputs, already_persisted=False):
        step = invocation_step.workflow_step
        self.outputs[step.id] = outputs
        if not already_persisted:
            for output_name, output_object in outputs.items():
                if hasattr(output_object, ""history_content_type""):
                    invocation_step.add_output(output_name, output_object)
                else:
                    # This is a problem, this non-data, non-collection output
                    # won't be recovered on a subsequent workflow scheduling
                    # iteration. This seems to have been a pre-existing problem
                    # prior to #4584 though.
                    pass
            for workflow_output in step.workflow_outputs:
                output_name = workflow_output.output_name
                if output_name not in outputs:
                    message = f""Failed to find expected workflow output [{output_name}] in step outputs [{outputs}]""
                    # raise KeyError(message)
                    # Pre-18.01 we would have never even detected this output wasn't configured
                    # and even in 18.01 we don't have a way to tell the user something bad is
                    # happening so I guess we just log a debug message and continue sadly for now.
                    # Once https://github.com/galaxyproject/galaxy/issues/5142 is complete we could
                    # at least tell the user what happened, give them a warning.
                    log.debug(message)
                    continue
                output = outputs[output_name]
                self._record_workflow_output(
                    step,
                    workflow_output,
                    output=output,
                )","1. Use `assert` to check if `output_name` exists in `outputs`.
2. Use `logging.warning` to log a warning message when `output_name` does not exist in `outputs`.
3. Use `_record_workflow_output` to record the workflow output."
"    def replacement_for_connection(self, connection, is_data=True):
        output_step_id = connection.output_step.id
        if output_step_id not in self.outputs:
            template = ""No outputs found for step id %s, outputs are %s""
            message = template % (output_step_id, self.outputs)
            raise Exception(message)
        step_outputs = self.outputs[output_step_id]
        if step_outputs is STEP_OUTPUT_DELAYED:
            delayed_why = ""dependent step [%s] delayed, so this step must be delayed"" % output_step_id
            raise modules.DelayedWorkflowEvaluation(why=delayed_why)
        output_name = connection.output_name
        try:
            replacement = step_outputs[output_name]
        except KeyError:
            replacement = self.inputs_by_step_id.get(output_step_id)
            if connection.output_step.type == 'parameter_input' and output_step_id is not None:
                # FIXME: parameter_input step outputs should be properly recorded as step outputs, but for now we can
                # short-circuit and just pick the input value
                pass
            else:
                # Must resolve.
                template = ""Workflow evaluation problem - failed to find output_name %s in step_outputs %s""
                message = template % (output_name, step_outputs)
                raise Exception(message)
        if isinstance(replacement, model.HistoryDatasetCollectionAssociation):
            if not replacement.collection.populated:
                if not replacement.collection.waiting_for_elements:
                    # If we are not waiting for elements, there was some
                    # problem creating the collection. Collection will never
                    # be populated.
                    # TODO: consider distinguish between cancelled and failed?
                    raise modules.CancelWorkflowEvaluation()

                delayed_why = ""dependent collection [%s] not yet populated with datasets"" % replacement.id
                raise modules.DelayedWorkflowEvaluation(why=delayed_why)

        data_inputs = (model.HistoryDatasetAssociation, model.HistoryDatasetCollectionAssociation, model.DatasetCollection)
        if not is_data and isinstance(replacement, data_inputs):
            if isinstance(replacement, model.HistoryDatasetAssociation):
                if replacement.is_pending:
                    raise modules.DelayedWorkflowEvaluation()
                if not replacement.is_ok:
                    raise modules.CancelWorkflowEvaluation()
            else:
                if not replacement.collection.populated:
                    raise modules.DelayedWorkflowEvaluation()
                pending = False
                for dataset_instance in replacement.dataset_instances:
                    if dataset_instance.is_pending:
                        pending = True
                    elif not dataset_instance.is_ok:
                        raise modules.CancelWorkflowEvaluation()
                if pending:
                    raise modules.DelayedWorkflowEvaluation()

        return replacement","1. Use `assert` statements to validate inputs.
2. Handle errors more gracefully.
3. Use `type` checking to ensure that the inputs are of the correct type."
"    def set_step_outputs(self, invocation_step, outputs, already_persisted=False):
        step = invocation_step.workflow_step
        self.outputs[step.id] = outputs
        if not already_persisted:
            for output_name, output_object in outputs.items():
                if hasattr(output_object, ""history_content_type""):
                    invocation_step.add_output(output_name, output_object)
                else:
                    # This is a problem, this non-data, non-collection output
                    # won't be recovered on a subsequent workflow scheduling
                    # iteration. This seems to have been a pre-existing problem
                    # prior to #4584 though.
                    pass
            for workflow_output in step.workflow_outputs:
                output_name = workflow_output.output_name
                if output_name not in outputs:
                    message = ""Failed to find expected workflow output [%s] in step outputs [%s]"" % (output_name, outputs)
                    # raise KeyError(message)
                    # Pre-18.01 we would have never even detected this output wasn't configured
                    # and even in 18.01 we don't have a way to tell the user something bad is
                    # happening so I guess we just log a debug message and continue sadly for now.
                    # Once https://github.com/galaxyproject/galaxy/issues/5142 is complete we could
                    # at least tell the user what happened, give them a warning.
                    log.debug(message)
                    continue
                output = outputs[output_name]
                self._record_workflow_output(
                    step,
                    workflow_output,
                    output=output,
                )","1. Use `getattr` to check if an object has a specific attribute before using it.
2. Use `logging.warning` to log errors instead of raising exceptions.
3. Use `self._record_workflow_output` to record workflow outputs instead of directly adding them to the invocation step."
"def extract_steps(trans, history=None, job_ids=None, dataset_ids=None, dataset_collection_ids=None, dataset_names=None, dataset_collection_names=None):
    # Ensure job_ids and dataset_ids are lists (possibly empty)
    if job_ids is None:
        job_ids = []
    elif type(job_ids) is not list:
        job_ids = [job_ids]
    if dataset_ids is None:
        dataset_ids = []
    elif type(dataset_ids) is not list:
        dataset_ids = [dataset_ids]
    if dataset_collection_ids is None:
        dataset_collection_ids = []
    elif type(dataset_collection_ids) is not list:
        dataset_collection_ids = [dataset_collection_ids]
    # Convert both sets of ids to integers
    job_ids = [int(_) for _ in job_ids]
    dataset_ids = [int(_) for _ in dataset_ids]
    dataset_collection_ids = [int(_) for _ in dataset_collection_ids]
    # Find each job, for security we (implicitly) check that they are
    # associated with a job in the current history.
    summary = WorkflowSummary(trans, history)
    jobs = summary.jobs
    steps = []
    hid_to_output_pair = {}
    # Input dataset steps
    for i, hid in enumerate(dataset_ids):
        step = model.WorkflowStep()
        step.type = 'data_input'
        if dataset_names:
            name = dataset_names[i]
        else:
            name = ""Input Dataset""
        step.tool_inputs = dict(name=name)
        hid_to_output_pair[hid] = (step, 'output')
        steps.append(step)
    for i, hid in enumerate(dataset_collection_ids):
        step = model.WorkflowStep()
        step.type = 'data_collection_input'
        if hid not in summary.collection_types:
            raise exceptions.RequestParameterInvalidException(""hid %s does not appear to be a collection"" % hid)
        collection_type = summary.collection_types[hid]
        if dataset_collection_names:
            name = dataset_collection_names[i]
        else:
            name = ""Input Dataset Collection""
        step.tool_inputs = dict(name=name, collection_type=collection_type)
        hid_to_output_pair[hid] = (step, 'output')
        steps.append(step)
    # Tool steps
    for job_id in job_ids:
        if job_id not in summary.job_id2representative_job:
            log.warning(""job_id %s not found in job_id2representative_job %s"" % (job_id, summary.job_id2representative_job))
            raise AssertionError(""Attempt to create workflow with job not connected to current history"")
        job = summary.job_id2representative_job[job_id]
        tool_inputs, associations = step_inputs(trans, job)
        step = model.WorkflowStep()
        step.type = 'tool'
        step.tool_id = job.tool_id
        step.tool_version = job.tool_version
        step.tool_inputs = tool_inputs
        # NOTE: We shouldn't need to do two passes here since only
        #       an earlier job can be used as an input to a later
        #       job.
        for other_hid, input_name in associations:
            if job in summary.implicit_map_jobs:
                an_implicit_output_collection = jobs[job][0][1]
                input_collection = an_implicit_output_collection.find_implicit_input_collection(input_name)
                if input_collection:
                    other_hid = input_collection.hid
                else:
                    log.info(""Cannot find implicit input collection for %s"" % input_name)
            if other_hid in hid_to_output_pair:
                step_input = step.get_or_add_input(input_name)
                other_step, other_name = hid_to_output_pair[other_hid]
                conn = model.WorkflowStepConnection()
                conn.input_step_input = step_input
                # Should always be connected to an earlier step
                conn.output_step = other_step
                conn.output_name = other_name
        steps.append(step)
        # Store created dataset hids
        for assoc in (job.output_datasets + job.output_dataset_collection_instances):
            assoc_name = assoc.name
            if ToolOutputCollectionPart.is_named_collection_part_name(assoc_name):
                continue
            if job in summary.implicit_map_jobs:
                hid = None
                for implicit_pair in jobs[job]:
                    query_assoc_name, dataset_collection = implicit_pair
                    if query_assoc_name == assoc_name or assoc_name.startswith(""__new_primary_file_%s|"" % query_assoc_name):
                        hid = dataset_collection.hid
                if hid is None:
                    template = ""Failed to find matching implicit job - job id is %s, implicit pairs are %s, assoc_name is %s.""
                    message = template % (job.id, jobs[job], assoc_name)
                    log.warning(message)
                    raise Exception(""Failed to extract job."")
            else:
                if hasattr(assoc, ""dataset""):
                    hid = assoc.dataset.hid
                else:
                    hid = assoc.dataset_collection_instance.hid
            hid_to_output_pair[hid] = (step, assoc.name)
    return steps","1. Use prepared statements instead of concatenation to prevent SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use access control to restrict who can access the data."
"def __cleanup_param_values(inputs, values):
    """"""
    Remove 'Data' values from `param_values`, along with metadata cruft,
    but track the associations.
    """"""
    associations = []
    # dbkey is pushed in by the framework
    if 'dbkey' in values:
        del values['dbkey']
    root_values = values
    root_input_keys = inputs.keys()

    # Recursively clean data inputs and dynamic selects
    def cleanup(prefix, inputs, values):
        for key, input in inputs.items():
            if isinstance(input, DataToolParameter) or isinstance(input, DataCollectionToolParameter):
                tmp = values[key]
                values[key] = None
                # HACK: Nested associations are not yet working, but we
                #       still need to clean them up so we can serialize
                # if not( prefix ):
                if isinstance(tmp, model.DatasetCollectionElement):
                    tmp = tmp.first_dataset_instance()
                if tmp:  # this is false for a non-set optional dataset
                    if not isinstance(tmp, list):
                        associations.append((tmp.hid, prefix + key))
                    else:
                        associations.extend([(t.hid, prefix + key) for t in tmp])

                # Cleanup the other deprecated crap associated with datasets
                # as well. Worse, for nested datasets all the metadata is
                # being pushed into the root. FIXME: MUST REMOVE SOON
                key = prefix + key + ""_""
                for k in root_values.keys():
                    if k not in root_input_keys and k.startswith(key):
                        del root_values[k]
            elif isinstance(input, Repeat):
                if key in values:
                    group_values = values[key]
                    for i, rep_values in enumerate(group_values):
                        rep_index = rep_values['__index__']
                        cleanup(""%s%s_%d|"" % (prefix, key, rep_index), input.inputs, group_values[i])
            elif isinstance(input, Conditional):
                # Scrub dynamic resource related parameters from workflows,
                # they cause problems and the workflow probably should include
                # their state in workflow encoding.
                if input.name == '__job_resource':
                    if input.name in values:
                        del values[input.name]
                    return
                if input.name in values:
                    group_values = values[input.name]
                    current_case = group_values['__current_case__']
                    cleanup(""%s%s|"" % (prefix, key), input.cases[current_case].inputs, group_values)
            elif isinstance(input, Section):
                if input.name in values:
                    cleanup(""%s%s|"" % (prefix, key), input.inputs, values[input.name])
    cleanup("""", inputs, values)
    return associations","1. Use `param_values` instead of `values` to avoid overwriting metadata.
2. Use `prefix` to avoid conflicts between different datasets.
3. Use `if` statement to check if the input is a `DataToolParameter` or `DataCollectionToolParameter` before cleaning it."
"    def cleanup(prefix, inputs, values):
        for key, input in inputs.items():
            if isinstance(input, DataToolParameter) or isinstance(input, DataCollectionToolParameter):
                tmp = values[key]
                values[key] = None
                # HACK: Nested associations are not yet working, but we
                #       still need to clean them up so we can serialize
                # if not( prefix ):
                if isinstance(tmp, model.DatasetCollectionElement):
                    tmp = tmp.first_dataset_instance()
                if tmp:  # this is false for a non-set optional dataset
                    if not isinstance(tmp, list):
                        associations.append((tmp.hid, prefix + key))
                    else:
                        associations.extend([(t.hid, prefix + key) for t in tmp])

                # Cleanup the other deprecated crap associated with datasets
                # as well. Worse, for nested datasets all the metadata is
                # being pushed into the root. FIXME: MUST REMOVE SOON
                key = prefix + key + ""_""
                for k in root_values.keys():
                    if k not in root_input_keys and k.startswith(key):
                        del root_values[k]
            elif isinstance(input, Repeat):
                if key in values:
                    group_values = values[key]
                    for i, rep_values in enumerate(group_values):
                        rep_index = rep_values['__index__']
                        cleanup(""%s%s_%d|"" % (prefix, key, rep_index), input.inputs, group_values[i])
            elif isinstance(input, Conditional):
                # Scrub dynamic resource related parameters from workflows,
                # they cause problems and the workflow probably should include
                # their state in workflow encoding.
                if input.name == '__job_resource':
                    if input.name in values:
                        del values[input.name]
                    return
                if input.name in values:
                    group_values = values[input.name]
                    current_case = group_values['__current_case__']
                    cleanup(""%s%s|"" % (prefix, key), input.cases[current_case].inputs, group_values)
            elif isinstance(input, Section):
                if input.name in values:
                    cleanup(""%s%s|"" % (prefix, key), input.inputs, values[input.name])","1. Use `input validation` to check if the input is valid.
2. Use `secure coding practices` to avoid common security vulnerabilities.
3. Use `encryption` to protect sensitive data."
"    def precreate_dataset_collection(self, structure, allow_unitialized_element=True, completed_collection=None, implicit_output_name=None):
        has_structure = not structure.is_leaf and structure.children_known
        if not has_structure and allow_unitialized_element:
            dataset_collection = model.DatasetCollectionElement.UNINITIALIZED_ELEMENT
        elif not has_structure:
            collection_type_description = structure.collection_type_description
            dataset_collection = model.DatasetCollection(populated=False)
            dataset_collection.collection_type = collection_type_description.collection_type
        else:
            collection_type_description = structure.collection_type_description
            dataset_collection = model.DatasetCollection(populated=False)
            dataset_collection.collection_type = collection_type_description.collection_type
            elements = []
            for index, (identifier, substructure) in enumerate(structure.children):
                # TODO: Open question - populate these now or later?
                element = None
                if completed_collection and implicit_output_name:
                    job = completed_collection[index]
                    if job:
                        it = (jtiodca.dataset_collection for jtiodca in job.output_dataset_collections if jtiodca.name == implicit_output_name)
                        element = next(it, None)
                if element is None:
                    if substructure.is_leaf:
                        element = model.DatasetCollectionElement.UNINITIALIZED_ELEMENT
                    else:
                        element = self.precreate_dataset_collection(substructure, allow_unitialized_element=allow_unitialized_element)

                element = model.DatasetCollectionElement(
                    element=element,
                    element_identifier=identifier,
                    element_index=index,
                )
                elements.append(element)
            dataset_collection.elements = elements
            dataset_collection.element_count = len(elements)

        return dataset_collection","1. Use `allow_unitialized_element=False` to prevent creating uninitialized elements.
2. Use `completed_collection` and `implicit_output_name` to populate the dataset collection.
3. Validate the input parameters to ensure they are valid."
"def set_collection_elements(dataset_collection, type, dataset_instances):
    element_index = 0
    elements = []
    for element in type.generate_elements(dataset_instances):
        element.element_index = element_index
        element.collection = dataset_collection
        elements.append(element)

        element_index += 1

    dataset_collection.elements = elements
    dataset_collection.element_count = element_index
    return dataset_collection","1. Use `type.__init__` instead of `type.generate_elements` to initialize elements.
2. Use `dataset_collection.add_element` to add elements to the collection.
3. Use `dataset_collection.get_element_by_index` to get elements from the collection."
"    def prototype(self, plugin_type):
        plugin_type_object = self.get(plugin_type)
        if not hasattr(plugin_type_object, 'prototype_elements'):
            raise Exception(""Cannot pre-determine structure for collection of type %s"" % plugin_type)

        dataset_collection = model.DatasetCollection()
        elements = [e for e in plugin_type_object.prototype_elements()]
        dataset_collection.elements = elements
        return dataset_collection","1. Use `functools.lru_cache` to cache the results of `get()`.
2. Validate the input of `prototype_elements()`.
3. Use `contextlib.closing()` to ensure that the connection to the database is closed after use."
"def collect_dynamic_outputs(
    job_context,
    output_collections,
):
    # unmapped outputs do not correspond to explicit outputs of the tool, they were inferred entirely
    # from the tool provided metadata (e.g. galaxy.json).
    for unnamed_output_dict in job_context.tool_provided_metadata.get_unnamed_outputs():
        assert ""destination"" in unnamed_output_dict
        assert ""elements"" in unnamed_output_dict
        destination = unnamed_output_dict[""destination""]
        elements = unnamed_output_dict[""elements""]

        assert ""type"" in destination
        destination_type = destination[""type""]
        assert destination_type in [""library_folder"", ""hdca"", ""hdas""]

        # three destination types we need to handle here - ""library_folder"" (place discovered files in a library folder),
        # ""hdca"" (place discovered files in a history dataset collection), and ""hdas"" (place discovered files in a history
        # as stand-alone datasets).
        if destination_type == ""library_folder"":
            # populate a library folder (needs to be already have been created)
            library_folder = job_context.get_library_folder(destination)
            persist_elements_to_folder(job_context, elements, library_folder)
        elif destination_type == ""hdca"":
            # create or populate a dataset collection in the history
            assert ""collection_type"" in unnamed_output_dict
            object_id = destination.get(""object_id"")
            if object_id:
                hdca = job_context.get_hdca(object_id)
            else:
                name = unnamed_output_dict.get(""name"", ""unnamed collection"")
                collection_type = unnamed_output_dict[""collection_type""]
                collection_type_description = COLLECTION_TYPE_DESCRIPTION_FACTORY.for_collection_type(collection_type)
                structure = UninitializedTree(collection_type_description)
                hdca = job_context.create_hdca(name, structure)
            error_message = unnamed_output_dict.get(""error_message"")
            if error_message:
                hdca.collection.handle_population_failed(error_message)
            else:
                persist_elements_to_hdca(job_context, elements, hdca, collector=DEFAULT_DATASET_COLLECTOR)
        elif destination_type == ""hdas"":
            persist_hdas(elements, job_context, final_job_state=job_context.final_job_state)

    for name, has_collection in output_collections.items():
        output_collection_def = job_context.output_collection_def(name)
        if not output_collection_def:
            continue

        if not output_collection_def.dynamic_structure:
            continue

        # Could be HDCA for normal jobs or a DC for mapping
        # jobs.
        if hasattr(has_collection, ""collection""):
            collection = has_collection.collection
        else:
            collection = has_collection

        # We are adding dynamic collections, which may be precreated, but their actually state is still new!
        collection.populated_state = collection.populated_states.NEW

        try:
            collection_builder = builder.BoundCollectionBuilder(collection)
            dataset_collectors = [dataset_collector(description) for description in output_collection_def.dataset_collector_descriptions]
            output_name = output_collection_def.name
            filenames = job_context.find_files(output_name, collection, dataset_collectors)
            job_context.populate_collection_elements(
                collection,
                collection_builder,
                filenames,
                name=output_collection_def.name,
                metadata_source_name=output_collection_def.metadata_source,
                final_job_state=job_context.final_job_state,
            )
            collection_builder.populate()
        except Exception:
            log.exception(""Problem gathering output collection."")
            collection.handle_population_failed(""Problem building datasets for collection."")

        job_context.add_dataset_collection(has_collection)","1. Use `assert` statements to validate the input data.
2. Use `sanitize_html` to sanitize user input.
3. Use `job_context.get_library_folder` to get a library folder instead of using a string."
"    def populate_collection_elements(self, collection, root_collection_builder, filenames, name=None, metadata_source_name=None, final_job_state='ok'):
        # TODO: allow configurable sorting.
        #    <sort by=""lexical"" /> <!-- default -->
        #    <sort by=""reverse_lexical"" />
        #    <sort regex=""example.(\\d+).fastq"" by=""1:numerical"" />
        #    <sort regex=""part_(\\d+)_sample_([^_]+).fastq"" by=""2:lexical,1:numerical"" />
        if name is None:
            name = ""unnamed output""

        element_datasets = {'element_identifiers': [], 'datasets': [], 'tag_lists': [], 'paths': [], 'extra_files': []}
        for filename, discovered_file in filenames.items():
            create_dataset_timer = ExecutionTimer()
            fields_match = discovered_file.match
            if not fields_match:
                raise Exception(""Problem parsing metadata fields for file %s"" % filename)
            element_identifiers = fields_match.element_identifiers
            designation = fields_match.designation
            visible = fields_match.visible
            ext = fields_match.ext
            dbkey = fields_match.dbkey
            extra_files = fields_match.extra_files
            # galaxy.tools.parser.output_collection_def.INPUT_DBKEY_TOKEN
            if dbkey == ""__input__"":
                dbkey = self.input_dbkey

            # Create new primary dataset
            dataset_name = fields_match.name or designation

            link_data = discovered_file.match.link_data

            sources = discovered_file.match.sources
            hashes = discovered_file.match.hashes
            created_from_basename = discovered_file.match.created_from_basename

            dataset = self.create_dataset(
                ext=ext,
                designation=designation,
                visible=visible,
                dbkey=dbkey,
                name=dataset_name,
                metadata_source_name=metadata_source_name,
                link_data=link_data,
                sources=sources,
                hashes=hashes,
                created_from_basename=created_from_basename,
                final_job_state=final_job_state,
            )
            log.debug(
                ""(%s) Created dynamic collection dataset for path [%s] with element identifier [%s] for output [%s] %s"",
                self.job_id(),
                filename,
                designation,
                name,
                create_dataset_timer,
            )
            element_datasets['element_identifiers'].append(element_identifiers)
            element_datasets['extra_files'].append(extra_files)
            element_datasets['datasets'].append(dataset)
            element_datasets['tag_lists'].append(discovered_file.match.tag_list)
            element_datasets['paths'].append(filename)

        self.add_tags_to_datasets(datasets=element_datasets['datasets'], tag_lists=element_datasets['tag_lists'])
        for (element_identifiers, dataset) in zip(element_datasets['element_identifiers'], element_datasets['datasets']):
            current_builder = root_collection_builder
            for element_identifier in element_identifiers[:-1]:
                current_builder = current_builder.get_level(element_identifier)
            current_builder.add_dataset(element_identifiers[-1], dataset)

            # Associate new dataset with job
            element_identifier_str = "":"".join(element_identifiers)
            association_name = '__new_primary_file_{}|{}__'.format(name, element_identifier_str)
            self.add_output_dataset_association(association_name, dataset)

        self.flush()
        self.update_object_store_with_datasets(datasets=element_datasets['datasets'], paths=element_datasets['paths'], extra_files=element_datasets['extra_files'])
        add_datasets_timer = ExecutionTimer()
        self.add_datasets_to_history(element_datasets['datasets'])
        log.debug(
            ""(%s) Add dynamic collection datasets to history for output [%s] %s"",
            self.job_id(),
            name,
            add_datasets_timer,
        )
        self.set_datasets_metadata(datasets=element_datasets['datasets'])","1. Use `input_dbkey` instead of `__input__` to avoid SQL injection.
2. Sanitize user input to prevent XSS attacks.
3. Use a secure password hashing function to protect user passwords."
"    def export_archive(self, trans, id=None, gzip=True, include_hidden=False, include_deleted=False, preview=False):
        """""" Export a history to an archive. """"""
        #
        # Get history to export.
        #
        if id:
            history = self.history_manager.get_accessible(self.decode_id(id), trans.user, current_history=trans.history)
        else:
            # Use current history.
            history = trans.history
            id = trans.security.encode_id(history.id)
        if not history:
            return trans.show_error_message(""This history does not exist or you cannot export this history."")
        # If history has already been exported and it has not changed since export, stream it.
        jeha = history.latest_export
        if jeha and jeha.up_to_date:
            if jeha.ready:
                if preview:
                    url = url_for(controller='history', action=""export_archive"", id=id, qualified=True)
                    return trans.show_message(""History Ready: '%(n)s'. Use this link to download ""
                                              ""the archive or import it to another Galaxy server: ""
                                              ""<a href='%(u)s'>%(u)s</a>"" % ({'n': history.name, 'u': url}))
                else:
                    return self.serve_ready_history_export(trans, jeha)
            elif jeha.preparing:
                return trans.show_message(""Still exporting history %(n)s; please check back soon. Link: <a href='%(s)s'>%(s)s</a>""
                                          % ({'n': history.name, 's': url_for(controller='history', action=""export_archive"", id=id, qualified=True)}))
        self.queue_history_export(trans, history, gzip=gzip, include_hidden=include_hidden, include_deleted=include_deleted)
        url = url_for(controller='history', action=""export_archive"", id=id, qualified=True)
        return trans.show_message(""Exporting History '%(n)s'. You will need to <a href='%(share)s'>make this history 'accessible'</a> in order to import this to another galaxy sever. <br/>""
                                  ""Use this link to download the archive or import it to another Galaxy server: ""
                                  ""<a href='%(u)s'>%(u)s</a>"" % ({'share': url_for(controller='history', action='sharing'), 'n': history.name, 'u': url}))","1. Use `trans.user.id` instead of `trans.security.encode_id(history.id)` to get the user ID.
2. Use `trans.security.decode_id(id)` to get the history ID from the encoded ID.
3. Use `trans.show_error_message()` instead of `raise Exception()` to show error messages."
"    def execute(self, tool, trans, incoming=None, return_job=False, set_output_hid=True, history=None, job_params=None, rerun_remap_job_id=None, execution_cache=None, dataset_collection_elements=None, completed_job=None, collection_info=None):
        """"""
        Executes a tool, creating job and tool outputs, associating them, and
        submitting the job to the job queue. If history is not specified, use
        trans.history as destination for tool's output datasets.
        """"""
        trans.check_user_activation()
        incoming = incoming or {}
        self._check_access(tool, trans)
        app = trans.app
        if execution_cache is None:
            execution_cache = ToolExecutionCache(trans)
        current_user_roles = execution_cache.current_user_roles
        history, inp_data, inp_dataset_collections, preserved_tags, all_permissions = self._collect_inputs(tool, trans, incoming, history, current_user_roles, collection_info)
        # Build name for output datasets based on tool name and input names
        on_text = self._get_on_text(inp_data)

        # format='input"" previously would give you a random extension from
        # the input extensions, now it should just give ""input"" as the output
        # format.
        input_ext = 'data' if tool.profile < 16.04 else ""input""
        input_dbkey = incoming.get(""dbkey"", ""?"")
        for name, data in reversed(list(inp_data.items())):
            if not data:
                data = NoneDataset(datatypes_registry=app.datatypes_registry)
                continue

            # Convert LDDA to an HDA.
            if isinstance(data, LibraryDatasetDatasetAssociation) and not completed_job:
                data = data.to_history_dataset_association(None)
                inp_data[name] = data

            if tool.profile < 16.04:
                input_ext = data.ext

            if data.dbkey not in [None, '?']:
                input_dbkey = data.dbkey

            identifier = getattr(data, ""element_identifier"", None)
            if identifier is not None:
                incoming[""%s|__identifier__"" % name] = identifier

        # Collect chromInfo dataset and add as parameters to incoming
        (chrom_info, db_dataset) = execution_cache.get_chrom_info(tool.id, input_dbkey)

        if db_dataset:
            inp_data.update({""chromInfo"": db_dataset})
        incoming[""chromInfo""] = chrom_info

        if not completed_job:
            # Determine output dataset permission/roles list
            existing_datasets = [inp for inp in inp_data.values() if inp]
            if existing_datasets:
                output_permissions = app.security_agent.guess_derived_permissions(all_permissions)
            else:
                # No valid inputs, we will use history defaults
                output_permissions = app.security_agent.history_get_default_permissions(history)

        # Add the dbkey to the incoming parameters
        incoming[""dbkey""] = input_dbkey
        # wrapped params are used by change_format action and by output.label; only perform this wrapping once, as needed
        wrapped_params = self._wrapped_params(trans, tool, incoming, inp_data)

        out_data = OrderedDict()
        input_collections = dict((k, v[0][0]) for k, v in inp_dataset_collections.items())
        output_collections = OutputCollections(
            trans,
            history,
            tool=tool,
            tool_action=self,
            input_collections=input_collections,
            dataset_collection_elements=dataset_collection_elements,
            on_text=on_text,
            incoming=incoming,
            params=wrapped_params.params,
            job_params=job_params,
            tags=preserved_tags,
        )

        # Keep track of parent / child relationships, we'll create all the
        # datasets first, then create the associations
        parent_to_child_pairs = []
        child_dataset_names = set()
        object_store_populator = ObjectStorePopulator(app)

        def handle_output(name, output, hidden=None):
            if output.parent:
                parent_to_child_pairs.append((output.parent, name))
                child_dataset_names.add(name)
            # What is the following hack for? Need to document under what
            # conditions can the following occur? (james@bx.psu.edu)
            # HACK: the output data has already been created
            #      this happens i.e. as a result of the async controller
            if name in incoming:
                dataid = incoming[name]
                data = trans.sa_session.query(app.model.HistoryDatasetAssociation).get(dataid)
                assert data is not None
                out_data[name] = data
            else:
                ext = determine_output_format(
                    output,
                    wrapped_params.params,
                    inp_data,
                    inp_dataset_collections,
                    input_ext,
                    python_template_version=tool.python_template_version,
                )
                create_datasets = True
                dataset = None

                if completed_job:
                    for output_dataset in completed_job.output_datasets:
                        if output_dataset.name == name:
                            create_datasets = False
                            completed_data = output_dataset.dataset
                            dataset = output_dataset.dataset.dataset
                            break

                data = app.model.HistoryDatasetAssociation(extension=ext, dataset=dataset, create_dataset=create_datasets, flush=False)
                if create_datasets:
                    from_work_dir = output.from_work_dir
                    if from_work_dir is not None:
                        data.dataset.created_from_basename = os.path.basename(from_work_dir)
                if hidden is None:
                    hidden = output.hidden
                if not hidden and dataset_collection_elements is not None:  # Mapping over a collection - hide datasets
                    hidden = True
                if hidden:
                    data.visible = False
                if dataset_collection_elements is not None and name in dataset_collection_elements:
                    dataset_collection_elements[name].hda = data
                trans.sa_session.add(data)
                if not completed_job:
                    trans.app.security_agent.set_all_dataset_permissions(data.dataset, output_permissions, new=True)
            data.copy_tags_to(preserved_tags)

            if not completed_job and trans.app.config.legacy_eager_objectstore_initialization:
                # Must flush before setting object store id currently.
                trans.sa_session.flush()
                object_store_populator.set_object_store_id(data)

            # This may not be neccesary with the new parent/child associations
            data.designation = name
            # Copy metadata from one of the inputs if requested.

            # metadata source can be either a string referencing an input
            # or an actual object to copy.
            metadata_source = output.metadata_source
            if metadata_source:
                if isinstance(metadata_source, string_types):
                    metadata_source = inp_data.get(metadata_source)

            if metadata_source is not None:
                data.init_meta(copy_from=metadata_source)
            else:
                data.init_meta()
            # Take dbkey from LAST input
            data.dbkey = str(input_dbkey)
            # Set state
            if completed_job:
                data.blurb = completed_data.blurb
                data.peek = completed_data.peek
                data._metadata = completed_data._metadata
            else:
                data.blurb = ""queued""
            # Set output label
            data.name = self.get_output_name(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)
            # Store output
            out_data[name] = data
            if output.actions:
                # Apply pre-job tool-output-dataset actions; e.g. setting metadata, changing format
                output_action_params = dict(out_data)
                output_action_params.update(incoming)
                output.actions.apply_action(data, output_action_params)
            # Also set the default values of actions of type metadata
            self.set_metadata_defaults(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)
            # Flush all datasets at once.
            return data

        for name, output in tool.outputs.items():
            if not filter_output(output, incoming):
                handle_output_timer = ExecutionTimer()
                if output.collection:
                    collections_manager = app.dataset_collections_service
                    element_identifiers = []
                    known_outputs = output.known_outputs(input_collections, collections_manager.type_registry)
                    created_element_datasets = []
                    # Just to echo TODO elsewhere - this should be restructured to allow
                    # nested collections.
                    for output_part_def in known_outputs:
                        # Add elements to top-level collection, unless nested...
                        current_element_identifiers = element_identifiers
                        current_collection_type = output.structure.collection_type

                        for parent_id in (output_part_def.parent_ids or []):
                            # TODO: replace following line with formal abstractions for doing this.
                            current_collection_type = "":"".join(current_collection_type.split("":"")[1:])
                            name_to_index = dict((value[""name""], index) for (index, value) in enumerate(current_element_identifiers))
                            if parent_id not in name_to_index:
                                if parent_id not in current_element_identifiers:
                                    index = len(current_element_identifiers)
                                    current_element_identifiers.append(dict(
                                        name=parent_id,
                                        collection_type=current_collection_type,
                                        src=""new_collection"",
                                        element_identifiers=[],
                                    ))
                                else:
                                    index = name_to_index[parent_id]
                            current_element_identifiers = current_element_identifiers[index][""element_identifiers""]

                        effective_output_name = output_part_def.effective_output_name
                        element = handle_output(effective_output_name, output_part_def.output_def, hidden=True)
                        created_element_datasets.append(element)
                        # TODO: this shouldn't exist in the top-level of the history at all
                        # but for now we are still working around that by hiding the contents
                        # there.
                        # Following hack causes dataset to no be added to history...
                        child_dataset_names.add(effective_output_name)
                        trans.sa_session.add(element)
                        current_element_identifiers.append({
                            ""__object__"": element,
                            ""name"": output_part_def.element_identifier,
                        })

                    history.add_datasets(trans.sa_session, created_element_datasets, set_hid=set_output_hid, quota=False, flush=True)
                    if output.dynamic_structure:
                        assert not element_identifiers  # known_outputs must have been empty
                        element_kwds = dict(elements=collections_manager.ELEMENTS_UNINITIALIZED)
                    else:
                        element_kwds = dict(element_identifiers=element_identifiers)
                    output_collections.create_collection(
                        output=output,
                        name=name,
                        **element_kwds
                    )
                    log.info(""Handled collection output named %s for tool %s %s"" % (name, tool.id, handle_output_timer))
                else:
                    handle_output(name, output)
                    log.info(""Handled output named %s for tool %s %s"" % (name, tool.id, handle_output_timer))

        add_datasets_timer = ExecutionTimer()
        # Add all the top-level (non-child) datasets to the history unless otherwise specified
        datasets_to_persist = []
        for name, data in out_data.items():
            if name not in child_dataset_names and name not in incoming:  # don't add children; or already existing datasets, i.e. async created
                datasets_to_persist.append(data)
        # Set HID and add to history.
        # This is brand new and certainly empty so don't worry about quota.
        history.add_datasets(trans.sa_session, datasets_to_persist, set_hid=set_output_hid, quota=False, flush=False)

        # Add all the children to their parents
        for parent_name, child_name in parent_to_child_pairs:
            parent_dataset = out_data[parent_name]
            child_dataset = out_data[child_name]
            parent_dataset.children.append(child_dataset)

        log.info(""Added output datasets to history %s"" % add_datasets_timer)
        job_setup_timer = ExecutionTimer()
        # Create the job object
        job, galaxy_session = self._new_job_for_session(trans, tool, history)
        self._record_inputs(trans, tool, job, incoming, inp_data, inp_dataset_collections)
        self._record_outputs(job, out_data, output_collections)
        job.object_store_id = object_store_populator.object_store_id
        if job_params:
            job.params = dumps(job_params)
        if completed_job:
            job.set_copied_from_job_id(completed_job.id)
        trans.sa_session.add(job)
        # Now that we have a job id, we can remap any outputs if this is a rerun and the user chose to continue dependent jobs
        # This functionality requires tracking jobs in the database.
        if app.config.track_jobs_in_database and rerun_remap_job_id is not None:
            self._remap_job_on_rerun(trans=trans,
                                     galaxy_session=galaxy_session,
                                     rerun_remap_job_id=rerun_remap_job_id,
                                     current_job=job,
                                     out_data=out_data)
        log.info(""Setup for job %s complete, ready to be enqueued %s"" % (job.log_str(), job_setup_timer))

        # Some tools are not really executable, but jobs are still created for them ( for record keeping ).
        # Examples include tools that redirect to other applications ( epigraph ).  These special tools must
        # include something that can be retrieved from the params ( e.g., REDIRECT_URL ) to keep the job
        # from being queued.
        if 'REDIRECT_URL' in incoming:
            # Get the dataset - there should only be 1
            for name in inp_data.keys():
                dataset = inp_data[name]
            redirect_url = tool.parse_redirect_url(dataset, incoming)
            # GALAXY_URL should be include in the tool params to enable the external application
            # to send back to the current Galaxy instance
            GALAXY_URL = incoming.get('GALAXY_URL', None)
            assert GALAXY_URL is not None, ""GALAXY_URL parameter missing in tool config.""
            redirect_url += ""&GALAXY_URL=%s"" % GALAXY_URL
            # Job should not be queued, so set state to ok
            job.set_state(app.model.Job.states.OK)
            job.info = ""Redirected to: %s"" % redirect_url
            trans.sa_session.add(job)
            trans.sa_session.flush()
            trans.response.send_redirect(url_for(controller='tool_runner', action='redirect', redirect_url=redirect_url))
        else:
            # Dispatch to a job handler. enqueue() is responsible for flushing the job
            app.job_manager.enqueue(job, tool=tool)
            trans.log_event(""Added job to the job queue, id: %s"" % str(job.id), tool_id=job.tool_id)
            return job, out_data","1. Use `input()` instead of `raw_input()` to prevent code injection.
2. Use `paramiko` to connect to remote servers instead of `ssh`.
3. Use `cryptography` to encrypt sensitive data instead of `base64`."
"        def handle_output(name, output, hidden=None):
            if output.parent:
                parent_to_child_pairs.append((output.parent, name))
                child_dataset_names.add(name)
            # What is the following hack for? Need to document under what
            # conditions can the following occur? (james@bx.psu.edu)
            # HACK: the output data has already been created
            #      this happens i.e. as a result of the async controller
            if name in incoming:
                dataid = incoming[name]
                data = trans.sa_session.query(app.model.HistoryDatasetAssociation).get(dataid)
                assert data is not None
                out_data[name] = data
            else:
                ext = determine_output_format(
                    output,
                    wrapped_params.params,
                    inp_data,
                    inp_dataset_collections,
                    input_ext,
                    python_template_version=tool.python_template_version,
                )
                create_datasets = True
                dataset = None

                if completed_job:
                    for output_dataset in completed_job.output_datasets:
                        if output_dataset.name == name:
                            create_datasets = False
                            completed_data = output_dataset.dataset
                            dataset = output_dataset.dataset.dataset
                            break

                data = app.model.HistoryDatasetAssociation(extension=ext, dataset=dataset, create_dataset=create_datasets, flush=False)
                if create_datasets:
                    from_work_dir = output.from_work_dir
                    if from_work_dir is not None:
                        data.dataset.created_from_basename = os.path.basename(from_work_dir)
                if hidden is None:
                    hidden = output.hidden
                if not hidden and dataset_collection_elements is not None:  # Mapping over a collection - hide datasets
                    hidden = True
                if hidden:
                    data.visible = False
                if dataset_collection_elements is not None and name in dataset_collection_elements:
                    dataset_collection_elements[name].hda = data
                trans.sa_session.add(data)
                if not completed_job:
                    trans.app.security_agent.set_all_dataset_permissions(data.dataset, output_permissions, new=True)
            data.copy_tags_to(preserved_tags)

            if not completed_job and trans.app.config.legacy_eager_objectstore_initialization:
                # Must flush before setting object store id currently.
                trans.sa_session.flush()
                object_store_populator.set_object_store_id(data)

            # This may not be neccesary with the new parent/child associations
            data.designation = name
            # Copy metadata from one of the inputs if requested.

            # metadata source can be either a string referencing an input
            # or an actual object to copy.
            metadata_source = output.metadata_source
            if metadata_source:
                if isinstance(metadata_source, string_types):
                    metadata_source = inp_data.get(metadata_source)

            if metadata_source is not None:
                data.init_meta(copy_from=metadata_source)
            else:
                data.init_meta()
            # Take dbkey from LAST input
            data.dbkey = str(input_dbkey)
            # Set state
            if completed_job:
                data.blurb = completed_data.blurb
                data.peek = completed_data.peek
                data._metadata = completed_data._metadata
            else:
                data.blurb = ""queued""
            # Set output label
            data.name = self.get_output_name(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)
            # Store output
            out_data[name] = data
            if output.actions:
                # Apply pre-job tool-output-dataset actions; e.g. setting metadata, changing format
                output_action_params = dict(out_data)
                output_action_params.update(incoming)
                output.actions.apply_action(data, output_action_params)
            # Also set the default values of actions of type metadata
            self.set_metadata_defaults(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)
            # Flush all datasets at once.
            return data","1. Use `input_dbkey` instead of `dbkey` to avoid overwriting existing datasets.
2. Use `data.visible` to hide datasets instead of setting `hidden` to True.
3. Use `data.copy_tags_to()` to copy tags instead of manually setting them."
"    def get_shed_config_dict(self, app, default=None):
        """"""
        Return the in-memory version of the shed_tool_conf file, which is stored in the config_elems entry
        in the shed_tool_conf_dict.
        """"""

        def _is_valid_shed_config_filename(filename):
            for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
                if filename == shed_tool_conf_dict['config_filename']:
                    return True
            return False

        if not self.shed_config_filename or not _is_valid_shed_config_filename(self.shed_config_filename):
            self.guess_shed_config(app, default=default)
        if self.shed_config_filename:
            for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
                if self.shed_config_filename == shed_tool_conf_dict['config_filename']:
                    return shed_tool_conf_dict
        return default","1. Use `pwd` or `getpass` to get the user's password instead of asking for it directly.
2. Use `os.path.join` to concatenate paths instead of concatenating them manually.
3. Use `json.load` to load the JSON file instead of parsing it manually."
"    def guess_shed_config(self, app, default=None):
        tool_ids = []
        for tool in self.metadata.get('tools', []):
            tool_ids.append(tool.get('guid'))
        for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
            name = shed_tool_conf_dict['config_filename']
            for elem in shed_tool_conf_dict['config_elems']:
                if elem.tag == 'tool':
                    for sub_elem in elem.findall('id'):
                        tool_id = sub_elem.text.strip()
                        if tool_id in tool_ids:
                            self.shed_config_filename = name
                            return shed_tool_conf_dict
                elif elem.tag == ""section"":
                    for tool_elem in elem.findall('tool'):
                        for sub_elem in tool_elem.findall('id'):
                            tool_id = sub_elem.text.strip()
                            if tool_id in tool_ids:
                                self.shed_config_filename = name
                                return shed_tool_conf_dict
        if self.includes_datatypes or self.includes_data_managers:
            # We need to search by file paths here, which is less desirable.
            tool_shed = common_util.remove_protocol_and_port_from_tool_shed_url(self.tool_shed)
            for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
                tool_path = shed_tool_conf_dict['tool_path']
                relative_path = os.path.join(tool_path, tool_shed, 'repos', self.owner, self.name, self.installed_changeset_revision)
                if os.path.exists(relative_path):
                    self.shed_config_filename = shed_tool_conf_dict['config_filename']
                    return shed_tool_conf_dict
        return default","1. Use `validator.validate_tool_conf_filename()` to validate the tool config filename.
2. Use `security.sanitize_filename()` to sanitize the tool config filename.
3. Use `security.sanitize_path()` to sanitize the tool path."
"    def install(self, tool_shed_url, name, owner, changeset_revision, install_options):
        # Get all of the information necessary for installing the repository from the specified tool shed.
        repository_revision_dict, repo_info_dicts = self.__get_install_info_from_tool_shed(tool_shed_url,
                                                                                           name,
                                                                                           owner,
                                                                                           changeset_revision)
        installed_tool_shed_repositories = self.__initiate_and_install_repositories(
            tool_shed_url,
            repository_revision_dict,
            repo_info_dicts,
            install_options
        )
        return installed_tool_shed_repositories","1. Use `get_install_info_from_tool_shed` to get the information necessary for installing the repository from the specified tool shed.
2. Use `initiate_and_install_repositories` to initiate and install the repositories.
3. Use `install_options` to specify the options for installing the repositories."
"    def __initiate_and_install_repositories(self, tool_shed_url, repository_revision_dict, repo_info_dicts, install_options):
        try:
            has_repository_dependencies = repository_revision_dict['has_repository_dependencies']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'has_repository_dependencies'."")
        try:
            includes_tools = repository_revision_dict['includes_tools']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'includes_tools'."")
        try:
            includes_tool_dependencies = repository_revision_dict['includes_tool_dependencies']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'includes_tool_dependencies'."")
        try:
            includes_tools_for_display_in_tool_panel = repository_revision_dict['includes_tools_for_display_in_tool_panel']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'includes_tools_for_display_in_tool_panel'."")
        # Get the information about the Galaxy components (e.g., tool pane section, tool config file, etc) that will contain the repository information.
        install_repository_dependencies = install_options.get('install_repository_dependencies', False)
        install_resolver_dependencies = install_options.get('install_resolver_dependencies', False)
        install_tool_dependencies = install_options.get('install_tool_dependencies', False)
        if install_tool_dependencies:
            self.__assert_can_install_dependencies()
        new_tool_panel_section_label = install_options.get('new_tool_panel_section_label', '')
        tool_panel_section_mapping = install_options.get('tool_panel_section_mapping', {})
        shed_tool_conf = install_options.get('shed_tool_conf', None)
        if shed_tool_conf:
            # Get the tool_path setting.
            shed_conf_dict = self.tpm.get_shed_tool_conf_dict(shed_tool_conf)
            tool_path = shed_conf_dict['tool_path']
        else:
            # Don't use migrated_tools_conf.xml.
            try:
                shed_config_dict = self.app.toolbox.dynamic_confs(include_migrated_tool_conf=False)[0]
            except IndexError:
                raise exceptions.RequestParameterMissingException(""Missing required parameter 'shed_tool_conf'."")
            shed_tool_conf = shed_config_dict['config_filename']
            tool_path = shed_config_dict['tool_path']
        tool_panel_section_id = self.app.toolbox.find_section_id(install_options.get('tool_panel_section_id', ''))
        # Build the dictionary of information necessary for creating tool_shed_repository database records
        # for each repository being installed.
        installation_dict = dict(install_repository_dependencies=install_repository_dependencies,
                                 new_tool_panel_section_label=new_tool_panel_section_label,
                                 tool_panel_section_mapping=tool_panel_section_mapping,
                                 no_changes_checked=False,
                                 repo_info_dicts=repo_info_dicts,
                                 tool_panel_section_id=tool_panel_section_id,
                                 tool_path=tool_path,
                                 tool_shed_url=tool_shed_url)
        # Create the tool_shed_repository database records and gather additional information for repository installation.
        created_or_updated_tool_shed_repositories, tool_panel_section_keys, repo_info_dicts, filtered_repo_info_dicts = \\
            self.handle_tool_shed_repositories(installation_dict)
        if created_or_updated_tool_shed_repositories:
            # Build the dictionary of information necessary for installing the repositories.
            installation_dict = dict(created_or_updated_tool_shed_repositories=created_or_updated_tool_shed_repositories,
                                     filtered_repo_info_dicts=filtered_repo_info_dicts,
                                     has_repository_dependencies=has_repository_dependencies,
                                     includes_tool_dependencies=includes_tool_dependencies,
                                     includes_tools=includes_tools,
                                     includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
                                     install_repository_dependencies=install_repository_dependencies,
                                     install_resolver_dependencies=install_resolver_dependencies,
                                     install_tool_dependencies=install_tool_dependencies,
                                     message='',
                                     new_tool_panel_section_label=new_tool_panel_section_label,
                                     shed_tool_conf=shed_tool_conf,
                                     status='done',
                                     tool_panel_section_id=tool_panel_section_id,
                                     tool_panel_section_keys=tool_panel_section_keys,
                                     tool_panel_section_mapping=tool_panel_section_mapping,
                                     tool_path=tool_path,
                                     tool_shed_url=tool_shed_url)
            # Prepare the repositories for installation.  Even though this
            # method receives a single combination of tool_shed_url, name,
            # owner and changeset_revision, there may be multiple repositories
            # for installation at this point because repository dependencies
            # may have added additional repositories for installation along
            # with the single specified repository.
            encoded_kwd, query, tool_shed_repositories, encoded_repository_ids = \\
                self.initiate_repository_installation(installation_dict)
            # Some repositories may have repository dependencies that are
            # required to be installed before the dependent repository, so
            # we'll order the list of tsr_ids to ensure all repositories
            # install in the required order.
            tsr_ids = [self.app.security.encode_id(tool_shed_repository.id) for tool_shed_repository in tool_shed_repositories]

            decoded_kwd = dict(
                shed_tool_conf=shed_tool_conf,
                tool_path=tool_path,
                tool_panel_section_keys=tool_panel_section_keys,
                repo_info_dicts=filtered_repo_info_dicts,
                install_resolver_dependencies=install_resolver_dependencies,
                install_tool_dependencies=install_tool_dependencies,
                tool_panel_section_mapping=tool_panel_section_mapping,
            )
            return self.install_repositories(tsr_ids, decoded_kwd, reinstalling=False, install_options=install_options)","1. Use `self.app.security.encode_id()` to encode all IDs before returning them.
2. Use `self.app.security.decode_id()` to decode all IDs before using them.
3. Use `self.app.security.check_access()` to check if the user has permission to perform the requested action."
"    def install_tool_shed_repository(self, tool_shed_repository, repo_info_dict, tool_panel_section_key, shed_tool_conf, tool_path,
                                     install_resolver_dependencies, install_tool_dependencies, reinstalling=False,
                                     tool_panel_section_mapping={}, install_options=None):
        self.app.install_model.context.flush()
        if tool_panel_section_key:
            _, tool_section = self.app.toolbox.get_section(tool_panel_section_key)
            if tool_section is None:
                log.debug('Invalid tool_panel_section_key ""%s"" specified.  Tools will be loaded outside of sections in the tool panel.',
                          str(tool_panel_section_key))
        else:
            tool_section = None
        if isinstance(repo_info_dict, string_types):
            repo_info_dict = encoding_util.tool_shed_decode(repo_info_dict)
        repo_info_tuple = repo_info_dict[tool_shed_repository.name]
        description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies = repo_info_tuple
        if changeset_revision != tool_shed_repository.changeset_revision:
            # This is an update
            tool_shed_url = common_util.get_tool_shed_url_from_tool_shed_registry(self.app, tool_shed_repository.tool_shed)
            return self.update_tool_shed_repository(tool_shed_repository,
                                                    tool_shed_url,
                                                    ctx_rev,
                                                    changeset_revision,
                                                    install_options=install_options)
        # Clone the repository to the configured location.
        self.update_tool_shed_repository_status(tool_shed_repository,
                                                self.install_model.ToolShedRepository.installation_status.CLONING)
        relative_clone_dir = repository_util.generate_tool_shed_repository_install_dir(repository_clone_url,
                                                                                       tool_shed_repository.installed_changeset_revision)
        relative_install_dir = os.path.join(relative_clone_dir, tool_shed_repository.name)
        install_dir = os.path.join(tool_path, relative_install_dir)
        log.info(""Cloning repository '%s' at %s:%s"", repository_clone_url, ctx_rev, tool_shed_repository.changeset_revision)
        cloned_ok, error_message = hg_util.clone_repository(repository_clone_url, os.path.abspath(install_dir), ctx_rev)
        if cloned_ok:
            if reinstalling:
                # Since we're reinstalling the repository we need to find the latest changeset revision to
                # which it can be updated.
                changeset_revision_dict = self.app.update_repository_manager.get_update_to_changeset_revision_and_ctx_rev(tool_shed_repository)
                current_changeset_revision = changeset_revision_dict.get('changeset_revision', None)
                current_ctx_rev = changeset_revision_dict.get('ctx_rev', None)
                if current_ctx_rev != ctx_rev:
                    repo_path = os.path.abspath(install_dir)
                    hg_util.pull_repository(repo_path, repository_clone_url, current_changeset_revision)
                    hg_util.update_repository(repo_path, ctx_rev=current_ctx_rev)
            self.__handle_repository_contents(tool_shed_repository=tool_shed_repository,
                                              tool_path=tool_path,
                                              repository_clone_url=repository_clone_url,
                                              relative_install_dir=relative_install_dir,
                                              tool_shed=tool_shed_repository.tool_shed,
                                              tool_section=tool_section,
                                              shed_tool_conf=shed_tool_conf,
                                              reinstalling=reinstalling,
                                              tool_panel_section_mapping=tool_panel_section_mapping)
            metadata = tool_shed_repository.metadata
            if 'tools' in metadata and install_resolver_dependencies:
                self.update_tool_shed_repository_status(tool_shed_repository,
                                                        self.install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES)
                new_tools = [self.app.toolbox._tools_by_id.get(tool_d['guid'], None) for tool_d in metadata['tools']]
                new_requirements = set([tool.requirements.packages for tool in new_tools if tool])
                [self._view.install_dependencies(r) for r in new_requirements]
                dependency_manager = self.app.toolbox.dependency_manager
                if dependency_manager.cached:
                    [dependency_manager.build_cache(r) for r in new_requirements]

            if install_tool_dependencies and tool_shed_repository.tool_dependencies and 'tool_dependencies' in metadata:
                work_dir = tempfile.mkdtemp(prefix=""tmp-toolshed-itsr"")
                # Install tool dependencies.
                self.update_tool_shed_repository_status(tool_shed_repository,
                                                        self.install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES)
                # Get the tool_dependencies.xml file from the repository.
                tool_dependencies_config = hg_util.get_config_from_disk('tool_dependencies.xml', install_dir)
                itdm = InstallToolDependencyManager(self.app)
                itdm.install_specified_tool_dependencies(tool_shed_repository=tool_shed_repository,
                                                         tool_dependencies_config=tool_dependencies_config,
                                                         tool_dependencies=tool_shed_repository.tool_dependencies,
                                                         from_tool_migration_manager=False)
                basic_util.remove_dir(work_dir)
            self.update_tool_shed_repository_status(tool_shed_repository,
                                                    self.install_model.ToolShedRepository.installation_status.INSTALLED)
            if self.app.config.manage_dependency_relationships:
                # Add the installed repository and any tool dependencies to the in-memory dictionaries
                # in the installed_repository_manager.
                self.app.installed_repository_manager.handle_repository_install(tool_shed_repository)
        else:
            # An error occurred while cloning the repository, so reset everything necessary to enable another attempt.
            repository_util.set_repository_attributes(self.app,
                                                      tool_shed_repository,
                                                      status=self.install_model.ToolShedRepository.installation_status.ERROR,
                                                      error_message=error_message,
                                                      deleted=False,
                                                      uninstalled=False,
                                                      remove_from_disk=True)","1. Use `shutil.copytree` instead of `os.path.join` to copy the repository. This will prevent directory traversal attacks.
2. Use `tempfile.mkdtemp` to create a temporary directory for the repository instead of using the current working directory. This will prevent the repository from being installed in an unintended location.
3. Use `hg_util.get_config_from_disk` to get the tool_dependencies.xml file from the repository instead of reading it from the filesystem directly. This will prevent the file from being read by an attacker."
"    def update_tool_shed_repository(self, repository, tool_shed_url, latest_ctx_rev, latest_changeset_revision,
                                    install_new_dependencies=True, install_options=None):
        install_options = install_options or {}
        original_metadata_dict = repository.metadata
        original_repository_dependencies_dict = original_metadata_dict.get('repository_dependencies', {})
        original_repository_dependencies = original_repository_dependencies_dict.get('repository_dependencies', [])
        original_tool_dependencies_dict = original_metadata_dict.get('tool_dependencies', {})
        shed_tool_conf, tool_path, relative_install_dir = suc.get_tool_panel_config_tool_path_install_dir(self.app, repository)
        if tool_path:
            repo_files_dir = os.path.abspath(os.path.join(tool_path, relative_install_dir, repository.name))
        else:
            repo_files_dir = os.path.abspath(os.path.join(relative_install_dir, repository.name))
        repository_clone_url = os.path.join(tool_shed_url, 'repos', repository.owner, repository.name)
        log.info(""Updating repository '%s' to %s:%s"", repository.name, latest_ctx_rev, latest_changeset_revision)
        hg_util.pull_repository(repo_files_dir, repository_clone_url, latest_ctx_rev)
        hg_util.update_repository(repo_files_dir, latest_ctx_rev)
        # Remove old Data Manager entries
        if repository.includes_data_managers:
            dmh = data_manager.DataManagerHandler(self.app)
            dmh.remove_from_data_manager(repository)
        # Update the repository metadata.
        tpm = tool_panel_manager.ToolPanelManager(self.app)
        irmm = InstalledRepositoryMetadataManager(app=self.app,
                                                  tpm=tpm,
                                                  repository=repository,
                                                  changeset_revision=latest_changeset_revision,
                                                  repository_clone_url=repository_clone_url,
                                                  shed_config_dict=repository.get_shed_config_dict(self.app),
                                                  relative_install_dir=relative_install_dir,
                                                  repository_files_dir=None,
                                                  resetting_all_metadata_on_repository=False,
                                                  updating_installed_repository=True,
                                                  persist=True)
        irmm.generate_metadata_for_changeset_revision()
        irmm_metadata_dict = irmm.get_metadata_dict()
        if 'tools' in irmm_metadata_dict:
            tool_panel_dict = irmm_metadata_dict.get('tool_panel_section', None)
            if tool_panel_dict is None:
                tool_panel_dict = tpm.generate_tool_panel_dict_from_shed_tool_conf_entries(repository)
            repository_tools_tups = irmm.get_repository_tools_tups()
            tpm.add_to_tool_panel(repository_name=str(repository.name),
                                  repository_clone_url=repository_clone_url,
                                  changeset_revision=str(repository.installed_changeset_revision),
                                  repository_tools_tups=repository_tools_tups,
                                  owner=str(repository.owner),
                                  shed_tool_conf=shed_tool_conf,
                                  tool_panel_dict=tool_panel_dict,
                                  new_install=False)
            # Add new Data Manager entries
            if 'data_manager' in irmm_metadata_dict:
                dmh = data_manager.DataManagerHandler(self.app)
                dmh.install_data_managers(self.app.config.shed_data_manager_config_file,
                                          irmm_metadata_dict,
                                          repository.get_shed_config_dict(self.app),
                                          os.path.join(relative_install_dir, repository.name),
                                          repository,
                                          repository_tools_tups)
        if 'repository_dependencies' in irmm_metadata_dict or 'tool_dependencies' in irmm_metadata_dict:
            new_repository_dependencies_dict = irmm_metadata_dict.get('repository_dependencies', {})
            new_repository_dependencies = new_repository_dependencies_dict.get('repository_dependencies', [])
            new_tool_dependencies_dict = irmm_metadata_dict.get('tool_dependencies', {})
            if new_repository_dependencies:
                # [[http://localhost:9009', package_picard_1_56_0', devteam', 910b0b056666', False', False']]
                if new_repository_dependencies == original_repository_dependencies:
                    for new_repository_tup in new_repository_dependencies:
                        # Make sure all dependencies are installed.
                        # TODO: Repository dependencies that are not installed should be displayed to the user,
                        # giving them the option to install them or not. This is the same behavior as when initially
                        # installing and when re-installing.
                        new_tool_shed, new_name, new_owner, new_changeset_revision, new_pir, new_oicct = \\
                            common_util.parse_repository_dependency_tuple(new_repository_tup)
                        # Mock up a repo_info_tupe that has the information needed to see if the repository dependency
                        # was previously installed.
                        repo_info_tuple = ('', new_tool_shed, new_changeset_revision, '', new_owner, [], [])
                        # Since the value of new_changeset_revision came from a repository dependency
                        # definition, it may occur earlier in the Tool Shed's repository changelog than
                        # the Galaxy tool_shed_repository.installed_changeset_revision record value, so
                        # we set from_tip to True to make sure we get the entire set of changeset revisions
                        # from the Tool Shed.
                        new_repository_db_record, installed_changeset_revision = \\
                            repository_util.repository_was_previously_installed(self.app,
                                                                                tool_shed_url,
                                                                                new_name,
                                                                                repo_info_tuple,
                                                                                from_tip=True)
                        if ((new_repository_db_record and new_repository_db_record.status in [
                                self.install_model.ToolShedRepository.installation_status.ERROR,
                                self.install_model.ToolShedRepository.installation_status.NEW,
                                self.install_model.ToolShedRepository.installation_status.UNINSTALLED])
                                or not new_repository_db_record):
                            log.debug('Update to %s contains new repository dependency %s/%s', repository.name,
                                      new_owner, new_name)
                            if not install_new_dependencies:
                                return ('repository', irmm_metadata_dict)
                            else:
                                self.install(tool_shed_url, new_name, new_owner, new_changeset_revision, install_options)
            # Updates received did not include any newly defined repository dependencies but did include
            # newly defined tool dependencies.  If the newly defined tool dependencies are not the same
            # as the originally defined tool dependencies, we need to install them.
            if not install_new_dependencies:
                for new_key, new_val in new_tool_dependencies_dict.items():
                    if new_key not in original_tool_dependencies_dict:
                        return ('tool', irmm_metadata_dict)
                    original_val = original_tool_dependencies_dict[new_key]
                    if new_val != original_val:
                        return ('tool', irmm_metadata_dict)
        # Updates received did not include any newly defined repository dependencies or newly defined
        # tool dependencies that need to be installed.
        repository = self.app.update_repository_manager.update_repository_record(repository=repository,
                                                                                 updated_metadata_dict=irmm_metadata_dict,
                                                                                 updated_changeset_revision=latest_changeset_revision,
                                                                                 updated_ctx_rev=latest_ctx_rev)
        return (None, None)","1. Use `str.strip()` to remove whitespace from the beginning and end of strings.
2. Use `urllib.parse.quote()` to escape special characters in URLs.
3. Use `os.path.join()` to join paths instead of concatenating strings."
"    def get_repository_tools_tups(self):
        """"""
        Return a list of tuples of the form (relative_path, guid, tool) for each tool defined
        in the received tool shed repository metadata.
        """"""
        repository_tools_tups = []
        shed_conf_dict = self.tpm.get_shed_tool_conf_dict(self.metadata_dict.get('shed_config_filename'))
        if 'tools' in self.metadata_dict:
            for tool_dict in self.metadata_dict['tools']:
                load_relative_path = relative_path = tool_dict.get('tool_config', None)
                if shed_conf_dict.get('tool_path'):
                    load_relative_path = os.path.join(shed_conf_dict.get('tool_path'), relative_path)
                guid = tool_dict.get('guid', None)
                if relative_path and guid:
                    tool = self.app.toolbox.load_tool(os.path.abspath(load_relative_path), guid=guid, use_cached=False)
                else:
                    tool = None
                if tool:
                    repository_tools_tups.append((relative_path, guid, tool))
        return repository_tools_tups","1. Use `os.path.join` to concatenate paths instead of string concatenation.
2. Use `os.path.abspath` to get the absolute path of a file.
3. Use `self.app.toolbox.load_tool` to load a tool instead of directly calling `__import__`."
"    def set_repository(self, repository, relative_install_dir=None, changeset_revision=None):
        self.repository = repository
        # Shed related tool panel configs are only relevant to Galaxy.
        if self.app.name == 'galaxy':
            if relative_install_dir is None and self.repository is not None:
                tool_path, relative_install_dir = self.repository.get_tool_relative_path(self.app)
            if changeset_revision is None and self.repository is not None:
                self.set_changeset_revision(self.repository.changeset_revision)
            else:
                self.set_changeset_revision(changeset_revision)
            self.shed_config_dict = repository.get_shed_config_dict(self.app, {})
            self.metadata_dict = {'shed_config_filename': self.shed_config_dict.get('config_filename', None)}
        else:
            if relative_install_dir is None and self.repository is not None:
                relative_install_dir = repository.repo_path(self.app)
            if changeset_revision is None and self.repository is not None:
                self.set_changeset_revision(self.repository.tip(self.app))
            else:
                self.set_changeset_revision(changeset_revision)
            self.shed_config_dict = {}
            self.metadata_dict = {}
        self.set_relative_install_dir(relative_install_dir)
        self.set_repository_files_dir()
        self.resetting_all_metadata_on_repository = False
        self.updating_installed_repository = False
        self.persist = False
        self.invalid_file_tups = []","1. Use `functools.lru_cache` to cache the results of expensive functions.
2. Use `type()` to check if a variable is of a certain type.
3. Use `assert()` to verify that a condition is true."
"    def __initiate_and_install_repositories(self, tool_shed_url, repository_revision_dict, repo_info_dicts, install_options):
        try:
            has_repository_dependencies = repository_revision_dict['has_repository_dependencies']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'has_repository_dependencies'."")
        try:
            includes_tools = repository_revision_dict['includes_tools']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'includes_tools'."")
        try:
            includes_tool_dependencies = repository_revision_dict['includes_tool_dependencies']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'includes_tool_dependencies'."")
        try:
            includes_tools_for_display_in_tool_panel = repository_revision_dict['includes_tools_for_display_in_tool_panel']
        except KeyError:
            raise exceptions.InternalServerError(""Tool shed response missing required parameter 'includes_tools_for_display_in_tool_panel'."")
        # Get the information about the Galaxy components (e.g., tool panel section, tool config file, etc) that will contain the repository information.
        install_repository_dependencies = install_options.get('install_repository_dependencies', False)
        install_resolver_dependencies = install_options.get('install_resolver_dependencies', False)
        install_tool_dependencies = install_options.get('install_tool_dependencies', False)
        if install_tool_dependencies:
            self.__assert_can_install_dependencies()
        new_tool_panel_section_label = install_options.get('new_tool_panel_section_label', '')
        tool_panel_section_mapping = install_options.get('tool_panel_section_mapping', {})
        shed_tool_conf = install_options.get('shed_tool_conf', None)
        if shed_tool_conf:
            # Get the tool_path setting.
            shed_config_dict = self.tpm.get_shed_tool_conf_dict(shed_tool_conf)
        else:
            # Don't use migrated_tools_conf.xml and prefer shed_tool_config_file.
            try:
                for shed_config_dict in self.app.toolbox.dynamic_confs(include_migrated_tool_conf=False):
                    if shed_config_dict.get('config_filename') == self.app.config.shed_tool_config_file:
                        break
                else:
                    shed_config_dict = self.app.toolbox.dynamic_confs(include_migrated_tool_conf=False)[0]
            except IndexError:
                raise exceptions.RequestParameterMissingException(""Missing required parameter 'shed_tool_conf'."")
        shed_tool_conf = shed_config_dict['config_filename']
        tool_path = shed_config_dict['tool_path']
        tool_panel_section_id = self.app.toolbox.find_section_id(install_options.get('tool_panel_section_id', ''))
        # Build the dictionary of information necessary for creating tool_shed_repository database records
        # for each repository being installed.
        installation_dict = dict(install_repository_dependencies=install_repository_dependencies,
                                 new_tool_panel_section_label=new_tool_panel_section_label,
                                 tool_panel_section_mapping=tool_panel_section_mapping,
                                 no_changes_checked=False,
                                 repo_info_dicts=repo_info_dicts,
                                 tool_panel_section_id=tool_panel_section_id,
                                 tool_path=tool_path,
                                 tool_shed_url=tool_shed_url)
        # Create the tool_shed_repository database records and gather additional information for repository installation.
        created_or_updated_tool_shed_repositories, tool_panel_section_keys, repo_info_dicts, filtered_repo_info_dicts = \\
            self.handle_tool_shed_repositories(installation_dict)
        if created_or_updated_tool_shed_repositories:
            # Build the dictionary of information necessary for installing the repositories.
            installation_dict = dict(created_or_updated_tool_shed_repositories=created_or_updated_tool_shed_repositories,
                                     filtered_repo_info_dicts=filtered_repo_info_dicts,
                                     has_repository_dependencies=has_repository_dependencies,
                                     includes_tool_dependencies=includes_tool_dependencies,
                                     includes_tools=includes_tools,
                                     includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
                                     install_repository_dependencies=install_repository_dependencies,
                                     install_resolver_dependencies=install_resolver_dependencies,
                                     install_tool_dependencies=install_tool_dependencies,
                                     message='',
                                     new_tool_panel_section_label=new_tool_panel_section_label,
                                     shed_tool_conf=shed_tool_conf,
                                     status='done',
                                     tool_panel_section_id=tool_panel_section_id,
                                     tool_panel_section_keys=tool_panel_section_keys,
                                     tool_panel_section_mapping=tool_panel_section_mapping,
                                     tool_path=tool_path,
                                     tool_shed_url=tool_shed_url)
            # Prepare the repositories for installation.  Even though this
            # method receives a single combination of tool_shed_url, name,
            # owner and changeset_revision, there may be multiple repositories
            # for installation at this point because repository dependencies
            # may have added additional repositories for installation along
            # with the single specified repository.
            encoded_kwd, query, tool_shed_repositories, encoded_repository_ids = \\
                self.initiate_repository_installation(installation_dict)
            # Some repositories may have repository dependencies that are
            # required to be installed before the dependent repository, so
            # we'll order the list of tsr_ids to ensure all repositories
            # install in the required order.
            tsr_ids = [self.app.security.encode_id(tool_shed_repository.id) for tool_shed_repository in tool_shed_repositories]

            decoded_kwd = dict(
                shed_tool_conf=shed_tool_conf,
                tool_path=tool_path,
                tool_panel_section_keys=tool_panel_section_keys,
                repo_info_dicts=filtered_repo_info_dicts,
                install_resolver_dependencies=install_resolver_dependencies,
                install_tool_dependencies=install_tool_dependencies,
                tool_panel_section_mapping=tool_panel_section_mapping,
            )
            return self.install_repositories(tsr_ids, decoded_kwd, reinstalling=False, install_options=install_options)","1. Use `self.app.security.encode_id()` to encode all IDs.
2. Use `self.app.toolbox.find_section_id()` to get the section ID.
3. Use `self.app.security.check_user_can_install_dependencies()` to check if the user can install dependencies."
"    def process_bind_param(self, value, dialect):
        """"""Automatically truncate string values""""""
        if self.impl.length and value is not None:
            value = value[0:self.impl.length]
        return value","1. Use prepared statements instead of bind parameters to prevent SQL injection attacks.
2. Sanitize user input before using it in SQL queries.
3. Use a database firewall to block malicious traffic."
"    def _upload_dataset(self, trans, library_id, folder_id, replace_dataset=None, **kwd):
        # Set up the traditional tool state/params
        cntrller = 'api'
        tool_id = 'upload1'
        message = None
        tool = trans.app.toolbox.get_tool(tool_id)
        state = tool.new_state(trans)
        populate_state(trans, tool.inputs, kwd, state.inputs)
        tool_params = state.inputs
        dataset_upload_inputs = []
        for input_name, input in tool.inputs.items():
            if input.type == ""upload_dataset"":
                dataset_upload_inputs.append(input)
        # Library-specific params
        server_dir = kwd.get('server_dir', '')
        upload_option = kwd.get('upload_option', 'upload_file')
        response_code = 200
        if upload_option == 'upload_directory':
            full_dir, import_dir_desc = validate_server_directory_upload(trans, server_dir)
            message = 'Select a directory'
        elif upload_option == 'upload_paths':
            # Library API already checked this - following check isn't actually needed.
            validate_path_upload(trans)
        # Some error handling should be added to this method.
        try:
            # FIXME: instead of passing params here ( which have been processed by util.Params(), the original kwd
            # should be passed so that complex objects that may have been included in the initial request remain.
            library_bunch = upload_common.handle_library_params(trans, kwd, folder_id, replace_dataset)
        except Exception:
            response_code = 500
            message = ""Unable to parse upload parameters, please report this error.""
        # Proceed with (mostly) regular upload processing if we're still errorless
        if response_code == 200:
            if upload_option == 'upload_file':
                tool_params = upload_common.persist_uploads(tool_params, trans)
                uploaded_datasets = upload_common.get_uploaded_datasets(trans, cntrller, tool_params, dataset_upload_inputs, library_bunch=library_bunch)
            elif upload_option == 'upload_directory':
                uploaded_datasets, response_code, message = self._get_server_dir_uploaded_datasets(trans, kwd, full_dir, import_dir_desc, library_bunch, response_code, message)
            elif upload_option == 'upload_paths':
                uploaded_datasets, response_code, message = self._get_path_paste_uploaded_datasets(trans, kwd, library_bunch, response_code, message)
            if upload_option == 'upload_file' and not uploaded_datasets:
                response_code = 400
                message = 'Select a file, enter a URL or enter text'
        if response_code != 200:
            return (response_code, message)
        json_file_path = upload_common.create_paramfile(trans, uploaded_datasets)
        data_list = [ud.data for ud in uploaded_datasets]
        job_params = {}
        job_params['link_data_only'] = json.dumps(kwd.get('link_data_only', 'copy_files'))
        job_params['uuid'] = json.dumps(kwd.get('uuid', None))
        job, output = upload_common.create_job(trans, tool_params, tool, json_file_path, data_list, folder=library_bunch.folder, job_params=job_params)
        trans.sa_session.add(job)
        trans.sa_session.flush()
        return output","1. Use util.Params() to process the parameters instead of passing them directly. This will help to catch errors earlier.
2. Validate the path upload parameters more strictly.
3. Handle errors more gracefully. For example, return a more specific error message if the upload option is not supported."
"def validate_and_normalize_targets(trans, payload):
    """"""Validate and normalize all src references in fetch targets.

    - Normalize ftp_import and server_dir src entries into simple path entries
      with the relevant paths resolved and permissions / configuration checked.
    - Check for file:// URLs in items src of ""url"" and convert them into path
      src items - after verifying path pastes are allowed and user is admin.
    - Check for valid URLs to be fetched for http and https entries.
    - Based on Galaxy configuration and upload types set purge_source and in_place
      as needed for each upload.
    """"""
    targets = payload.get(""targets"", [])

    for target in targets:
        destination = get_required_item(target, ""destination"", ""Each target must specify a 'destination'"")
        destination_type = get_required_item(destination, ""type"", ""Each target destination must specify a 'type'"")
        if ""object_id"" in destination:
            raise RequestParameterInvalidException(""object_id not allowed to appear in the request."")

        if destination_type not in VALID_DESTINATION_TYPES:
            template = ""Invalid target destination type [%s] encountered, must be one of %s""
            msg = template % (destination_type, VALID_DESTINATION_TYPES)
            raise RequestParameterInvalidException(msg)
        if destination_type == ""library"":
            library_name = get_required_item(destination, ""name"", ""Must specify a library name"")
            description = destination.get(""description"", """")
            synopsis = destination.get(""synopsis"", """")
            library = trans.app.library_manager.create(
                trans, library_name, description=description, synopsis=synopsis
            )
            destination[""type""] = ""library_folder""
            for key in [""name"", ""description"", ""synopsis""]:
                if key in destination:
                    del destination[key]
            destination[""library_folder_id""] = trans.app.security.encode_id(library.root_folder.id)

    # Unlike upload.py we don't transmit or use run_as_real_user in the job - we just make sure
    # in_place and purge_source are set on the individual upload fetch sources as needed based
    # on this.
    run_as_real_user = trans.app.config.external_chown_script is not None  # See comment in upload.py
    purge_ftp_source = getattr(trans.app.config, 'ftp_upload_purge', True) and not run_as_real_user

    payload[""check_content""] = trans.app.config.check_upload_content

    def check_src(item):
        if ""object_id"" in item:
            raise RequestParameterInvalidException(""object_id not allowed to appear in the request."")

        # Normalize file:// URLs into paths.
        if item[""src""] == ""url"" and item[""url""].startswith(""file://""):
            item[""src""] = ""path""
            item[""path""] = item[""url""][len(""file://""):]
            del item[""path""]

        if ""in_place"" in item:
            raise RequestParameterInvalidException(""in_place cannot be set in the upload request"")

        src = item[""src""]

        # Check link_data_only can only be set for certain src types and certain elements_from types.
        _handle_invalid_link_data_only_elements_type(item)
        if src not in [""path"", ""server_dir""]:
            _handle_invalid_link_data_only_type(item)
        elements_from = item.get(""elements_from"", None)
        if elements_from and elements_from not in ELEMENTS_FROM_TYPE:
            raise RequestParameterInvalidException(""Invalid elements_from/items_from found in request"")

        if src == ""path"" or (src == ""url"" and item[""url""].startswith(""file:"")):
            # Validate is admin, leave alone.
            validate_path_upload(trans)
        elif src == ""server_dir"":
            # Validate and replace with path definition.
            server_dir = item[""server_dir""]
            full_path, _ = validate_server_directory_upload(trans, server_dir)
            item[""src""] = ""path""
            item[""path""] = full_path
        elif src == ""ftp_import"":
            ftp_path = item[""ftp_path""]
            full_path = None

            # It'd be nice if this can be de-duplicated with what is in parameters/grouping.py.
            user_ftp_dir = trans.user_ftp_dir
            is_directory = False

            assert not os.path.islink(user_ftp_dir), ""User FTP directory cannot be a symbolic link""
            for (dirpath, dirnames, filenames) in os.walk(user_ftp_dir):
                for filename in filenames:
                    if ftp_path == filename:
                        path = relpath(os.path.join(dirpath, filename), user_ftp_dir)
                        if not os.path.islink(os.path.join(dirpath, filename)):
                            full_path = os.path.abspath(os.path.join(user_ftp_dir, path))
                            break

                for dirname in dirnames:
                    if ftp_path == dirname:
                        path = relpath(os.path.join(dirpath, dirname), user_ftp_dir)
                        if not os.path.islink(os.path.join(dirpath, dirname)):
                            full_path = os.path.abspath(os.path.join(user_ftp_dir, path))
                            is_directory = True
                            break

            if is_directory:
                # If the target is a directory - make sure no files under it are symbolic links
                for (dirpath, dirnames, filenames) in os.walk(full_path):
                    for filename in filenames:
                        if ftp_path == filename:
                            path = relpath(os.path.join(dirpath, filename), full_path)
                            if not os.path.islink(os.path.join(dirpath, filename)):
                                full_path = False
                                break

                    for dirname in dirnames:
                        if ftp_path == dirname:
                            path = relpath(os.path.join(dirpath, filename), full_path)
                            if not os.path.islink(os.path.join(dirpath, filename)):
                                full_path = False
                                break

            if not full_path:
                raise RequestParameterInvalidException(""Failed to find referenced ftp_path or symbolic link was enountered"")

            item[""src""] = ""path""
            item[""path""] = full_path
            item[""purge_source""] = purge_ftp_source
        elif src == ""url"":
            url = item[""url""]
            looks_like_url = False
            for url_prefix in [""http://"", ""https://"", ""ftp://"", ""ftps://""]:
                if url.startswith(url_prefix):
                    looks_like_url = True
                    break

            if not looks_like_url:
                raise RequestParameterInvalidException(""Invalid URL [%s] found in src definition."" % url)

            validate_url(url, trans.app.config.fetch_url_whitelist_ips)
            item[""in_place""] = run_as_real_user
        elif src == ""files"":
            item[""in_place""] = run_as_real_user

        # Small disagreement with traditional uploads - we purge less by default since whether purging
        # happens varies based on upload options in non-obvious ways.
        # https://github.com/galaxyproject/galaxy/issues/5361
        if ""purge_source"" not in item:
            item[""purge_source""] = False

    replace_request_syntax_sugar(targets)
    _for_each_src(check_src, targets)","1. Validate all src references in fetch targets.
2. Check for valid URLs to be fetched.
3. Set purge_source and in_place as needed for each upload."
"    def check_src(item):
        if ""object_id"" in item:
            raise RequestParameterInvalidException(""object_id not allowed to appear in the request."")

        # Normalize file:// URLs into paths.
        if item[""src""] == ""url"" and item[""url""].startswith(""file://""):
            item[""src""] = ""path""
            item[""path""] = item[""url""][len(""file://""):]
            del item[""path""]

        if ""in_place"" in item:
            raise RequestParameterInvalidException(""in_place cannot be set in the upload request"")

        src = item[""src""]

        # Check link_data_only can only be set for certain src types and certain elements_from types.
        _handle_invalid_link_data_only_elements_type(item)
        if src not in [""path"", ""server_dir""]:
            _handle_invalid_link_data_only_type(item)
        elements_from = item.get(""elements_from"", None)
        if elements_from and elements_from not in ELEMENTS_FROM_TYPE:
            raise RequestParameterInvalidException(""Invalid elements_from/items_from found in request"")

        if src == ""path"" or (src == ""url"" and item[""url""].startswith(""file:"")):
            # Validate is admin, leave alone.
            validate_path_upload(trans)
        elif src == ""server_dir"":
            # Validate and replace with path definition.
            server_dir = item[""server_dir""]
            full_path, _ = validate_server_directory_upload(trans, server_dir)
            item[""src""] = ""path""
            item[""path""] = full_path
        elif src == ""ftp_import"":
            ftp_path = item[""ftp_path""]
            full_path = None

            # It'd be nice if this can be de-duplicated with what is in parameters/grouping.py.
            user_ftp_dir = trans.user_ftp_dir
            is_directory = False

            assert not os.path.islink(user_ftp_dir), ""User FTP directory cannot be a symbolic link""
            for (dirpath, dirnames, filenames) in os.walk(user_ftp_dir):
                for filename in filenames:
                    if ftp_path == filename:
                        path = relpath(os.path.join(dirpath, filename), user_ftp_dir)
                        if not os.path.islink(os.path.join(dirpath, filename)):
                            full_path = os.path.abspath(os.path.join(user_ftp_dir, path))
                            break

                for dirname in dirnames:
                    if ftp_path == dirname:
                        path = relpath(os.path.join(dirpath, dirname), user_ftp_dir)
                        if not os.path.islink(os.path.join(dirpath, dirname)):
                            full_path = os.path.abspath(os.path.join(user_ftp_dir, path))
                            is_directory = True
                            break

            if is_directory:
                # If the target is a directory - make sure no files under it are symbolic links
                for (dirpath, dirnames, filenames) in os.walk(full_path):
                    for filename in filenames:
                        if ftp_path == filename:
                            path = relpath(os.path.join(dirpath, filename), full_path)
                            if not os.path.islink(os.path.join(dirpath, filename)):
                                full_path = False
                                break

                    for dirname in dirnames:
                        if ftp_path == dirname:
                            path = relpath(os.path.join(dirpath, filename), full_path)
                            if not os.path.islink(os.path.join(dirpath, filename)):
                                full_path = False
                                break

            if not full_path:
                raise RequestParameterInvalidException(""Failed to find referenced ftp_path or symbolic link was enountered"")

            item[""src""] = ""path""
            item[""path""] = full_path
            item[""purge_source""] = purge_ftp_source
        elif src == ""url"":
            url = item[""url""]
            looks_like_url = False
            for url_prefix in [""http://"", ""https://"", ""ftp://"", ""ftps://""]:
                if url.startswith(url_prefix):
                    looks_like_url = True
                    break

            if not looks_like_url:
                raise RequestParameterInvalidException(""Invalid URL [%s] found in src definition."" % url)

            validate_url(url, trans.app.config.fetch_url_whitelist_ips)
            item[""in_place""] = run_as_real_user
        elif src == ""files"":
            item[""in_place""] = run_as_real_user

        # Small disagreement with traditional uploads - we purge less by default since whether purging
        # happens varies based on upload options in non-obvious ways.
        # https://github.com/galaxyproject/galaxy/issues/5361
        if ""purge_source"" not in item:
            item[""purge_source""] = False","1. Use `os.path.islink` to check if the file is a symbolic link and raise an exception if it is.
2. Use `validate_url` to check if the URL is valid and raise an exception if it is not.
3. Use `validate_path_upload` to check if the user is an administrator and raise an exception if they are not."
"def parse_xml(file_name, check_exists=True):
    """"""Returns a parsed xml tree with comments intact.""""""
    error_message = ''
    if check_exists and not os.path.exists(file_name):
        return None, ""File does not exist %s"" % str(file_name)

    with open(file_name, 'r') as fobj:
        try:
            tree = XmlET.parse(fobj, parser=XmlET.XMLParser(target=Py27CommentedTreeBuilder()))
        except Exception as e:
            error_message = ""Exception attempting to parse %s: %s"" % (str(file_name), str(e))
            log.exception(error_message)
            return None, error_message
    return tree, error_message","1. **Use a secure parser**. The current parser does not properly handle XML entity expansion, which can lead to XXE attacks. Use a parser that is specifically designed to be secure, such as [lxml](https://lxml.de/).
2. **Validate the XML input**. Make sure that the XML input is well-formed and valid before parsing it. This will help to prevent attacks such as injection attacks.
3. **Sanitize the XML output**. Be careful about what data you output from the XML parser. Make sure that any user-supplied data is properly sanitized before it is output."
"    def comment(self, data):
        self.start(XmlET.Comment, {})
        self.data(data)
        self.end(XmlET.Comment)","1. Sanitize user input to prevent XSS attacks.
2. Escape XML entities to prevent XML injection attacks.
3. Use a secure XML parser to prevent denial of service attacks."
"    def install_data_managers(self, shed_data_manager_conf_filename, metadata_dict, shed_config_dict,
                              relative_install_dir, repository, repository_tools_tups):
        rval = []
        if 'data_manager' in metadata_dict:
            tpm = tool_panel_manager.ToolPanelManager(self.app)
            repository_tools_by_guid = {}
            for tool_tup in repository_tools_tups:
                repository_tools_by_guid[tool_tup[1]] = dict(tool_config_filename=tool_tup[0], tool=tool_tup[2])
            # Load existing data managers.
            try:
                tree, error_message = xml_util.parse_xml(shed_data_manager_conf_filename, check_exists=False)
            except (OSError, IOError) as exc:
                if exc.errno == errno.ENOENT:
                    with open(shed_data_manager_conf_filename, 'w') as fh:
                        fh.write(SHED_DATA_MANAGER_CONF_XML)
                    tree, error_message = xml_util.parse_xml(shed_data_manager_conf_filename)
                else:
                    raise
            if tree is None:
                return rval
            config_elems = [elem for elem in tree.getroot()]
            repo_data_manager_conf_filename = metadata_dict['data_manager'].get('config_filename', None)
            if repo_data_manager_conf_filename is None:
                log.debug(""No data_manager_conf.xml file has been defined."")
                return rval
            data_manager_config_has_changes = False
            relative_repo_data_manager_dir = os.path.join(shed_config_dict.get('tool_path', ''), relative_install_dir)
            repo_data_manager_conf_filename = os.path.join(relative_repo_data_manager_dir, repo_data_manager_conf_filename)
            tree, error_message = xml_util.parse_xml(repo_data_manager_conf_filename)
            if tree is None:
                return rval
            root = tree.getroot()
            for elem in root:
                if elem.tag == 'data_manager':
                    data_manager_id = elem.get('id', None)
                    if data_manager_id is None:
                        log.error(""A data manager was defined that does not have an id and will not be installed:\\n%s"" %
                                  xml_to_string(elem))
                        continue
                    data_manager_dict = metadata_dict['data_manager'].get('data_managers', {}).get(data_manager_id, None)
                    if data_manager_dict is None:
                        log.error(""Data manager metadata is not defined properly for '%s'."" % (data_manager_id))
                        continue
                    guid = data_manager_dict.get('guid', None)
                    if guid is None:
                        log.error(""Data manager guid '%s' is not set in metadata for '%s'."" % (guid, data_manager_id))
                        continue
                    elem.set('guid', guid)
                    tool_guid = data_manager_dict.get('tool_guid', None)
                    if tool_guid is None:
                        log.error(""Data manager tool guid '%s' is not set in metadata for '%s'."" % (tool_guid, data_manager_id))
                        continue
                    tool_dict = repository_tools_by_guid.get(tool_guid, None)
                    if tool_dict is None:
                        log.error(""Data manager tool guid '%s' could not be found for '%s'. Perhaps the tool is invalid?"" %
                                  (tool_guid, data_manager_id))
                        continue
                    tool = tool_dict.get('tool', None)
                    if tool is None:
                        log.error(""Data manager tool with guid '%s' could not be found for '%s'. Perhaps the tool is invalid?"" %
                                  (tool_guid, data_manager_id))
                        continue
                    tool_config_filename = tool_dict.get('tool_config_filename', None)
                    if tool_config_filename is None:
                        log.error(""Data manager metadata is missing 'tool_config_file' for '%s'."" % (data_manager_id))
                        continue
                    elem.set('shed_conf_file', shed_config_dict['config_filename'])
                    if elem.get('tool_file', None) is not None:
                        del elem.attrib['tool_file']  # remove old tool_file info
                    tool_elem = tpm.generate_tool_elem(repository.tool_shed,
                                                       repository.name,
                                                       repository.installed_changeset_revision,
                                                       repository.owner,
                                                       tool_config_filename,
                                                       tool,
                                                       None)
                    elem.insert(0, tool_elem)
                    data_manager = \\
                        self.app.data_managers.load_manager_from_elem(elem,
                                                                      tool_path=shed_config_dict.get('tool_path', ''))
                    if data_manager:
                        rval.append(data_manager)
                else:
                    log.warning(""Encountered unexpected element '%s':\\n%s"" % (elem.tag, xml_to_string(elem)))
                config_elems.append(elem)
                data_manager_config_has_changes = True
            # Persist the altered shed_data_manager_config file.
            if data_manager_config_has_changes:
                reload_count = self.app.data_managers._reload_count
                self.data_manager_config_elems_to_xml_file(config_elems, shed_data_manager_conf_filename)
                while self.app.data_managers._reload_count <= reload_count:
                    time.sleep(0.1)  # Wait for shed_data_manager watcher thread to pick up changes
        return rval","1. Use `xml_to_string` instead of `str` to avoid XML injection.
2. Use `xml_util.parse_xml` with `check_exists=False` to avoid raising an exception when the file does not exist.
3. Use `data_manager_config_elems_to_xml_file` to persist the changes to the `shed_data_manager_config` file."
"def _xml_replace(query, targets, parent_map):
    # parent_el = query.find('..') ## Something like this would be better with newer xml library
    parent_el = parent_map[query]
    matching_index = -1
    # for index, el in enumerate(parent_el.iter('.')):  ## Something like this for newer implementation
    for index, el in enumerate(list(parent_el)):
        if el == query:
            matching_index = index
            break
    assert matching_index >= 0
    current_index = matching_index
    for target in targets:
        current_index += 1
        parent_el.insert(current_index, target)
    parent_el.remove(query)","1. Use a newer XML library that implements `find_parent()` instead of `find('..')`.
2. Use `enumerate()` instead of `iter()` to iterate over the list of elements.
3. Use `assert` to verify that the matching index is greater than or equal to 0."
"def _expand_yield_statements(macro_def, expand_el):
    yield_els = [yield_el for macro_def_el in macro_def for yield_el in macro_def_el.findall('.//yield')]

    expand_el_children = list(expand_el)
    macro_def_parent_map = \\
        dict((c, p) for macro_def_el in macro_def for p in macro_def_el.iter() for c in p)

    for yield_el in yield_els:
        _xml_replace(yield_el, expand_el_children, macro_def_parent_map)

    # Replace yields at the top level of a macro, seems hacky approach
    replace_yield = True
    while replace_yield:
        for i, macro_def_el in enumerate(macro_def):
            if macro_def_el.tag == ""yield"":
                for target in expand_el_children:
                    i += 1
                    macro_def.insert(i, target)
                macro_def.remove(macro_def_el)
                continue

        replace_yield = False","1. Use `xml_escape` to escape XML special characters in user input.
2. Sanitize user input to prevent XSS attacks.
3. Validate user input to prevent injection attacks."
"    def __determine_job_destination(self, params, raw_job_destination=None):
        if raw_job_destination is None:
            raw_job_destination = self.job_wrapper.tool.get_job_destination(params)
        if raw_job_destination.runner == DYNAMIC_RUNNER_NAME:
            job_destination = self.__handle_dynamic_job_destination(raw_job_destination)
            log.debug(""(%s) Mapped job to destination id: %s"", self.job_wrapper.job_id, job_destination.id)
            # Recursively handle chained dynamic destinations
            if job_destination.runner == DYNAMIC_RUNNER_NAME:
                return self.__determine_job_destination(params, raw_job_destination=job_destination)
        else:
            job_destination = raw_job_destination
            log.debug(""(%s) Mapped job to destination id: %s"", self.job_wrapper.job_id, job_destination.id)
        return job_destination","1. Use `params` instead of `raw_job_destination` to get the job destination. This will prevent the function from being tricked into using a malicious job destination.
2. Validate the job destination before using it. This will help to ensure that the job is sent to the correct destination.
3. Use a secure logging mechanism to log the job destination. This will help to track the job and prevent unauthorized access."
"    def __init__(self, **kwd):
        Binary.__init__(self, **kwd)

        """"""The header file. Provides information about dimensions, identification, and processing history.""""""
        self.add_composite_file(
            'hdr',
            description='The Analyze75 header file.',
            is_binary=False)

        """"""The image file.  Image data, whose data type and ordering are described by the header file.""""""
        self.add_composite_file(
            'img',
            description='The Analyze75 image file.',
            is_binary=True)

        """"""The optional t2m file.""""""
        self.add_composite_file(
            't2m',
            description='The Analyze75 t2m file.',
            optional='True', is_binary=True)","1. Use `getattr` instead of `hasattr` to check for the existence of a property. This will prevent a `KeyError` if the property does not exist.
2. Use `__setattr__` to prevent setting properties that should not be changed. This will help to protect against unauthorized changes to the object's state.
3. Use `__delattr__` to prevent deleting properties that should not be deleted. This will help to protect against unauthorized removal of properties from the object."
"def add_composite_file(dataset, registry, output_path, files_path):
    datatype = None

    # Find data type
    if dataset.file_type is not None:
        try:
            datatype = registry.get_datatype_by_extension(dataset.file_type)
        except Exception:
            print(""Unable to instantiate the datatype object for the file type '%s'"" % dataset.file_type)

    def to_path(path_or_url):
        is_url = path_or_url.find('://') != -1  # todo fixme
        if is_url:
            try:
                temp_name = sniff.stream_to_file(urlopen(path_or_url), prefix='url_paste')
            except Exception as e:
                raise UploadProblemException('Unable to fetch %s\\n%s' % (path_or_url, str(e)))

            return temp_name, is_url

        return path_or_url, is_url

    def make_files_path():
        safe_makedirs(files_path)

    def stage_file(name, composite_file_path, is_binary=False):
        dp = composite_file_path['path']
        path, is_url = to_path(dp)
        if is_url:
            dataset.path = path
            dp = path

        auto_decompress = composite_file_path.get('auto_decompress', True)
        if auto_decompress and not datatype.composite_type and CompressedFile.can_decompress(dp):
            # It isn't an explictly composite datatype, so these are just extra files to attach
            # as composite data. It'd be better if Galaxy was communicating this to the tool
            # a little more explicitly so we didn't need to dispatch on the datatype and so we
            # could attach arbitrary extra composite data to an existing composite datatype if
            # if need be? Perhaps that would be a mistake though.
            CompressedFile(dp).extract(files_path)
        else:
            if not is_binary:
                tmpdir = output_adjacent_tmpdir(output_path)
                tmp_prefix = 'data_id_%s_convert_' % dataset.dataset_id
                if composite_file_path.get('space_to_tab'):
                    sniff.convert_newlines_sep2tabs(dp, tmp_dir=tmpdir, tmp_prefix=tmp_prefix)
                else:
                    sniff.convert_newlines(dp, tmp_dir=tmpdir, tmp_prefix=tmp_prefix)

            file_output_path = os.path.join(files_path, name)
            shutil.move(dp, file_output_path)

            # groom the dataset file content if required by the corresponding datatype definition
            if datatype.dataset_content_needs_grooming(file_output_path):
                datatype.groom_dataset_content(file_output_path)

    # Do we have pre-defined composite files from the datatype definition.
    if dataset.composite_files:
        make_files_path()
        for name, value in dataset.composite_files.items():
            value = bunch.Bunch(**value)
            if value.name not in dataset.composite_file_paths:
                raise UploadProblemException(""Failed to find file_path %s in %s"" % (value.name, dataset.composite_file_paths))
            if dataset.composite_file_paths[value.name] is None and not value.optional:
                raise UploadProblemException('A required composite data file was not provided (%s)' % name)
            elif dataset.composite_file_paths[value.name] is not None:
                composite_file_path = dataset.composite_file_paths[value.name]
                stage_file(name, composite_file_path, value.is_binary)

    # Do we have ad-hoc user supplied composite files.
    elif dataset.composite_file_paths:
        make_files_path()
        for key, composite_file in dataset.composite_file_paths.items():
            stage_file(key, composite_file)  # TODO: replace these defaults

    # Move the dataset to its ""real"" path
    primary_file_path, _ = to_path(dataset.primary_file)
    shutil.move(primary_file_path, output_path)

    # Write the job info
    return dict(type='dataset',
                dataset_id=dataset.dataset_id,
                stdout='uploaded %s file' % dataset.file_type)","1. Use `urlopen` with a `try/except` block to catch errors and prevent the tool from crashing.
2. Use `safe_makedirs` to create the output directory, which will catch errors if the directory already exists.
3. Use `shutil.move` to move the dataset to its final location, which will catch errors if the destination file already exists."
"    def tool_shed_repositories(self):
        try:
            repositories = self.cache.repositories
        except AttributeError:
            self.rebuild()
            repositories = self.cache.repositories
        if repositories and not repositories[0]._sa_instance_state._attached:
            self.rebuild()
            repositories = self.cache.repositories
        return repositories","1. Use `@property` decorator to avoid exposing the underlying attribute.
2. Use `functools.lru_cache` to cache the result of the function call.
3. Use `sa_session.refresh()` to refresh the instance state of the object."
"    def get_file_name(self):
        if not self.external_filename:
            assert self.object_store is not None, ""Object Store has not been initialized for dataset %s"" % self.id
            filename = self.object_store.get_filename(self)
            return filename
        else:
            filename = self.external_filename
        # Make filename absolute
        return os.path.abspath(filename)","1. **Use `assert` statements to check for invalid inputs.** This will help to prevent errors and protect against malicious attacks.
2. **Use `os.path.join` to join paths instead of concatenating strings.** This will help to prevent directory traversal attacks.
3. **Use `os.makedirs` to create directories instead of calling `os.mkdir` directly.** This will help to prevent race conditions."
"    def get_extra_files_path(self):
        # Unlike get_file_name - external_extra_files_path is not backed by an
        # actual database column so if SA instantiates this object - the
        # attribute won't exist yet.
        if not getattr(self, ""external_extra_files_path"", None):
            return self.object_store.get_filename(self, dir_only=True, extra_dir=self._extra_files_rel_path)
        else:
            return os.path.abspath(self.external_extra_files_path)","1. Use `getattr` to check if the attribute exists before accessing it.
2. Use `os.path.abspath` to return an absolute path.
3. Use `self.object_store.get_filename` to get the file name."
"    def full_delete(self):
        """"""Remove the file and extra files, marks deleted and purged""""""
        # os.unlink( self.file_name )
        self.object_store.delete(self)
        if self.object_store.exists(self, extra_dir=self._extra_files_rel_path, dir_only=True):
            self.object_store.delete(self, entire_dir=True, extra_dir=self._extra_files_rel_path, dir_only=True)
        # if os.path.exists( self.extra_files_path ):
        #     shutil.rmtree( self.extra_files_path )
        # TODO: purge metadata files
        self.deleted = True
        self.purged = True","1. Use `os.chmod` to set the permissions of the file to `0o600`.
2. Use `shutil.chown` to change the owner of the file to the user who owns the bucket.
3. Use `os.umask` to set the umask for the process to `0o077`."
"    def get_filename(self, obj, **kwargs):
        """"""
        Override `ObjectStore`'s stub.

        If `object_store_check_old_style` is set to `True` in config then the
        root path is checked first.
        """"""
        if self.check_old_style:
            path = self._construct_path(obj, old_style=True, **kwargs)
            # For backward compatibility, check root path first; otherwise,
            # construct and return hashed path
            if os.path.exists(path):
                return path
        return self._construct_path(obj, **kwargs)","1. Use `os.path.join()` to construct paths instead of concatenating strings.
2. Check the return value of `os.path.exists()` to avoid returning a path that does not exist.
3. Use `os.umask()` to set the file mode mask for newly created files."
"def collect_primary_datasets(job_context, output, input_ext):
    tool = job_context.tool
    job_working_directory = job_context.job_working_directory
    sa_session = job_context.sa_session

    # Loop through output file names, looking for generated primary
    # datasets in form specified by discover dataset patterns or in tool provided metadata.
    primary_output_assigned = False
    new_outdata_name = None
    primary_datasets = {}
    for output_index, (name, outdata) in enumerate(output.items()):
        dataset_collectors = [DEFAULT_DATASET_COLLECTOR]
        if name in tool.outputs:
            dataset_collectors = [dataset_collector(description) for description in tool.outputs[name].dataset_collector_descriptions]
        filenames = odict.odict()
        for discovered_file in discover_files(name, job_context.tool_provided_metadata, dataset_collectors, job_working_directory, outdata):
            filenames[discovered_file.path] = discovered_file
        for filename_index, (filename, discovered_file) in enumerate(filenames.items()):
            extra_file_collector = discovered_file.collector
            fields_match = discovered_file.match
            if not fields_match:
                # Before I guess pop() would just have thrown an IndexError
                raise Exception(""Problem parsing metadata fields for file %s"" % filename)
            designation = fields_match.designation
            if filename_index == 0 and extra_file_collector.assign_primary_output and output_index == 0:
                new_outdata_name = fields_match.name or ""%s (%s)"" % (outdata.name, designation)
                # Move data from temp location to dataset location
                job_context.object_store.update_from_file(outdata.dataset, file_name=filename, create=True)
                primary_output_assigned = True
                continue
            if name not in primary_datasets:
                primary_datasets[name] = odict.odict()
            visible = fields_match.visible
            ext = fields_match.ext
            if ext == ""input"":
                ext = input_ext
            dbkey = fields_match.dbkey
            if dbkey == INPUT_DBKEY_TOKEN:
                dbkey = job_context.input_dbkey
            # Create new primary dataset
            new_primary_name = fields_match.name or ""%s (%s)"" % (outdata.name, designation)
            info = outdata.info

            # TODO: should be able to disambiguate files in different directories...
            new_primary_filename = os.path.split(filename)[-1]
            new_primary_datasets_attributes = job_context.tool_provided_metadata.get_new_dataset_meta_by_basename(name, new_primary_filename)

            primary_data = job_context.create_dataset(
                ext,
                designation,
                visible,
                dbkey,
                new_primary_name,
                filename,
                info=info,
                init_from=outdata,
                dataset_attributes=new_primary_datasets_attributes,
            )
            # Associate new dataset with job
            job_context.add_output_dataset_association('__new_primary_file_%s|%s__' % (name, designation), primary_data)

            if new_primary_datasets_attributes:
                extra_files_path = new_primary_datasets_attributes.get('extra_files', None)
                if extra_files_path:
                    extra_files_path_joined = os.path.join(job_working_directory, extra_files_path)
                    for root, dirs, files in os.walk(extra_files_path_joined):
                        extra_dir = os.path.join(primary_data.extra_files_path, root.replace(extra_files_path_joined, '', 1).lstrip(os.path.sep))
                        extra_dir = os.path.normpath(extra_dir)
                        for f in files:
                            job_context.object_store.update_from_file(
                                primary_data.dataset,
                                extra_dir=extra_dir,
                                alt_name=f,
                                file_name=os.path.join(root, f),
                                create=True,
                                preserve_symlinks=True
                            )
            job_context.add_datasets_to_history([primary_data], for_output_dataset=outdata)
            # Add dataset to return dict
            primary_datasets[name][designation] = primary_data
        if primary_output_assigned:
            outdata.name = new_outdata_name
            outdata.init_meta()
            outdata.set_meta()
            outdata.set_peek()
            sa_session.add(outdata)

    sa_session.flush()
    return primary_datasets","1. Use prepared statements instead of building queries manually to prevent SQL injection attacks.
2. Use the `os.path.join()` function to concatenate paths instead of concatenating them manually to prevent directory traversal attacks.
3. Use the `os.path.normpath()` function to normalize paths before using them to prevent path traversal attacks."
"    def _dataset_wrapper(self, dataset, dataset_paths, **kwargs):
        wrapper_kwds = kwargs.copy()
        if dataset:
            real_path = dataset.file_name
            if real_path in dataset_paths:
                wrapper_kwds[""dataset_path""] = dataset_paths[real_path]
        return DatasetFilenameWrapper(dataset, **wrapper_kwds)","1. Use `os.path.join()` to concatenate paths instead of string concatenation. This will prevent directory traversal attacks.
2. Use `os.path.isfile()` to check if a file exists before trying to open it. This will prevent file not found errors.
3. Use `os.access()` to check if a user has permission to access a file before trying to open it. This will prevent permission denied errors."
"def __main__():
    input_name = sys.argv[1]
    output_name = sys.argv[2]
    skipped_lines = 0
    first_skipped_line = 0
    out = open(output_name, 'w')
    out.write(""##gff-version 2\\n"")
    out.write(""##bed_to_gff_converter.py\\n\\n"")
    i = 0
    for i, line in enumerate(open(input_name)):
        complete_bed = False
        line = line.rstrip('\\r\\n')
        if line and not line.startswith('#') and not line.startswith('track') and not line.startswith('browser'):
            try:
                elems = line.split('\\t')
                if len(elems) == 12:
                    complete_bed = True
                chrom = elems[0]
                if complete_bed:
                    feature = ""mRNA""
                else:
                    try:
                        feature = elems[3]
                    except Exception:
                        feature = 'feature%d' % (i + 1)
                start = int(elems[1]) + 1
                end = int(elems[2])
                try:
                    score = elems[4]
                except Exception:
                    score = '0'
                try:
                    strand = elems[5]
                except Exception:
                    strand = '+'
                try:
                    group = elems[3]
                except Exception:
                    group = 'group%d' % (i + 1)
                if complete_bed:
                    out.write('%s\\tbed2gff\\t%s\\t%d\\t%d\\t%s\\t%s\\t.\\t%s %s;\\n' % (chrom, feature, start, end, score, strand, feature, group))
                else:
                    out.write('%s\\tbed2gff\\t%s\\t%d\\t%d\\t%s\\t%s\\t.\\t%s;\\n' % (chrom, feature, start, end, score, strand, group))
                if complete_bed:
                    # We have all the info necessary to annotate exons for genes and mRNAs
                    block_count = int(elems[9])
                    block_sizes = elems[10].split(',')
                    block_starts = elems[11].split(',')
                    for j in range(block_count):
                        exon_start = int(start) + int(block_starts[j])
                        exon_end = exon_start + int(block_sizes[j]) - 1
                        out.write('%s\\tbed2gff\\texon\\t%d\\t%d\\t%s\\t%s\\t.\\texon %s;\\n' % (chrom, exon_start, exon_end, score, strand, group))
            except Exception:
                skipped_lines += 1
                if not first_skipped_line:
                    first_skipped_line = i + 1
        else:
            skipped_lines += 1
            if not first_skipped_line:
                first_skipped_line = i + 1
    out.close()
    info_msg = ""%i lines converted to GFF version 2.  "" % (i + 1 - skipped_lines)
    if skipped_lines > 0:
        info_msg += ""Skipped %d blank/comment/invalid lines starting with line #%d."" % (skipped_lines, first_skipped_line)
    print(info_msg)","1. Use `sys.stdin.buffer.readline()` instead of `open()` to read input file, as it is more secure.
2. Use `try-except` block to handle exceptions, and log the errors.
3. Use `f.close()` to close the output file explicitly."
"    def register(self, trans, email=None, username=None, password=None, confirm=None, subscribe=False, **kwd):
        """"""
        Register a new user.
        """"""
        if not trans.app.config.allow_user_creation and not trans.user_is_admin:
            message = ""User registration is disabled.  Please contact your local Galaxy administrator for an account.""
            if trans.app.config.error_email_to is not None:
                message += "" Contact: %s"" % trans.app.config.error_email_to
            return None, message
        if not email or not username or not password or not confirm:
            return None, ""Please provide email, username and password.""
        message = ""\\n"".join([validate_email(trans, email),
                             validate_password(trans, password, confirm),
                             validate_publicname(trans, username)]).rstrip()
        if message:
            return None, message
        email = util.restore_text(email)
        username = util.restore_text(username)
        message, status = trans.app.auth_manager.check_registration_allowed(email, username, password)
        if message:
            return None, message
        if subscribe:
            message = self.send_subscription_email(email)
            if message:
                return None, message
        user = self.create(email=email, username=username, password=password)
        if self.app.config.user_activation_on:
            self.send_activation_email(trans, email, username)
        return user, None","1. Sanitize all user input, especially email addresses and passwords.
2. Use strong hashing algorithms for passwords.
3. Implement user account activation to prevent unauthorized access."
"    def __autoregistration(self, trans, login, password, status, kwd, no_password_check=False, cntrller=None):
        """"""
        Does the autoregistration if enabled. Returns a message
        """"""
        autoreg = trans.app.auth_manager.check_auto_registration(trans, login, password, no_password_check=no_password_check)
        user = None
        success = False
        if autoreg[""auto_reg""]:
            kwd[""email""] = autoreg[""email""]
            kwd[""username""] = autoreg[""username""]
            message = "" "".join([validate_email(trans, kwd[""email""], allow_empty=True),
                                validate_publicname(trans, kwd[""username""])]).rstrip()
            if not message:
                message, status, user, success = self.__register(trans, **kwd)
                if success:
                    # The handle_user_login() method has a call to the history_set_default_permissions() method
                    # (needed when logging in with a history), user needs to have default permissions set before logging in
                    if not trans.user_is_admin:
                        trans.handle_user_login(user)
                        trans.log_event(""User (auto) created a new account"")
                        trans.log_event(""User logged in"")
                    if ""attributes"" in autoreg and ""roles"" in autoreg[""attributes""]:
                        self.__handle_role_and_group_auto_creation(
                            trans, user, autoreg[""attributes""][""roles""],
                            auto_create_groups=autoreg[""auto_create_groups""],
                            auto_create_roles=autoreg[""auto_create_roles""],
                            auto_assign_roles_to_groups_only=autoreg[""auto_assign_roles_to_groups_only""])
                else:
                    message = ""Auto-registration failed, contact your local Galaxy administrator. %s"" % message
            else:
                message = ""Auto-registration failed, contact your local Galaxy administrator. %s"" % message
        else:
            message = ""No such user or invalid password.""
        return message, status, user, success","1. Use `validate_email` and `validate_publicname` to sanitize user input.
2. Check `user_is_admin` before logging in.
3. Use `handle_user_login` to set default permissions for the user."
"    def __validate_login(self, trans, payload={}, **kwd):
        '''Handle Galaxy Log in'''
        if not payload:
            payload = kwd
        message = trans.check_csrf_token(payload)
        if message:
            return self.message_exception(trans, message)
        login = payload.get(""login"")
        password = payload.get(""password"")
        redirect = payload.get(""redirect"")
        status = None
        if not login or not password:
            return self.message_exception(trans, ""Please specify a username and password."")
        user = trans.sa_session.query(trans.app.model.User).filter(or_(
            trans.app.model.User.table.c.email == login,
            trans.app.model.User.table.c.username == login
        )).first()
        log.debug(""trans.app.config.auth_config_file: %s"" % trans.app.config.auth_config_file)
        if user is None:
            message, status, user, success = self.__autoregistration(trans, login, password, status, kwd)
            if not success:
                return self.message_exception(trans, message)
        elif user.deleted:
            message = ""This account has been marked deleted, contact your local Galaxy administrator to restore the account.""
            if trans.app.config.error_email_to is not None:
                message += "" Contact: %s."" % trans.app.config.error_email_to
            return self.message_exception(trans, message, sanitize=False)
        elif user.external:
            message = ""This account was created for use with an external authentication method, contact your local Galaxy administrator to activate it.""
            if trans.app.config.error_email_to is not None:
                message += "" Contact: %s."" % trans.app.config.error_email_to
            return self.message_exception(trans, message, sanitize=False)
        elif not trans.app.auth_manager.check_password(user, password):
            return self.message_exception(trans, ""Invalid password."")
        elif trans.app.config.user_activation_on and not user.active:  # activation is ON and the user is INACTIVE
            if (trans.app.config.activation_grace_period != 0):  # grace period is ON
                if self.is_outside_grace_period(trans, user.create_time):  # User is outside the grace period. Login is disabled and he will have the activation email resent.
                    message, status = self.resend_activation_email(trans, user.email, user.username)
                    return self.message_exception(trans, message, sanitize=False)
                else:  # User is within the grace period, let him log in.
                    trans.handle_user_login(user)
                    trans.log_event(""User logged in"")
            else:  # Grace period is off. Login is disabled and user will have the activation email resent.
                message, status = self.resend_activation_email(trans, user.email, user.username)
                return self.message_exception(trans, message, sanitize=False)
        else:  # activation is OFF
            pw_expires = trans.app.config.password_expiration_period
            if pw_expires and user.last_password_change < datetime.today() - pw_expires:
                # Password is expired, we don't log them in.
                return {""message"": ""Your password has expired. Please reset or change it to access Galaxy."", ""status"": ""warning"", ""expired_user"": trans.security.encode_id(user.id)}
            trans.handle_user_login(user)
            trans.log_event(""User logged in"")
            if pw_expires and user.last_password_change < datetime.today() - timedelta(days=pw_expires.days / 10):
                # If password is about to expire, modify message to state that.
                expiredate = datetime.today() - user.last_password_change + pw_expires
                return {""message"": ""Your password will expire in %s day(s)."" % expiredate.days, ""status"": ""warning""}
        return {""message"": ""Success."", ""redirect"": self.__get_redirect_url(redirect)}","1. Use `trans.check_csrf_token()` to verify the CSRF token.
2. Sanitize the input data to prevent XSS attacks.
3. Use `trans.handle_user_login()` to log the user in and record the event."
"    def resend_verification(self, trans):
        """"""
        Exposed function for use outside of the class. E.g. when user click on the resend link in the masthead.
        """"""
        message, status = self.resend_activation_email(trans, None, None)
        if status == 'done':
            return trans.show_ok_message(message)
        else:
            return trans.show_error_message(message)","1. Use `cryptography` to generate a secure random token and store it in the database.
2. Use `hashlib` to hash the user's password and store it in the database.
3. Use `signed cookies` to verify that the user is logged in when they access sensitive pages."
"    def resend_activation_email(self, trans, email, username):
        """"""
        Function resends the verification email in case user wants to log in with an inactive account or he clicks the resend link.
        """"""
        if email is None:  # User is coming from outside registration form, load email from trans
            email = trans.user.email
        if username is None:  # User is coming from outside registration form, load email from trans
            username = trans.user.username
        is_activation_sent = self.user_manager.send_activation_email(trans, email, username)
        if is_activation_sent:
            message = 'This account has not been activated yet. The activation link has been sent again. Please check your email address <b>%s</b> including the spam/trash folder.<br><a target=""_top"" href=""%s"">Return to the home page</a>.' % (escape(email), url_for('/'))
            status = 'error'
        else:
            message = 'This account has not been activated yet but we are unable to send the activation link. Please contact your local Galaxy administrator.<br><a target=""_top"" href=""%s"">Return to the home page</a>.' % url_for('/')
            status = 'error'
            if trans.app.config.error_email_to is not None:
                message += '<br>Error contact: %s' % trans.app.config.error_email_to
        return message, status","1. Use `email` and `username` parameters instead of `trans.user.email` and `trans.user.username` to avoid leaking user information.
2. Use `trans.app.config.error_email_to` instead of a hard-coded email address to protect against email spoofing attacks.
3. Sanitize the email address before sending it in the message body to prevent cross-site scripting attacks."
"    def create(self, trans, payload={}, **kwd):
        if not payload:
            payload = kwd
        message = trans.check_csrf_token(payload)
        if message:
            return self.message_exception(trans, message)
        user, message = self.user_manager.register(trans, **payload)
        if message:
            return self.message_exception(trans, message, sanitize=False)
        elif user and not trans.user_is_admin:
            trans.handle_user_login(user)
            trans.log_event(""User created a new account"")
            trans.log_event(""User logged in"")
        return {""message"": ""Success.""}","1. Sanitize the user input.
2. Check the CSRF token.
3. Log the user out after logging in."
"def send_mail(frm, to, subject, body, config, html=None):
    """"""
    Sends an email.

    :type  frm: str
    :param frm: from address

    :type  to: str
    :param to: to address

    :type  subject: str
    :param subject: Subject line

    :type  body: str
    :param body: Body text (should be plain text)

    :type  config: object
    :param config: Galaxy configuration object

    :type  html: str
    :param html: Alternative HTML representation of the body content. If
                 provided will convert the message to a MIMEMultipart. (Default 'None')
    """"""

    to = listify(to)
    if html:
        msg = email_mime_multipart.MIMEMultipart('alternative')
    else:
        msg = email_mime_text.MIMEText(body.encode('ascii', 'replace'))

    msg['To'] = ', '.join(to)
    msg['From'] = frm
    msg['Subject'] = subject

    if config.smtp_server is None:
        log.error(""Mail is not configured for this Galaxy instance."")
        log.info(msg)
        return

    if html:
        mp_text = email_mime_text.MIMEText(body.encode('ascii', 'replace'), 'plain')
        mp_html = email_mime_text.MIMEText(html.encode('ascii', 'replace'), 'html')
        msg.attach(mp_text)
        msg.attach(mp_html)

    smtp_ssl = asbool(getattr(config, 'smtp_ssl', False))
    if smtp_ssl:
        s = smtplib.SMTP_SSL()
    else:
        s = smtplib.SMTP()
    s.connect(config.smtp_server)
    if not smtp_ssl:
        try:
            s.starttls()
            log.debug('Initiated SSL/TLS connection to SMTP server: %s' % config.smtp_server)
        except RuntimeError as e:
            log.warning('SSL/TLS support is not available to your Python interpreter: %s' % e)
        except smtplib.SMTPHeloError as e:
            log.error(""The server didn't reply properly to the HELO greeting: %s"" % e)
            s.close()
            raise
        except smtplib.SMTPException as e:
            log.warning('The server does not support the STARTTLS extension: %s' % e)
    if config.smtp_username and config.smtp_password:
        try:
            s.login(config.smtp_username, config.smtp_password)
        except smtplib.SMTPHeloError as e:
            log.error(""The server didn't reply properly to the HELO greeting: %s"" % e)
            s.close()
            raise
        except smtplib.SMTPAuthenticationError as e:
            log.error(""The server didn't accept the username/password combination: %s"" % e)
            s.close()
            raise
        except smtplib.SMTPException as e:
            log.error(""No suitable authentication method was found: %s"" % e)
            s.close()
            raise
    s.sendmail(frm, to, msg.as_string())
    s.quit()","1. Use `email_utils.send_email` instead of `smtplib` directly, which provides more security features.
2. Use `asctime` instead of `strftime` to format the timestamp, which is less vulnerable to injection attacks.
3. Use `base64.urlsafe_b64encode` to encode the password, which is more secure than `base64.b64encode`."
"    def add_output(self, workflow_output, step, output_object):
        if output_object.history_content_type == ""dataset"":
            output_assoc = WorkflowInvocationOutputDatasetAssociation()
            output_assoc.workflow_invocation = self
            output_assoc.workflow_output = workflow_output
            output_assoc.workflow_step = step
            output_assoc.dataset = output_object
            self.output_datasets.append(output_assoc)
        elif output_object.history_content_type == ""dataset_collection"":
            output_assoc = WorkflowInvocationOutputDatasetCollectionAssociation()
            output_assoc.workflow_invocation = self
            output_assoc.workflow_output = workflow_output
            output_assoc.workflow_step = step
            output_assoc.dataset_collection = output_object
            self.output_dataset_collections.append(output_assoc)
        else:
            raise Exception(""Unknown output type encountered"")","1. Use [parameterized queries](https://cloud.google.com/bigquery/docs/parameterized-queries) to avoid SQL injection.
2. [Validate user input](https://cloud.google.com/apis/design/design_principles#validate_user_input) to prevent malicious attacks.
3. [Use strong access control](https://cloud.google.com/bigquery/docs/access-control) to restrict who can access your data."
"def url_get(base_url, password_mgr=None, pathspec=None, params=None):
    """"""Make contact with the uri provided and return any contents.""""""
    # Uses system proxy settings if they exist.
    proxy = urlrequest.ProxyHandler()
    if password_mgr is not None:
        auth = urlrequest.HTTPDigestAuthHandler(password_mgr)
        urlopener = urlrequest.build_opener(proxy, auth)
    else:
        urlopener = urlrequest.build_opener(proxy)
    urlrequest.install_opener(urlopener)
    full_url = build_url(base_url, pathspec=pathspec, params=params)
    response = urlopener.open(full_url)
    content = response.read()
    response.close()
    return content","1. Use HTTPS instead of HTTP to protect the data from being intercepted.
2. Use a secure password manager to store passwords.
3. Use HTTP Basic Auth or OAuth 2.0 to authenticate the user."
"    def get_app_kwds(cls, config_section, app_name=None):
        kwds = {
            'config_file': None,
            'config_section': config_section,
        }
        uwsgi_opt = uwsgi.opt
        config_file = None
        # check for --set galaxy_config_file=<path>, this overrides whatever config file uWSGI was loaded with (which
        # may not actually include a Galaxy config)
        if uwsgi_opt.get(""galaxy_config_file""):
            config_file = uwsgi_opt.get(""galaxy_config_file"")
        # check for --yaml or --json uWSGI config options next
        if config_file is None:
            config_file = (UWSGIApplicationStack._get_config_file(uwsgi_opt.get(""yaml""), yaml.safe_load, config_section)
                           or UWSGIApplicationStack._get_config_file(uwsgi_opt.get(""json""), json.load, config_section))
        # --ini and --ini-paste don't behave the same way, but this method will only be called by mules if the main
        # application was loaded with --ini-paste, so we can make some assumptions, most notably, uWSGI does not have
        # any way to set the app name when loading with paste.deploy:loadapp(), so hardcoding the alternate section
        # name to `app:main` is fine.
        has_ini_config = config_file is None and uwsgi_opt.get(""ini"") or uwsgi_opt.get(""ini-paste"")
        has_ini_config = has_ini_config or (config_file and has_ext(config_file, ""ini"", aliases=True, ignore=""sample""))
        if has_ini_config:
            config_file = config_file or uwsgi_opt.get(""ini"") or uwsgi_opt.get(""ini-paste"")
            parser = nice_config_parser(config_file)
            if not parser.has_section(config_section) and parser.has_section('app:main'):
                kwds['config_section'] = 'app:main'
        kwds['config_file'] = config_file
        return kwds","1. Use `uwsgi_opt.get(""galaxy_config_file"")` to get the config file path instead of using `uwsgi_opt.get(""ini"")` or `uwsgi_opt.get(""ini-paste"")`.
2. Use `parser.has_section(config_section)` to check if the config section exists before using it.
3. Use `parser.has_section('app:main')` to check if the `app:main` section exists and use it if the config section does not exist."
"def guess_ext(fname, sniff_order, is_binary=False):
    """"""
    Returns an extension that can be used in the datatype factory to
    generate a data for the 'fname' file

    >>> from galaxy.datatypes.registry import example_datatype_registry_for_sample
    >>> datatypes_registry = example_datatype_registry_for_sample()
    >>> sniff_order = datatypes_registry.sniff_order
    >>> fname = get_test_fname('megablast_xml_parser_test1.blastxml')
    >>> guess_ext(fname, sniff_order)
    'blastxml'
    >>> fname = get_test_fname('interval.interval')
    >>> guess_ext(fname, sniff_order)
    'interval'
    >>> fname = get_test_fname('interval1.bed')
    >>> guess_ext(fname, sniff_order)
    'bed'
    >>> fname = get_test_fname('test_tab.bed')
    >>> guess_ext(fname, sniff_order)
    'bed'
    >>> fname = get_test_fname('sequence.maf')
    >>> guess_ext(fname, sniff_order)
    'maf'
    >>> fname = get_test_fname('sequence.fasta')
    >>> guess_ext(fname, sniff_order)
    'fasta'
    >>> fname = get_test_fname('1.genbank')
    >>> guess_ext(fname, sniff_order)
    'genbank'
    >>> fname = get_test_fname('1.genbank.gz')
    >>> guess_ext(fname, sniff_order)
    'genbank.gz'
    >>> fname = get_test_fname('file.html')
    >>> guess_ext(fname, sniff_order)
    'html'
    >>> fname = get_test_fname('test.gtf')
    >>> guess_ext(fname, sniff_order)
    'gtf'
    >>> fname = get_test_fname('test.gff')
    >>> guess_ext(fname, sniff_order)
    'gff'
    >>> fname = get_test_fname('gff_version_3.gff')
    >>> guess_ext(fname, sniff_order)
    'gff3'
    >>> fname = get_test_fname('2.txt')
    >>> guess_ext(fname, sniff_order)  # 2.txt
    'txt'
    >>> fname = get_test_fname('2.tabular')
    >>> guess_ext(fname, sniff_order)
    'tabular'
    >>> fname = get_test_fname('3.txt')
    >>> guess_ext(fname, sniff_order)  # 3.txt
    'txt'
    >>> fname = get_test_fname('test_tab1.tabular')
    >>> guess_ext(fname, sniff_order)
    'tabular'
    >>> fname = get_test_fname('alignment.lav')
    >>> guess_ext(fname, sniff_order)
    'lav'
    >>> fname = get_test_fname('1.sff')
    >>> guess_ext(fname, sniff_order)
    'sff'
    >>> fname = get_test_fname('1.bam')
    >>> guess_ext(fname, sniff_order)
    'bam'
    >>> fname = get_test_fname('3unsorted.bam')
    >>> guess_ext(fname, sniff_order)
    'unsorted.bam'
    >>> fname = get_test_fname('test.idpDB')
    >>> guess_ext(fname, sniff_order)
    'idpdb'
    >>> fname = get_test_fname('test.mz5')
    >>> guess_ext(fname, sniff_order)
    'h5'
    >>> fname = get_test_fname('issue1818.tabular')
    >>> guess_ext(fname, sniff_order)
    'tabular'
    >>> fname = get_test_fname('drugbank_drugs.cml')
    >>> guess_ext(fname, sniff_order)
    'cml'
    >>> fname = get_test_fname('q.fps')
    >>> guess_ext(fname, sniff_order)
    'fps'
    >>> fname = get_test_fname('drugbank_drugs.inchi')
    >>> guess_ext(fname, sniff_order)
    'inchi'
    >>> fname = get_test_fname('drugbank_drugs.mol2')
    >>> guess_ext(fname, sniff_order)
    'mol2'
    >>> fname = get_test_fname('drugbank_drugs.sdf')
    >>> guess_ext(fname, sniff_order)
    'sdf'
    >>> fname = get_test_fname('5e5z.pdb')
    >>> guess_ext(fname, sniff_order)
    'pdb'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.otu')
    >>> guess_ext(fname, sniff_order)
    'mothur.otu'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.lower.dist')
    >>> guess_ext(fname, sniff_order)
    'mothur.lower.dist'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.square.dist')
    >>> guess_ext(fname, sniff_order)
    'mothur.square.dist'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.pair.dist')
    >>> guess_ext(fname, sniff_order)
    'mothur.pair.dist'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.freq')
    >>> guess_ext(fname, sniff_order)
    'mothur.freq'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.quan')
    >>> guess_ext(fname, sniff_order)
    'mothur.quan'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.ref.taxonomy')
    >>> guess_ext(fname, sniff_order)
    'mothur.ref.taxonomy'
    >>> fname = get_test_fname('mothur_datatypetest_true.mothur.axes')
    >>> guess_ext(fname, sniff_order)
    'mothur.axes'
    >>> guess_ext(get_test_fname('infernal_model.cm'), sniff_order)
    'cm'
    >>> fname = get_test_fname('1.gg')
    >>> guess_ext(fname, sniff_order)
    'gg'
    >>> fname = get_test_fname('diamond_db.dmnd')
    >>> guess_ext(fname, sniff_order)
    'dmnd'
    >>> fname = get_test_fname('1.xls')
    >>> guess_ext(fname, sniff_order)
    'excel.xls'
    >>> fname = get_test_fname('biom2_sparse_otu_table_hdf5.biom')
    >>> guess_ext(fname, sniff_order)
    'biom2'
    >>> fname = get_test_fname('454Score.pdf')
    >>> guess_ext(fname, sniff_order)
    'pdf'
    >>> fname = get_test_fname('1.obo')
    >>> guess_ext(fname, sniff_order)
    'obo'
    >>> fname = get_test_fname('1.arff')
    >>> guess_ext(fname, sniff_order)
    'arff'
    >>> fname = get_test_fname('1.afg')
    >>> guess_ext(fname, sniff_order)
    'afg'
    >>> fname = get_test_fname('1.owl')
    >>> guess_ext(fname, sniff_order)
    'owl'
    >>> fname = get_test_fname('Acanium.hmm')
    >>> guess_ext(fname, sniff_order)
    'snaphmm'
    >>> fname = get_test_fname('wiggle.wig')
    >>> guess_ext(fname, sniff_order)
    'wig'
    >>> fname = get_test_fname('example.iqtree')
    >>> guess_ext(fname, sniff_order)
    'iqtree'
    >>> fname = get_test_fname('1.stockholm')
    >>> guess_ext(fname, sniff_order)
    'stockholm'
    >>> fname = get_test_fname('1.xmfa')
    >>> guess_ext(fname, sniff_order)
    'xmfa'
    >>> fname = get_test_fname('test.blib')
    >>> guess_ext(fname, sniff_order)
    'blib'
    >>> fname = get_test_fname('test.phylip')
    >>> guess_ext(fname, sniff_order)
    'phylip'
    >>> fname = get_test_fname('1.smat')
    >>> guess_ext(fname, sniff_order)
    'smat'
    >>> fname = get_test_fname('1.ttl')
    >>> guess_ext(fname, sniff_order)
    'ttl'
    >>> fname = get_test_fname('1.hdt')
    >>> guess_ext(fname, sniff_order)
    'hdt'
    >>> fname = get_test_fname('1.phyloxml')
    >>> guess_ext(fname, sniff_order)
    'phyloxml'
    >>> fname = get_test_fname('1.fastqsanger.gz')
    >>> guess_ext(fname, sniff_order)  # See test_datatype_registry for more compressed type tests.
    'fastqsanger.gz'
    """"""
    file_prefix = FilePrefix(fname)
    file_ext = run_sniffers_raw(file_prefix, sniff_order, is_binary)

    # Ugly hack for tsv vs tabular sniffing, we want to prefer tabular
    # to tsv but it doesn't have a sniffer - is TSV was sniffed just check
    # if it is an okay tabular and use that instead.
    if file_ext == 'tsv':
        if is_column_based(file_prefix, '\\t', 1):
            file_ext = 'tabular'
    if file_ext is not None:
        return file_ext

    # skip header check if data is already known to be binary
    if is_binary:
        return file_ext or 'binary'
    try:
        get_headers(file_prefix, None)
    except UnicodeDecodeError:
        return 'data'  # default data type file extension
    if is_column_based(file_prefix, '\\t', 1):
        return 'tabular'  # default tabular data type file extension
    return 'txt'  # default text data type file extension","1. Use `get_headers` to check if the file is a tabular file instead of `is_column_based`.
2. Use `is_binary` to check if the file is binary before calling `get_headers`.
3. Handle UnicodeDecodeError more gracefully."
"    def __int__(self):
        return int(str(self))","1. **Use `__hash__` to implement a custom hash function.** The default hash function for strings is not secure, as it is susceptible to hash collisions. A custom hash function can be used to mitigate this risk.
2. **Use `__eq__` to implement a custom equality function.** The default equality function for strings compares objects lexicographically. This can be a security risk if the string contains sensitive data, as an attacker could craft a malicious string that compares equal to a legitimate string. A custom equality function can be used to mitigate this risk.
3. **Use `__repr__` to implement a custom representation function.** The default representation function for strings simply returns the string itself. This can be a security risk if the string contains sensitive data, as an attacker could view the string in memory. A custom representation function can be used to mitigate this risk."
"    def execute(self, cmd, persist=False, timeout=DEFAULT_TIMEOUT, timeout_check_interval=DEFAULT_TIMEOUT_CHECK_INTERVAL, **kwds):
        outf = TemporaryFile()
        p = Popen(cmd, stdin=None, stdout=outf, stderr=PIPE)
        # poll until timeout

        for i in range(int(timeout / timeout_check_interval)):
            sleep(0.1)  # For fast returning commands
            r = p.poll()
            if r is not None:
                break
            sleep(timeout_check_interval)
        else:
            kill_pid(p.pid)
            return Bunch(stdout=u'', stderr=TIMEOUT_ERROR_MESSAGE, returncode=TIMEOUT_RETURN_CODE)
        outf.seek(0)
        return Bunch(stdout=_read_str(outf), stderr=_read_str(p.stderr), returncode=p.returncode)","1. Use `subprocess.run()` instead of `Popen()` to avoid race conditions.
2. Use `shlex.quote()` to escape arguments to avoid command injection.
3. Use `os.fdopen()` to open the pipe to the child process instead of `TemporaryFile()` to avoid file descriptor leaks."
"    def top_level_workflow(self):
        """""" If this workflow is not attached to stored workflow directly,
        recursively grab its parents until it is the top level workflow
        which must have a stored workflow associated with it.
        """"""
        top_level_workflow = self
        if self.stored_workflow is None:
            # TODO: enforce this at creation...
            assert len(self.parent_workflow_steps) == 1
            return self.parent_workflow_steps[0].workflow.top_level_workflow
        return top_level_workflow","1. Use `assert` statements to validate the input data.
2. Use `functools.lru_cache` to memoize the results of expensive computations.
3. Use `contextlib.closing` to ensure that resources are closed properly."
"    def update_permissions(self, trans, dataset_id, payload, **kwd):
        """"""
        PUT /api/datasets/{encoded_dataset_id}/permissions
        Updates permissions of a dataset.

        :rtype:     dict
        :returns:   dictionary containing new permissions
        """"""
        if payload:
            kwd.update(payload)
        hda_ldda = kwd.get('hda_ldda', 'hda')
        dataset_assoc = self.get_hda_or_ldda(trans, hda_ldda=hda_ldda, dataset_id=dataset_id)
        if hda_ldda == ""hda"":
            self.hda_manager.update_permissions(trans, dataset_assoc, **kwd)
            return self.hda_manager.serialize_dataset_association_roles(trans, dataset_assoc)
        else:
            self.ldda_manager.update_permissions(trans, dataset_assoc, **kwd)
            return self.hda_manager.serialize_dataset_association_roles(trans, dataset_assoc)","1. Use `payload.get()` to get the payload data instead of directly accessing the `payload` parameter. This will prevent accidentally accessing data that is not present in the payload.
2. Use `self.get_hda_or_ldda()` to get the dataset association object instead of directly accessing the `dataset_id` parameter. This will prevent accidentally accessing data for a different dataset.
3. Use `self.hda_manager.serialize_dataset_association_roles()` to serialize the dataset association roles instead of directly returning the `dataset_assoc` object. This will prevent accidentally exposing sensitive information about the dataset association."
"    def _get_current_roles(self, trans, library_dataset):
        """"""
        Find all roles currently connected to relevant permissions
        on the library dataset and the underlying dataset.

        :param  library_dataset:      the model object
        :type   library_dataset:      LibraryDataset

        :rtype:     dictionary
        :returns:   dict of current roles for all available permission types
        """"""
        return self.serialize_dataset_association_roles(library_dataset)","1. Use `get_roles_for_dataset` instead of `serialize_dataset_association_roles` to get the current roles.
2. Use `check_ownership` to check if the current user has the permission to get the roles.
3. Use `is_public` to check if the dataset is public."
"def process_key(incoming_key, d):
    key_parts = incoming_key.split('|')
    if len(key_parts) == 1:
        # Regular parameter
        d[incoming_key] = object()
    elif key_parts[0].rsplit('_', 1)[-1].isdigit():
        # Repeat
        input_name_index = key_parts[0].rsplit('_', 1)
        input_name, index = input_name_index
        if input_name not in d:
            d[input_name] = []
        subdict = {}
        d[input_name].append(subdict)
        process_key(""|"".join(key_parts[1:]), d=subdict)
    else:
        # Section / Conditional
        input_name = key_parts[0]
        subdict = {}
        d[input_name] = subdict
        process_key(""|"".join(key_parts[1:]), d=subdict)","1. Use `input()` instead of `eval()` to prevent code injection.
2. Sanitize user input to prevent XSS attacks.
3. Use a secure hashing algorithm to store passwords."
"def expand_meta_parameters(trans, tool, incoming):
    """"""
    Take in a dictionary of raw incoming parameters and expand to a list
    of expanded incoming parameters (one set of parameters per tool
    execution).
    """"""

    to_remove = []
    for key in incoming.keys():
        if key.endswith(""|__identifier__""):
            to_remove.append(key)
    for key in to_remove:
        incoming.pop(key)

    # If we're going to multiply input dataset combinations
    # order matters, so the following reorders incoming
    # according to tool.inputs (which is ordered).
    incoming_copy = incoming.copy()
    nested_dict = {}
    for incoming_key in incoming_copy:
        if not incoming_key.startswith('__'):
            process_key(incoming_key, d=nested_dict)

    reordered_incoming = OrderedDict()

    def visitor(input, value, prefix, prefixed_name, prefixed_label, error, **kwargs):
        if prefixed_name in incoming_copy:
            reordered_incoming[prefixed_name] = incoming_copy[prefixed_name]
            del incoming_copy[prefixed_name]

    visit_input_values(inputs=tool.inputs, input_values=nested_dict, callback=visitor)
    reordered_incoming.update(incoming_copy)

    def classifier(input_key):
        value = incoming[input_key]
        if isinstance(value, dict) and 'values' in value:
            # Explicit meta wrapper for inputs...
            is_batch = value.get('batch', False)
            is_linked = value.get('linked', True)
            if is_batch and is_linked:
                classification = permutations.input_classification.MATCHED
            elif is_batch:
                classification = permutations.input_classification.MULTIPLIED
            else:
                classification = permutations.input_classification.SINGLE
            if __collection_multirun_parameter(value):
                collection_value = value['values'][0]
                values = __expand_collection_parameter(trans, input_key, collection_value, collections_to_match, linked=is_linked)
            else:
                values = value['values']
        else:
            classification = permutations.input_classification.SINGLE
            values = value
        return classification, values

    from galaxy.dataset_collections import matching
    collections_to_match = matching.CollectionsToMatch()

    # Stick an unexpanded version of multirun keys so they can be replaced,
    # by expand_mult_inputs.
    incoming_template = reordered_incoming

    expanded_incomings = permutations.expand_multi_inputs(incoming_template, classifier)
    if collections_to_match.has_collections():
        collection_info = trans.app.dataset_collections_service.match_collections(collections_to_match)
    else:
        collection_info = None
    return expanded_incomings, collection_info","1. Use `input_values` instead of `incoming_copy` to avoid overwriting values.
2. Use `collections_to_match` to track collections that need to be matched.
3. Use `expand_mult_inputs` to replace multirun keys with expanded values."
"    def get_private_user_role(self, user, auto_create=False):
        role = self.sa_session.query(self.model.Role) \\
                              .filter(and_(self.model.Role.table.c.name == user.email,
                                           self.model.Role.table.c.type == self.model.Role.types.PRIVATE)) \\
                              .first()
        if not role:
            if auto_create:
                return self.create_private_user_role(user)
            else:
                return None
        return role","1. Use `sa.orm.aliased()` to avoid implicit string concatenation.
2. Use `sa.orm.with_entities()` to avoid loading unnecessary data.
3. Use `sa.orm.joinedload()` to load related data eagerly."
"    def __init__(self, config_file, tool_source, app, guid=None, repository_id=None, allow_code_files=True):
        """"""Load a tool from the config named by `config_file`""""""
        # Determine the full path of the directory where the tool config is
        self.config_file = config_file
        self.tool_dir = os.path.dirname(config_file)
        self.app = app
        self.repository_id = repository_id
        self._allow_code_files = allow_code_files
        # setup initial attribute values
        self.inputs = odict()
        self.stdio_exit_codes = list()
        self.stdio_regexes = list()
        self.inputs_by_page = list()
        self.display_by_page = list()
        self.action = '/tool_runner/index'
        self.target = 'galaxy_main'
        self.method = 'post'
        self.labels = []
        self.check_values = True
        self.nginx_upload = False
        self.input_required = False
        self.display_interface = True
        self.require_login = False
        self.rerun = False
        # Define a place to keep track of all input   These
        # differ from the inputs dictionary in that inputs can be page
        # elements like conditionals, but input_params are basic form
        # parameters like SelectField objects.  This enables us to more
        # easily ensure that parameter dependencies like index files or
        # tool_data_table_conf.xml entries exist.
        self.input_params = []
        # Attributes of tools installed from Galaxy tool sheds.
        self.tool_shed = None
        self.repository_name = None
        self.repository_owner = None
        self.changeset_revision = None
        self.installed_changeset_revision = None
        self.sharable_url = None
        # The tool.id value will be the value of guid, but we'll keep the
        # guid attribute since it is useful to have.
        self.guid = guid
        self.old_id = None
        self.version = None
        self._lineage = None
        self.dependencies = []
        # populate toolshed repository info, if available
        self.populate_tool_shed_info()
        # add tool resource parameters
        self.populate_resource_parameters(tool_source)
        # Parse XML element containing configuration
        try:
            self.parse(tool_source, guid=guid)
        except Exception as e:
            global_tool_errors.add_error(config_file, ""Tool Loading"", e)
            raise e
        # The job search is only relevant in a galaxy context, and breaks
        # loading tools into the toolshed for validation.
        if self.app.name == 'galaxy':
            self.job_search = JobSearch(app=self.app)","1. Use `os.path.join()` to concatenate paths instead of string concatenation.
2. Use `str.format()` to interpolate variables into strings instead of string concatenation.
3. Use `json.dumps()` to serialize objects into JSON strings instead of using `str()`."
"    def to_json(self, trans, kwd={}, job=None, workflow_building_mode=False):
        """"""
        Recursively creates a tool dictionary containing repeats, dynamic options and updated states.
        """"""
        history_id = kwd.get('history_id', None)
        history = None
        if workflow_building_mode is workflow_building_modes.USE_HISTORY or workflow_building_mode is workflow_building_modes.DISABLED:
            # We don't need a history when exporting a workflow for the workflow editor or when downloading a workflow
            try:
                if history_id is not None:
                    history = self.history_manager.get_owned(trans.security.decode_id(history_id), trans.user, current_history=trans.history)
                else:
                    history = trans.get_history()
                if history is None and job is not None:
                    history = self.history_manager.get_owned(job.history.id, trans.user, current_history=trans.history)
                if history is None:
                    raise exceptions.MessageException('History unavailable. Please specify a valid history id')
            except Exception as e:
                raise exceptions.MessageException('[history_id=%s] Failed to retrieve history. %s.' % (history_id, str(e)))

        # build request context
        request_context = WorkRequestContext(app=trans.app, user=trans.user, history=history, workflow_building_mode=workflow_building_mode)

        # load job parameters into incoming
        tool_message = ''
        tool_warnings = ''
        if job:
            try:
                job_params = job.get_param_values(self.app, ignore_errors=True)
                tool_warnings = self.check_and_update_param_values(job_params, request_context, update_values=True)
                self._map_source_to_history(request_context, self.inputs, job_params)
                tool_message = self._compare_tool_version(job)
                params_to_incoming(kwd, self.inputs, job_params, self.app)
            except Exception as e:
                raise exceptions.MessageException(str(e))

        # create parameter object
        params = Params(kwd, sanitize=False)

        # expand incoming parameters (parameters might trigger multiple tool executions,
        # here we select the first execution only in order to resolve dynamic parameters)
        expanded_incomings, _ = expand_meta_parameters(trans, self, params.__dict__)
        if expanded_incomings:
            params.__dict__ = expanded_incomings[0]

        # do param translation here, used by datasource tools
        if self.input_translator:
            self.input_translator.translate(params)

        set_dataset_matcher_factory(request_context, self)
        # create tool state
        state_inputs = {}
        state_errors = {}
        populate_state(request_context, self.inputs, params.__dict__, state_inputs, state_errors)

        # create tool model
        tool_model = self.to_dict(request_context)
        tool_model['inputs'] = []
        self.populate_model(request_context, self.inputs, state_inputs, tool_model['inputs'])
        unset_dataset_matcher_factory(request_context)

        # create tool help
        tool_help = ''
        if self.help:
            tool_help = self.help.render(static_path=url_for('/static'), host_url=url_for('/', qualified=True))
            tool_help = unicodify(tool_help, 'utf-8')

        # update tool model
        tool_model.update({
            'id'            : self.id,
            'help'          : tool_help,
            'citations'     : bool(self.citations),
            'biostar_url'   : self.app.config.biostar_url,
            'sharable_url'  : self.sharable_url,
            'message'       : tool_message,
            'warnings'      : tool_warnings,
            'versions'      : self.tool_versions,
            'requirements'  : [{'name' : r.name, 'version' : r.version} for r in self.requirements],
            'errors'        : state_errors,
            'state_inputs'  : params_to_strings(self.inputs, state_inputs, self.app),
            'job_id'        : trans.security.encode_id(job.id) if job else None,
            'job_remap'     : self._get_job_remap(job),
            'history_id'    : trans.security.encode_id(history.id) if history else None,
            'display'       : self.display_interface,
            'action'        : url_for(self.action),
            'method'        : self.method,
            'enctype'       : self.enctype
        })
        return tool_model","1. Use `sanitize=True` when creating the parameter object. This will help to prevent malicious code from being injected into the tool.
2. Use `url_for` to generate URLs instead of hard-coding them. This will help to prevent phishing attacks.
3. Use `trans.security.encode_id()` to encode IDs before sending them to the client. This will help to prevent ID spoofing attacks."
"    def __init__(self):
        self._hash_by_tool_paths = {}
        self._tools_by_path = {}
        self._tool_paths_by_id = {}
        self._macro_paths_by_id = {}
        self._tool_ids_by_macro_paths = {}
        self._mod_time_by_path = {}
        self._new_tool_ids = set()
        self._removed_tool_ids = set()","1. Use `functools.lru_cache` to cache the results of expensive functions.
2. Use `typing` to annotate the types of arguments and return values.
3. Use `f-strings` to format strings instead of concatenation."
"    def cleanup(self):
        """"""
        Remove uninstalled tools from tool cache if they are not on disk anymore or if their content has changed.

        Returns list of tool_ids that have been removed.
        """"""
        removed_tool_ids = []
        try:
            paths_to_cleanup = {path: tool.all_ids for path, tool in self._tools_by_path.items() if self._should_cleanup(path)}
            for config_filename, tool_ids in paths_to_cleanup.items():
                del self._hash_by_tool_paths[config_filename]
                del self._tools_by_path[config_filename]
                for tool_id in tool_ids:
                    if tool_id in self._tool_paths_by_id:
                        del self._tool_paths_by_id[tool_id]
                removed_tool_ids.extend(tool_ids)
            for tool_id in removed_tool_ids:
                self._removed_tool_ids.add(tool_id)
                if tool_id in self._new_tool_ids:
                    self._new_tool_ids.remove(tool_id)
        except Exception:
            # If by chance the file is being removed while calculating the hash or modtime
            # we don't want the thread to die.
            pass
        return removed_tool_ids","1. Use `functools.lru_cache` to cache the results of the `_should_cleanup` function.
2. Use `os.path.isfile` to check if the file exists before calling `os.stat`.
3. Use `os.access` to check if the file is readable before calling `os.stat`."
"    def reset_status(self):
        """"""Reset self._new_tool_ids and self._removed_tool_ids once
        all operations that need to know about new tools have finished running.""""""
        self._new_tool_ids = set()
        self._removed_tool_ids = set()","1. Use `set()` instead of `list()` to avoid duplicate entries.
2. Use `.add()` and `.remove()` to add and remove items from the set.
3. Use `isdisjoint()` to check if two sets have any common elements."
"    def to_dict(self, view='collection', value_mapper=None, app=None):
        as_dict = super(ToolOutput, self).to_dict(view=view, value_mapper=value_mapper)
        format = self.format
        if format and format != ""input"" and app:
            edam_format = app.datatypes_registry.edam_formats.get(self.format)
            as_dict[""edam_format""] = edam_format
            edam_data = app.datatypes_registry.edam_data.get(self.format)
            as_dict[""edam_data""] = edam_data
        return as_dict","1. Use `input_validator` to validate user input.
2. Use `output_validator` to validate user output.
3. Use `access_control` to restrict user access."
"    def sliced_input_collection_structure(self, input_name):
        input_collection = self.example_params[input_name]
        collection_type_description = self.trans.app.dataset_collections_service.collection_type_descriptions.for_collection_type(input_collection.collection.collection_type)
        subcollection_mapping_type = None
        if self.is_implicit_input(input_name):
            subcollection_mapping_type = self.collection_info.subcollection_mapping_type(input_name)

        return get_structure(input_collection, collection_type_description, leaf_subcollection_type=subcollection_mapping_type)","1. Use `input_collection.collection.collection_type` instead of `input_collection.collection_type` to get the collection type.
2. Use `get_structure(input_collection, collection_type_description, leaf_subcollection_type=subcollection_mapping_type)` to get the structure of the input collection.
3. Use `self.is_implicit_input(input_name)` to check if the input collection is implicit."
"def add_file(dataset, registry, output_path):
    ext = None
    compression_type = None
    line_count = None
    converted_path = None
    stdout = None
    link_data_only_str = dataset.get('link_data_only', 'copy_files')
    if link_data_only_str not in ['link_to_files', 'copy_files']:
        raise UploadProblemException(""Invalid setting '%s' for option link_data_only - upload request misconfigured"" % link_data_only_str)
    link_data_only = link_data_only_str == 'link_to_files'

    # run_as_real_user is estimated from galaxy config (external chmod indicated of inputs executed)
    # If this is True we always purge supplied upload inputs so they are cleaned up and we reuse their
    # paths during data conversions since this user already owns that path.
    # Older in_place check for upload jobs created before 18.01, TODO remove in 19.XX. xref #5206
    run_as_real_user = dataset.get('run_as_real_user', False) or dataset.get(""in_place"", False)

    # purge_source defaults to True unless this is an FTP import and
    # ftp_upload_purge has been overridden to False in Galaxy's config.
    # We set purge_source to False if:
    # - the job does not have write access to the file, e.g. when running as the
    #   real user
    # - the files are uploaded from external paths.
    purge_source = dataset.get('purge_source', True) and not run_as_real_user and dataset.type not in ('server_dir', 'path_paste')

    # in_place is True unless we are running as a real user or importing external paths (i.e.
    # this is a real upload and not a path paste or ftp import).
    # in_place should always be False if running as real user because the uploaded file will
    # be owned by Galaxy and not the user and it should be False for external paths so Galaxy doesn't
    # modify files not controlled by Galaxy.
    in_place = not run_as_real_user and dataset.type not in ('server_dir', 'path_paste', 'ftp_import')

    # Base on the check_upload_content Galaxy config option and on by default, this enables some
    # security related checks on the uploaded content, but can prevent uploads from working in some cases.
    check_content = dataset.get('check_content' , True)

    # auto_decompress is a request flag that can be swapped off to prevent Galaxy from automatically
    # decompressing archive files before sniffing.
    auto_decompress = dataset.get('auto_decompress', True)
    try:
        dataset.file_type
    except AttributeError:
        raise UploadProblemException('Unable to process uploaded file, missing file_type parameter.')

    if dataset.type == 'url':
        try:
            dataset.path = sniff.stream_url_to_file(dataset.path)
        except Exception as e:
            raise UploadProblemException('Unable to fetch %s\\n%s' % (dataset.path, str(e)))

    # See if we have an empty file
    if not os.path.exists(dataset.path):
        raise UploadProblemException('Uploaded temporary file (%s) does not exist.' % dataset.path)

    if not os.path.getsize(dataset.path) > 0:
        raise UploadProblemException('The uploaded file is empty')

    # Does the first 1K contain a null?
    is_binary = check_binary(dataset.path)

    # Decompress if needed/desired and determine/validate filetype. If a keep-compressed datatype is explicitly selected
    # or if autodetection is selected and the file sniffs as a keep-compressed datatype, it will not be decompressed.
    if not link_data_only:
        if is_zip(dataset.path) and not is_single_file_zip(dataset.path):
            stdout = 'ZIP file contained more than one file, only the first file was added to Galaxy.'
        try:
            ext, converted_path, compression_type = sniff.handle_uploaded_dataset_file(
                dataset.path,
                registry,
                ext=dataset.file_type,
                tmp_prefix='data_id_%s_upload_' % dataset.dataset_id,
                tmp_dir=output_adjacent_tmpdir(output_path),
                in_place=in_place,
                check_content=check_content,
                is_binary=is_binary,
                auto_decompress=auto_decompress,
                uploaded_file_ext=os.path.splitext(dataset.name)[1].lower().lstrip('.'),
                convert_to_posix_lines=dataset.to_posix_lines,
                convert_spaces_to_tabs=dataset.space_to_tab,
            )
        except sniff.InappropriateDatasetContentError as exc:
            raise UploadProblemException(str(exc))
    elif dataset.file_type == 'auto':
        # Link mode can't decompress anyway, so enable sniffing for keep-compressed datatypes even when auto_decompress
        # is enabled
        os.environ['GALAXY_SNIFFER_VALIDATE_MODE'] = '1'
        ext = sniff.guess_ext(dataset.path, registry.sniff_order, is_binary=is_binary)
        os.environ.pop('GALAXY_SNIFFER_VALIDATE_MODE')

    # The converted path will be the same as the input path if no conversion was done (or in-place conversion is used)
    converted_path = None if converted_path == dataset.path else converted_path

    # Validate datasets where the filetype was explicitly set using the filetype's sniffer (if any)
    if dataset.file_type != 'auto':
        datatype = registry.get_datatype_by_extension(dataset.file_type)
        # Enable sniffer ""validate mode"" (prevents certain sniffers from disabling themselves)
        os.environ['GALAXY_SNIFFER_VALIDATE_MODE'] = '1'
        if hasattr(datatype, 'sniff') and not datatype.sniff(dataset.path):
            stdout = (""Warning: The file 'Type' was set to '{ext}' but the file does not appear to be of that""
                      "" type"".format(ext=dataset.file_type))
        os.environ.pop('GALAXY_SNIFFER_VALIDATE_MODE')

    # Handle unsniffable binaries
    if is_binary and ext == 'binary':
        upload_ext = os.path.splitext(dataset.name)[1].lower().lstrip('.')
        if registry.is_extension_unsniffable_binary(upload_ext):
            stdout = (""Warning: The file's datatype cannot be determined from its contents and was guessed based on""
                     "" its extension, to avoid this warning, manually set the file 'Type' to '{ext}' when uploading""
                     "" this type of file"".format(ext=upload_ext))
            ext = upload_ext
        else:
            stdout = (""The uploaded binary file format cannot be determined automatically, please set the file 'Type'""
                      "" manually"")

    datatype = registry.get_datatype_by_extension(ext)

    # Strip compression extension from name
    if compression_type and not getattr(datatype, 'compressed', False) and dataset.name.endswith('.' + compression_type):
        dataset.name = dataset.name[:-len('.' + compression_type)]

    # Move dataset
    if link_data_only:
        # Never alter a file that will not be copied to Galaxy's local file store.
        if datatype.dataset_content_needs_grooming(dataset.path):
            err_msg = 'The uploaded files need grooming, so change your <b>Copy data into Galaxy?</b> selection to be ' + \\
                '<b>Copy files into Galaxy</b> instead of <b>Link to files without copying into Galaxy</b> so grooming can be performed.'
            raise UploadProblemException(err_msg)
    if not link_data_only:
        # Move the dataset to its ""real"" path. converted_path is a tempfile so we move it even if purge_source is False.
        if purge_source or converted_path:
            try:
                shutil.move(converted_path or dataset.path, output_path)
            except OSError as e:
                # We may not have permission to remove the input
                if e.errno != errno.EACCES:
                    raise
        else:
            shutil.copy(dataset.path, output_path)

    # Write the job info
    stdout = stdout or 'uploaded %s file' % ext
    info = dict(type='dataset',
                dataset_id=dataset.dataset_id,
                ext=ext,
                stdout=stdout,
                name=dataset.name,
                line_count=line_count)
    if dataset.get('uuid', None) is not None:
        info['uuid'] = dataset.get('uuid')
    # FIXME: does this belong here? also not output-adjacent-tmpdir aware =/
    if not link_data_only and datatype and datatype.dataset_content_needs_grooming(output_path):
        # Groom the dataset content if necessary
        datatype.groom_dataset_content(output_path)
    return info","1. Use `os.path.basename` to get the file name instead of `os.path.splitext` to avoid leaking file extensions.
2. Use `shutil.copyfile` to copy the file instead of `shutil.move` to avoid overwriting the original file.
3. Use `tempfile.NamedTemporaryFile` to create a temporary file instead of using the user-provided file path to avoid leaking sensitive information."
"def copy_sample_file(app, filename, dest_path=None):
    """"""
    Copies a sample file at `filename` to `the dest_path`
    directory and strips the '.sample' extensions from `filename`.
    """"""
    if dest_path is None:
        dest_path = os.path.abspath(app.config.tool_data_path)
    sample_file_name = basic_util.strip_path(filename)
    copied_file = sample_file_name.rsplit('.sample', 1)[0]
    full_source_path = os.path.abspath(filename)
    full_destination_path = os.path.join(dest_path, sample_file_name)
    # Don't copy a file to itself - not sure how this happens, but sometimes it does...
    if full_source_path != full_destination_path:
        # It's ok to overwrite the .sample version of the file.
        shutil.copy(full_source_path, full_destination_path)
    # Only create the .loc file if it does not yet exist.  We don't overwrite it in case it
    # contains stuff proprietary to the local instance.
    if not os.path.lexists(os.path.join(dest_path, copied_file)):
        shutil.copy(full_source_path, os.path.join(dest_path, copied_file))","1. Use `os.path.relpath()` to prevent directory traversal attacks.
2. Use `shutil.copyfile()` to avoid overwriting existing files.
3. Use `os.makedirs()` to create the destination directory if it does not exist."
"def handle_missing_index_file(app, tool_path, sample_files, repository_tools_tups, sample_files_copied):
    """"""
    Inspect each tool to see if it has any input parameters that are dynamically
    generated select lists that depend on a .loc file.  This method is not called
    from the tool shed, but from Galaxy when a repository is being installed.
    """"""
    for index, repository_tools_tup in enumerate(repository_tools_tups):
        tup_path, guid, repository_tool = repository_tools_tup
        params_with_missing_index_file = repository_tool.params_with_missing_index_file
        for param in params_with_missing_index_file:
            options = param.options
            missing_file_name = basic_util.strip_path(options.missing_index_file)
            if missing_file_name not in sample_files_copied:
                # The repository must contain the required xxx.loc.sample file.
                for sample_file in sample_files:
                    sample_file_name = basic_util.strip_path(sample_file)
                    if sample_file_name == '%s.sample' % missing_file_name:
                        copy_sample_file(app, os.path.join(tool_path, sample_file))
                        if options.tool_data_table and options.tool_data_table.missing_index_file:
                            options.tool_data_table.handle_found_index_file(options.missing_index_file)
                        sample_files_copied.append(options.missing_index_file)
                        break
    return repository_tools_tups, sample_files_copied","1. Use `os.path.join` to concatenate paths instead of string concatenation.
2. Use `basic_util.strip_path` to remove leading and trailing slashes from paths.
3. Use `copy_sample_file` to copy sample files instead of directly copying them."
"    def __link_file_check(self):
        """""" outputs_to_working_directory breaks library uploads where data is
        linked.  This method is a hack that solves that problem, but is
        specific to the upload tool and relies on an injected job param.  This
        method should be removed ASAP and replaced with some properly generic
        and stateful way of determining link-only datasets. -nate
        """"""
        job = self.get_job()
        param_dict = job.get_param_values(self.app)
        return self.tool.id == 'upload1' and param_dict.get('link_data_only', None) == 'link_to_files'","1. Use proper input validation to check if the job param is valid.
2. Use a more generic and stateful way to determine link-only datasets.
3. Remove the injected job param as it is not secure."
"    def _write_integrated_tool_panel_config_file(self):
        """"""
        Write the current in-memory version of the integrated_tool_panel.xml file to disk.  Since Galaxy administrators
        use this file to manage the tool panel, we'll not use xml_to_string() since it doesn't write XML quite right.
        """"""
        tracking_directory = self._integrated_tool_panel_tracking_directory
        if not tracking_directory:
            fd, filename = tempfile.mkstemp()
        else:
            if not os.path.exists(tracking_directory):
                os.makedirs(tracking_directory)
            name = ""integrated_tool_panel_%.10f.xml"" % time.time()
            filename = os.path.join(tracking_directory, name)
            open_file = open(filename, ""w"")
            fd = open_file.fileno()
        os.write(fd, '<?xml version=""1.0""?>\\n')
        os.write(fd, '<toolbox>\\n')
        os.write(fd, '    <!--\\n    ')
        os.write(fd, '\\n    '.join([l for l in INTEGRATED_TOOL_PANEL_DESCRIPTION.split(""\\n"") if l]))
        os.write(fd, '\\n    -->\\n')
        for key, item_type, item in self._integrated_tool_panel.panel_items_iter():
            if item:
                if item_type == panel_item_types.TOOL:
                    os.write(fd, '    <tool id=""%s"" />\\n' % item.id)
                elif item_type == panel_item_types.WORKFLOW:
                    os.write(fd, '    <workflow id=""%s"" />\\n' % item.id)
                elif item_type == panel_item_types.LABEL:
                    label_id = item.id or ''
                    label_text = item.text or ''
                    label_version = item.version or ''
                    os.write(fd, '    <label id=""%s"" text=""%s"" version=""%s"" />\\n' % (label_id, label_text, label_version))
                elif item_type == panel_item_types.SECTION:
                    section_id = item.id or ''
                    section_name = item.name or ''
                    section_version = item.version or ''
                    os.write(fd, '    <section id=""%s"" name=""%s"" version=""%s"">\\n' % (escape(section_id), escape(section_name), section_version))
                    for section_key, section_item_type, section_item in item.panel_items_iter():
                        if section_item_type == panel_item_types.TOOL:
                            if section_item:
                                os.write(fd, '        <tool id=""%s"" />\\n' % section_item.id)
                        elif section_item_type == panel_item_types.WORKFLOW:
                            if section_item:
                                os.write(fd, '        <workflow id=""%s"" />\\n' % section_item.id)
                        elif section_item_type == panel_item_types.LABEL:
                            if section_item:
                                label_id = section_item.id or ''
                                label_text = section_item.text or ''
                                label_version = section_item.version or ''
                                os.write(fd, '        <label id=""%s"" text=""%s"" version=""%s"" />\\n' % (label_id, label_text, label_version))
                    os.write(fd, '    </section>\\n')
        os.write(fd, '</toolbox>\\n')
        os.close(fd)
        destination = os.path.abspath(self._integrated_tool_panel_config)
        if tracking_directory:
            open(filename + "".stack"", ""w"").write(''.join(traceback.format_stack()))
            shutil.copy(filename, filename + "".copy"")
            filename = filename + "".copy""
        shutil.move(filename, destination)
        os.chmod(self._integrated_tool_panel_config, 0o644)","1. Use xml_to_string() to write XML correctly.
2. Use a secure directory to store the file.
3. Set the file permissions to 0o644."
"    def guess_shed_config(self, app, default=None):
        tool_ids = []
        metadata = self.metadata or {}
        for tool in metadata.get('tools', []):
            tool_ids.append(tool.get('guid'))
        for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
            name = shed_tool_conf_dict['config_filename']
            for elem in shed_tool_conf_dict['config_elems']:
                if elem.tag == 'tool':
                    for sub_elem in elem.findall('id'):
                        tool_id = sub_elem.text.strip()
                        if tool_id in tool_ids:
                            self.shed_config_filename = name
                            return shed_tool_conf_dict
                elif elem.tag == ""section"":
                    for tool_elem in elem.findall('tool'):
                        for sub_elem in tool_elem.findall('id'):
                            tool_id = sub_elem.text.strip()
                            if tool_id in tool_ids:
                                self.shed_config_filename = name
                                return shed_tool_conf_dict
        if self.includes_datatypes:
            # We need to search by file paths here, which is less desirable.
            tool_shed = common_util.remove_protocol_and_port_from_tool_shed_url(self.tool_shed)
            for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
                tool_path = shed_tool_conf_dict['tool_path']
                relative_path = os.path.join(tool_path, tool_shed, 'repos', self.owner, self.name, self.installed_changeset_revision)
                if os.path.exists(relative_path):
                    self.shed_config_filename = shed_tool_conf_dict['config_filename']
                    return shed_tool_conf_dict
        return default","1. Use `verify=False` when loading XML files from untrusted sources.
2. Use `os.path.join` to construct file paths instead of concatenating strings.
3. Use `common_util.remove_protocol_and_port_from_tool_shed_url` to remove the protocol and port from tool shed URLs."
"    def to_xml_file(self, shed_tool_data_table_config, new_elems=None, remove_elems=None):
        """"""
        Write the current in-memory version of the shed_tool_data_table_conf.xml file to disk.
        remove_elems are removed before new_elems are added.
        """"""
        if not (new_elems or remove_elems):
            log.debug('ToolDataTableManager.to_xml_file called without any elements to add or remove.')
            return  # no changes provided, no need to persist any changes
        if not new_elems:
            new_elems = []
        if not remove_elems:
            remove_elems = []
        full_path = os.path.abspath(shed_tool_data_table_config)
        # FIXME: we should lock changing this file by other threads / head nodes
        try:
            tree = util.parse_xml(full_path)
            root = tree.getroot()
            out_elems = [elem for elem in root]
        except Exception as e:
            out_elems = []
            log.debug('Could not parse existing tool data table config, assume no existing elements: %s', e)
        for elem in remove_elems:
            # handle multiple occurrences of remove elem in existing elems
            while elem in out_elems:
                remove_elems.remove(elem)
        # add new elems
        out_elems.extend(new_elems)
        out_path_is_new = not os.path.exists(full_path)
        with open(full_path, 'wb') as out:
            out.write('<?xml version=""1.0""?>\\n<tables>\\n')
            for elem in out_elems:
                out.write(util.xml_to_string(elem, pretty=True))
            out.write('</tables>\\n')
        os.chmod(full_path, 0o644)
        if out_path_is_new:
            self.tool_data_path_files.update_files()","1. **Use proper file permissions.** The file `shed_tool_data_table_conf.xml` should be mode 644, which prevents unauthorized users from reading or modifying its contents.
2. **Lock the file while it is being written to.** This will prevent other threads or processes from trying to access the file while it is being updated.
3. **Validate the input data before it is added to the file.** This will help to prevent malicious users from injecting harmful content into the file."
"    def pause(self, job=None, message=None):
        if job is None:
            job = self.get_job()
        if message is None:
            message = ""Execution of this dataset's job is paused""
        if job.state == job.states.NEW:
            for dataset_assoc in job.output_datasets + job.output_library_datasets:
                dataset_assoc.dataset.dataset.state = dataset_assoc.dataset.dataset.states.PAUSED
                dataset_assoc.dataset.info = message
                self.sa_session.add(dataset_assoc.dataset)
            job.set_state(job.states.PAUSED)
            self.sa_session.add(job)","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Use the `sa_session.commit()` method to commit changes to the database.
3. Use the `sa_session.rollback()` method to roll back changes to the database if an error occurs."
"    def execute(self, tool, trans, incoming={}, return_job=False, set_output_hid=True, history=None, job_params=None, rerun_remap_job_id=None, execution_cache=None, dataset_collection_elements=None):
        """"""
        Executes a tool, creating job and tool outputs, associating them, and
        submitting the job to the job queue. If history is not specified, use
        trans.history as destination for tool's output datasets.
        """"""
        self._check_access(tool, trans)
        app = trans.app
        if execution_cache is None:
            execution_cache = ToolExecutionCache(trans)
        current_user_roles = execution_cache.current_user_roles
        history, inp_data, inp_dataset_collections = self._collect_inputs(tool, trans, incoming, history, current_user_roles)

        # Build name for output datasets based on tool name and input names
        on_text = self._get_on_text(inp_data)

        # format='input"" previously would give you a random extension from
        # the input extensions, now it should just give ""input"" as the output
        # format.
        input_ext = 'data' if tool.profile < 16.04 else ""input""
        input_dbkey = incoming.get(""dbkey"", ""?"")
        preserved_tags = {}
        for name, data in reversed(inp_data.items()):
            if not data:
                data = NoneDataset(datatypes_registry=app.datatypes_registry)
                continue

            # Convert LDDA to an HDA.
            if isinstance(data, LibraryDatasetDatasetAssociation):
                data = data.to_history_dataset_association(None)
                inp_data[name] = data

            if tool.profile < 16.04:
                input_ext = data.ext

            if data.dbkey not in [None, '?']:
                input_dbkey = data.dbkey

            identifier = getattr(data, ""element_identifier"", None)
            if identifier is not None:
                incoming[""%s|__identifier__"" % name] = identifier

            for tag in [t for t in data.tags if t.user_tname == 'name']:
                preserved_tags[tag.value] = tag

        # Collect chromInfo dataset and add as parameters to incoming
        (chrom_info, db_dataset) = app.genome_builds.get_chrom_info(input_dbkey, trans=trans, custom_build_hack_get_len_from_fasta_conversion=tool.id != 'CONVERTER_fasta_to_len')
        if db_dataset:
            inp_data.update({""chromInfo"": db_dataset})
        incoming[""chromInfo""] = chrom_info

        # Determine output dataset permission/roles list
        existing_datasets = [inp for inp in inp_data.values() if inp]
        if existing_datasets:
            output_permissions = app.security_agent.guess_derived_permissions_for_datasets(existing_datasets)
        else:
            # No valid inputs, we will use history defaults
            output_permissions = app.security_agent.history_get_default_permissions(history)

        # Add the dbkey to the incoming parameters
        incoming[""dbkey""] = input_dbkey
        # wrapped params are used by change_format action and by output.label; only perform this wrapping once, as needed
        wrapped_params = self._wrapped_params(trans, tool, incoming, inp_data)

        out_data = odict()
        input_collections = dict((k, v[0][0]) for k, v in inp_dataset_collections.items())
        output_collections = OutputCollections(
            trans,
            history,
            tool=tool,
            tool_action=self,
            input_collections=input_collections,
            dataset_collection_elements=dataset_collection_elements,
            on_text=on_text,
            incoming=incoming,
            params=wrapped_params.params,
            job_params=job_params,
        )

        # Keep track of parent / child relationships, we'll create all the
        # datasets first, then create the associations
        parent_to_child_pairs = []
        child_dataset_names = set()
        object_store_populator = ObjectStorePopulator(app)

        def handle_output(name, output, hidden=None):
            if output.parent:
                parent_to_child_pairs.append((output.parent, name))
                child_dataset_names.add(name)
            # What is the following hack for? Need to document under what
            # conditions can the following occur? (james@bx.psu.edu)
            # HACK: the output data has already been created
            #      this happens i.e. as a result of the async controller
            if name in incoming:
                dataid = incoming[name]
                data = trans.sa_session.query(app.model.HistoryDatasetAssociation).get(dataid)
                assert data is not None
                out_data[name] = data
            else:
                ext = determine_output_format(
                    output,
                    wrapped_params.params,
                    inp_data,
                    inp_dataset_collections,
                    input_ext
                )
                data = app.model.HistoryDatasetAssociation(extension=ext, create_dataset=True, flush=False)
                if hidden is None:
                    hidden = output.hidden
                if not hidden and dataset_collection_elements is not None:  # Mapping over a collection - hide datasets
                    hidden = True
                if hidden:
                    data.visible = False
                if dataset_collection_elements is not None and name in dataset_collection_elements:
                    dataset_collection_elements[name].hda = data
                trans.sa_session.add(data)
                trans.app.security_agent.set_all_dataset_permissions(data.dataset, output_permissions, new=True)
            for _, tag in preserved_tags.items():
                data.tags.append(tag.copy())

            # Must flush before setting object store id currently.
            # TODO: optimize this.
            trans.sa_session.flush()
            object_store_populator.set_object_store_id(data)

            # This may not be neccesary with the new parent/child associations
            data.designation = name
            # Copy metadata from one of the inputs if requested.

            # metadata source can be either a string referencing an input
            # or an actual object to copy.
            metadata_source = output.metadata_source
            if metadata_source:
                if isinstance(metadata_source, string_types):
                    metadata_source = inp_data.get(metadata_source)

            if metadata_source is not None:
                data.init_meta(copy_from=metadata_source)
            else:
                data.init_meta()
            # Take dbkey from LAST input
            data.dbkey = str(input_dbkey)
            # Set state
            data.blurb = ""queued""
            # Set output label
            data.name = self.get_output_name(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)
            # Store output
            out_data[name] = data
            if output.actions:
                # Apply pre-job tool-output-dataset actions; e.g. setting metadata, changing format
                output_action_params = dict(out_data)
                output_action_params.update(incoming)
                output.actions.apply_action(data, output_action_params)
            # Also set the default values of actions of type metadata
            self.set_metadata_defaults(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)
            # Flush all datasets at once.
            return data

        for name, output in tool.outputs.items():
            if not filter_output(output, incoming):
                handle_output_timer = ExecutionTimer()
                if output.collection:
                    collections_manager = app.dataset_collections_service
                    element_identifiers = []
                    known_outputs = output.known_outputs(input_collections, collections_manager.type_registry)
                    # Just to echo TODO elsewhere - this should be restructured to allow
                    # nested collections.
                    for output_part_def in known_outputs:
                        # Add elements to top-level collection, unless nested...
                        current_element_identifiers = element_identifiers
                        current_collection_type = output.structure.collection_type

                        for parent_id in (output_part_def.parent_ids or []):
                            # TODO: replace following line with formal abstractions for doing this.
                            current_collection_type = "":"".join(current_collection_type.split("":"")[1:])
                            name_to_index = dict((value[""name""], index) for (index, value) in enumerate(current_element_identifiers))
                            if parent_id not in name_to_index:
                                if parent_id not in current_element_identifiers:
                                    index = len(current_element_identifiers)
                                    current_element_identifiers.append(dict(
                                        name=parent_id,
                                        collection_type=current_collection_type,
                                        src=""new_collection"",
                                        element_identifiers=[],
                                    ))
                                else:
                                    index = name_to_index[parent_id]
                            current_element_identifiers = current_element_identifiers[index][""element_identifiers""]

                        effective_output_name = output_part_def.effective_output_name
                        element = handle_output(effective_output_name, output_part_def.output_def, hidden=True)
                        # TODO: this shouldn't exist in the top-level of the history at all
                        # but for now we are still working around that by hiding the contents
                        # there.
                        # Following hack causes dataset to no be added to history...
                        child_dataset_names.add(effective_output_name)

                        history.add_dataset(element, set_hid=set_output_hid, quota=False)
                        trans.sa_session.add(element)
                        trans.sa_session.flush()

                        current_element_identifiers.append({
                            ""__object__"": element,
                            ""name"": output_part_def.element_identifier,
                        })
                        log.info(element_identifiers)

                    if output.dynamic_structure:
                        assert not element_identifiers  # known_outputs must have been empty
                        element_kwds = dict(elements=collections_manager.ELEMENTS_UNINITIALIZED)
                    else:
                        element_kwds = dict(element_identifiers=element_identifiers)
                    output_collections.create_collection(
                        output=output,
                        name=name,
                        tags=preserved_tags,
                        **element_kwds
                    )
                    log.info(""Handled collection output named %s for tool %s %s"" % (name, tool.id, handle_output_timer))
                else:
                    handle_output(name, output)
                    log.info(""Handled output named %s for tool %s %s"" % (name, tool.id, handle_output_timer))

        add_datasets_timer = ExecutionTimer()
        # Add all the top-level (non-child) datasets to the history unless otherwise specified
        datasets_to_persist = []
        for name in out_data.keys():
            if name not in child_dataset_names and name not in incoming:  # don't add children; or already existing datasets, i.e. async created
                data = out_data[name]
                datasets_to_persist.append(data)
        # Set HID and add to history.
        # This is brand new and certainly empty so don't worry about quota.
        # TOOL OPTIMIZATION NOTE - from above loop to the job create below 99%+
        # of execution time happens within in history.add_datasets.
        history.add_datasets(trans.sa_session, datasets_to_persist, set_hid=set_output_hid, quota=False, flush=False)

        # Add all the children to their parents
        for parent_name, child_name in parent_to_child_pairs:
            parent_dataset = out_data[parent_name]
            child_dataset = out_data[child_name]
            parent_dataset.children.append(child_dataset)

        log.info(""Added output datasets to history %s"" % add_datasets_timer)
        job_setup_timer = ExecutionTimer()
        # Create the job object
        job, galaxy_session = self._new_job_for_session(trans, tool, history)
        self._record_inputs(trans, tool, job, incoming, inp_data, inp_dataset_collections, current_user_roles)
        self._record_outputs(job, out_data, output_collections)
        job.object_store_id = object_store_populator.object_store_id
        if job_params:
            job.params = dumps(job_params)
        job.set_handler(tool.get_job_handler(job_params))
        trans.sa_session.add(job)
        # Now that we have a job id, we can remap any outputs if this is a rerun and the user chose to continue dependent jobs
        # This functionality requires tracking jobs in the database.
        if app.config.track_jobs_in_database and rerun_remap_job_id is not None:
            try:
                old_job = trans.sa_session.query(app.model.Job).get(rerun_remap_job_id)
                assert old_job is not None, '(%s/%s): Old job id is invalid' % (rerun_remap_job_id, job.id)
                assert old_job.tool_id == job.tool_id, '(%s/%s): Old tool id (%s) does not match rerun tool id (%s)' % (old_job.id, job.id, old_job.tool_id, job.tool_id)
                if trans.user is not None:
                    assert old_job.user_id == trans.user.id, '(%s/%s): Old user id (%s) does not match rerun user id (%s)' % (old_job.id, job.id, old_job.user_id, trans.user.id)
                elif trans.user is None and type(galaxy_session) == trans.model.GalaxySession:
                    assert old_job.session_id == galaxy_session.id, '(%s/%s): Old session id (%s) does not match rerun session id (%s)' % (old_job.id, job.id, old_job.session_id, galaxy_session.id)
                else:
                    raise Exception('(%s/%s): Remapping via the API is not (yet) supported' % (old_job.id, job.id))
                # Duplicate PJAs before remap.
                for pjaa in old_job.post_job_actions:
                    job.add_post_job_action(pjaa.post_job_action)
                for jtod in old_job.output_datasets:
                    for (job_to_remap, jtid) in [(jtid.job, jtid) for jtid in jtod.dataset.dependent_jobs]:
                        if (trans.user is not None and job_to_remap.user_id == trans.user.id) or (trans.user is None and job_to_remap.session_id == galaxy_session.id):
                            if job_to_remap.state == job_to_remap.states.PAUSED:
                                job_to_remap.state = job_to_remap.states.NEW
                            for hda in [dep_jtod.dataset for dep_jtod in job_to_remap.output_datasets]:
                                if hda.state == hda.states.PAUSED:
                                    hda.state = hda.states.NEW
                                    hda.info = None
                            input_values = dict([(p.name, json.loads(p.value)) for p in job_to_remap.parameters])
                            update_param(jtid.name, input_values, str(out_data[jtod.name].id))
                            for p in job_to_remap.parameters:
                                p.value = json.dumps(input_values[p.name])
                            jtid.dataset = out_data[jtod.name]
                            jtid.dataset.hid = jtod.dataset.hid
                            log.info('Job %s input HDA %s remapped to new HDA %s' % (job_to_remap.id, jtod.dataset.id, jtid.dataset.id))
                            trans.sa_session.add(job_to_remap)
                            trans.sa_session.add(jtid)
                    jtod.dataset.visible = False
                    trans.sa_session.add(jtod)
            except Exception:
                log.exception('Cannot remap rerun dependencies.')

        log.info(""Setup for job %s complete, ready to flush %s"" % (job.log_str(), job_setup_timer))

        job_flush_timer = ExecutionTimer()
        trans.sa_session.flush()
        log.info(""Flushed transaction for job %s %s"" % (job.log_str(), job_flush_timer))
        # Some tools are not really executable, but jobs are still created for them ( for record keeping ).
        # Examples include tools that redirect to other applications ( epigraph ).  These special tools must
        # include something that can be retrieved from the params ( e.g., REDIRECT_URL ) to keep the job
        # from being queued.
        if 'REDIRECT_URL' in incoming:
            # Get the dataset - there should only be 1
            for name in inp_data.keys():
                dataset = inp_data[name]
            redirect_url = tool.parse_redirect_url(dataset, incoming)
            # GALAXY_URL should be include in the tool params to enable the external application
            # to send back to the current Galaxy instance
            GALAXY_URL = incoming.get('GALAXY_URL', None)
            assert GALAXY_URL is not None, ""GALAXY_URL parameter missing in tool config.""
            redirect_url += ""&GALAXY_URL=%s"" % GALAXY_URL
            # Job should not be queued, so set state to ok
            job.set_state(app.model.Job.states.OK)
            job.info = ""Redirected to: %s"" % redirect_url
            trans.sa_session.add(job)
            trans.sa_session.flush()
            trans.response.send_redirect(url_for(controller='tool_runner', action='redirect', redirect_url=redirect_url))
        else:
            # Put the job in the queue if tracking in memory
            app.job_manager.job_queue.put(job.id, job.tool_id)
            trans.log_event(""Added job to the job queue, id: %s"" % str(job.id), tool_id=job.tool_id)
            return job, out_data","1. Use `flask.session` to store user data instead of `trans.user`.
2. Use `trans.sa_session.add()` to add data to the database instead of `trans.sa_session.flush()`.
3. Use `trans.response.send_redirect()` to redirect the user instead of `return`."
"    def shutdown(self):
        self.workflow_scheduling_manager.shutdown()
        self.job_manager.shutdown()
        self.object_store.shutdown()
        if self.heartbeat:
            self.heartbeat.shutdown()
        self.update_repository_manager.shutdown()
        try:
            self.control_worker.shutdown()
        except AttributeError:
            # There is no control_worker
            pass","1. Use `@staticmethod` to mark methods that do not need an instance to be called.
2. Use `@property` to mark methods that only read data.
3. Use `@abstractmethod` to mark methods that must be implemented by subclasses."
"    def create( self, trans, parent, name, collection_type, element_identifiers=None,
                elements=None, implicit_collection_info=None, trusted_identifiers=None,
                hide_source_items=False, tags=None):
        """"""
        PRECONDITION: security checks on ability to add to parent
        occurred during load.
        """"""
        # Trust embedded, newly created objects created by tool subsystem.
        if trusted_identifiers is None:
            trusted_identifiers = implicit_collection_info is not None

        if element_identifiers and not trusted_identifiers:
            validate_input_element_identifiers( element_identifiers )

        dataset_collection = self.create_dataset_collection(
            trans=trans,
            collection_type=collection_type,
            element_identifiers=element_identifiers,
            elements=elements,
            hide_source_items=hide_source_items,
        )

        if isinstance( parent, model.History ):
            dataset_collection_instance = self.model.HistoryDatasetCollectionAssociation(
                collection=dataset_collection,
                name=name,
            )
            if implicit_collection_info:
                for input_name, input_collection in implicit_collection_info[ ""implicit_inputs"" ]:
                    dataset_collection_instance.add_implicit_input_collection( input_name, input_collection )
                for output_dataset in implicit_collection_info.get( ""outputs"" ):
                    if output_dataset not in trans.sa_session:
                        output_dataset = trans.sa_session.query( type( output_dataset ) ).get( output_dataset.id )
                    if isinstance( output_dataset, model.HistoryDatasetAssociation ):
                        output_dataset.hidden_beneath_collection_instance = dataset_collection_instance
                    elif isinstance( output_dataset, model.HistoryDatasetCollectionAssociation ):
                        dataset_collection_instance.add_implicit_input_collection( input_name, input_collection )
                    else:
                        # dataset collection, don't need to do anything...
                        pass
                    trans.sa_session.add( output_dataset )

                dataset_collection_instance.implicit_output_name = implicit_collection_info[ ""implicit_output_name"" ]

            log.debug(""Created collection with %d elements"" % ( len( dataset_collection_instance.collection.elements ) ) )
            # Handle setting hid
            parent.add_dataset_collection( dataset_collection_instance )

        elif isinstance( parent, model.LibraryFolder ):
            dataset_collection_instance = self.model.LibraryDatasetCollectionAssociation(
                collection=dataset_collection,
                folder=parent,
                name=name,
            )

        else:
            message = ""Internal logic error - create called with unknown parent type %s"" % type( parent )
            log.exception( message )
            raise MessageException( message )
        tags = tags or {}
        if implicit_collection_info:
            for k, v in implicit_collection_info.get('implicit_inputs', []):
                for tag in [t for t in v.tags if t.user_tname == 'name']:
                    tags[tag.value] = tag
        for _, tag in tags.items():
            dataset_collection_instance.tags.append(tag.copy())

        return self.__persist( dataset_collection_instance )","1. Validate input element identifiers before creating a dataset collection.
2. Use trusted identifiers for newly created objects created by the tool subsystem.
3. Handle setting hid when creating a dataset collection."
"    def copy(self):
        new_ta = type(self)()
        new_ta.tag_id = self.tag_id
        new_ta.user_tname = self.user_tname
        new_ta.value = self.value
        new_ta.user_value = self.user_value
        return new_ta","1. Use `getattr` and `setattr` to access and modify object attributes instead of direct access.
2. Use `functools.wraps` to preserve the metadata of the wrapped function.
3. Use `inspect.isfunction` to check if an object is a function before calling it."
"def create_paramfile(trans, uploaded_datasets):
    """"""
    Create the upload tool's JSON ""param"" file.
    """"""
    def _chown(path):
        try:
            # get username from email/username
            pwent = trans.user.system_user_pwent(trans.app.config.real_system_username)
            cmd = shlex.split(trans.app.config.external_chown_script)
            cmd.extend([path, pwent[0], str(pwent[3])])
            log.debug('Changing ownership of %s with: %s' % (path, ' '.join(cmd)))
            p = subprocess.Popen(cmd, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdout, stderr = p.communicate()
            assert p.returncode == 0, stderr
        except Exception as e:
            log.warning('Changing ownership of uploaded file %s failed: %s' % (path, str(e)))

    # TODO: json_file should go in the working directory
    json_file = tempfile.mkstemp()
    json_file_path = json_file[1]
    json_file = os.fdopen(json_file[0], 'w')
    for uploaded_dataset in uploaded_datasets:
        data = uploaded_dataset.data
        if uploaded_dataset.type == 'composite':
            # we need to init metadata before the job is dispatched
            data.init_meta()
            for meta_name, meta_value in uploaded_dataset.metadata.items():
                setattr(data.metadata, meta_name, meta_value)
            trans.sa_session.add(data)
            trans.sa_session.flush()
            json = dict(file_type=uploaded_dataset.file_type,
                        dataset_id=data.dataset.id,
                        dbkey=uploaded_dataset.dbkey,
                        type=uploaded_dataset.type,
                        metadata=uploaded_dataset.metadata,
                        primary_file=uploaded_dataset.primary_file,
                        composite_file_paths=uploaded_dataset.composite_files,
                        composite_files=dict((k, v.__dict__) for k, v in data.datatype.get_composite_files(data).items()))
        else:
            try:
                is_binary = uploaded_dataset.datatype.is_binary
            except:
                is_binary = None
            try:
                link_data_only = uploaded_dataset.link_data_only
            except:
                link_data_only = 'copy_files'
            try:
                uuid_str = uploaded_dataset.uuid
            except:
                uuid_str = None
            try:
                purge_source = uploaded_dataset.purge_source
            except:
                purge_source = True
            json = dict(file_type=uploaded_dataset.file_type,
                        ext=uploaded_dataset.ext,
                        name=uploaded_dataset.name,
                        dataset_id=data.dataset.id,
                        dbkey=uploaded_dataset.dbkey,
                        type=uploaded_dataset.type,
                        is_binary=is_binary,
                        link_data_only=link_data_only,
                        uuid=uuid_str,
                        to_posix_lines=getattr(uploaded_dataset, ""to_posix_lines"", True),
                        auto_decompress=getattr(uploaded_dataset, ""auto_decompress"", True),
                        purge_source=purge_source,
                        space_to_tab=uploaded_dataset.space_to_tab,
                        in_place=trans.app.config.external_chown_script is None,
                        check_content=trans.app.config.check_upload_content,
                        path=uploaded_dataset.path)
            # TODO: This will have to change when we start bundling inputs.
            # Also, in_place above causes the file to be left behind since the
            # user cannot remove it unless the parent directory is writable.
            if link_data_only == 'copy_files' and trans.app.config.external_chown_script:
                _chown(uploaded_dataset.path)
        json_file.write(dumps(json) + '\\n')
    json_file.close()
    if trans.app.config.external_chown_script:
        _chown(json_file_path)
    return json_file_path","1. Use `os.fchmod` to set the file mode of the JSON file to 0600. This will prevent other users from reading the file.
2. Use `shlex.quote` to escape the path of the JSON file when using it as an argument to the external chown script. This will prevent the script from being tricked into changing the ownership of a different file.
3. Use `subprocess.check_call` to execute the external chown script. This will ensure that the script exits with a non-zero exit code if it encounters an error."
"def add_file(dataset, registry, json_file, output_path):
    data_type = None
    line_count = None
    converted_path = None
    stdout = None
    link_data_only = dataset.get('link_data_only', 'copy_files')
    in_place = dataset.get('in_place', True)
    purge_source = dataset.get('purge_source', True)
    check_content = dataset.get('check_content' , True)
    auto_decompress = dataset.get('auto_decompress', True)
    try:
        ext = dataset.file_type
    except AttributeError:
        file_err('Unable to process uploaded file, missing file_type parameter.', dataset, json_file)
        return

    if dataset.type == 'url':
        try:
            page = urlopen(dataset.path)  # page will be .close()ed by sniff methods
            temp_name, dataset.is_multi_byte = sniff.stream_to_file(page, prefix='url_paste', source_encoding=util.get_charset_from_http_headers(page.headers))
        except Exception as e:
            file_err('Unable to fetch %s\\n%s' % (dataset.path, str(e)), dataset, json_file)
            return
        dataset.path = temp_name
    # See if we have an empty file
    if not os.path.exists(dataset.path):
        file_err('Uploaded temporary file (%s) does not exist.' % dataset.path, dataset, json_file)
        return
    if not os.path.getsize(dataset.path) > 0:
        file_err('The uploaded file is empty', dataset, json_file)
        return
    if not dataset.type == 'url':
        # Already set is_multi_byte above if type == 'url'
        try:
            dataset.is_multi_byte = multi_byte.is_multi_byte(codecs.open(dataset.path, 'r', 'utf-8').read(100))
        except UnicodeDecodeError as e:
            dataset.is_multi_byte = False
    # Is dataset an image?
    i_ext = get_image_ext(dataset.path)
    if i_ext:
        ext = i_ext
        data_type = ext
    # Is dataset content multi-byte?
    elif dataset.is_multi_byte:
        data_type = 'multi-byte char'
        ext = sniff.guess_ext(dataset.path, registry.sniff_order, is_multi_byte=True)
    # Is dataset content supported sniffable binary?
    else:
        # FIXME: This ignores the declared sniff order in datatype_conf.xml
        # resulting in improper behavior
        type_info = Binary.is_sniffable_binary(dataset.path)
        if type_info:
            data_type = type_info[0]
            ext = type_info[1]
    if not data_type:
        root_datatype = registry.get_datatype_by_extension(dataset.file_type)
        if getattr(root_datatype, 'compressed', False):
            data_type = 'compressed archive'
            ext = dataset.file_type
        else:
            # See if we have a gzipped file, which, if it passes our restrictions, we'll uncompress
            is_gzipped, is_valid = check_gzip(dataset.path, check_content=check_content)
            if is_gzipped and not is_valid:
                file_err('The gzipped uploaded file contains inappropriate content', dataset, json_file)
                return
            elif is_gzipped and is_valid and auto_decompress:
                if link_data_only == 'copy_files':
                    # We need to uncompress the temp_name file, but BAM files must remain compressed in the BGZF format
                    CHUNK_SIZE = 2 ** 20  # 1Mb
                    fd, uncompressed = tempfile.mkstemp(prefix='data_id_%s_upload_gunzip_' % dataset.dataset_id, dir=os.path.dirname(output_path), text=False)
                    gzipped_file = gzip.GzipFile(dataset.path, 'rb')
                    while 1:
                        try:
                            chunk = gzipped_file.read(CHUNK_SIZE)
                        except IOError:
                            os.close(fd)
                            os.remove(uncompressed)
                            file_err('Problem decompressing gzipped data', dataset, json_file)
                            return
                        if not chunk:
                            break
                        os.write(fd, chunk)
                    os.close(fd)
                    gzipped_file.close()
                    # Replace the gzipped file with the decompressed file if it's safe to do so
                    if dataset.type in ('server_dir', 'path_paste') or not in_place:
                        dataset.path = uncompressed
                    else:
                        shutil.move(uncompressed, dataset.path)
                    os.chmod(dataset.path, 0o644)
                dataset.name = dataset.name.rstrip('.gz')
                data_type = 'gzip'
            if not data_type:
                # See if we have a bz2 file, much like gzip
                is_bzipped, is_valid = check_bz2(dataset.path, check_content)
                if is_bzipped and not is_valid:
                    file_err('The gzipped uploaded file contains inappropriate content', dataset, json_file)
                    return
                elif is_bzipped and is_valid and auto_decompress:
                    if link_data_only == 'copy_files':
                        # We need to uncompress the temp_name file
                        CHUNK_SIZE = 2 ** 20  # 1Mb
                        fd, uncompressed = tempfile.mkstemp(prefix='data_id_%s_upload_bunzip2_' % dataset.dataset_id, dir=os.path.dirname(output_path), text=False)
                        bzipped_file = bz2.BZ2File(dataset.path, 'rb')
                        while 1:
                            try:
                                chunk = bzipped_file.read(CHUNK_SIZE)
                            except IOError:
                                os.close(fd)
                                os.remove(uncompressed)
                                file_err('Problem decompressing bz2 compressed data', dataset, json_file)
                                return
                            if not chunk:
                                break
                            os.write(fd, chunk)
                        os.close(fd)
                        bzipped_file.close()
                        # Replace the bzipped file with the decompressed file if it's safe to do so
                        if dataset.type in ('server_dir', 'path_paste') or not in_place:
                            dataset.path = uncompressed
                        else:
                            shutil.move(uncompressed, dataset.path)
                        os.chmod(dataset.path, 0o644)
                    dataset.name = dataset.name.rstrip('.bz2')
                    data_type = 'bz2'
            if not data_type:
                # See if we have a zip archive
                is_zipped = check_zip(dataset.path)
                if is_zipped and auto_decompress:
                    if link_data_only == 'copy_files':
                        CHUNK_SIZE = 2 ** 20  # 1Mb
                        uncompressed = None
                        uncompressed_name = None
                        unzipped = False
                        z = zipfile.ZipFile(dataset.path)
                        for name in z.namelist():
                            if name.endswith('/'):
                                continue
                            if unzipped:
                                stdout = 'ZIP file contained more than one file, only the first file was added to Galaxy.'
                                break
                            fd, uncompressed = tempfile.mkstemp(prefix='data_id_%s_upload_zip_' % dataset.dataset_id, dir=os.path.dirname(output_path), text=False)
                            if sys.version_info[:2] >= (2, 6):
                                zipped_file = z.open(name)
                                while 1:
                                    try:
                                        chunk = zipped_file.read(CHUNK_SIZE)
                                    except IOError:
                                        os.close(fd)
                                        os.remove(uncompressed)
                                        file_err('Problem decompressing zipped data', dataset, json_file)
                                        return
                                    if not chunk:
                                        break
                                    os.write(fd, chunk)
                                os.close(fd)
                                zipped_file.close()
                                uncompressed_name = name
                                unzipped = True
                            else:
                                # python < 2.5 doesn't have a way to read members in chunks(!)
                                try:
                                    outfile = open(uncompressed, 'wb')
                                    outfile.write(z.read(name))
                                    outfile.close()
                                    uncompressed_name = name
                                    unzipped = True
                                except IOError:
                                    os.close(fd)
                                    os.remove(uncompressed)
                                    file_err('Problem decompressing zipped data', dataset, json_file)
                                    return
                        z.close()
                        # Replace the zipped file with the decompressed file if it's safe to do so
                        if uncompressed is not None:
                            if dataset.type in ('server_dir', 'path_paste') or not in_place:
                                dataset.path = uncompressed
                            else:
                                shutil.move(uncompressed, dataset.path)
                            os.chmod(dataset.path, 0o644)
                            dataset.name = uncompressed_name
                    data_type = 'zip'
            if not data_type:
                # TODO refactor this logic.  check_binary isn't guaranteed to be
                # correct since it only looks at whether the first 100 chars are
                # printable or not.  If someone specifies a known unsniffable
                # binary datatype and check_binary fails, the file gets mangled.
                if check_binary(dataset.path) or Binary.is_ext_unsniffable(dataset.file_type):
                    # We have a binary dataset, but it is not Bam, Sff or Pdf
                    data_type = 'binary'
                    # binary_ok = False
                    parts = dataset.name.split(""."")
                    if len(parts) > 1:
                        ext = parts[-1].strip().lower()
                        if check_content and not Binary.is_ext_unsniffable(ext):
                            file_err('The uploaded binary file contains inappropriate content', dataset, json_file)
                            return
                        elif Binary.is_ext_unsniffable(ext) and dataset.file_type != ext:
                            err_msg = ""You must manually set the 'File Format' to '%s' when uploading %s files."" % (ext.capitalize(), ext)
                            file_err(err_msg, dataset, json_file)
                            return
            if not data_type:
                # We must have a text file
                if check_content and check_html(dataset.path):
                    file_err('The uploaded file contains inappropriate HTML content', dataset, json_file)
                    return
            if data_type != 'binary':
                if link_data_only == 'copy_files':
                    if dataset.type in ('server_dir', 'path_paste') and data_type not in ['gzip', 'bz2', 'zip']:
                        in_place = False
                    # Convert universal line endings to Posix line endings, but allow the user to turn it off,
                    # so that is becomes possible to upload gzip, bz2 or zip files with binary data without
                    # corrupting the content of those files.
                    if dataset.to_posix_lines:
                        tmpdir = output_adjacent_tmpdir(output_path)
                        tmp_prefix = 'data_id_%s_convert_' % dataset.dataset_id
                        if dataset.space_to_tab:
                            line_count, converted_path = sniff.convert_newlines_sep2tabs(dataset.path, in_place=in_place, tmp_dir=tmpdir, tmp_prefix=tmp_prefix)
                        else:
                            line_count, converted_path = sniff.convert_newlines(dataset.path, in_place=in_place, tmp_dir=tmpdir, tmp_prefix=tmp_prefix)
                if dataset.file_type == 'auto':
                    ext = sniff.guess_ext(dataset.path, registry.sniff_order)
                else:
                    ext = dataset.file_type
                data_type = ext
    # Save job info for the framework
    if ext == 'auto' and data_type == 'binary':
        ext = 'data'
    if ext == 'auto' and dataset.ext:
        ext = dataset.ext
    if ext == 'auto':
        ext = 'data'
    datatype = registry.get_datatype_by_extension(ext)
    if dataset.type in ('server_dir', 'path_paste') and link_data_only == 'link_to_files':
        # Never alter a file that will not be copied to Galaxy's local file store.
        if datatype.dataset_content_needs_grooming(dataset.path):
            err_msg = 'The uploaded files need grooming, so change your <b>Copy data into Galaxy?</b> selection to be ' + \\
                '<b>Copy files into Galaxy</b> instead of <b>Link to files without copying into Galaxy</b> so grooming can be performed.'
            file_err(err_msg, dataset, json_file)
            return
    if link_data_only == 'copy_files' and dataset.type in ('server_dir', 'path_paste') and data_type not in ['gzip', 'bz2', 'zip']:
        # Move the dataset to its ""real"" path
        if converted_path is not None:
            shutil.copy(converted_path, output_path)
            try:
                os.remove(converted_path)
            except:
                pass
        else:
            # This should not happen, but it's here just in case
            shutil.copy(dataset.path, output_path)
    elif link_data_only == 'copy_files':
        if purge_source:
            shutil.move(dataset.path, output_path)
        else:
            shutil.copy(dataset.path, output_path)
    # Write the job info
    stdout = stdout or 'uploaded %s file' % data_type
    info = dict(type='dataset',
                dataset_id=dataset.dataset_id,
                ext=ext,
                stdout=stdout,
                name=dataset.name,
                line_count=line_count)
    if dataset.get('uuid', None) is not None:
        info['uuid'] = dataset.get('uuid')
    json_file.write(dumps(info) + ""\\n"")
    if link_data_only == 'copy_files' and datatype and datatype.dataset_content_needs_grooming(output_path):
        # Groom the dataset content if necessary
        datatype.groom_dataset_content(output_path)","1. Use `os.path.join` instead of `+` to concatenate paths to avoid directory traversal attacks.
2. Use `shutil.copy` instead of `os.rename` to avoid race conditions.
3. Use `os.chmod` to set the file permissions to `0o644` to prevent unauthorized users from reading the file."
"def add_file(dataset, registry, json_file, output_path):
    data_type = None
    line_count = None
    converted_path = None
    stdout = None
    link_data_only = dataset.get('link_data_only', 'copy_files')
    in_place = dataset.get('in_place', True)
    purge_source = dataset.get('purge_source', True)
    # in_place is True if there is no external chmod in place,
    # however there are other instances where modifications should not occur in_place:
    # in-place unpacking or editing of line-ending when linking in data or when
    # importing data from the FTP folder while purge_source is set to false
    if not purge_source and dataset.get('type') == 'ftp_import':
        # If we do not purge the source we should not modify it in place.
        in_place = False
    if dataset.type in ('server_dir', 'path_paste'):
        in_place = False
    check_content = dataset.get('check_content' , True)
    auto_decompress = dataset.get('auto_decompress', True)
    try:
        ext = dataset.file_type
    except AttributeError:
        file_err('Unable to process uploaded file, missing file_type parameter.', dataset, json_file)
        return

    if dataset.type == 'url':
        try:
            page = urlopen(dataset.path)  # page will be .close()ed by sniff methods
            temp_name, dataset.is_multi_byte = sniff.stream_to_file(page, prefix='url_paste', source_encoding=util.get_charset_from_http_headers(page.headers))
        except Exception as e:
            file_err('Unable to fetch %s\\n%s' % (dataset.path, str(e)), dataset, json_file)
            return
        dataset.path = temp_name
    # See if we have an empty file
    if not os.path.exists(dataset.path):
        file_err('Uploaded temporary file (%s) does not exist.' % dataset.path, dataset, json_file)
        return
    if not os.path.getsize(dataset.path) > 0:
        file_err('The uploaded file is empty', dataset, json_file)
        return
    if not dataset.type == 'url':
        # Already set is_multi_byte above if type == 'url'
        try:
            dataset.is_multi_byte = multi_byte.is_multi_byte(codecs.open(dataset.path, 'r', 'utf-8').read(100))
        except UnicodeDecodeError as e:
            dataset.is_multi_byte = False
    # Is dataset an image?
    i_ext = get_image_ext(dataset.path)
    if i_ext:
        ext = i_ext
        data_type = ext
    # Is dataset content multi-byte?
    elif dataset.is_multi_byte:
        data_type = 'multi-byte char'
        ext = sniff.guess_ext(dataset.path, registry.sniff_order, is_multi_byte=True)
    # Is dataset content supported sniffable binary?
    else:
        # FIXME: This ignores the declared sniff order in datatype_conf.xml
        # resulting in improper behavior
        type_info = Binary.is_sniffable_binary(dataset.path)
        if type_info:
            data_type = type_info[0]
            ext = type_info[1]
    if not data_type:
        root_datatype = registry.get_datatype_by_extension(dataset.file_type)
        if getattr(root_datatype, 'compressed', False):
            data_type = 'compressed archive'
            ext = dataset.file_type
        else:
            # See if we have a gzipped file, which, if it passes our restrictions, we'll uncompress
            is_gzipped, is_valid = check_gzip(dataset.path, check_content=check_content)
            if is_gzipped and not is_valid:
                file_err('The gzipped uploaded file contains inappropriate content', dataset, json_file)
                return
            elif is_gzipped and is_valid and auto_decompress:
                if link_data_only == 'copy_files':
                    # We need to uncompress the temp_name file, but BAM files must remain compressed in the BGZF format
                    CHUNK_SIZE = 2 ** 20  # 1Mb
                    fd, uncompressed = tempfile.mkstemp(prefix='data_id_%s_upload_gunzip_' % dataset.dataset_id, dir=os.path.dirname(output_path), text=False)
                    gzipped_file = gzip.GzipFile(dataset.path, 'rb')
                    while 1:
                        try:
                            chunk = gzipped_file.read(CHUNK_SIZE)
                        except IOError:
                            os.close(fd)
                            os.remove(uncompressed)
                            file_err('Problem decompressing gzipped data', dataset, json_file)
                            return
                        if not chunk:
                            break
                        os.write(fd, chunk)
                    os.close(fd)
                    gzipped_file.close()
                    # Replace the gzipped file with the decompressed file if it's safe to do so
                    if not in_place:
                        dataset.path = uncompressed
                    else:
                        shutil.move(uncompressed, dataset.path)
                    os.chmod(dataset.path, 0o644)
                dataset.name = dataset.name.rstrip('.gz')
                data_type = 'gzip'
            if not data_type and bz2 is not None:
                # See if we have a bz2 file, much like gzip
                is_bzipped, is_valid = check_bz2(dataset.path, check_content)
                if is_bzipped and not is_valid:
                    file_err('The gzipped uploaded file contains inappropriate content', dataset, json_file)
                    return
                elif is_bzipped and is_valid and auto_decompress:
                    if link_data_only == 'copy_files':
                        # We need to uncompress the temp_name file
                        CHUNK_SIZE = 2 ** 20  # 1Mb
                        fd, uncompressed = tempfile.mkstemp(prefix='data_id_%s_upload_bunzip2_' % dataset.dataset_id, dir=os.path.dirname(output_path), text=False)
                        bzipped_file = bz2.BZ2File(dataset.path, 'rb')
                        while 1:
                            try:
                                chunk = bzipped_file.read(CHUNK_SIZE)
                            except IOError:
                                os.close(fd)
                                os.remove(uncompressed)
                                file_err('Problem decompressing bz2 compressed data', dataset, json_file)
                                return
                            if not chunk:
                                break
                            os.write(fd, chunk)
                        os.close(fd)
                        bzipped_file.close()
                        # Replace the bzipped file with the decompressed file if it's safe to do so
                        if not in_place:
                            dataset.path = uncompressed
                        else:
                            shutil.move(uncompressed, dataset.path)
                        os.chmod(dataset.path, 0o644)
                    dataset.name = dataset.name.rstrip('.bz2')
                    data_type = 'bz2'
            if not data_type:
                # See if we have a zip archive
                is_zipped = check_zip(dataset.path)
                if is_zipped and auto_decompress:
                    if link_data_only == 'copy_files':
                        CHUNK_SIZE = 2 ** 20  # 1Mb
                        uncompressed = None
                        uncompressed_name = None
                        unzipped = False
                        z = zipfile.ZipFile(dataset.path)
                        for name in z.namelist():
                            if name.endswith('/'):
                                continue
                            if unzipped:
                                stdout = 'ZIP file contained more than one file, only the first file was added to Galaxy.'
                                break
                            fd, uncompressed = tempfile.mkstemp(prefix='data_id_%s_upload_zip_' % dataset.dataset_id, dir=os.path.dirname(output_path), text=False)
                            if sys.version_info[:2] >= (2, 6):
                                zipped_file = z.open(name)
                                while 1:
                                    try:
                                        chunk = zipped_file.read(CHUNK_SIZE)
                                    except IOError:
                                        os.close(fd)
                                        os.remove(uncompressed)
                                        file_err('Problem decompressing zipped data', dataset, json_file)
                                        return
                                    if not chunk:
                                        break
                                    os.write(fd, chunk)
                                os.close(fd)
                                zipped_file.close()
                                uncompressed_name = name
                                unzipped = True
                            else:
                                # python < 2.5 doesn't have a way to read members in chunks(!)
                                try:
                                    outfile = open(uncompressed, 'wb')
                                    outfile.write(z.read(name))
                                    outfile.close()
                                    uncompressed_name = name
                                    unzipped = True
                                except IOError:
                                    os.close(fd)
                                    os.remove(uncompressed)
                                    file_err('Problem decompressing zipped data', dataset, json_file)
                                    return
                        z.close()
                        # Replace the zipped file with the decompressed file if it's safe to do so
                        if uncompressed is not None:
                            if not in_place:
                                dataset.path = uncompressed
                            else:
                                shutil.move(uncompressed, dataset.path)
                            os.chmod(dataset.path, 0o644)
                            dataset.name = uncompressed_name
                    data_type = 'zip'
            if not data_type:
                # TODO refactor this logic.  check_binary isn't guaranteed to be
                # correct since it only looks at whether the first 100 chars are
                # printable or not.  If someone specifies a known unsniffable
                # binary datatype and check_binary fails, the file gets mangled.
                if check_binary(dataset.path) or Binary.is_ext_unsniffable(dataset.file_type):
                    # We have a binary dataset, but it is not Bam, Sff or Pdf
                    data_type = 'binary'
                    # binary_ok = False
                    parts = dataset.name.split(""."")
                    if len(parts) > 1:
                        ext = parts[-1].strip().lower()
                        if check_content and not Binary.is_ext_unsniffable(ext):
                            file_err('The uploaded binary file contains inappropriate content', dataset, json_file)
                            return
                        elif Binary.is_ext_unsniffable(ext) and dataset.file_type != ext:
                            err_msg = ""You must manually set the 'File Format' to '%s' when uploading %s files."" % (ext.capitalize(), ext)
                            file_err(err_msg, dataset, json_file)
                            return
            if not data_type:
                # We must have a text file
                if check_content and check_html(dataset.path):
                    file_err('The uploaded file contains inappropriate HTML content', dataset, json_file)
                    return
            if data_type != 'binary':
                if link_data_only == 'copy_files' and data_type not in ('gzip', 'bz2', 'zip'):
                    # Convert universal line endings to Posix line endings if to_posix_lines is True
                    # and the data is not binary or gzip-, bz2- or zip-compressed.
                    if dataset.to_posix_lines:
                        tmpdir = output_adjacent_tmpdir(output_path)
                        tmp_prefix = 'data_id_%s_convert_' % dataset.dataset_id
                        if dataset.space_to_tab:
                            line_count, converted_path = sniff.convert_newlines_sep2tabs(dataset.path, in_place=in_place, tmp_dir=tmpdir, tmp_prefix=tmp_prefix)
                        else:
                            line_count, converted_path = sniff.convert_newlines(dataset.path, in_place=in_place, tmp_dir=tmpdir, tmp_prefix=tmp_prefix)
                if dataset.file_type == 'auto':
                    ext = sniff.guess_ext(dataset.path, registry.sniff_order)
                else:
                    ext = dataset.file_type
                data_type = ext
    # Save job info for the framework
    if ext == 'auto' and data_type == 'binary':
        ext = 'data'
    if ext == 'auto' and dataset.ext:
        ext = dataset.ext
    if ext == 'auto':
        ext = 'data'
    datatype = registry.get_datatype_by_extension(ext)
    if dataset.type in ('server_dir', 'path_paste') and link_data_only == 'link_to_files':
        # Never alter a file that will not be copied to Galaxy's local file store.
        if datatype.dataset_content_needs_grooming(dataset.path):
            err_msg = 'The uploaded files need grooming, so change your <b>Copy data into Galaxy?</b> selection to be ' + \\
                '<b>Copy files into Galaxy</b> instead of <b>Link to files without copying into Galaxy</b> so grooming can be performed.'
            file_err(err_msg, dataset, json_file)
            return
    if link_data_only == 'copy_files' and converted_path:
        # Move the dataset to its ""real"" path
        shutil.copy(converted_path, output_path)
        try:
            os.remove(converted_path)
        except Exception:
            pass
    elif link_data_only == 'copy_files':
        if purge_source:
            shutil.move(dataset.path, output_path)
        else:
            shutil.copy(dataset.path, output_path)
    # Write the job info
    stdout = stdout or 'uploaded %s file' % data_type
    info = dict(type='dataset',
                dataset_id=dataset.dataset_id,
                ext=ext,
                stdout=stdout,
                name=dataset.name,
                line_count=line_count)
    if dataset.get('uuid', None) is not None:
        info['uuid'] = dataset.get('uuid')
    json_file.write(dumps(info) + ""\\n"")
    if link_data_only == 'copy_files' and datatype and datatype.dataset_content_needs_grooming(output_path):
        # Groom the dataset content if necessary
        datatype.groom_dataset_content(output_path)","1. Use `tempfile.NamedTemporaryFile` to create a temporary file instead of `tempfile.mkstemp`. This will ensure that the file is deleted when the function exits.
2. Use `os.chmod` to set the permissions of the temporary file to 0o644. This will prevent other users from reading or writing to the file.
3. Use `shutil.move` to move the temporary file to its final destination instead of `os.rename`. This will ensure that the file is not left in an intermediate state if the function exits unexpectedly."
"def build_command(
    runner,
    job_wrapper,
    container=None,
    modify_command_for_container=True,
    include_metadata=False,
    include_work_dir_outputs=True,
    create_tool_working_directory=True,
    remote_command_params={},
    metadata_directory=None,
):
    """"""
    Compose the sequence of commands necessary to execute a job. This will
    currently include:

        - environment settings corresponding to any requirement tags
        - preparing input files
        - command line taken from job wrapper
        - commands to set metadata (if include_metadata is True)
    """"""
    shell = job_wrapper.shell
    base_command_line = job_wrapper.get_command_line()
    # job_id = job_wrapper.job_id
    # log.debug( 'Tool evaluation for job (%s) produced command-line: %s' % ( job_id, base_command_line ) )
    if not base_command_line:
        raise Exception(""Attempting to run a tool with empty command definition."")

    commands_builder = CommandsBuilder(base_command_line)

    # All job runners currently handle this case which should never occur
    if not commands_builder.commands:
        return None

    __handle_version_command(commands_builder, job_wrapper)
    __handle_task_splitting(commands_builder, job_wrapper)

    # One could imagine also allowing dependencies inside of the container but
    # that is too sophisticated for a first crack at this - build your
    # containers ready to go!
    if not container or container.resolve_dependencies:
        __handle_dependency_resolution(commands_builder, job_wrapper, remote_command_params)

    if (container and modify_command_for_container) or job_wrapper.commands_in_new_shell:
        if container and modify_command_for_container:
            # Many Docker containers do not have /bin/bash.
            external_command_shell = container.shell
        else:
            external_command_shell = shell
        externalized_commands = __externalize_commands(job_wrapper, external_command_shell, commands_builder, remote_command_params)
        if container and modify_command_for_container:
            # Stop now and build command before handling metadata and copying
            # working directory files back. These should always happen outside
            # of docker container - no security implications when generating
            # metadata and means no need for Galaxy to be available to container
            # and not copying workdir outputs back means on can be more restrictive
            # of where container can write to in some circumstances.
            run_in_container_command = container.containerize_command(
                externalized_commands
            )
            commands_builder = CommandsBuilder( run_in_container_command )
        else:
            commands_builder = CommandsBuilder( externalized_commands )

    # Don't need to create a separate tool working directory for Pulsar
    # jobs - that is handled by Pulsar.
    if create_tool_working_directory:
        # usually working will already exist, but it will not for task
        # split jobs.

        # Remove the working directory incase this is for instance a SLURM re-submission.
        # xref https://github.com/galaxyproject/galaxy/issues/3289
        commands_builder.prepend_command(""rm -rf working; mkdir -p working; cd working"")

    if include_work_dir_outputs:
        __handle_work_dir_outputs(commands_builder, job_wrapper, runner, remote_command_params)

    commands_builder.capture_return_code()

    if include_metadata and job_wrapper.requires_setting_metadata:
        metadata_directory = metadata_directory or job_wrapper.working_directory
        commands_builder.append_command(""cd '%s'"" % metadata_directory)
        __handle_metadata(commands_builder, job_wrapper, runner, remote_command_params)

    return commands_builder.build()","1. Use `subprocess.check_output()` instead of `subprocess.call()` to avoid leaking sensitive information to the terminal.
2. Use `shlex.quote()` to escape shell metacharacters in arguments to avoid command injection attacks.
3. Use `subprocess.Popen()` with the `universal_newlines=True` flag to handle text files with newlines correctly."
"def build_command(
    runner,
    job_wrapper,
    container=None,
    modify_command_for_container=True,
    include_metadata=False,
    include_work_dir_outputs=True,
    create_tool_working_directory=True,
    remote_command_params={},
    metadata_directory=None,
):
    """"""
    Compose the sequence of commands necessary to execute a job. This will
    currently include:

        - environment settings corresponding to any requirement tags
        - preparing input files
        - command line taken from job wrapper
        - commands to set metadata (if include_metadata is True)
    """"""
    shell = job_wrapper.shell
    base_command_line = job_wrapper.get_command_line()
    # job_id = job_wrapper.job_id
    # log.debug( 'Tool evaluation for job (%s) produced command-line: %s' % ( job_id, base_command_line ) )
    if not base_command_line:
        raise Exception(""Attempting to run a tool with empty command definition."")

    commands_builder = CommandsBuilder(base_command_line)

    # All job runners currently handle this case which should never occur
    if not commands_builder.commands:
        return None

    __handle_version_command(commands_builder, job_wrapper)

    # One could imagine also allowing dependencies inside of the container but
    # that is too sophisticated for a first crack at this - build your
    # containers ready to go!
    if not container or container.resolve_dependencies:
        __handle_dependency_resolution(commands_builder, job_wrapper, remote_command_params)

    __handle_task_splitting(commands_builder, job_wrapper)

    if (container and modify_command_for_container) or job_wrapper.commands_in_new_shell:
        if container and modify_command_for_container:
            # Many Docker containers do not have /bin/bash.
            external_command_shell = container.shell
        else:
            external_command_shell = shell
        externalized_commands = __externalize_commands(job_wrapper, external_command_shell, commands_builder, remote_command_params)
        if container and modify_command_for_container:
            # Stop now and build command before handling metadata and copying
            # working directory files back. These should always happen outside
            # of docker container - no security implications when generating
            # metadata and means no need for Galaxy to be available to container
            # and not copying workdir outputs back means on can be more restrictive
            # of where container can write to in some circumstances.
            run_in_container_command = container.containerize_command(
                externalized_commands
            )
            commands_builder = CommandsBuilder( run_in_container_command )
        else:
            commands_builder = CommandsBuilder( externalized_commands )

    # Don't need to create a separate tool working directory for Pulsar
    # jobs - that is handled by Pulsar.
    if create_tool_working_directory:
        # usually working will already exist, but it will not for task
        # split jobs.

        # Remove the working directory incase this is for instance a SLURM re-submission.
        # xref https://github.com/galaxyproject/galaxy/issues/3289
        commands_builder.prepend_command(""rm -rf working; mkdir -p working; cd working"")

    if include_work_dir_outputs:
        __handle_work_dir_outputs(commands_builder, job_wrapper, runner, remote_command_params)

    commands_builder.capture_return_code()

    if include_metadata and job_wrapper.requires_setting_metadata:
        metadata_directory = metadata_directory or job_wrapper.working_directory
        commands_builder.append_command(""cd '%s'"" % metadata_directory)
        __handle_metadata(commands_builder, job_wrapper, runner, remote_command_params)

    return commands_builder.build()","1. Use `externalized_commands` to escape shell characters.
2. Use `container.containerize_command` to run commands in a docker container.
3. Use `CommandsBuilder.capture_return_code` to capture the return code of the command."
"    def get_data_inputs( self ):
        """""" Get configure time data input descriptions. """"""
        # Filter subworkflow steps and get inputs
        step_to_input_type = {
            ""data_input"": ""dataset"",
            ""data_collection_input"": ""dataset_collection"",
        }
        inputs = []
        if hasattr( self.subworkflow, 'input_steps' ):
            for step in self.subworkflow.input_steps:
                name = step.label
                if not name:
                    step_module = module_factory.from_workflow_step( self.trans, step )
                    name = step_module.get_name()
                step_type = step.type
                assert step_type in step_to_input_type
                input = dict(
                    input_subworkflow_step_id=step.order_index,
                    name=name,
                    label=name,
                    multiple=False,
                    extensions=""input"",
                    input_type=step_to_input_type[step_type],
                )
                inputs.append(input)
        return inputs","1. Use `assert` statements to validate input data.
2. Use `type` checking to ensure that input data is of the correct type.
3. Sanitize input data to prevent against injection attacks."
"    def get_data_outputs( self ):
        outputs = []
        if hasattr( self.subworkflow, 'workflow_outputs' ):
            for workflow_output in self.subworkflow.workflow_outputs:
                output_step = workflow_output.workflow_step
                label = workflow_output.label
                if label is None:
                    label = ""%s:%s"" % (output_step.order_index, workflow_output.output_name)
                output = dict(
                    name=label,
                    label=label,
                    extensions=['input'],  # TODO
                )
                outputs.append(output)
        return outputs","1. Use `getattr` instead of `hasattr` to check for attributes.
2. Use `format` to sanitize user input.
3. Use `assert` to validate user input."
"    def execute( self, trans, progress, invocation, step ):
        """""" Execute the given workflow step in the given workflow invocation.
        Use the supplied workflow progress object to track outputs, find
        inputs, etc...
        """"""
        subworkflow_invoker = progress.subworkflow_invoker( trans, step )
        subworkflow_invoker.invoke()
        subworkflow = subworkflow_invoker.workflow
        subworkflow_progress = subworkflow_invoker.progress
        outputs = {}
        for workflow_output in subworkflow.workflow_outputs:
            workflow_output_label = workflow_output.label
            replacement = subworkflow_progress.get_replacement_workflow_output( workflow_output )
            outputs[ workflow_output_label ] = replacement
        progress.set_step_outputs( step, outputs )
        return None","1. Use `assert` statements to validate input parameters.
2. Sanitize user input to prevent injection attacks.
3. Use `cryptography` library to encrypt sensitive data."
"    def get_filter_set( self, connections=None ):
        filter_set = []
        if connections:
            for oc in connections:
                for ic in oc.input_step.module.get_data_inputs():
                    if 'extensions' in ic and ic[ 'name' ] == oc.input_name:
                        filter_set += ic[ 'extensions' ]
        if not filter_set:
            filter_set = [ 'data' ]
        return ', '.join( filter_set )","1. Use `filter_set` instead of `input_name` to avoid SQL injection.
2. Use `input_step.module.get_data_inputs()` instead of `ic` to avoid accessing invalid data.
3. Use `filter_set = ['data']` instead of `filter_set = []` to avoid returning empty data."
"def populate_state( request_context, inputs, incoming, state, errors={}, prefix='', context=None, check=True ):
    """"""
    Populates nested state dict from incoming parameter values.
    >>> from xml.etree.ElementTree import XML
    >>> from galaxy.util.bunch import Bunch
    >>> from galaxy.util.odict import odict
    >>> from galaxy.tools.parameters.basic import TextToolParameter, BooleanToolParameter
    >>> from galaxy.tools.parameters.grouping import Repeat
    >>> trans = Bunch( workflow_building_mode=False )
    >>> a = TextToolParameter( None, XML( '<param name=""a""/>' ) )
    >>> b = Repeat()
    >>> b.min = 0
    >>> b.max = 1
    >>> c = TextToolParameter( None, XML( '<param name=""c""/>' ) )
    >>> d = Repeat()
    >>> d.min = 0
    >>> d.max = 1
    >>> e = TextToolParameter( None, XML( '<param name=""e""/>' ) )
    >>> f = Conditional()
    >>> g = BooleanToolParameter( None, XML( '<param name=""g""/>' ) )
    >>> h = TextToolParameter( None, XML( '<param name=""h""/>' ) )
    >>> i = TextToolParameter( None, XML( '<param name=""i""/>' ) )
    >>> b.name = 'b'
    >>> b.inputs = odict([ ('c', c), ('d', d) ])
    >>> d.name = 'd'
    >>> d.inputs = odict([ ('e', e), ('f', f) ])
    >>> f.test_param = g
    >>> f.name = 'f'
    >>> f.cases = [ Bunch( value='true', inputs= { 'h': h } ), Bunch( value='false', inputs= { 'i': i } ) ]
    >>> inputs = odict([('a',a),('b',b)])
    >>> flat = odict([ ('a', 1 ), ( 'b_0|c', 2 ), ( 'b_0|d_0|e', 3 ), ( 'b_0|d_0|f|h', 4 ), ( 'b_0|d_0|f|g', True ) ])
    >>> state = odict()
    >>> populate_state( trans, inputs, flat, state, check=False )
    >>> print state[ 'a' ]
    1
    >>> print state[ 'b' ][ 0 ][ 'c' ]
    2
    >>> print state[ 'b' ][ 0 ][ 'd' ][ 0 ][ 'e' ]
    3
    >>> print state[ 'b' ][ 0 ][ 'd' ][ 0 ][ 'f' ][ 'h' ]
    4
    """"""
    context = ExpressionContext( state, context )
    for input in inputs.values():
        state[ input.name ] = input.get_initial_value( request_context, context )
        key = prefix + input.name
        group_state = state[ input.name ]
        group_prefix = '%s|' % ( key )
        if input.type == 'repeat':
            rep_index = 0
            del group_state[:]
            while True:
                rep_prefix = '%s_%d' % ( key, rep_index )
                if not any( incoming_key.startswith( rep_prefix ) for incoming_key in incoming.keys() ) and rep_index >= input.min:
                    break
                if rep_index < input.max:
                    new_state = { '__index__' : rep_index }
                    group_state.append( new_state )
                    populate_state( request_context, input.inputs, incoming, new_state, errors, prefix=rep_prefix + '|', context=context )
                rep_index += 1
        elif input.type == 'conditional':
            if input.value_ref and not input.value_ref_in_group:
                test_param_key = prefix + input.test_param.name
            else:
                test_param_key = group_prefix + input.test_param.name
            test_param_value = incoming.get( test_param_key, group_state.get( input.test_param.name ) )
            value, error = check_param( request_context, input.test_param, test_param_value, context ) if check else [ test_param_value, None ]
            if error:
                errors[ test_param_key ] = error
            else:
                try:
                    current_case = input.get_current_case( value )
                    group_state = state[ input.name ] = {}
                    populate_state( request_context, input.cases[ current_case ].inputs, incoming, group_state, errors, prefix=group_prefix, context=context )
                    group_state[ '__current_case__' ] = current_case
                except Exception:
                    errors[ test_param_key ] = 'The selected case is unavailable/invalid.'
                    pass
            group_state[ input.test_param.name ] = value
        elif input.type == 'section':
            populate_state( request_context, input.inputs, incoming, group_state, errors, prefix=group_prefix, context=context )
        elif input.type == 'upload_dataset':
            d_type = input.get_datatype( request_context, context=context )
            writable_files = d_type.writable_files
            while len( group_state ) > len( writable_files ):
                del group_state[ -1 ]
            while len( writable_files ) > len( group_state ):
                new_state = { '__index__' : len( group_state ) }
                for upload_item in input.inputs.values():
                    new_state[ upload_item.name ] = upload_item.get_initial_value( request_context, context )
                group_state.append( new_state )
            for i, rep_state in enumerate( group_state ):
                rep_index = rep_state[ '__index__' ]
                rep_prefix = '%s_%d|' % ( key, rep_index )
                populate_state( request_context, input.inputs, incoming, rep_state, errors, prefix=rep_prefix, context=context )
        else:
            param_value = _get_incoming_value( incoming, key, state.get( input.name ) )
            value, error = check_param( request_context, input, param_value, context ) if check else [ param_value, None ]
            if error:
                errors[ key ] = error
            state[ input.name ] = value","1. Use `input()` instead of `eval()` to avoid code injection.
2. Sanitize user input to prevent XSS attacks.
3. Use prepared statements to prevent SQL injection attacks."
"def reload_data_managers(app, **kwargs):
    from galaxy.tools.data_manager.manager import DataManagers
    log.debug(""Executing data managers reload on '%s'"", app.config.server_name)
    app._configure_tool_data_tables(from_shed_config=False)
    reload_tool_data_tables(app)
    reload_count = app.data_managers._reload_count
    app.data_managers = DataManagers(app, conf_watchers=app.data_managers.conf_watchers)
    app.data_managers._reload_count = reload_count + 1","1. Use `from galaxy.tools.data_manager.manager import DataManagers` instead of `from galaxy.tools.data_manager import DataManagers` to avoid importing the deprecated `DataManagers` class.
2. Use `app.config.server_name` instead of `app.config['server_name']` to avoid accidentally accessing a key that does not exist.
3. Use `app.data_managers._reload_count + 1` instead of `reload_count + 1` to avoid accidentally incrementing the reload count twice."
"    def __init__( self, job_wrapper, job_destination ):
        self.runner_state_handled = False
        self.job_wrapper = job_wrapper
        self.job_destination = job_destination","1. Use `functools.wraps` to preserve the function signature of the wrapped function.
2. Use `inspect.getfullargspec` to get the argument names of the wrapped function.
3. Use `inspect.iscoroutinefunction` to check if the wrapped function is a coroutine function."
"    def __init__( self, files_dir=None, job_wrapper=None, job_id=None, job_file=None, output_file=None, error_file=None, exit_code_file=None, job_name=None, job_destination=None  ):
        super( AsynchronousJobState, self ).__init__( job_wrapper, job_destination )
        self.old_state = None
        self._running = False
        self.check_count = 0
        self.start_time = None

        # job_id is the DRM's job id, not the Galaxy job id
        self.job_id = job_id

        self.job_file = job_file
        self.output_file = output_file
        self.error_file = error_file
        self.exit_code_file = exit_code_file
        self.job_name = job_name

        self.set_defaults( files_dir )

        self.cleanup_file_attributes = [ 'job_file', 'output_file', 'error_file', 'exit_code_file' ]","1. Use [functools.lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache) to cache the results of expensive function calls.
2. Use [type hints](https://docs.python.org/3/library/typing.html) to annotate the types of function arguments and return values.
3. Use [f-strings](https://docs.python.org/3/reference/lexical_analysis.html#f-strings) to format strings in a more readable way."
"    def cleanup( self ):
        for file in [ getattr( self, a ) for a in self.cleanup_file_attributes if hasattr( self, a ) ]:
            try:
                os.unlink( file )
            except Exception as e:
                log.debug( ""(%s/%s) Unable to cleanup %s: %s"" % ( self.job_wrapper.get_id_tag(), self.job_id, file, str( e ) ) )","1. Use `with` statement to open files to ensure they are closed properly.
2. Use `try-except` block to handle exceptions and log errors.
3. Use `os.remove` to delete files instead of `os.unlink`."
"    def queue_job( self, job_wrapper ):
        """"""Create job script and submit it to the DRM""""""

        # prepare the job
        include_metadata = asbool( job_wrapper.job_destination.params.get( ""embed_metadata_in_job"", True ) )
        if not self.prepare_job( job_wrapper, include_metadata=include_metadata):
            return

        # get configured job destination
        job_destination = job_wrapper.job_destination

        # wrapper.get_id_tag() instead of job_id for compatibility with TaskWrappers.
        galaxy_id_tag = job_wrapper.get_id_tag()

        # get destination params
        query_params = submission_params(prefix="""", **job_destination.params)
        container = None
        universe = query_params.get('universe', None)
        if universe and universe.strip().lower() == 'docker':
            container = self.find_container( job_wrapper )
            if container:
                # HTCondor needs the image as 'docker_image'
                query_params.update({'docker_image': container})

        galaxy_slots = query_params.get('request_cpus', None)
        if galaxy_slots:
            galaxy_slots_statement = 'GALAXY_SLOTS=""%s""; export GALAXY_SLOTS_CONFIGURED=""1""' % galaxy_slots
        else:
            galaxy_slots_statement = 'GALAXY_SLOTS=""1""'

        # define job attributes
        cjs = CondorJobState(
            files_dir=self.app.config.cluster_files_directory,
            job_wrapper=job_wrapper
        )

        cluster_directory = self.app.config.cluster_files_directory
        cjs.user_log = os.path.join( cluster_directory, 'galaxy_%s.condor.log' % galaxy_id_tag )
        cjs.register_cleanup_file_attribute( 'user_log' )
        submit_file = os.path.join( cluster_directory, 'galaxy_%s.condor.desc' % galaxy_id_tag )
        executable = cjs.job_file

        build_submit_params = dict(
            executable=executable,
            output=cjs.output_file,
            error=cjs.error_file,
            user_log=cjs.user_log,
            query_params=query_params,
        )

        submit_file_contents = build_submit_description(**build_submit_params)
        script = self.get_job_file(
            job_wrapper,
            exit_code_path=cjs.exit_code_file,
            slots_statement=galaxy_slots_statement,
        )
        try:
            self.write_executable_script( executable, script )
        except:
            job_wrapper.fail( ""failure preparing job script"", exception=True )
            log.exception( ""(%s) failure preparing job script"" % galaxy_id_tag )
            return

        cleanup_job = job_wrapper.cleanup_job
        try:
            open(submit_file, ""w"").write(submit_file_contents)
        except Exception:
            if cleanup_job == ""always"":
                cjs.cleanup()
                # job_wrapper.fail() calls job_wrapper.cleanup()
            job_wrapper.fail( ""failure preparing submit file"", exception=True )
            log.exception( ""(%s) failure preparing submit file"" % galaxy_id_tag )
            return

        # job was deleted while we were preparing it
        if job_wrapper.get_state() == model.Job.states.DELETED:
            log.debug( ""Job %s deleted by user before it entered the queue"" % galaxy_id_tag )
            if cleanup_job in (""always"", ""onsuccess""):
                os.unlink( submit_file )
                cjs.cleanup()
                job_wrapper.cleanup()
            return

        log.debug( ""(%s) submitting file %s"" % ( galaxy_id_tag, executable ) )

        external_job_id, message = condor_submit(submit_file)
        if external_job_id is None:
            log.debug( ""condor_submit failed for job %s: %s"" % (job_wrapper.get_id_tag(), message) )
            if self.app.config.cleanup_job == ""always"":
                os.unlink( submit_file )
                cjs.cleanup()
            job_wrapper.fail( ""condor_submit failed"", exception=True )
            return

        os.unlink( submit_file )

        log.info( ""(%s) queued as %s"" % ( galaxy_id_tag, external_job_id ) )

        # store runner information for tracking if Galaxy restarts
        job_wrapper.set_job_destination( job_destination, external_job_id )

        # Store DRM related state information for job
        cjs.job_id = external_job_id
        cjs.job_destination = job_destination

        # Add to our 'queue' of jobs to monitor
        self.monitor_queue.put( cjs )","1. Use `pwd.getpwuid()` to get the user name instead of `os.getuid()`.
2. Use `subprocess.check_call()` to run the command instead of `os.system()`.
3. Use `shlex.quote()` to escape the arguments to the command."
"    def _write_integrated_tool_panel_config_file( self ):
        """"""
        Write the current in-memory version of the integrated_tool_panel.xml file to disk.  Since Galaxy administrators
        use this file to manage the tool panel, we'll not use xml_to_string() since it doesn't write XML quite right.
        """"""
        tracking_directory = self._integrated_tool_panel_tracking_directory
        if not tracking_directory:
            fd, filename = tempfile.mkstemp()
        else:
            if not os.path.exists(tracking_directory):
                os.makedirs(tracking_directory)
            name = ""integrated_tool_panel_%.10f.xml"" % time.time()
            filename = os.path.join(tracking_directory, name)
            open_file = open(filename, ""w"")
            fd = open_file.fileno()
        os.write( fd, '<?xml version=""1.0""?>\\n' )
        os.write( fd, '<toolbox>\\n' )
        os.write( fd, '    <!--\\n    ')
        os.write( fd, '\\n    '.join( [ l for l in INTEGRATED_TOOL_PANEL_DESCRIPTION.split(""\\n"") if l ] ) )
        os.write( fd, '\\n    -->\\n')
        for key, item_type, item in self._integrated_tool_panel.panel_items_iter():
            if item:
                if item_type == panel_item_types.TOOL:
                    os.write( fd, '    <tool id=""%s"" />\\n' % item.id )
                elif item_type == panel_item_types.WORKFLOW:
                    os.write( fd, '    <workflow id=""%s"" />\\n' % item.id )
                elif item_type == panel_item_types.LABEL:
                    label_id = item.id or ''
                    label_text = item.text or ''
                    label_version = item.version or ''
                    os.write( fd, '    <label id=""%s"" text=""%s"" version=""%s"" />\\n' % ( label_id, label_text, label_version ) )
                elif item_type == panel_item_types.SECTION:
                    section_id = item.id or ''
                    section_name = item.name or ''
                    section_version = item.version or ''
                    os.write( fd, '    <section id=""%s"" name=""%s"" version=""%s"">\\n' % ( section_id, section_name, section_version ) )
                    for section_key, section_item_type, section_item in item.panel_items_iter():
                        if section_item_type == panel_item_types.TOOL:
                            if section_item:
                                os.write( fd, '        <tool id=""%s"" />\\n' % section_item.id )
                        elif section_item_type == panel_item_types.WORKFLOW:
                            if section_item:
                                os.write( fd, '        <workflow id=""%s"" />\\n' % section_item.id )
                        elif section_item_type == panel_item_types.LABEL:
                            if section_item:
                                label_id = section_item.id or ''
                                label_text = section_item.text or ''
                                label_version = section_item.version or ''
                                os.write( fd, '        <label id=""%s"" text=""%s"" version=""%s"" />\\n' % ( label_id, label_text, label_version ) )
                    os.write( fd, '    </section>\\n' )
        os.write( fd, '</toolbox>\\n' )
        os.close( fd )
        destination = os.path.abspath( self._integrated_tool_panel_config )
        if tracking_directory:
            open(filename + "".stack"", ""w"").write(''.join(traceback.format_stack()))
            shutil.copy( filename, filename + "".copy"" )
            filename = filename + "".copy""
        shutil.move( filename, destination )
        os.chmod( self._integrated_tool_panel_config, 0o644 )","1. Use `xml_to_string()` to write XML correctly.
2. Use `tempfile.mkstemp()` to create a temporary file instead of hard-coding the filename.
3. Use `os.chmod()` to set the permissions of the file to 0o644."
"def parse_xml( fname ):
    """"""Returns a parsed xml tree""""""
    # handle deprecation warning for XMLParsing a file with DOCTYPE
    class DoctypeSafeCallbackTarget( ElementTree.TreeBuilder ):
        def doctype( *args ):
            pass
    tree = ElementTree.ElementTree()
    root = tree.parse( fname, parser=ElementTree.XMLParser( target=DoctypeSafeCallbackTarget() ) )
    ElementInclude.include( root )
    return tree","1. **Use a secure parser**. The default XML parser in Python is not secure and should not be used for parsing untrusted input. Use a secure parser such as [xmlsec](https://pypi.org/project/xmlsec/) or [lxml](https://pypi.org/project/lxml/).
2. **Sanitize input data**. Before parsing XML input, it is important to sanitize the data to remove any malicious content. This can be done using a tool such as [xmllint](https://xmlsoft.org/xmllint.html) or [libxml2](https://xmlsoft.org/libxml2/).
3. **Use proper permissions**. Make sure that the XML parser is only run with the necessary permissions. This will help to prevent malicious code from being executed."
"    def start(self):
        if not self._active:
            self._active = True
            self.thread.start()","1. Use `threading.Lock()` to protect shared state between threads.
2. Use `threading.Event()` to wait for a thread to finish.
3. Use `os.setpgid()` to set the process group ID of a thread."
"    def monitor(self, path):
        mod_time = None
        if os.path.exists(path):
            mod_time = time.ctime(os.path.getmtime(path))
        with self._lock:
            self.paths[path] = mod_time","1. Use `os.stat()` instead of `os.path.getmtime()` to get the file's modification time. This will prevent a race condition where the file's modification time could change between the time it is checked and the time it is added to the dictionary.
2. Use `fcntl.flock()` to lock the dictionary while it is being updated. This will prevent other threads from accessing the dictionary while it is being modified.
3. Use a `threading.Lock` to protect the `_lock` attribute. This will prevent multiple threads from accessing the lock simultaneously."
"    def encode_dict_ids( self, a_dict, kind=None ):
        """"""
        Encode all ids in dictionary. Ids are identified by (a) an 'id' key or
        (b) a key that ends with '_id'
        """"""
        for key, val in a_dict.items():
            if key == 'id' or key.endswith('_id'):
                a_dict[ key ] = self.encode_id( val, kind=kind )

        return a_dict","1. Use `cryptography` instead of `base64` to encode IDs.
2. Use a secure hashing algorithm such as `SHA256` or `SHA512`.
3. Salt the hashed IDs to prevent rainbow table attacks."
"    def execute(cls, app, sa_session, action, job, replacement_dict):
        # TODO Optimize this later.  Just making it work for now.
        # TODO Support purging as well as deletion if user_purge is enabled.
        # Dataset candidates for deletion must be
        # 1) Created by the workflow.
        # 2) Not have any job_to_input_dataset associations with states other
        # than OK or DELETED.  If a step errors, we don't want to delete/purge it
        # automatically.
        # 3) Not marked as a workflow output.
        # POTENTIAL ISSUES:  When many outputs are being finish()ed
        # concurrently, sometimes non-terminal steps won't be cleaned up
        # because of the lag in job state updates.
        sa_session.flush()
        if not job.workflow_invocation_step:
            log.debug(""This job is not part of a workflow invocation, delete intermediates aborted."")
            return
        wfi = job.workflow_invocation_step.workflow_invocation
        sa_session.refresh(wfi)
        if wfi.active:
            log.debug(""Workflow still scheduling so new jobs may appear, skipping deletion of intermediate files."")
            # Still evaluating workflow so we don't yet have all workflow invocation
            # steps to start looking at.
            return
        outputs_defined = wfi.workflow.has_outputs_defined()
        if outputs_defined:
            wfi_steps = [wfistep for wfistep in wfi.steps if not wfistep.workflow_step.workflow_outputs and wfistep.workflow_step.type == ""tool""]
            jobs_to_check = []
            for wfi_step in wfi_steps:
                sa_session.refresh(wfi_step)
                wfi_step_job = wfi_step.job
                if wfi_step_job:
                    jobs_to_check.append(wfi_step_job)
                else:
                    log.debug(""No job found yet for wfi_step %s, (step %s)"" % (wfi_step, wfi_step.workflow_step))
            for j2c in jobs_to_check:
                creating_jobs = [(x, x.dataset.creating_job) for x in j2c.input_datasets if x.dataset.creating_job]
                for (x, creating_job) in creating_jobs:
                    sa_session.refresh(creating_job)
                    sa_session.refresh(x)
                for input_dataset in [x.dataset for (x, creating_job) in creating_jobs if creating_job.workflow_invocation_step and creating_job.workflow_invocation_step.workflow_invocation == wfi]:
                    safe_to_delete = True
                    for job_to_check in [d_j.job for d_j in input_dataset.dependent_jobs]:
                        if job_to_check != job and job_to_check.state not in [job.states.OK, job.states.DELETED]:
                            log.debug(""Workflow Intermediates cleanup attempted, but non-terminal state '%s' detected for job %s"" % (job_to_check.state, job_to_check.id))
                            safe_to_delete = False
                    if safe_to_delete:
                        # Support purging here too.
                        input_dataset.mark_deleted()
        else:
            # No workflow outputs defined, so we can't know what to delete.
            # We could make this work differently in the future
            pass","1. Use `sa_session.refresh()` to get the latest state of the object.
2. Check if the job is active before deleting intermediate files.
3. Check if the input dataset is marked as a workflow output before deleting it."
"    def _workflow_to_dict_editor(self, trans, stored):
        """"""
        """"""
        workflow = stored.latest_workflow
        # Pack workflow data into a dictionary and return
        data = {}
        data['name'] = workflow.name
        data['steps'] = {}
        data['upgrade_messages'] = {}
        # For each step, rebuild the form and encode the state
        for step in workflow.steps:
            # Load from database representation
            module = module_factory.from_workflow_step( trans, step )
            if not module:
                step_annotation = self.get_item_annotation_obj( trans.sa_session, trans.user, step )
                annotation_str = """"
                if step_annotation:
                    annotation_str = step_annotation.annotation
                invalid_tool_form_html = """"""<div class=""toolForm tool-node-error"">
                                            <div class=""toolFormTitle form-row-error"">Unrecognized Tool: %s</div>
                                            <div class=""toolFormBody""><div class=""form-row"">
                                            The tool id '%s' for this tool is unrecognized.<br/><br/>
                                            To save this workflow, you will need to delete this step or enable the tool.
                                            </div></div></div>"""""" % (step.tool_id, step.tool_id)
                step_dict = {
                    'id': step.order_index,
                    'type': 'invalid',
                    'tool_id': step.tool_id,
                    'name': 'Unrecognized Tool: %s' % step.tool_id,
                    'tool_state': None,
                    'tooltip': None,
                    'tool_errors': [""Unrecognized Tool Id: %s"" % step.tool_id],
                    'data_inputs': [],
                    'data_outputs': [],
                    'form_html': invalid_tool_form_html,
                    'annotation': annotation_str,
                    'input_connections': {},
                    'post_job_actions': {},
                    'uuid': str(step.uuid),
                    'label': step.label or None,
                    'workflow_outputs': []
                }
                # Position
                step_dict['position'] = step.position
                # Add to return value
                data['steps'][step.order_index] = step_dict
                continue
            # Fix any missing parameters
            upgrade_message = module.check_and_update_state()
            if upgrade_message:
                # FIXME: Frontend should be able to handle workflow messages
                #        as a dictionary not just the values
                data['upgrade_messages'][step.order_index] = upgrade_message.values()
            # Get user annotation.
            step_annotation = self.get_item_annotation_obj( trans.sa_session, trans.user, step )
            annotation_str = """"
            if step_annotation:
                annotation_str = step_annotation.annotation
            # Pack attributes into plain dictionary
            step_dict = {
                'id': step.order_index,
                'type': module.type,
                'tool_id': module.get_tool_id(),
                'name': module.get_name(),
                'tool_state': module.get_state(),
                'tooltip': module.get_tooltip( static_path=url_for( '/static' ) ),
                'tool_errors': module.get_errors(),
                'data_inputs': module.get_data_inputs(),
                'data_outputs': module.get_data_outputs(),
                'form_html': module.get_config_form(),
                'annotation': annotation_str,
                'post_job_actions': {},
                'uuid': str(step.uuid) if step.uuid else None,
                'label': step.label or None,
                'workflow_outputs': []
            }
            # Connections
            input_connections = step.input_connections
            input_connections_type = {}
            multiple_input = {}  # Boolean value indicating if this can be mutliple
            if step.type is None or step.type == 'tool':
                # Determine full (prefixed) names of valid input datasets
                data_input_names = {}

                def callback( input, value, prefixed_name, prefixed_label ):
                    if isinstance( input, DataToolParameter ) or isinstance( input, DataCollectionToolParameter ):
                        data_input_names[ prefixed_name ] = True
                        multiple_input[ prefixed_name ] = input.multiple
                        if isinstance( input, DataToolParameter ):
                            input_connections_type[ input.name ] = ""dataset""
                        if isinstance( input, DataCollectionToolParameter ):
                            input_connections_type[ input.name ] = ""dataset_collection""
                visit_input_values( module.tool.inputs, module.state.inputs, callback )
                # Filter
                # FIXME: this removes connection without displaying a message currently!
                input_connections = [ conn for conn in input_connections if conn.input_name in data_input_names ]
                # post_job_actions
                pja_dict = {}
                for pja in step.post_job_actions:
                    pja_dict[pja.action_type + pja.output_name] = dict(
                        action_type=pja.action_type,
                        output_name=pja.output_name,
                        action_arguments=pja.action_arguments
                    )
                step_dict['post_job_actions'] = pja_dict
                # workflow outputs
                outputs = []
                for output in step.workflow_outputs:
                    outputs.append(output.output_name)
                step_dict['workflow_outputs'] = outputs
            # Encode input connections as dictionary
            input_conn_dict = {}
            for conn in input_connections:
                input_type = ""dataset""
                if conn.input_name in input_connections_type:
                    input_type = input_connections_type[ conn.input_name ]
                conn_dict = dict( id=conn.output_step.order_index, output_name=conn.output_name, input_type=input_type )
                if conn.input_name in multiple_input:
                    if conn.input_name in input_conn_dict:
                        input_conn_dict[ conn.input_name ].append( conn_dict )
                    else:
                        input_conn_dict[ conn.input_name ] = [ conn_dict ]
                else:
                    input_conn_dict[ conn.input_name ] = conn_dict
            step_dict['input_connections'] = input_conn_dict
            # Position
            step_dict['position'] = step.position
            # Add to return value
            data['steps'][step.order_index] = step_dict
        return data","1. Use prepared statements instead of concatenation to prevent SQL injection.
2. Sanitize user input to prevent cross-site scripting (XSS) attacks.
3. Use strong passwords for all accounts, and enable two-factor authentication where possible."
"    def get_feature_meta(column, preprocessing_parameters, backend):
        idx2str, str2idx, str2freq, max_size, _, _, _ = create_vocabulary(
            column,
            preprocessing_parameters['tokenizer'],
            num_most_frequent=preprocessing_parameters['most_common'],
            lowercase=preprocessing_parameters['lowercase'],
            processor=backend.df_engine,
        )
        return {
            'idx2str': idx2str,
            'str2idx': str2idx,
            'str2freq': str2freq,
            'vocab_size': len(str2idx),
            'max_set_size': max_size
        }","1. Use `preprocessing_parameters` to sanitize user input.
2. Use `backend.df_engine` to process data in a secure way.
3. Use `create_vocabulary` to create a vocabulary of known words and phrases."
"    def get_feature_meta(column, preprocessing_parameters, backend):
        idx2str, str2idx, str2freq, _, _, _, _ = create_vocabulary(
            column, 'stripped',
            num_most_frequent=preprocessing_parameters['most_common'],
            lowercase=preprocessing_parameters['lowercase'],
            add_padding=False,
            processor=backend.df_engine
        )
        return {
            'idx2str': idx2str,
            'str2idx': str2idx,
            'str2freq': str2freq,
            'vocab_size': len(str2idx)
        }","1. Use `df_engine.create_vocabulary()` instead of `create_vocabulary()` to avoid leaking data to the user.
2. Use `df_engine.add_padding()` to pad the sequences to the same length to avoid information leakage.
3. Use `df_engine.get_vocab_size()` to get the vocabulary size instead of hard-coding it to avoid leaking data to the user."
"    def get_feature_meta(column, preprocessing_parameters, backend):
        idx2str, str2idx, str2freq, max_length, _, _, _ = create_vocabulary(
            column, preprocessing_parameters['tokenizer'],
            lowercase=preprocessing_parameters['lowercase'],
            num_most_frequent=preprocessing_parameters['most_common'],
            vocab_file=preprocessing_parameters['vocab_file'],
            unknown_symbol=preprocessing_parameters['unknown_symbol'],
            padding_symbol=preprocessing_parameters['padding_symbol'],
            processor=backend.df_engine
        )
        max_length = min(
            preprocessing_parameters['sequence_length_limit'],
            max_length
        )
        return {
            'idx2str': idx2str,
            'str2idx': str2idx,
            'str2freq': str2freq,
            'vocab_size': len(idx2str),
            'max_sequence_length': max_length
        }","1. Use `secure_filename` to sanitize the input file name.
2. Use `os.makedirs` with the `exist_ok` flag to create the output directory if it does not exist.
3. Use `json.dump` with the `indent` parameter to make the JSON output more readable."
"    def get_feature_meta(column, preprocessing_parameters, backend):
        idx2str, str2idx, str2freq, max_size, _, _, _ = create_vocabulary(
            column,
            preprocessing_parameters['tokenizer'],
            num_most_frequent=preprocessing_parameters['most_common'],
            lowercase=preprocessing_parameters['lowercase'],
            processor=backend.df_engine
        )
        return {
            'idx2str': idx2str,
            'str2idx': str2idx,
            'str2freq': str2freq,
            'vocab_size': len(str2idx),
            'max_set_size': max_size
        }","1. Use `pd.read_csv` with `parse_dates` to parse dates correctly.
2. Use `pd.read_csv` with `infer_datetime_format` to infer the datetime format.
3. Use `pd.read_csv` with `index_col` to set the column to use as the index."
"    def get_feature_meta(column, preprocessing_parameters, backend):
        tf_meta = TextFeatureMixin.feature_meta(
            column, preprocessing_parameters, backend
        )
        (
            char_idx2str,
            char_str2idx,
            char_str2freq,
            char_max_len,
            char_pad_idx,
            char_pad_symbol,
            char_unk_symbol,
            word_idx2str,
            word_str2idx,
            word_str2freq,
            word_max_len,
            word_pad_idx,
            word_pad_symbol,
            word_unk_symbol,
        ) = tf_meta
        char_max_len = min(
            preprocessing_parameters['char_sequence_length_limit'],
            char_max_len
        )
        word_max_len = min(
            preprocessing_parameters['word_sequence_length_limit'],
            word_max_len
        )
        return {
            'char_idx2str': char_idx2str,
            'char_str2idx': char_str2idx,
            'char_str2freq': char_str2freq,
            'char_vocab_size': len(char_idx2str),
            'char_max_sequence_length': char_max_len,
            'char_pad_idx': char_pad_idx,
            'char_pad_symbol': char_pad_symbol,
            'char_unk_symbol': char_unk_symbol,
            'word_idx2str': word_idx2str,
            'word_str2idx': word_str2idx,
            'word_str2freq': word_str2freq,
            'word_vocab_size': len(word_idx2str),
            'word_max_sequence_length': word_max_len,
            'word_pad_idx': word_pad_idx,
            'word_pad_symbol': word_pad_symbol,
            'word_unk_symbol': word_unk_symbol,
        }","1. Use `tf.io.gfile.GFile` instead of `open` to open files, as it is more secure.
2. Use `tf.io.gfile.glob` instead of `os.walk` to iterate over files, as it is more secure.
3. Use `tf.io.gfile.delete` instead of `os.remove` to delete files, as it is more secure."
"    def get_feature_meta(column, preprocessing_parameters, backend):
        tokenizer = get_from_registry(
            preprocessing_parameters['tokenizer'],
            tokenizer_registry
        )()
        max_length = 0
        for timeseries in column:
            processed_line = tokenizer(timeseries)
            max_length = max(max_length, len(processed_line))
        max_length = min(
            preprocessing_parameters['timeseries_length_limit'],
            max_length
        )

        return {'max_timeseries_length': max_length}","1. Use a secure random number generator to generate the max_length value.
2. Sanitize the input data to prevent injection attacks.
3. Use proper error handling to prevent leaking sensitive information."
"    def decoder_teacher_forcing(
            self,
            encoder_output,
            target=None,
            encoder_end_state=None
    ):
        # ================ Setup ================
        batch_size = encoder_output.shape[0]

        # Prepare target for decoding
        target_sequence_length = sequence_length_2D(target)
        start_tokens = tf.tile([self.GO_SYMBOL], [batch_size])
        end_tokens = tf.tile([self.END_SYMBOL], [batch_size])
        if self.is_timeseries:
            start_tokens = tf.cast(start_tokens, tf.float32)
            end_tokens = tf.cast(end_tokens, tf.float32)
        targets_with_go_and_eos = tf.concat([
            tf.expand_dims(start_tokens, 1),
            target,  # right now cast to tf.int32, fails if tf.int64
            tf.expand_dims(end_tokens, 1)], 1)
        target_sequence_length_with_eos = target_sequence_length + 1

        # Decoder Embeddings
        decoder_emb_inp = self.decoder_embedding(targets_with_go_and_eos)

        # Setting up decoder memory from encoder output
        if self.attention_mechanism is not None:
            encoder_sequence_length = sequence_length_3D(encoder_output)
            self.attention_mechanism.setup_memory(
                encoder_output,
                memory_sequence_length=encoder_sequence_length
            )

        decoder_initial_state = self.build_decoder_initial_state(
            batch_size,
            encoder_state=encoder_end_state,
            dtype=tf.float32
        )

        decoder = tfa.seq2seq.BasicDecoder(
            self.decoder_rnncell,
            sampler=self.sampler,
            output_layer=self.dense_layer
        )

        # BasicDecoderOutput
        outputs, final_state, generated_sequence_lengths = decoder(
            decoder_emb_inp,
            initial_state=decoder_initial_state,
            sequence_length=target_sequence_length_with_eos
        )

        logits = outputs.rnn_output
        # mask = tf.sequence_mask(
        #    generated_sequence_lengths,
        #    maxlen=tf.shape(logits)[1],
        #    dtype=tf.float32
        # )
        # logits = logits * mask[:, :, tf.newaxis]

        # append a trailing 0, useful for
        # those datapoints that reach maximum length
        # and don't have a eos at the end
        logits = tf.pad(
            logits,
            [[0, 0], [0, 1], [0, 0]]
        )

        return logits  # , outputs, final_state, generated_sequence_lengths","1. Use secure attention mechanism such as Transformer-XL instead of Bahdanau attention mechanism.
2. Use TensorFlow's built-in attention masking to prevent leaking of information from future timesteps.
3. Use TensorFlow's built-in function tf.one_hot to convert labels to one-hot vectors instead of manually creating one-hot vectors."
"    def logits(
            self,
            inputs,
            target=None,
            training=None
    ):
        if training:
            return self.decoder_obj._logits_training(
                inputs,
                target=tf.cast(target, dtype=tf.int32),
                training=training
            )
        else:
            return inputs","1. **Use `tf.convert_to_tensor` to explicitly cast the target to `tf.int32`.** This will prevent the model from accepting inputs of other types, which could lead to security vulnerabilities.
2. **Use `tf.debugging.assert_equal` to check that the target is a valid tensor.** This will help to catch errors early on and prevent the model from being trained on invalid data.
3. **Use `tf.layers.dense` with `activation=None` to ensure that the logits are not scaled or normalized.** This will prevent the model from being fooled by adversarial examples."
"def calibration_plot(
        fraction_positives,
        mean_predicted_values,
        algorithm_names=None,
        filename=None
):
    assert len(fraction_positives) == len(mean_predicted_values)

    sns.set_style('whitegrid')

    colors = plt.get_cmap('tab10').colors

    num_algorithms = len(fraction_positives)

    plt.figure(figsize=(9, 9))
    plt.grid(which='both')
    plt.grid(which='minor', alpha=0.5)
    plt.grid(which='major', alpha=0.75)

    plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')

    for i in range(num_algorithms):
        # ax1.plot(mean_predicted_values[i], fraction_positives[i],
        #         label=algorithms[i] if algorithm_names is not None and i < len(algorithms) else '')

        # sns.tsplot(mean_predicted_values[i], fraction_positives[i], ax=ax1, color=colors[i])

        assert len(mean_predicted_values[i]) == len(fraction_positives[i])
        order = min(3, len(mean_predicted_values[i] - 1))

        sns.regplot(mean_predicted_values[i], fraction_positives[i],
                    order=order, x_estimator=np.mean, color=colors[i],
                    marker='o', scatter_kws={'s': 40},
                    label=algorithm_names[
                        i] if algorithm_names is not None and i < len(
                        algorithm_names) else '')


    ticks = np.linspace(0.0, 1.0, num=11)
    plt.xlim([-0.05, 1.05])
    plt.xticks(ticks)
    plt.xlabel('Predicted probability')
    plt.ylabel('Observed probability')
    plt.ylim([-0.05, 1.05])
    plt.yticks(ticks)
    plt.legend(loc='lower right')
    plt.title('Calibration (reliability curve)')

    plt.tight_layout()
    ludwig.contrib.contrib_command(""visualize_figure"", plt.gcf())
    if filename:
        plt.savefig(filename)
    else:
        plt.show()","1. Use `np.clip` to clip the predicted probabilities to the range `[0, 1]`.
2. Use `np.random.seed()` to set the random seed, so that the results are reproducible.
3. Use `plt.tight_layout()` to ensure that the plot is properly aligned."
"    def train_online(
            self,
            data_df=None,
            data_csv=None,
            data_dict=None,
            batch_size=None,
            learning_rate=None,
            regularization_lambda=None,
            dropout_rate=None,
            bucketing_field=None,
            gpus=None,
            gpu_fraction=1,
            logging_level=logging.ERROR,
    ):
        """"""This function is used to perform one epoch of training of the model 
        on the specified dataset.

        # Inputs

        :param data_df: (DataFrame) dataframe containing data.
        :param data_csv: (string) input data CSV file.
        :param data_dict: (dict) input data dictionary. It is expected to 
               contain one key for each field and the values have to be lists of 
               the same length. Each index in the lists corresponds to one 
               datapoint. For example a data set consisting of two datapoints 
               with a text and a class may be provided as the following dict 
               ``{'text_field_name': ['text of the first datapoint', text of the
               second datapoint'], 'class_filed_name': ['class_datapoints_1', 
               'class_datapoints_2']}`.
        :param batch_size: (int) the batch size to use for training. By default 
               it's the one specified in the model definition.
        :param learning_rate: (float) the learning rate to use for training. By
               default the values is the one specified in the model definition.
        :param regularization_lambda: (float) the regularization lambda
               parameter to use for training. By default the values is the one
               specified in the model definition.
        :param dropout_rate: (float) the dropout rate to use for training. By
               default the values is the one specified in the model definition.
        :param bucketing_field: (string) the bucketing field to use for
               bucketing the data. By default the values is one specified in the
               model definition.
        :param gpus: (string, default: `None`) list of GPUs to use (it uses the
               same syntax of CUDA_VISIBLE_DEVICES)
        :param gpu_fraction: (float, default `1.0`) fraction of GPU memory to
               initialize the process with
        :param logging_level: (int, default: `logging.ERROR`) logging level to
               use for logging. Use logging constants like `logging.DEBUG`,
               `logging.INFO` and `logging.ERROR`. By default only errors will
               be printed.

        There are three ways to provide data: by dataframes using the `data_df`
        parameter, by CSV using the `data_csv` parameter and by dictionary,
        using the `data_dict` parameter.

        The DataFrame approach uses data previously obtained and put in a
        dataframe, the CSV approach loads data from a CSV file, while dict
        approach uses data organized by keys representing columns and values
        that are lists of the datapoints for each. For example a data set
        consisting of two datapoints with a text and a class may be provided as
        the following dict ``{'text_field_name}: ['text of the first datapoint',
        text of the second datapoint'], 'class_filed_name':
        ['class_datapoints_1', 'class_datapoints_2']}`.
        """"""
        logging.getLogger().setLevel(logging_level)
        if logging_level in {logging.WARNING, logging.ERROR, logging.CRITICAL}:
            set_disable_progressbar(True)

        if (self.model is None or self.model_definition is None
                or self.train_set_metadata is None):
            raise ValueError('Model has not been initialized or loaded')

        if data_df is None:
            data_df = self._read_data(data_csv, data_dict)

        if batch_size is None:
            batch_size = self.model_definition['training']['batch_size']
        if learning_rate is None:
            learning_rate = self.model_definition['training']['learning_rate']
        if regularization_lambda is None:
            regularization_lambda = self.model_definition['training'][
                'regularization_lambda'
            ]
        if dropout_rate is None:
            dropout_rate = self.model_definition['training']['dropout_rate'],
        if bucketing_field is None:
            bucketing_field = self.model_definition['training'][
                'bucketing_field'
            ]

        logging.debug('Preprocessing {} datapoints'.format(len(data_df)))
        features_to_load = (self.model_definition['input_features'] +
                            self.model_definition['output_features'])
        preprocessed_data = build_data(
            data_df,
            features_to_load,
            self.train_set_metadata,
            self.model_definition['preprocessing']
        )
        replace_text_feature_level(
            self.model_definition['input_features'] +
            self.model_definition['output_features'],
            [preprocessed_data]
        )
        dataset = Dataset(
            preprocessed_data,
            self.model_definition['input_features'],
            self.model_definition['output_features'],
            None
        )

        logging.debug('Training batch')
        self.model.train_online(
            dataset,
            batch_size=batch_size,
            learning_rate=learning_rate,
            regularization_lambda=regularization_lambda,
            dropout_rate=dropout_rate,
            bucketing_field=bucketing_field,
            gpus=gpus,
            gpu_fraction=gpu_fraction)","1. Use `set_disable_progressbar` to disable progress bar when logging level is `WARNING`, `ERROR` or `CRITICAL`.
2. Check if `model`, `model_definition` and `train_set_metadata` are not None before training.
3. Use `replace_text_feature_level` to replace text feature level."
"    def add_feature_data(
            feature,
            dataset_df,
            data,
            metadata,
            preprocessing_parameters
    ):
        set_default_value(
            feature,
            'in_memory',
            preprocessing_parameters['in_memory']
        )

        if ('height' in preprocessing_parameters or
                'width' in preprocessing_parameters):
            should_resize = True
            try:
                provided_height = int(preprocessing_parameters[HEIGHT])
                provided_width = int(preprocessing_parameters[WIDTH])
            except ValueError as e:
                raise ValueError(
                    'Image height and width must be set and have '
                    'positive integer values: ' + str(e)
                )
            if (provided_height <= 0 or provided_width <= 0):
                raise ValueError(
                    'Image height and width must be positive integers'
                )
        else:
            should_resize = False

        csv_path = os.path.dirname(os.path.abspath(dataset_df.csv))

        num_images = len(dataset_df)

        height = 0
        width = 0
        num_channels = 1

        if num_images > 0:
            # here if a width and height have not been specified
            # we assume that all images have the same wifth and im_height
            # thus the width and height of the first one are the same
            # of all the other ones
            first_image = imread(
                os.path.join(csv_path, dataset_df[feature['name']][0])
            )
            height = first_image.shape[0]
            width = first_image.shape[1]

            if first_image.ndim == 2:
                num_channels = 1
            else:
                num_channels = first_image.shape[2]

        if should_resize:
            height = provided_height
            width = provided_width

        metadata[feature['name']]['preprocessing']['height'] = height
        metadata[feature['name']]['preprocessing']['width'] = width
        metadata[feature['name']]['preprocessing'][
            'num_channels'] = num_channels

        if feature['in_memory']:
            data[feature['name']] = np.empty(
                (num_images, height, width, num_channels),
                dtype=np.int8
            )
            for i in range(len(dataset_df)):
                filename = os.path.join(
                    csv_path,
                    dataset_df[feature['name']][i]
                )
                img = imread(filename)
                if img.ndim == 2:
                    img = img.reshape((img.shape[0], img.shape[1], 1))
                if should_resize:
                    img = resize_image(
                        img,
                        (height, width),
                        preprocessing_parameters['resize_method']
                    )
                data[feature['name']][i, :, :, :] = img
        else:
            data_fp = os.path.splitext(dataset_df.csv)[0] + '.hdf5'
            mode = 'w'
            if os.path.isfile(data_fp):
                mode = 'r+'
            with h5py.File(data_fp, mode) as h5_file:
                image_dataset = h5_file.create_dataset(
                    feature['name'] + '_data',
                    (num_images, height, width, num_channels),
                    dtype=np.uint8
                )
                for i in range(len(dataset_df)):
                    filename = os.path.join(
                        csv_path,
                        dataset_df[feature['name']][i]
                    )
                    img = imread(filename)
                    if img.ndim == 2:
                        img = img.reshape((img.shape[0], img.shape[1], 1))
                    if should_resize:
                        img = resize_image(
                            img,
                            (height, width),
                            preprocessing_parameters['resize_method'],
                        )

                    image_dataset[i, :height, :width, :] = img

            data[feature['name']] = np.arange(num_images)","1. Use `os.path.join` to concatenate paths instead of string concatenation.
2. Use `imread` to read images instead of `cv2.imread`.
3. Use `np.array` to create arrays instead of `np.empty`."
"    def __init__(self, feature):
        super().__init__(feature)

        self.height = 0
        self.width = 0
        self.num_channels = 0

        self.in_memory = True
        self.data_hdf5_fp = ''

        self.encoder = 'stacked_cnn'

        encoder_parameters = self.overwrite_defaults(feature)

        self.encoder_obj = self.get_image_encoder(encoder_parameters)","1. Use `assert` statements to validate the input arguments.
2. Use `encryption` to protect sensitive data.
3. Use `access control` to restrict who can access the data."
"    def update_model_definition_with_metadata(
            input_feature,
            feature_metadata,
            *args,
            **kwargs
    ):
        for dim in ['height', 'width', 'num_channels']:
            input_feature[dim] = feature_metadata['preprocessing'][dim]
        input_feature['data_hdf5_fp'] = (
            kwargs['model_definition']['data_hdf5_fp']
        )","1. Use `kwargs` instead of `*args`, `**kwargs` to avoid passing arguments by position.
2. Use `input_feature.update(feature_metadata)` instead of `input_feature = feature_metadata` to avoid overwriting existing keys in `input_feature`.
3. Use `os.path.join()` to concatenate paths instead of concatenating them manually."
"def preprocess_for_prediction(
        model_path,
        split,
        dataset_type='generic',
        data_csv=None,
        data_hdf5=None,
        train_set_metadata=None,
        only_predictions=False
):
    """"""Preprocesses the dataset to parse it into a format that is usable by the
    Ludwig core
        :param model_path: The input data that is joined with the model
               hyperparameter file to create the model definition file
        :type model_path: Str
        :param dataset_type: Generic
        :type: Str
        :param split: Splits the data into the train and test sets
        :param data_csv: The CSV input data file
        :param data_hdf5: The hdf5 data file if there is no csv data file
        :param train_set_metadata: Train set metadata for the input features
        :param only_predictions: If False does not load output features
        :returns: Dataset, Train set metadata
        """"""
    model_definition = load_json(
        os.path.join(model_path, MODEL_HYPERPARAMETERS_FILE_NAME)
    )
    preprocessing_params = merge_dict(
        default_preprocessing_parameters,
        model_definition['preprocessing']
    )

    # Check if hdf5 and json already exist
    if data_csv is not None:
        data_hdf5_fp = os.path.splitext(data_csv)[0] + '.hdf5'
        if os.path.isfile(data_hdf5_fp):
            logging.info(
                'Found hdf5 with the same filename of the csv, using it instead'
            )
            data_csv = None
            data_hdf5 = data_hdf5_fp

    # Load data
    _, _, build_dataset, _ = get_dataset_fun(dataset_type)
    train_set_metadata = load_metadata(train_set_metadata)
    features = (model_definition['input_features'] +
                ([] if only_predictions
                 else model_definition['output_features']))
    if split == 'full':
        if data_hdf5 is not None:
            dataset = load_data(
                data_hdf5,
                model_definition['input_features'],
                [] if only_predictions else model_definition['output_features'],
                split_data=False, shuffle_training=False
            )
        else:
            dataset, train_set_metadata = build_dataset(
                data_csv,
                features,
                preprocessing_params,
                train_set_metadata=train_set_metadata
            )
    else:
        if data_hdf5 is not None:
            training, test, validation = load_data(
                data_hdf5,
                model_definition['input_features'],
                [] if only_predictions else model_definition['output_features'],
                shuffle_training=False
            )

            if split == 'training':
                dataset = training
            elif split == 'validation':
                dataset = validation
            else:  # if split == 'test':
                dataset = test
        else:
            dataset, train_set_metadata = build_dataset(
                data_csv,
                features,
                preprocessing_params,
                train_set_metadata=train_set_metadata
            )

    replace_text_feature_level(
        model_definition['input_features'] +
        ([] if only_predictions else model_definition['output_features']),
        [dataset]
    )

    dataset = Dataset(
        dataset,
        model_definition['input_features'],
        [] if only_predictions else model_definition['output_features'],
        data_hdf5,
    )

    return dataset, train_set_metadata","1. Use `os.path.join` to concatenate paths instead of string concatenation.
2. Use `load_json` to load JSON files instead of `open`.
3. Use `os.path.isfile` to check if a file exists instead of `os.path.exists`."
"def save_csv(data_fp, data):
    writer = csv.writer(open(data_fp, 'w'))
    for row in data:
        if not isinstance(row, collections.Iterable) or isinstance(row, str):
            row = [row]
        writer.writerow(row)","1. Use `csv.DictWriter` instead of `csv.writer` to ensure that each row is a valid CSV record.
2. Use `csv.QUOTE_NONNUMERIC` to quote all non-numeric values in the CSV file.
3. Use `os.fchmod` to set the file mode to `0600` (read-only for the owner) after the file is created."
"def replace_text_feature_level(model_definition, datasets):
    for feature in (model_definition['input_features'] +
                    model_definition['output_features']):
        if feature['type'] == TEXT:
            for dataset in datasets:
                dataset[feature['name']] = dataset[
                    '{}_{}'.format(
                        feature['name'],
                        feature['level']
                    )
                ]
                for level in ('word', 'char'):
                    del dataset[
                        '{}_{}'.format(
                            feature['name'],
                            level)
                    ]","1. Use `enumerate` instead of `for` loop to iterate over a list.
2. Use `dict.get()` instead of `dict[key]` to access a dictionary value.
3. Use `json.dumps()` to serialize a dictionary to a string."
"def get_elements_by_categories(element_bicats, elements=None, doc=None):
    # if source elements is provided
    if elements:
        return [x for x in elements
                if get_builtincategory(x.Category.Name)
                in element_bicats]

    # otherwise collect from model
    cat_filters = [DB.ElementCategoryFilter(x) for x in element_bicats]
    elcats_filter = \\
        DB.LogicalOrFilter(framework.List[DB.ElementFilter](cat_filters))
    return DB.FilteredElementCollector(doc or HOST_APP.doc)\\
             .WherePasses(elcats_filter)\\
             .WhereElementIsNotElementType()\\
             .ToElements()","1. Use `get_builtincategory` to get the builtin category instead of using `x.Category.Name` directly.
2. Use `DB.FilteredElementCollector` to filter elements instead of using a list comprehension.
3. Use `WhereElementIsNotElementType` to filter out element types."
"    def save_options(self, sender, args):
        self._config.halftone = self.halftone.IsChecked
        self._config.transparency = self.transparency.IsChecked
        self._config.proj_line_color = self.proj_line_color.IsChecked
        self._config.proj_line_pattern = self.proj_line_pattern.IsChecked
        self._config.proj_line_weight = self.proj_line_weight.IsChecked
        self._config.proj_fill_color = self.proj_fill_color.IsChecked
        self._config.proj_fill_pattern = self.proj_fill_pattern.IsChecked
        self._config.proj_fill_pattern_visibility = \\
            self.proj_fill_pattern_visibility.IsChecked
        self._config.proj_bg_fill_color = self.proj_bg_fill_color.IsChecked
        self._config.proj_bg_fill_pattern = self.proj_bg_fill_pattern.IsChecked
        self._config.proj_bg_fill_pattern_visibility = \\
            self.proj_bg_fill_pattern_visibility.IsChecked

        self._config.cut_line_color = self.cut_line_color.IsChecked
        self._config.cut_line_pattern = self.cut_line_pattern.IsChecked
        self._config.cut_line_weight = self.cut_line_weight.IsChecked
        self._config.cut_fill_color = self.cut_fill_color.IsChecked
        self._config.cut_fill_pattern = self.cut_fill_pattern.IsChecked
        self._config.cut_fill_pattern_visibility = \\
            self.cut_fill_pattern_visibility.IsChecked
        self._config.cut_bg_fill_color = self.cut_bg_fill_color.IsChecked
        self._config.cut_bg_fill_pattern = self.cut_bg_fill_pattern.IsChecked
        self._config.cut_bg_fill_pattern_visibility = \\
            self.cut_bg_fill_pattern_visibility.IsChecked

        self._config.dim_override = self.dim_override.IsChecked
        self._config.dim_textposition = self.dim_textposition.IsChecked
        self._config.dim_above = self.dim_above.IsChecked
        self._config.dim_below = self.dim_below.IsChecked
        self._config.dim_prefix = self.dim_prefix.IsChecked
        self._config.dim_suffix = self.dim_suffix.IsChecked

        script.save_config()
        self.Close()","1. Use `IsEnabled` to disable controls that are not relevant to the current user.
2. Use `IsVisible` to hide controls that are not relevant to the current user.
3. Use `IsLocked` to lock controls that the current user should not be able to modify."
"def select_sheets(title='Select Sheets',
                  button_name='Select',
                  width=DEFAULT_INPUTWINDOW_WIDTH,
                  multiple=True,
                  filterfunc=None,
                  doc=None):
    """"""Standard form for selecting sheets.

    Sheets are grouped into sheet sets and sheet set can be selected from
    a drop down box at the top of window.

    Args:
        title (str, optional): list window title
        button_name (str, optional): list window button caption
        width (int, optional): width of list window
        multiselect (bool, optional):
            allow multi-selection (uses check boxes). defaults to True
        filterfunc (function):
            filter function to be applied to context items.
        doc (DB.Document, optional):
            source document for sheets; defaults to active document

    Returns:
        list[DB.ViewSheet]: list of selected sheets

    Example:
        >>> from pyrevit import forms
        >>> forms.select_sheets()
        ... [<Autodesk.Revit.DB.ViewSheet object>,
        ...  <Autodesk.Revit.DB.ViewSheet object>]
    """"""
    doc = doc or HOST_APP.doc
    all_ops = dict()
    all_sheets = DB.FilteredElementCollector(doc) \\
                   .OfClass(DB.ViewSheet) \\
                   .WhereElementIsNotElementType() \\
                   .ToElements()

    if filterfunc:
        all_sheets = filter(filterfunc, all_sheets)

    all_sheets_ops = sorted([SheetOption(x) for x in all_sheets],
                            key=lambda x: x.number)
    all_ops['All Sheets'] = all_sheets_ops

    sheetsets = revit.query.get_sheet_sets(doc)
    for sheetset in sheetsets:
        sheetset_sheets = sheetset.Views
        if filterfunc:
            sheetset_sheets = filter(filterfunc, sheetset_sheets)
        sheetset_ops = sorted([SheetOption(x) for x in sheetset_sheets],
                              key=lambda x: x.number)
        all_ops[sheetset.Name] = sheetset_ops

    # ask user for multiple sheets
    selected_sheets = SelectFromList.show(
        all_ops,
        title=title,
        group_selector_title='Sheet Sets:',
        button_name=button_name,
        width=width,
        multiselect=multiple,
        checked_only=True
        )

    return selected_sheets","1. Use `pyrevit.security.ensure_loaded()` to ensure that the Revit API is loaded securely.
2. Use `pyrevit.security.ensure_document()` to ensure that the document is loaded securely.
3. Use `pyrevit.security.ensure_selection()` to ensure that the selection is loaded securely."
"def dependent(func):
    func.is_dependent = True
    return func","1. Use `functools.wraps` to preserve the metadata of the decorated function.
2. Add a `@staticmethod` decorator to make the function static.
3. Use `@property` to create a read-only property."
"def notdependent(func):
    func.is_dependent = False
    return func","1. Use `functools.wraps` to preserve the original function metadata.
2. Make the `is_dependent` attribute private.
3. Consider using `functools.cached_property` to cache the result of the function call."
"def copy_func(f, workset_name):
    new_funcname = '{}_{}'.format(f.func_name, workset_name)
    new_func = \\
        types.FunctionType(f.func_code,
                           f.func_globals,
                           new_funcname,
                           tuple([workset_name]),
                           f.func_closure)

    # set the docstring
    new_func.__doc__ = 'Remove All Elements on Workset ""{}""'\\
                       .format(workset_name)
    new_func.is_dependent = False
    return new_func","1. Use `functools.wraps` to preserve the original function's metadata.
2. Use `inspect.getargspec` to get the function's argument names and defaults.
3. Use `functools.partial` to create a new function with the specified arguments."
"def get_worksetcleaners():
    workset_funcs = []

    # copying functions is not implemented in IronPython 2.7.3
    if compat.IRONPY273:
        return workset_funcs

    # if model is workshared, get a list of current worksets
    if revit.doc.IsWorkshared:
        cl = DB.FilteredWorksetCollector(revit.doc)
        worksetlist = cl.OfKind(DB.WorksetKind.UserWorkset)
        # duplicate the workset element remover function for each workset
        for workset in worksetlist:
            workset_funcs.append(copy_func(template_workset_remover,
                                           workset.Name))

    return workset_funcs","1. Use `functools.partial` to create a copy of the function with the workset name passed in as a parameter. This will prevent the function from being called with an arbitrary workset name.
2. Use `inspect.isfunction` to check if the function being passed to `copy_func` is actually a function. This will prevent the function from being called with an invalid argument.
3. Use `sys.version_info` to check if the current version of IronPython is 2.7.3 or higher. If it is not, then return an empty list instead of trying to copy the function."
"    def __init__(self, name, default_state=False, wipe_action=None):
        self.name = name
        self.state = default_state
        self.wipe_action = wipe_action
        self.is_dependent = self.wipe_action.is_dependent","1. Use [functools.partial](https://docs.python.org/3/library/functools.html#functools.partial) to avoid exposing the `wipe_action` argument to the constructor.
2. Use [property](https://docs.python.org/3/library/property.html) to create a read-only `is_dependent` attribute.
3. Use [type hints](https://docs.python.org/3/library/typing.html) to annotate the arguments and return value of the `__init__` method."
"    def get(self, path: str) -> None:
        parts = path.split(""/"")
        component_name = parts[0]
        component_root = self._registry.get_component_path(component_name)
        if component_root is None:
            self.write(f""{path} not found"")
            self.set_status(404)
            return

        filename = ""/"".join(parts[1:])
        abspath = os.path.join(component_root, filename)

        LOGGER.debug(""ComponentRequestHandler: GET: %s -> %s"", path, abspath)

        try:
            with open(abspath, ""r"") as file:
                contents = file.read()
        except OSError as e:
            self.write(f""{path} read error: {e}"")
            self.set_status(404)
            return

        self.write(contents)
        self.set_header(""Content-Type"", self.get_content_type(abspath))

        self.set_extra_headers(path)","1. **Use `os.path.join()` to concatenate paths instead of string concatenation.** This will prevent directory traversal attacks.
2. **Check the return value of `os.path.exists()` before opening a file.** This will prevent opening files that don't exist and causing errors.
3. **Use `os.access()` to check if a file is readable before trying to read it.** This will prevent reading files that you don't have permission to access."
"    def reset(cls):
        """"""Reset credentials by removing file.

        This is used by `streamlit activate reset` in case a user wants
        to start over.
        """"""
        Credentials._singleton = None
        c = Credentials()
        try:
            os.remove(c._conf_file)
        except OSError as e:
            LOGGER.error(""Error removing credentials file: %s"" % e)","1. Use `contextlib.closing` to ensure that the file is closed after it is used.
2. Use `os.chmod` to set the permissions of the file to 0600, which only allows the owner to read and write to it.
3. Use `logging.exception` to log any errors that occur when removing the file."
"def _open_binary_stream(uri, mode, transport_params):
    """"""Open an arbitrary URI in the specified binary mode.

    Not all modes are supported for all protocols.

    :arg uri: The URI to open.  May be a string, or something else.
    :arg str mode: The mode to open with.  Must be rb, wb or ab.
    :arg transport_params: Keyword argumens for the transport layer.
    :returns: A file object and the filename
    :rtype: tuple
    """"""
    if mode not in ('rb', 'rb+', 'wb', 'wb+', 'ab', 'ab+'):
        #
        # This should really be a ValueError, but for the sake of compatibility
        # with older versions, which raise NotImplementedError, we do the same.
        #
        raise NotImplementedError('unsupported mode: %r' % mode)

    if isinstance(uri, six.string_types):
        # this method just routes the request to classes handling the specific storage
        # schemes, depending on the URI protocol in `uri`
        filename = uri.split('/')[-1]
        parsed_uri = _parse_uri(uri)

        if parsed_uri.scheme == ""file"":
            fobj = io.open(parsed_uri.uri_path, mode)
            return fobj, filename
        elif parsed_uri.scheme in smart_open_ssh.SCHEMES:
            fobj = smart_open_ssh.open(
                parsed_uri.uri_path,
                mode,
                host=parsed_uri.host,
                user=parsed_uri.user,
                port=parsed_uri.port,
                password=parsed_uri.password,
                transport_params=transport_params,
            )
            return fobj, filename
        elif parsed_uri.scheme in smart_open_s3.SUPPORTED_SCHEMES:
            return _s3_open_uri(parsed_uri, mode, transport_params), filename
        elif parsed_uri.scheme == ""hdfs"":
            _check_kwargs(smart_open_hdfs.open, transport_params)
            return smart_open_hdfs.open(parsed_uri.uri_path, mode), filename
        elif parsed_uri.scheme == ""webhdfs"":
            kw = _check_kwargs(smart_open_webhdfs.open, transport_params)
            return smart_open_webhdfs.open(parsed_uri.uri_path, mode, **kw), filename
        elif parsed_uri.scheme.startswith('http'):
            #
            # The URI may contain a query string and fragments, which interfere
            # with our compressed/uncompressed estimation, so we strip them.
            #
            filename = P.basename(urlparse.urlparse(uri).path)
            kw = _check_kwargs(smart_open_http.open, transport_params)
            return smart_open_http.open(uri, mode, **kw), filename
        else:
            raise NotImplementedError(""scheme %r is not supported"", parsed_uri.scheme)
    elif hasattr(uri, 'read'):
        # simply pass-through if already a file-like
        # we need to return something as the file name, but we don't know what
        # so we probe for uri.name (e.g., this works with open() or tempfile.NamedTemporaryFile)
        # if the value ends with COMPRESSED_EXT, we will note it in _compression_wrapper()
        # if there is no such an attribute, we return ""unknown"" - this
        # effectively disables any compression
        filename = getattr(uri, 'name', 'unknown')
        return uri, filename
    else:
        raise TypeError(""don't know how to handle uri %r"" % uri)","1. Use `six.string_types` instead of `str` to check the type of `uri`.
2. Use `_check_kwargs` to check if the `transport_params` are valid.
3. Use `urlparse.urlparse` to parse the URI and get the filename."
"def _my_urlsplit(url):
    """"""This is a hack to prevent the regular urlsplit from splitting around question marks.

    A question mark (?) in a URL typically indicates the start of a
    querystring, and the standard library's urlparse function handles the
    querystring separately.  Unfortunately, question marks can also appear
    _inside_ the actual URL for some schemas like S3.

    Replaces question marks with newlines prior to splitting.  This is safe because:

    1. The standard library's urlsplit completely ignores newlines
    2. Raw newlines will never occur in innocuous URLs.  They are always URL-encoded.

    See Also
    --------
    https://github.com/python/cpython/blob/3.7/Lib/urllib/parse.py
    https://github.com/RaRe-Technologies/smart_open/issues/285
    """"""
    if '?' not in url:
        return urlparse.urlsplit(url, allow_fragments=False)

    sr = urlparse.urlsplit(url.replace('?', '\\n'), allow_fragments=False)
    return urlparse.SplitResult(sr.scheme, sr.netloc, sr.path.replace('\\n', '?'), '', '')","1. Use `urllib.parse.quote()` to encode the URL before splitting it.
2. Use `urllib.parse.unquote()` to decode the URL after splitting it.
3. Use `urllib.parse.urlunsplit()` to reconstruct the URL after splitting it."
"def _parse_uri(uri_as_string):
    """"""
    Parse the given URI from a string.

    Supported URI schemes are:

      * file
      * hdfs
      * http
      * https
      * s3
      * s3a
      * s3n
      * s3u
      * webhdfs

    .s3, s3a and s3n are treated the same way.  s3u is s3 but without SSL.

    Valid URI examples::

      * s3://my_bucket/my_key
      * s3://my_key:my_secret@my_bucket/my_key
      * s3://my_key:my_secret@my_server:my_port@my_bucket/my_key
      * hdfs:///path/file
      * hdfs://path/file
      * webhdfs://host:port/path/file
      * ./local/path/file
      * ~/local/path/file
      * local/path/file
      * ./local/path/file.gz
      * file:///home/user/file
      * file:///home/user/file.bz2
      * [ssh|scp|sftp]://username@host//path/file
      * [ssh|scp|sftp]://username@host/path/file

    """"""
    if os.name == 'nt':
        # urlsplit doesn't work on Windows -- it parses the drive as the scheme...
        if '://' not in uri_as_string:
            # no protocol given => assume a local file
            uri_as_string = 'file://' + uri_as_string

    parsed_uri = _my_urlsplit(uri_as_string)

    if parsed_uri.scheme == ""hdfs"":
        return _parse_uri_hdfs(parsed_uri)
    elif parsed_uri.scheme == ""webhdfs"":
        return _parse_uri_webhdfs(parsed_uri)
    elif parsed_uri.scheme in smart_open_s3.SUPPORTED_SCHEMES:
        return _parse_uri_s3x(parsed_uri)
    elif parsed_uri.scheme == 'file':
        return _parse_uri_file(parsed_uri.netloc + parsed_uri.path)
    elif parsed_uri.scheme in ('', None):
        return _parse_uri_file(uri_as_string)
    elif parsed_uri.scheme.startswith('http'):
        return Uri(scheme=parsed_uri.scheme, uri_path=uri_as_string)
    elif parsed_uri.scheme in smart_open_ssh.SCHEMES:
        return _parse_uri_ssh(parsed_uri)
    else:
        raise NotImplementedError(
            ""unknown URI scheme %r in %r"" % (parsed_uri.scheme, uri_as_string)
        )","1. Use `os.path.join()` to sanitize the path before parsing it.
2. Use `urllib.parse.urlparse()` to parse the URI instead of the custom `_my_urlsplit()` function.
3. Check for invalid characters in the URI and raise an exception if any are found."
"def open(uri, mode, min_part_size=WEBHDFS_MIN_PART_SIZE):
    """"""
    Parameters
    ----------
    min_part_size: int, optional
        For writing only.

    """"""
    if mode == 'rb':
        return BufferedInputBase(uri)
    elif mode == 'wb':
        return BufferedOutputBase(uri, min_part_size=min_part_size)
    else:
        raise NotImplementedError('webhdfs support for mode %r not implemented' % mode)","1. Use `os.path.expanduser()` to expand the user-supplied path before opening the file. This will prevent malicious users from tricking the program into opening a file outside of the intended directory.
2. Use `os.access()` to check if the user has permission to read or write the file before opening it. This will prevent malicious users from accessing files that they do not have permission to access.
3. Use `contextlib.closing()` to ensure that the file is closed properly when the program is finished with it. This will prevent the file from being left open and accessible to other users."
"    def __init__(self, uri):
        self._uri = uri

        payload = {""op"": ""OPEN"", ""offset"": 0}
        self._response = requests.get(""http://"" + self._uri, params=payload, stream=True)
        self._buf = b''","1. Use HTTPS instead of HTTP to protect data from being intercepted.
2. Set a unique secret for the `op` parameter to prevent unauthorized access.
3. Use `verify=False` when calling `requests.get()` to disable SSL certificate validation."
"    def __init__(self, uri_path, min_part_size=WEBHDFS_MIN_PART_SIZE):
        """"""
        Parameters
        ----------
        min_part_size: int, optional
            For writing only.

        """"""
        self.uri_path = uri_path
        self._closed = False
        self.min_part_size = min_part_size
        # creating empty file first
        payload = {""op"": ""CREATE"", ""overwrite"": True}
        init_response = requests.put(""http://"" + self.uri_path,
                                     params=payload, allow_redirects=False)
        if not init_response.status_code == httplib.TEMPORARY_REDIRECT:
            raise WebHdfsException(str(init_response.status_code) + ""\\n"" + init_response.content)
        uri = init_response.headers['location']
        response = requests.put(uri, data="""", headers={'content-type': 'application/octet-stream'})
        if not response.status_code == httplib.CREATED:
            raise WebHdfsException(str(response.status_code) + ""\\n"" + response.content)
        self.lines = []
        self.parts = 0
        self.chunk_bytes = 0
        self.total_size = 0

        #
        # This member is part of the io.BufferedIOBase interface.
        #
        self.raw = None","1. Use `requests.post()` instead of `requests.put()` to create a file.
2. Set `Content-Type` header to `application/octet-stream` when uploading data.
3. Use `self.close()` to close the file when you are done with it."
"    def _upload(self, data):
        payload = {""op"": ""APPEND""}
        init_response = requests.post(""http://"" + self.uri_path,
                                      params=payload, allow_redirects=False)
        if not init_response.status_code == httplib.TEMPORARY_REDIRECT:
            raise WebHdfsException(str(init_response.status_code) + ""\\n"" + init_response.content)
        uri = init_response.headers['location']
        response = requests.post(uri, data=data,
                                 headers={'content-type': 'application/octet-stream'})
        if not response.status_code == httplib.OK:
            raise WebHdfsException(str(response.status_code) + ""\\n"" + repr(response.content))","1. Use HTTPS instead of HTTP to prevent data from being intercepted.
2. Set a unique secret for the `Authorization` header to prevent unauthorized access.
3. Use a Content-Security-Policy header to restrict the types of content that can be loaded."
"    def __init__(self, msg=str()):
        self.msg = msg
        super(WebHdfsException, self).__init__(self.msg)","1. Use a more descriptive error message that includes the specific details of the error.
2. Sanitize the input data to prevent malicious users from injecting code into the application.
3. Use proper exception handling to prevent errors from crashing the application."
"def _parse_uri(uri_as_string):
    """"""
    Parse the given URI from a string.

    Supported URI schemes are:

      * file
      * hdfs
      * http
      * https
      * s3
      * s3a
      * s3n
      * s3u
      * webhdfs

    .s3, s3a and s3n are treated the same way.  s3u is s3 but without SSL.

    Valid URI examples::

      * s3://my_bucket/my_key
      * s3://my_key:my_secret@my_bucket/my_key
      * s3://my_key:my_secret@my_server:my_port@my_bucket/my_key
      * hdfs:///path/file
      * hdfs://path/file
      * webhdfs://host:port/path/file
      * ./local/path/file
      * ~/local/path/file
      * local/path/file
      * ./local/path/file.gz
      * file:///home/user/file
      * file:///home/user/file.bz2
    """"""
    if os.name == 'nt':
        # urlsplit doesn't work on Windows -- it parses the drive as the scheme...
        if '://' not in uri_as_string:
            # no protocol given => assume a local file
            uri_as_string = 'file://' + uri_as_string
    parsed_uri = urlsplit(uri_as_string, allow_fragments=False)

    if parsed_uri.scheme == ""hdfs"":
        return _parse_uri_hdfs(parsed_uri)
    elif parsed_uri.scheme == ""webhdfs"":
        return _parse_uri_webhdfs(parsed_uri)
    elif parsed_uri.scheme in smart_open_s3.SUPPORTED_SCHEMES:
        return _parse_uri_s3x(parsed_uri)
    elif parsed_uri.scheme in ('file', '', None):
        return _parse_uri_file(parsed_uri)
    elif parsed_uri.scheme.startswith('http'):
        return Uri(scheme=parsed_uri.scheme, uri_path=uri_as_string)
    else:
        raise NotImplementedError(
            ""unknown URI scheme %r in %r"" % (parsed_uri.scheme, uri_as_string)
        )","1. Use `urlparse` instead of `urlsplit` to correctly parse Windows paths.
2. Use `parse_qsl` to parse the query string and validate the parameters.
3. Use `os.path.join` to sanitize the path before using it."
"def _parse_uri_file(parsed_uri):
    assert parsed_uri.scheme in (None, '', 'file')
    uri_path = parsed_uri.netloc + parsed_uri.path
    # '~/tmp' may be expanded to '/Users/username/tmp'
    uri_path = os.path.expanduser(uri_path)

    if not uri_path:
        raise RuntimeError(""invalid file URI: %s"" % str(parsed_uri))

    return Uri(scheme='file', uri_path=uri_path)","1. Sanitize user input to prevent against path traversal attacks.
2. Use `os.path.normpath()` to normalize the path before expanding it.
3. Use `os.path.abspath()` to get the absolute path of the file."
"def _shortcut_open(uri, mode, **kw):
    """"""Try to open the URI using the standard library io.open function.

    This can be much faster than the alternative of opening in binary mode and
    then decoding.

    This is only possible under the following conditions:

        1. Opening a local file
        2. Ignore extension is set to True

    If it is not possible to use the built-in open for the specified URI, returns None.

    :param str uri: A string indicating what to open.
    :param str mode: The mode to pass to the open function.
    :param dict kw:
    :returns: The opened file
    :rtype: file
    """"""
    if not isinstance(uri, six.string_types):
        return None

    parsed_uri = _parse_uri(uri)
    if parsed_uri.scheme != 'file':
        return None

    _, extension = P.splitext(parsed_uri.uri_path)
    ignore_extension = kw.get('ignore_extension', False)
    if extension in ('.gz', '.bz2') and not ignore_extension:
        return None

    open_kwargs = {}
    errors = kw.get('errors')
    if errors is not None:
        open_kwargs['errors'] = errors

    encoding = kw.get('encoding')
    if encoding is not None:
        open_kwargs['encoding'] = encoding
        mode = mode.replace('b', '')

    return io.open(parsed_uri.uri_path, mode, **open_kwargs)","1. Use `six.moves.urllib.parse.urlparse` instead of `urllib.parse.urlparse` to avoid security vulnerabilities.
2. Use `io.open` with `mode='rb'` to open the file in binary mode.
3. Use `os.path.basename` to get the file name without the extension."
"def open(bucket_id, key_id, mode, **kwargs):
    logger.debug('%r', locals())
    if mode not in MODES:
        raise NotImplementedError('bad mode: %r expected one of %r' % (mode, MODES))

    buffer_size = kwargs.pop(""buffer_size"", io.DEFAULT_BUFFER_SIZE)
    encoding = kwargs.pop(""encoding"", ""utf-8"")
    errors = kwargs.pop(""errors"", None)
    newline = kwargs.pop(""newline"", None)
    line_buffering = kwargs.pop(""line_buffering"", False)
    s3_min_part_size = kwargs.pop(""s3_min_part_size"", DEFAULT_MIN_PART_SIZE)

    if mode in (READ, READ_BINARY):
        fileobj = BufferedInputBase(bucket_id, key_id, **kwargs)
    elif mode in (WRITE, WRITE_BINARY):
        fileobj = BufferedOutputBase(bucket_id, key_id, min_part_size=s3_min_part_size, **kwargs)
    else:
        assert False

    if mode in (READ, WRITE):
        return io.TextIOWrapper(fileobj, encoding=encoding, errors=errors,
                                newline=newline, line_buffering=line_buffering)
    elif mode in (READ_BINARY, WRITE_BINARY):
        return fileobj
    else:
        assert False","1. Use `os.fchmod` to set the file mode to 0600 to restrict permissions for the file.
2. Use `contextlib.closing` to ensure that the file is closed after it is used.
3. Use `boto3.session.Session` to create a boto3 session and pass it to the `open` function to avoid leaking credentials."
"    def __init__(self, s3_object):
        self.position = 0
        self._object = s3_object
        self._content_length = self._object.content_length","1. Use `boto3.resource` instead of `boto3.client` to get a higher level of abstraction and avoid leaking credentials.
2. Use `boto3.session.Session` to manage temporary credentials and avoid storing them in the code.
3. Use `boto3.s3.Bucket.Object.get` to get the object content instead of downloading it directly to avoid leaking credentials."
"    def read(self, size=-1):
        if self.position == self._content_length:
            return b''
        if size <= 0:
            end = None
        else:
            end = min(self._content_length, self.position + size)
        range_string = _range_string(self.position, stop=end)
        logger.debug('range_string: %r', range_string)
        body = self._object.get(Range=range_string)['Body'].read()
        self.position += len(body)
        return body","1. Use `boto3.client` instead of `boto3.resource` to avoid leaking credentials.
2. Authenticate the request using `boto3.session.Session`.
3. Use `boto3.s3.ObjectAcl` to set the correct permissions for the object."
"    def __init__(self, bucket, key, **kwargs):
        session = boto3.Session(profile_name=kwargs.pop('profile_name', None))
        s3 = session.resource('s3', **kwargs)
        self._object = s3.Object(bucket, key)
        self._raw_reader = RawReader(self._object)
        self._content_length = self._object.content_length
        self._current_pos = 0
        self._buffer = b''
        self._eof = False

        #
        # This member is part of the io.BufferedIOBase interface.
        #
        self.raw = None","1. Use boto3.Session() with a specific profile name to avoid leaking credentials to the environment.
2. Use the s3.Object() object's get_object_to_file() method to read the object directly into a file, rather than using the BufferedReader() class.
3. Close the BufferedReader() object when you are finished with it to release any resources that it is holding."
"    def seekable(self):
        """"""If False, seek(), tell() and truncate() will raise IOError.

        We offer only seek support, and no truncate support.""""""
        return True","1. Use `os.ftruncate()` instead of `truncate()` to truncate the file.
2. Check the return value of `os.ftruncate()` and raise an `IOError` if it fails.
3. Use `os.fsync()` to flush the file data to disk after truncating it."
"    def read(self, size=-1):
        """"""Read up to size bytes from the object and return them.""""""
        if size <= 0:
            if len(self._buffer):
                from_buf = self._read_from_buffer(len(self._buffer))
            else:
                from_buf = b''
            self._current_pos = self._content_length
            return from_buf + self._raw_reader.read()

        #
        # Return unused data first
        #
        if len(self._buffer) >= size:
            return self._read_from_buffer(size)

        #
        # If the stream is finished, return what we have.
        #
        if self._eof:
            return self._read_from_buffer(len(self._buffer))

        #
        # Fill our buffer to the required size.
        #
        # logger.debug('filling %r byte-long buffer up to %r bytes', len(self._buffer), size)
        while len(self._buffer) < size and not self._eof:
            raw = self._raw_reader.read(size=io.DEFAULT_BUFFER_SIZE)
            if len(raw):
                self._buffer += raw
            else:
                logger.debug('reached EOF while filling buffer')
                self._eof = True

        return self._read_from_buffer(size)","1. Use `io.BytesIO` instead of `io.StringIO` to avoid data conversion.
2. Validate the size of the data being read to prevent buffer overflows.
3. Close the underlying stream when the object is closed."
"    def __init__(self, bucket, key, min_part_size=DEFAULT_MIN_PART_SIZE, **kwargs):
        if min_part_size < MIN_MIN_PART_SIZE:
            logger.warning(""S3 requires minimum part size >= 5MB; \\
multipart upload may fail"")

        session = boto3.Session(profile_name=kwargs.pop('profile_name', None))
        s3 = session.resource('s3', **kwargs)

        #
        # https://stackoverflow.com/questions/26871884/how-can-i-easily-determine-if-a-boto-3-s3-bucket-resource-exists
        #
        s3.create_bucket(Bucket=bucket)
        self._object = s3.Object(bucket, key)
        self._min_part_size = min_part_size
        self._mp = self._object.initiate_multipart_upload()

        self._buf = io.BytesIO()
        self._total_bytes = 0
        self._total_parts = 0
        self._parts = []

        #
        # This member is part of the io.BufferedIOBase interface.
        #
        self.raw = None","1. Use `boto3.session.get_credentials()` to get credentials instead of passing them directly to the constructor.
2. Use `boto3.s3.Bucket.upload_fileobj()` to upload the file instead of using the multipart upload API.
3. Use `boto3.s3.Object.Acl().put()` to set the ACL on the object after it has been uploaded."
"def smart_open(uri, mode=""rb"", **kw):
    """"""
    Open the given S3 / HDFS / filesystem file pointed to by `uri` for reading or writing.

    The only supported modes for now are 'rb' (read, default) and 'wb' (replace & write).

    The reads/writes are memory efficient (streamed) and therefore suitable for
    arbitrarily large files.

    The `uri` can be either:

    1. a URI for the local filesystem (compressed ``.gz`` or ``.bz2`` files handled automatically):
       `./lines.txt`, `/home/joe/lines.txt.gz`, `file:///home/joe/lines.txt.bz2`
    2. a URI for HDFS: `hdfs:///some/path/lines.txt`
    3. a URI for Amazon's S3 (can also supply credentials inside the URI):
       `s3://my_bucket/lines.txt`, `s3://my_aws_key_id:key_secret@my_bucket/lines.txt`
    4. an instance of the boto.s3.key.Key class.

    Examples::

      >>> # stream lines from http; you can use context managers too:
      >>> with smart_open.smart_open('http://www.google.com') as fin:
      ...     for line in fin:
      ...         print line

      >>> # stream lines from S3; you can use context managers too:
      >>> with smart_open.smart_open('s3://mybucket/mykey.txt') as fin:
      ...     for line in fin:
      ...         print line

      >>> # you can also use a boto.s3.key.Key instance directly:
      >>> key = boto.connect_s3().get_bucket(""my_bucket"").get_key(""my_key"")
      >>> with smart_open.smart_open(key) as fin:
      ...     for line in fin:
      ...         print line

      >>> # stream line-by-line from an HDFS file
      >>> for line in smart_open.smart_open('hdfs:///user/hadoop/my_file.txt'):
      ...    print line

      >>> # stream content *into* S3:
      >>> with smart_open.smart_open('s3://mybucket/mykey.txt', 'wb') as fout:
      ...     for line in ['first line', 'second line', 'third line']:
      ...          fout.write(line + '\\n')

      >>> # stream from/to (compressed) local files:
      >>> for line in smart_open.smart_open('/home/radim/my_file.txt'):
      ...    print line
      >>> for line in smart_open.smart_open('/home/radim/my_file.txt.gz'):
      ...    print line
      >>> with smart_open.smart_open('/home/radim/my_file.txt.gz', 'wb') as fout:
      ...    fout.write(""hello world!\\n"")
      >>> with smart_open.smart_open('/home/radim/another.txt.bz2', 'wb') as fout:
      ...    fout.write(""good bye!\\n"")
      >>> # stream from/to (compressed) local files with Expand ~ and ~user constructions:
      >>> for line in smart_open.smart_open('~/my_file.txt'):
      ...    print line
      >>> for line in smart_open.smart_open('my_file.txt'):
      ...    print line

    """"""
    logger.debug('%r', locals())

    #
    # This is a work-around for the problem described in Issue #144.
    # If the user has explicitly specified an encoding, then assume they want
    # us to open the destination in text mode, instead of the default binary.
    #
    # If we change the default mode to be text, and match the normal behavior
    # of Py2 and 3, then the above assumption will be unnecessary.
    #
    if kw.get('encoding') is not None and 'b' in mode:
        mode = mode.replace('b', '')

    # validate mode parameter
    if not isinstance(mode, six.string_types):
        raise TypeError('mode should be a string')

    if isinstance(uri, six.string_types):
        # this method just routes the request to classes handling the specific storage
        # schemes, depending on the URI protocol in `uri`
        parsed_uri = ParseUri(uri)

        if parsed_uri.scheme in (""file"", ):
            # local files -- both read & write supported
            # compression, if any, is determined by the filename extension (.gz, .bz2)
            return file_smart_open(parsed_uri.uri_path, mode, encoding=kw.pop('encoding', None))
        elif parsed_uri.scheme in (""s3"", ""s3n"", 's3u'):
            return s3_open_uri(parsed_uri, mode, **kw)
        elif parsed_uri.scheme in (""hdfs"", ):
            encoding = kw.pop('encoding', None)
            if encoding is not None:
                warnings.warn(_ISSUE_146_FSTR % {'encoding': encoding, 'scheme': parsed_uri.scheme})
            if mode in ('r', 'rb'):
                return HdfsOpenRead(parsed_uri, **kw)
            if mode in ('w', 'wb'):
                return HdfsOpenWrite(parsed_uri, **kw)
            else:
                raise NotImplementedError(""file mode %s not supported for %r scheme"", mode, parsed_uri.scheme)
        elif parsed_uri.scheme in (""webhdfs"", ):
            encoding = kw.pop('encoding', None)
            if encoding is not None:
                warnings.warn(_ISSUE_146_FSTR % {'encoding': encoding, 'scheme': parsed_uri.scheme})
            if mode in ('r', 'rb'):
                return WebHdfsOpenRead(parsed_uri, **kw)
            elif mode in ('w', 'wb'):
                return WebHdfsOpenWrite(parsed_uri, **kw)
            else:
                raise NotImplementedError(""file mode %s not supported for %r scheme"", mode, parsed_uri.scheme)
        elif parsed_uri.scheme.startswith('http'):
            encoding = kw.pop('encoding', None)
            if encoding is not None:
                warnings.warn(_ISSUE_146_FSTR % {'encoding': encoding, 'scheme': parsed_uri.scheme})
            if mode in ('r', 'rb'):
                return HttpOpenRead(parsed_uri, **kw)
            else:
                raise NotImplementedError(""file mode %s not supported for %r scheme"", mode, parsed_uri.scheme)
        else:
            raise NotImplementedError(""scheme %r is not supported"", parsed_uri.scheme)
    elif isinstance(uri, boto.s3.key.Key):
        return s3_open_key(uri, mode, **kw)
    elif hasattr(uri, 'read'):
        # simply pass-through if already a file-like
        return uri
    else:
        raise TypeError('don\\'t know how to handle uri %s' % repr(uri))","1. Use `six.string_types` to validate the mode parameter.
2. Use `ParseUri` to parse the URI and route the request to the corresponding handler.
3. Use `warnings.warn` to notify users of unsupported features."
"def s3_open_uri(parsed_uri, mode, **kwargs):
    logger.debug('%r', locals())
    if parsed_uri.access_id is not None:
        kwargs['aws_access_key_id'] = parsed_uri.access_id
    if parsed_uri.access_secret is not None:
        kwargs['aws_secret_access_key'] = parsed_uri.access_secret

    # Get an S3 host. It is required for sigv4 operations.
    host = kwargs.pop('host', None)
    if host is not None:
        kwargs['endpoint_url'] = 'http://' + host

    #
    # TODO: this is the wrong place to handle ignore_extension.
    # It should happen at the highest level in the smart_open function, because
    # it influences other file systems as well, not just S3.
    #
    if kwargs.pop(""ignore_extension"", False):
        codec = None
    else:
        codec = _detect_codec(parsed_uri.key_id)

    #
    # Codecs work on a byte-level, so the underlying S3 object should
    # always be reading bytes.
    #
    if codec and mode in (smart_open_s3.READ, smart_open_s3.READ_BINARY):
        s3_mode = smart_open_s3.READ_BINARY
    elif codec and mode in (smart_open_s3.WRITE, smart_open_s3.WRITE_BINARY):
        s3_mode = smart_open_s3.WRITE_BINARY
    else:
        s3_mode = mode

    #
    # TODO: I'm not sure how to handle this with boto3.  Any ideas?
    #
    # https://github.com/boto/boto3/issues/334
    #
    # _setup_unsecured_mode()

    fobj = smart_open_s3.open(parsed_uri.bucket_id, parsed_uri.key_id, s3_mode, **kwargs)
    return _CODECS[codec](fobj, mode)","1. Use secure mode for boto3.
2. Use access key id and secret access key only in secure environment.
3. Handle ignore_extension at the highest level."
"def s3_open_key(key, mode, **kwargs):
    logger.debug('%r', locals())
    #
    # TODO: handle boto3 keys as well
    #
    host = kwargs.pop('host', None)
    if host is not None:
        kwargs['endpoint_url'] = 'http://' + host

    if kwargs.pop(""ignore_extension"", False):
        codec = None
    else:
        codec = _detect_codec(key.name)

    #
    # Codecs work on a byte-level, so the underlying S3 object should
    # always be reading bytes.
    #
    if codec and mode in (smart_open_s3.READ, smart_open_s3.READ_BINARY):
        s3_mode = smart_open_s3.READ_BINARY
    elif codec and mode in (smart_open_s3.WRITE, smart_open_s3.WRITE_BINARY):
        s3_mode = smart_open_s3.WRITE_BINARY
    else:
        s3_mode = mode

    logging.debug('codec: %r mode: %r s3_mode: %r', codec, mode, s3_mode)
    fobj = smart_open_s3.open(key.bucket.name, key.name, s3_mode, **kwargs)
    return _CODECS[codec](fobj, mode)","1. Use boto3 instead of smart_open_s3 to handle S3 keys.
2. Use a secure connection to S3 by setting the `endpoint_url` parameter to `https://`.
3. Encrypt the data on disk using a strong encryption algorithm such as AES-256."
"def _wrap_gzip(fileobj, mode):
    return contextlib.closing(gzip.GzipFile(fileobj=fileobj, mode=mode))","1. Use `with` statement to ensure that the `GzipFile` object is closed after use.
2. Check the `mode` argument to ensure that it is a valid value.
3. Use `os.fchmod` to set the file mode to `0644` after the file is closed."
"def encoding_wrapper(fileobj, mode, encoding=None):
    """"""Decode bytes into text, if necessary.

    If mode specifies binary access, does nothing, unless the encoding is
    specified.  A non-null encoding implies text mode.

    :arg fileobj: must quack like a filehandle object.
    :arg str mode: is the mode which was originally requested by the user.
    :arg encoding: The text encoding to use.  If mode is binary, overrides mode.
    :returns: a file object
    """"""
    logger.debug('encoding_wrapper: %r', locals())

    #
    # If the mode is binary, but the user specified an encoding, assume they
    # want text.  If we don't make this assumption, ignore the encoding and
    # return bytes, smart_open behavior will diverge from the built-in open:
    #
    #   open(filename, encoding='utf-8') returns a text stream in Py3
    #   smart_open(filename, encoding='utf-8') would return a byte stream
    #       without our assumption, because the default mode is rb.
    #
    if 'b' in mode and encoding is None:
        return fileobj

    if encoding is None:
        encoding = SYSTEM_ENCODING

    if mode[0] == 'r':
        decoder = codecs.getreader(encoding)
    else:
        decoder = codecs.getwriter(encoding)
    return decoder(fileobj)","1. Use `contextlib.closing` to ensure that the file is closed after it is used.
2. Check the mode of the file before decoding it. If the mode is binary, do not decode the file.
3. Use a secure default encoding, such as `utf-8`."
"def file_smart_open(fname, mode='rb', encoding=None):
    """"""
    Stream from/to local filesystem, transparently (de)compressing gzip and bz2
    files if necessary.

    :arg str fname: The path to the file to open.
    :arg str mode: The mode in which to open the file.
    :arg str encoding: The text encoding to use.
    :returns: A file object
    """"""
    #
    # This is how we get from the filename to the end result.
    # Decompression is optional, but it always accepts bytes and returns bytes.
    # Decoding is also optional, accepts bytes and returns text.
    # The diagram below is for reading, for writing, the flow is from right to
    # left, but the code is identical.
    #
    #           open as binary         decompress?          decode?
    # filename ---------------> bytes -------------> bytes ---------> text
    #                          raw_fobj        decompressed_fobj   decoded_fobj
    #
    try:  # TODO need to fix this place (for cases with r+ and so on)
        raw_mode = {'r': 'rb', 'w': 'wb', 'a': 'ab'}[mode]
    except KeyError:
        raw_mode = mode
    raw_fobj = open(fname, raw_mode)
    decompressed_fobj = compression_wrapper(raw_fobj, fname, raw_mode)
    decoded_fobj = encoding_wrapper(decompressed_fobj, mode, encoding=encoding)
    return decoded_fobj","1. Use `os.fchmod` to set the file mode to `0600` to prevent unauthorized access.
2. Use `contextlib.closing` to ensure that the file is closed properly, even in the case of an exception.
3. Use `shutil.copyfileobj` to copy data between files, rather than using `open` and `read` directly."
"    def __init__(self, uri, default_scheme=""file""):
        """"""
        Assume `default_scheme` if no scheme given in `uri`.

        """"""
        if os.name == 'nt':
            # urlsplit doesn't work on Windows -- it parses the drive as the scheme...
            if '://' not in uri:
                # no protocol given => assume a local file
                uri = 'file://' + uri
        parsed_uri = urlsplit(uri)
        self.scheme = parsed_uri.scheme if parsed_uri.scheme else default_scheme

        if self.scheme == ""hdfs"":
            self.uri_path = parsed_uri.netloc + parsed_uri.path
            self.uri_path = ""/"" + self.uri_path.lstrip(""/"")

            if not self.uri_path:
                raise RuntimeError(""invalid HDFS URI: %s"" % uri)
        elif self.scheme == ""webhdfs"":
            self.uri_path = parsed_uri.netloc + ""/webhdfs/v1"" + parsed_uri.path
            if parsed_uri.query:
                self.uri_path += ""?"" + parsed_uri.query

            if not self.uri_path:
                raise RuntimeError(""invalid WebHDFS URI: %s"" % uri)
        elif self.scheme in (""s3"", ""s3n""):
            self.bucket_id = (parsed_uri.netloc + parsed_uri.path).split('@')
            self.key_id = None

            if len(self.bucket_id) == 1:
                # URI without credentials: s3://bucket/object
                self.bucket_id, self.key_id = self.bucket_id[0].split('/', 1)
                # ""None"" credentials are interpreted as ""look for credentials in other locations"" by boto
                self.access_id, self.access_secret = None, None
            elif len(self.bucket_id) == 2 and len(self.bucket_id[0].split(':')) == 2:
                # URI in full format: s3://key:secret@bucket/object
                # access key id: [A-Z0-9]{20}
                # secret access key: [A-Za-z0-9/+=]{40}
                acc, self.bucket_id = self.bucket_id
                self.access_id, self.access_secret = acc.split(':')
                self.bucket_id, self.key_id = self.bucket_id.split('/', 1)
            else:
                # more than 1 '@' means invalid uri
                # Bucket names must be at least 3 and no more than 63 characters long.
                # Bucket names must be a series of one or more labels.
                # Adjacent labels are separated by a single period (.).
                # Bucket names can contain lowercase letters, numbers, and hyphens.
                # Each label must start and end with a lowercase letter or a number.
                raise RuntimeError(""invalid S3 URI: %s"" % uri)
        elif self.scheme == 'file':
            self.uri_path = parsed_uri.netloc + parsed_uri.path

            # '~/tmp' may be expanded to '/Users/username/tmp'
            self.uri_path = os.path.expanduser(self.uri_path)

            if not self.uri_path:
                raise RuntimeError(""invalid file URI: %s"" % uri)
        else:
            raise NotImplementedError(""unknown URI scheme %r in %r"" % (self.scheme, uri))","1. Use `urllib.parse.urlparse()` instead of `urlsplit()` to parse the URI.
2. Validate the URI against a whitelist of allowed schemes.
3. Sanitize the URI path to prevent malicious code injection."
"    def _get_many_from_db_backend(self, async_tasks) -> Mapping[str, EventBufferValueType]:
        task_ids = _tasks_list_to_task_ids(async_tasks)
        session = app.backend.ResultSession()
        task_cls = app.backend.task_cls
        with session_cleanup(session):
            tasks = session.query(task_cls).filter(task_cls.task_id.in_(task_ids)).all()

        task_results = [app.backend.meta_from_decoded(task.to_dict()) for task in tasks]
        task_results_by_task_id = {task_result[""task_id""]: task_result for task_result in task_results}
        return self._prepare_state_and_info_by_task_dict(task_ids, task_results_by_task_id)","1. Use prepared statements instead of building queries manually to prevent SQL injection attacks.
2. Use a database connection pool to avoid creating unnecessary database connections.
3. Use proper escaping when inserting user input into the database."
"    def _execute(self, session=None):
        """"""
        Initializes all components required to run a dag for a specified date range and
        calls helper method to execute the tasks.
        """"""
        ti_status = BackfillJob._DagRunTaskStatus()

        start_date = self.bf_start_date

        # Get intervals between the start/end dates, which will turn into dag runs
        run_dates = self.dag.get_run_dates(start_date=start_date, end_date=self.bf_end_date)
        if self.run_backwards:
            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]
            if tasks_that_depend_on_past:
                raise AirflowException(
                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(
                        "","".join(tasks_that_depend_on_past)
                    )
                )
            run_dates = run_dates[::-1]

        if len(run_dates) == 0:
            self.log.info(""No run dates were found for the given dates and dag interval."")
            return

        # picklin'
        pickle_id = None

        if not self.donot_pickle and self.executor_class not in (
            executor_constants.LOCAL_EXECUTOR,
            executor_constants.SEQUENTIAL_EXECUTOR,
            executor_constants.DASK_EXECUTOR,
        ):
            pickle = DagPickle(self.dag)
            session.add(pickle)
            session.commit()
            pickle_id = pickle.id

        executor = self.executor
        executor.start()

        ti_status.total_runs = len(run_dates)  # total dag runs in backfill

        try:  # pylint: disable=too-many-nested-blocks
            remaining_dates = ti_status.total_runs
            while remaining_dates > 0:
                dates_to_process = [
                    run_date for run_date in run_dates if run_date not in ti_status.executed_dag_run_dates
                ]

                self._execute_for_run_dates(
                    run_dates=dates_to_process,
                    ti_status=ti_status,
                    executor=executor,
                    pickle_id=pickle_id,
                    start_date=start_date,
                    session=session,
                )

                remaining_dates = ti_status.total_runs - len(ti_status.executed_dag_run_dates)
                err = self._collect_errors(ti_status=ti_status, session=session)
                if err:
                    raise BackfillUnfinished(err, ti_status)

                if remaining_dates > 0:
                    self.log.info(
                        ""max_active_runs limit for dag %s has been reached ""
                        "" - waiting for other dag runs to finish"",
                        self.dag_id,
                    )
                    time.sleep(self.delay_on_limit_secs)
        except (KeyboardInterrupt, SystemExit):
            self.log.warning(""Backfill terminated by user."")

            # TODO: we will need to terminate running task instances and set the
            # state to failed.
            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)
        finally:
            session.commit()
            executor.end()

        self.log.info(""Backfill done. Exiting."")","1. Use Airflow's built-in backfill functionality instead of this custom code.
2. Use a secure executor like Celery or Kubernetes instead of the default LocalExecutor.
3. Encrypt sensitive data like passwords and tokens in Airflow's metadata database."
"    def run(self):
        """"""Runs Task Instance.""""""
        dag_id = request.form.get('dag_id')
        task_id = request.form.get('task_id')
        origin = get_safe_url(request.form.get('origin'))
        dag = current_app.dag_bag.get_dag(dag_id)
        task = dag.get_task(task_id)

        execution_date = request.form.get('execution_date')
        execution_date = timezone.parse(execution_date)
        ignore_all_deps = request.form.get('ignore_all_deps') == ""true""
        ignore_task_deps = request.form.get('ignore_task_deps') == ""true""
        ignore_ti_state = request.form.get('ignore_ti_state') == ""true""

        executor = ExecutorLoader.get_default_executor()
        valid_celery_config = False
        valid_kubernetes_config = False

        try:
            from airflow.executors.celery_executor import CeleryExecutor  # noqa

            valid_celery_config = isinstance(executor, CeleryExecutor)
        except ImportError:
            pass

        try:
            from airflow.executors.kubernetes_executor import KubernetesExecutor  # noqa

            valid_kubernetes_config = isinstance(executor, KubernetesExecutor)
        except ImportError:
            pass

        if not valid_celery_config and not valid_kubernetes_config:
            flash(""Only works with the Celery or Kubernetes executors, sorry"", ""error"")
            return redirect(origin)

        ti = models.TaskInstance(task=task, execution_date=execution_date)
        ti.refresh_from_db()

        # Make sure the task instance can be run
        dep_context = DepContext(
            deps=RUNNING_DEPS,
            ignore_all_deps=ignore_all_deps,
            ignore_task_deps=ignore_task_deps,
            ignore_ti_state=ignore_ti_state,
        )
        failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))
        if failed_deps:
            failed_deps_str = "", "".join([f""{dep.dep_name}: {dep.reason}"" for dep in failed_deps])
            flash(
                ""Could not queue task instance for execution, dependencies not met: ""
                ""{}"".format(failed_deps_str),
                ""error"",
            )
            return redirect(origin)

        executor.start()
        executor.queue_task_instance(
            ti,
            ignore_all_deps=ignore_all_deps,
            ignore_task_deps=ignore_task_deps,
            ignore_ti_state=ignore_ti_state,
        )
        executor.heartbeat()
        flash(f""Sent {ti} to the message queue, it should start any moment now."")
        return redirect(origin)","1. Use `get_safe_url` to sanitize the `origin` parameter.
2. Use `isinstance` to check if the executor is a CeleryExecutor or KubernetesExecutor.
3. Use `DepContext` to check if the task instance can be run."
"    def _emit_duration_stats_for_finished_state(self):
        if self.state == State.RUNNING:
            return

        duration = self.end_date - self.start_date
        if self.state is State.SUCCESS:
            Stats.timing(f'dagrun.duration.success.{self.dag_id}', duration)
        elif self.state == State.FAILED:
            Stats.timing(f'dagrun.duration.failed.{self.dag_id}', duration)","1. Use `functools.lru_cache` to cache the duration calculation.
2. Use `os.fchmod` to set the permissions of the log file to 0600.
3. Use `contextlib.closing` to ensure that the log file is closed after the function exits."
"    def _check_missing_providers(providers):

        current_airflow_version = Version(__import__(""airflow"").__version__)
        if current_airflow_version.major >= 2:
            prefix = ""apache-airflow-providers-""
        else:
            prefix = ""apache-airflow-backport-providers-""

        for provider in providers:
            dist_name = prefix + provider
            try:
                distribution(dist_name)
            except PackageNotFoundError:
                yield ""Please install `{}`"".format(dist_name)","1. Use `importlib.metadata` instead of `importlib.util.find_spec` to get the package version.
2. Use `importlib.metadata.distributions` to get the package distribution.
3. Handle the `PackageNotFoundError` exception more gracefully."
"    def __init__(self, *args, **kwargs) -> None:
        kwargs['client_type'] = 's3'

        self.extra_args = {}
        if 'extra_args' in kwargs:
            self.extra_args = kwargs['extra_args']
            if not isinstance(self.extra_args, dict):
                raise ValueError(f""extra_args '{self.extra_args!r}' must be of type {dict}"")
            del kwargs['extra_args']

        super().__init__(*args, **kwargs)","1. Use `kwargs` instead of `*args` to avoid positional arguments.
2. Validate the type of `extra_args` to ensure it is a dictionary.
3. Use `super().__init__()` to call the parent class's constructor."
"    def load_file(
        self,
        filename: str,
        key: str,
        bucket_name: Optional[str] = None,
        replace: bool = False,
        encrypt: bool = False,
        gzip: bool = False,
        acl_policy: Optional[str] = None,
    ) -> None:
        """"""
        Loads a local file to S3

        :param filename: name of the file to load.
        :type filename: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists. If replace is False and the key exists, an
            error will be raised.
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        :param gzip: If True, the file will be compressed locally
        :type gzip: bool
        :param acl_policy: String specifying the canned ACL policy for the file being
            uploaded to the S3 bucket.
        :type acl_policy: str
        """"""
        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(f""The key {key} already exists."")

        extra_args = self.extra_args
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""
        if gzip:
            with open(filename, 'rb') as f_in:
                filename_gz = f_in.name + '.gz'
                with gz.open(filename_gz, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
                    filename = filename_gz
        if acl_policy:
            extra_args['ACL'] = acl_policy

        client = self.get_conn()
        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)","1. Use `boto3.session.Session` to create a client, instead of using `boto3.client` directly. This will ensure that the client is created with the correct credentials and region.
2. Use `boto3.s3.Bucket.Object.upload_fileobj` to upload the file, instead of using `boto3.client.upload_file`. This will ensure that the file is uploaded in the correct format and with the correct permissions.
3. Use `boto3.s3.Bucket.Object.Acl.put` to set the ACL for the object, instead of passing the ACL as a parameter to `boto3.client.upload_file`. This will ensure that the ACL is set correctly, even if the object already exists."
"    def _upload_file_obj(
        self,
        file_obj: BytesIO,
        key: str,
        bucket_name: Optional[str] = None,
        replace: bool = False,
        encrypt: bool = False,
        acl_policy: Optional[str] = None,
    ) -> None:
        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(f""The key {key} already exists."")

        extra_args = self.extra_args
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""
        if acl_policy:
            extra_args['ACL'] = acl_policy

        client = self.get_conn()
        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)","1. Use `boto3.session.Session` to create a client instead of directly creating a client. This will ensure that the client is created with the correct credentials and region.
2. Use `boto3.s3.Bucket.Object.upload_fileobj` to upload the file object. This method will automatically set the correct Content-Type header for the file.
3. Use `boto3.s3.Bucket.Object.Acl.put` to set the ACL for the object. This method will ensure that the ACL is set correctly for the object."
"    def hook(self):
        """"""Returns S3Hook.""""""
        remote_conn_id = conf.get('logging', 'REMOTE_LOG_CONN_ID')
        try:
            from airflow.providers.amazon.aws.hooks.s3 import S3Hook

            return S3Hook(remote_conn_id)
        except Exception as e:  # pylint: disable=broad-except
            self.log.exception(
                'Could not create an S3Hook with connection id ""%s"". '
                'Please make sure that airflow[aws] is installed and '
                'the S3 connection exists. Exception : ""%s""',
                remote_conn_id,
                e,
            )
            return None","1. Use `get_hook` instead of `import` to avoid circular import.
2. Wrap the `import` in a `try` block to catch errors.
3. Log the exception with `exception.args[0]` instead of `exception`."
"    def _init_file(self, ti):
        """"""
        Create log directory and give it correct permissions.

        :param ti: task instance object
        :return: relative log path of the given task instance
        """"""
        # To handle log writing when tasks are impersonated, the log files need to
        # be writable by the user that runs the Airflow command and the user
        # that is impersonated. This is mainly to handle corner cases with the
        # SubDagOperator. When the SubDagOperator is run, all of the operators
        # run under the impersonated user and create appropriate log files
        # as the impersonated user. However, if the user manually runs tasks
        # of the SubDagOperator through the UI, then the log files are created
        # by the user that runs the Airflow command. For example, the Airflow
        # run command may be run by the `airflow_sudoable` user, but the Airflow
        # tasks may be run by the `airflow` user. If the log files are not
        # writable by both users, then it's possible that re-running a task
        # via the UI (or vice versa) results in a permission error as the task
        # tries to write to a log file created by the other user.
        relative_path = self._render_filename(ti, ti.try_number)
        full_path = os.path.join(self.local_base, relative_path)
        directory = os.path.dirname(full_path)
        # Create the log file and give it group writable permissions
        # TODO(aoen): Make log dirs and logs globally readable for now since the SubDag
        # operator is not compatible with impersonation (e.g. if a Celery executor is used
        # for a SubDag operator and the SubDag operator has a different owner than the
        # parent DAG)
        Path(directory).mkdir(mode=0o777, parents=True, exist_ok=True)

        if not os.path.exists(full_path):
            open(full_path, ""a"").close()
            # TODO: Investigate using 444 instead of 666.
            os.chmod(full_path, 0o666)

        return full_path","1. Use mode=0o755 instead of 0o777 to make the log directory only writable by the owner and group.
2. Use mode=0o644 instead of 0o666 to make the log file only readable by the owner.
3. Consider using a different location for the log file, such as a location outside of the Airflow home directory."
"    def _init_file(self, ti):
        """"""
        Create log directory and give it correct permissions.

        :param ti: task instance object
        :return: relative log path of the given task instance
        """"""
        # To handle log writing when tasks are impersonated, the log files need to
        # be writable by the user that runs the Airflow command and the user
        # that is impersonated. This is mainly to handle corner cases with the
        # SubDagOperator. When the SubDagOperator is run, all of the operators
        # run under the impersonated user and create appropriate log files
        # as the impersonated user. However, if the user manually runs tasks
        # of the SubDagOperator through the UI, then the log files are created
        # by the user that runs the Airflow command. For example, the Airflow
        # run command may be run by the `airflow_sudoable` user, but the Airflow
        # tasks may be run by the `airflow` user. If the log files are not
        # writable by both users, then it's possible that re-running a task
        # via the UI (or vice versa) results in a permission error as the task
        # tries to write to a log file created by the other user.
        relative_path = self._render_filename(ti, ti.try_number)
        full_path = os.path.join(self.local_base, relative_path)
        directory = os.path.dirname(full_path)
        # Create the log file and give it group writable permissions
        # TODO(aoen): Make log dirs and logs globally readable for now since the SubDag
        # operator is not compatible with impersonation (e.g. if a Celery executor is used
        # for a SubDag operator and the SubDag operator has a different owner than the
        # parent DAG)
        if not os.path.exists(directory):
            # Create the directory as globally writable using custom mkdirs
            # as os.makedirs doesn't set mode properly.
            mkdirs(directory, 0o777)

        if not os.path.exists(full_path):
            open(full_path, ""a"").close()
            # TODO: Investigate using 444 instead of 666.
            os.chmod(full_path, 0o666)

        return full_path","1. Use `os.makedirs` instead of `mkdirs` to create the directory, as it sets the mode properly.
2. Use a more restrictive mode for the log file, such as `0o644`.
3. Consider using a different location for the log files, such as a location that is not accessible by the web server."
"    def execute_async(
        self,
        key: TaskInstanceKey,
        command: CommandType,
        queue: Optional[str] = None,
        executor_config: Optional[Any] = None,
    ) -> None:
        """"""Executes task asynchronously""""""
        self.log.info('Add task %s with command %s with executor_config %s', key, command, executor_config)
        kube_executor_config = PodGenerator.from_obj(executor_config)
        if executor_config:
            pod_template_file = executor_config.get(""pod_template_override"", None)
        else:
            pod_template_file = None
        if not self.task_queue:
            raise AirflowException(NOT_STARTED_MESSAGE)
        self.event_buffer[key] = (State.QUEUED, self.scheduler_job_id)
        self.task_queue.put((key, command, kube_executor_config, pod_template_file))","1. Use `airflow.utils.helpers.apply_defaults` to sanitize user input.
2. Use `airflow.models.TaskInstance.get` to get the task instance instead of passing it in directly.
3. Use `airflow.utils.session.Session` to manage database transactions."
"def clear(args):
    logging.basicConfig(
        level=settings.LOGGING_LEVEL,
        format=settings.SIMPLE_LOG_FORMAT)
    dags = get_dags(args)

    if args.task_regex:
        for idx, dag in enumerate(dags):
            dags[idx] = dag.sub_dag(
                task_regex=args.task_regex,
                include_downstream=args.downstream,
                include_upstream=args.upstream)

    DAG.clear_dags(
        dags,
        start_date=args.start_date,
        end_date=args.end_date,
        only_failed=args.only_failed,
        only_running=args.only_running,
        confirm_prompt=not args.no_confirm,
        include_subdags=not args.exclude_subdags,
        include_parentdag=not args.exclude_parentdag,
    )","1. Use [args.task_regex] to filter dags instead of [args.dag_id], which could be dangerous if the user has access to all dags.
2. Use [DAG.clear_dags()] to clear dags instead of [dag.clear()].
3. Use [confirm_prompt=True] to prompt the user to confirm before clearing dags."
"    def __init__(self, celery_executor, kubernetes_executor):
        super().__init__()
        self.celery_executor = celery_executor
        self.kubernetes_executor = kubernetes_executor","1. Use `@property` decorators to protect the `celery_executor` and `kubernetes_executor` attributes from being directly modified.
2. Use `assert` statements to validate the types of the arguments passed to the constructor.
3. Use `functools.partial` to create a new function that wraps the `run` method and passes the appropriate executors to it."
"    def _change_state_for_tis_without_dagrun(
        self, old_states: List[str], new_state: str, session: Session = None
    ) -> None:
        """"""
        For all DAG IDs in the DagBag, look for task instances in the
        old_states and set them to new_state if the corresponding DagRun
        does not exist or exists but is not in the running state. This
        normally should not happen, but it can if the state of DagRuns are
        changed manually.

        :param old_states: examine TaskInstances in this state
        :type old_states: list[airflow.utils.state.State]
        :param new_state: set TaskInstances to this state
        :type new_state: airflow.utils.state.State
        """"""
        tis_changed = 0
        query = (
            session.query(models.TaskInstance)
            .outerjoin(models.TaskInstance.dag_run)
            .filter(models.TaskInstance.dag_id.in_(list(self.dagbag.dag_ids)))
            .filter(models.TaskInstance.state.in_(old_states))
            .filter(
                or_(
                    # pylint: disable=comparison-with-callable
                    models.DagRun.state != State.RUNNING,
                    # pylint: disable=no-member
                    models.DagRun.state.is_(None),
                )
            )
        )
        # We need to do this for mysql as well because it can cause deadlocks
        # as discussed in https://issues.apache.org/jira/browse/AIRFLOW-2516
        if self.using_sqlite or self.using_mysql:
            tis_to_change: List[TI] = with_row_locks(query, of=TI, **skip_locked(session=session)).all()
            for ti in tis_to_change:
                ti.set_state(new_state, session=session)
                tis_changed += 1
        else:
            subq = query.subquery()
            current_time = timezone.utcnow()
            ti_prop_update = {
                models.TaskInstance.state: new_state,
                models.TaskInstance.start_date: current_time,
            }

            # Only add end_date and duration if the new_state is 'success', 'failed' or 'skipped'
            if new_state in State.finished:
                ti_prop_update.update(
                    {
                        models.TaskInstance.end_date: current_time,
                        models.TaskInstance.duration: 0,
                    }
                )

            tis_changed = (
                session.query(models.TaskInstance)
                .filter(
                    models.TaskInstance.dag_id == subq.c.dag_id,
                    models.TaskInstance.task_id == subq.c.task_id,
                    models.TaskInstance.execution_date == subq.c.execution_date,
                )
                .update(ti_prop_update, synchronize_session=False)
            )

        if tis_changed > 0:
            session.flush()
            self.log.warning(
                ""Set %s task instances to state=%s as their associated DagRun was not in RUNNING state"",
                tis_changed,
                new_state,
            )
            Stats.gauge('scheduler.tasks.without_dagrun', tis_changed)","1. Use `with_row_locks()` to avoid deadlocks.
2. Use `skip_locked()` to skip locked rows.
3. Use `synchronize_session=False` to avoid implicit transaction."
"    def _executable_task_instances_to_queued(self, max_tis: int, session: Session = None) -> List[TI]:
        """"""
        Finds TIs that are ready for execution with respect to pool limits,
        dag concurrency, executor state, and priority.

        :param max_tis: Maximum number of TIs to queue in this loop.
        :type max_tis: int
        :return: list[airflow.models.TaskInstance]
        """"""
        executable_tis: List[TI] = []

        # Get the pool settings. We get a lock on the pool rows, treating this as a ""critical section""
        # Throws an exception if lock cannot be obtained, rather than blocking
        pools = models.Pool.slots_stats(lock_rows=True, session=session)

        # If the pools are full, there is no point doing anything!
        # If _somehow_ the pool is overfull, don't let the limit go negative - it breaks SQL
        pool_slots_free = max(0, sum(pool['open'] for pool in pools.values()))

        if pool_slots_free == 0:
            self.log.debug(""All pools are full!"")
            return executable_tis

        max_tis = min(max_tis, pool_slots_free)

        # Get all task instances associated with scheduled
        # DagRuns which are not backfilled, in the given states,
        # and the dag is not paused
        query = (
            session.query(TI)
            .outerjoin(TI.dag_run)
            .filter(or_(DR.run_id.is_(None), DR.run_type != DagRunType.BACKFILL_JOB))
            .join(TI.dag_model)
            .filter(not_(DM.is_paused))
            .filter(TI.state == State.SCHEDULED)
            .options(selectinload('dag_model'))
            .limit(max_tis)
        )

        task_instances_to_examine: List[TI] = with_row_locks(
            query,
            of=TI,
            **skip_locked(session=session),
        ).all()
        # TODO[HA]: This was wrong before anyway, as it only looked at a sub-set of dags, not everything.
        # Stats.gauge('scheduler.tasks.pending', len(task_instances_to_examine))

        if len(task_instances_to_examine) == 0:
            self.log.debug(""No tasks to consider for execution."")
            return executable_tis

        # Put one task instance on each line
        task_instance_str = ""\\n\\t"".join([repr(x) for x in task_instances_to_examine])
        self.log.info(""%s tasks up for execution:\\n\\t%s"", len(task_instances_to_examine), task_instance_str)

        pool_to_task_instances: DefaultDict[str, List[models.Pool]] = defaultdict(list)
        for task_instance in task_instances_to_examine:
            pool_to_task_instances[task_instance.pool].append(task_instance)

        # dag_id to # of running tasks and (dag_id, task_id) to # of running tasks.
        dag_concurrency_map: DefaultDict[str, int]
        task_concurrency_map: DefaultDict[Tuple[str, str], int]
        dag_concurrency_map, task_concurrency_map = self.__get_concurrency_maps(
            states=list(EXECUTION_STATES), session=session
        )

        num_tasks_in_executor = 0
        # Number of tasks that cannot be scheduled because of no open slot in pool
        num_starving_tasks_total = 0

        # Go through each pool, and queue up a task for execution if there are
        # any open slots in the pool.
        # pylint: disable=too-many-nested-blocks
        for pool, task_instances in pool_to_task_instances.items():
            pool_name = pool
            if pool not in pools:
                self.log.warning(""Tasks using non-existent pool '%s' will not be scheduled"", pool)
                continue

            open_slots = pools[pool][""open""]

            num_ready = len(task_instances)
            self.log.info(
                ""Figuring out tasks to run in Pool(name=%s) with %s open slots ""
                ""and %s task instances ready to be queued"",
                pool,
                open_slots,
                num_ready,
            )

            priority_sorted_task_instances = sorted(
                task_instances, key=lambda ti: (-ti.priority_weight, ti.execution_date)
            )

            num_starving_tasks = 0
            for current_index, task_instance in enumerate(priority_sorted_task_instances):
                if open_slots <= 0:
                    self.log.info(""Not scheduling since there are %s open slots in pool %s"", open_slots, pool)
                    # Can't schedule any more since there are no more open slots.
                    num_unhandled = len(priority_sorted_task_instances) - current_index
                    num_starving_tasks += num_unhandled
                    num_starving_tasks_total += num_unhandled
                    break

                # Check to make sure that the task concurrency of the DAG hasn't been
                # reached.
                dag_id = task_instance.dag_id

                current_dag_concurrency = dag_concurrency_map[dag_id]
                dag_concurrency_limit = task_instance.dag_model.concurrency
                self.log.info(
                    ""DAG %s has %s/%s running and queued tasks"",
                    dag_id,
                    current_dag_concurrency,
                    dag_concurrency_limit,
                )
                if current_dag_concurrency >= dag_concurrency_limit:
                    self.log.info(
                        ""Not executing %s since the number of tasks running or queued ""
                        ""from DAG %s is >= to the DAG's task concurrency limit of %s"",
                        task_instance,
                        dag_id,
                        dag_concurrency_limit,
                    )
                    continue

                task_concurrency_limit: Optional[int] = None
                if task_instance.dag_model.has_task_concurrency_limits:
                    # Many dags don't have a task_concurrency, so where we can avoid loading the full
                    # serialized DAG the better.
                    serialized_dag = self.dagbag.get_dag(dag_id, session=session)
                    if serialized_dag.has_task(task_instance.task_id):
                        task_concurrency_limit = serialized_dag.get_task(
                            task_instance.task_id
                        ).task_concurrency

                    if task_concurrency_limit is not None:
                        current_task_concurrency = task_concurrency_map[
                            (task_instance.dag_id, task_instance.task_id)
                        ]

                        if current_task_concurrency >= task_concurrency_limit:
                            self.log.info(
                                ""Not executing %s since the task concurrency for""
                                "" this task has been reached."",
                                task_instance,
                            )
                            continue

                if task_instance.pool_slots > open_slots:
                    self.log.info(
                        ""Not executing %s since it requires %s slots ""
                        ""but there are %s open slots in the pool %s."",
                        task_instance,
                        task_instance.pool_slots,
                        open_slots,
                        pool,
                    )
                    num_starving_tasks += 1
                    num_starving_tasks_total += 1
                    # Though we can execute tasks with lower priority if there's enough room
                    continue

                executable_tis.append(task_instance)
                open_slots -= task_instance.pool_slots
                dag_concurrency_map[dag_id] += 1
                task_concurrency_map[(task_instance.dag_id, task_instance.task_id)] += 1

            Stats.gauge(f'pool.starving_tasks.{pool_name}', num_starving_tasks)

        Stats.gauge('scheduler.tasks.starving', num_starving_tasks_total)
        Stats.gauge('scheduler.tasks.running', num_tasks_in_executor)
        Stats.gauge('scheduler.tasks.executable', len(executable_tis))

        task_instance_str = ""\\n\\t"".join([repr(x) for x in executable_tis])
        self.log.info(""Setting the following tasks to queued state:\\n\\t%s"", task_instance_str)

        # set TIs to queued state
        filter_for_tis = TI.filter_for_tis(executable_tis)
        session.query(TI).filter(filter_for_tis).update(
            # TODO[ha]: should we use func.now()? How does that work with DB timezone on mysql when it's not
            # UTC?
            {TI.state: State.QUEUED, TI.queued_dttm: timezone.utcnow(), TI.queued_by_job_id: self.id},
            synchronize_session=False,
        )

        for ti in executable_tis:
            make_transient(ti)
        return executable_tis","1. Use `func.now()` instead of `timezone.utcnow()` to avoid potential timezone issues.
2. Use `synchronize_session=False` to avoid potential deadlocks.
3. Make sure to `make_transient()` the task instances after updating their state."
"    def _change_state_for_tasks_failed_to_execute(self, session: Session = None):
        """"""
        If there are tasks left over in the executor,
        we set them back to SCHEDULED to avoid creating hanging tasks.

        :param session: session for ORM operations
        """"""
        if not self.executor.queued_tasks:
            return

        filter_for_ti_state_change = [
            and_(
                TI.dag_id == dag_id,
                TI.task_id == task_id,
                TI.execution_date == execution_date,
                # The TI.try_number will return raw try_number+1 since the
                # ti is not running. And we need to -1 to match the DB record.
                TI._try_number == try_number - 1,  # pylint: disable=protected-access
                TI.state == State.QUEUED,
            )
            for dag_id, task_id, execution_date, try_number in self.executor.queued_tasks.keys()
        ]
        ti_query = session.query(TI).filter(or_(*filter_for_ti_state_change))
        tis_to_set_to_scheduled: List[TI] = with_row_locks(ti_query).all()
        if not tis_to_set_to_scheduled:
            return

        # set TIs to queued state
        filter_for_tis = TI.filter_for_tis(tis_to_set_to_scheduled)
        session.query(TI).filter(filter_for_tis).update(
            {TI.state: State.SCHEDULED, TI.queued_dttm: None}, synchronize_session=False
        )

        for task_instance in tis_to_set_to_scheduled:
            self.executor.queued_tasks.pop(task_instance.key)

        task_instance_str = ""\\n\\t"".join(repr(x) for x in tis_to_set_to_scheduled)
        self.log.info(""Set the following tasks to scheduled state:\\n\\t%s"", task_instance_str)","1. Use `session.query(TI).filter(and_(*filter_for_ti_state_change))` instead of `session.query(TI).filter(or_(*filter_for_ti_state_change))` to avoid race condition.
2. Use `with_row_locks(ti_query).all()` to lock the rows before updating them.
3. Use `TI.filter_for_tis(tis_to_set_to_scheduled)` to filter the tasks before updating them."
"    def adopt_or_reset_orphaned_tasks(self, session: Session = None):
        """"""
        Reset any TaskInstance still in QUEUED or SCHEDULED states that were
        enqueued by a SchedulerJob that is no longer running.

        :return: the number of TIs reset
        :rtype: int
        """"""
        self.log.info(""Resetting orphaned tasks for active dag runs"")
        timeout = conf.getint('scheduler', 'scheduler_health_check_threshold')

        num_failed = (
            session.query(SchedulerJob)
            .filter(
                SchedulerJob.state == State.RUNNING,
                SchedulerJob.latest_heartbeat < (timezone.utcnow() - timedelta(seconds=timeout)),
            )
            .update({""state"": State.FAILED})
        )

        if num_failed:
            self.log.info(""Marked %d SchedulerJob instances as failed"", num_failed)
            Stats.incr(self.__class__.__name__.lower() + '_end', num_failed)

        resettable_states = [State.SCHEDULED, State.QUEUED, State.RUNNING]
        query = (
            session.query(TI)
            .filter(TI.state.in_(resettable_states))
            # outerjoin is because we didn't use to have queued_by_job
            # set, so we need to pick up anything pre upgrade. This (and the
            # ""or queued_by_job_id IS NONE"") can go as soon as scheduler HA is
            # released.
            .outerjoin(TI.queued_by_job)
            .filter(or_(TI.queued_by_job_id.is_(None), SchedulerJob.state != State.RUNNING))
            .join(TI.dag_run)
            .filter(
                DagRun.run_type != DagRunType.BACKFILL_JOB,
                # pylint: disable=comparison-with-callable
                DagRun.state == State.RUNNING,
            )
            .options(load_only(TI.dag_id, TI.task_id, TI.execution_date))
        )

        # Lock these rows, so that another scheduler can't try and adopt these too
        tis_to_reset_or_adopt = with_row_locks(query, of=TI, **skip_locked(session=session)).all()
        to_reset = self.executor.try_adopt_task_instances(tis_to_reset_or_adopt)

        reset_tis_message = []
        for ti in to_reset:
            reset_tis_message.append(repr(ti))
            ti.state = State.NONE
            ti.queued_by_job_id = None

        for ti in set(tis_to_reset_or_adopt) - set(to_reset):
            ti.queued_by_job_id = self.id

        Stats.incr('scheduler.orphaned_tasks.cleared', len(to_reset))
        Stats.incr('scheduler.orphaned_tasks.adopted', len(tis_to_reset_or_adopt) - len(to_reset))

        if to_reset:
            task_instance_str = '\\n\\t'.join(reset_tis_message)
            self.log.info(
                ""Reset the following %s orphaned TaskInstances:\\n\\t%s"", len(to_reset), task_instance_str
            )

        # Issue SQL/finish ""Unit of Work"", but let @provide_session commit (or if passed a session, let caller
        # decide when to commit
        session.flush()
        return len(to_reset)","1. Use `with_row_locks` to lock the rows, so that another scheduler can't try and adopt these too.
2. Use `try_adopt_task_instances` to adopt the task instances.
3. Use `queued_by_job_id` to track which scheduler adopted the task instances."
"    def bulk_write_to_db(cls, dags: Collection[""DAG""], session=None):
        """"""
        Ensure the DagModel rows for the given dags are up-to-date in the dag table in the DB, including
        calculated fields.

        Note that this method can be called for both DAGs and SubDAGs. A SubDag is actually a SubDagOperator.

        :param dags: the DAG objects to save to the DB
        :type dags: List[airflow.models.dag.DAG]
        :return: None
        """"""
        if not dags:
            return

        log.info(""Sync %s DAGs"", len(dags))
        dag_by_ids = {dag.dag_id: dag for dag in dags}
        dag_ids = set(dag_by_ids.keys())
        query = (
            session.query(DagModel)
            .options(joinedload(DagModel.tags, innerjoin=False))
            .filter(DagModel.dag_id.in_(dag_ids))
        )
        orm_dags = with_row_locks(query, of=DagModel).all()

        existing_dag_ids = {orm_dag.dag_id for orm_dag in orm_dags}
        missing_dag_ids = dag_ids.difference(existing_dag_ids)

        for missing_dag_id in missing_dag_ids:
            orm_dag = DagModel(dag_id=missing_dag_id)
            dag = dag_by_ids[missing_dag_id]
            if dag.is_paused_upon_creation is not None:
                orm_dag.is_paused = dag.is_paused_upon_creation
            orm_dag.tags = []
            log.info(""Creating ORM DAG for %s"", dag.dag_id)
            session.add(orm_dag)
            orm_dags.append(orm_dag)

        # Get the latest dag run for each existing dag as a single query (avoid n+1 query)
        most_recent_dag_runs = dict(
            session.query(DagRun.dag_id, func.max_(DagRun.execution_date))
            .filter(
                DagRun.dag_id.in_(existing_dag_ids),
                or_(
                    DagRun.run_type == DagRunType.BACKFILL_JOB,
                    DagRun.run_type == DagRunType.SCHEDULED,
                ),
            )
            .group_by(DagRun.dag_id)
            .all()
        )

        # Get number of active dagruns for all dags we are processing as a single query.
        num_active_runs = dict(
            session.query(DagRun.dag_id, func.count('*'))
            .filter(
                DagRun.dag_id.in_(existing_dag_ids),
                DagRun.state == State.RUNNING,  # pylint: disable=comparison-with-callable
                DagRun.external_trigger.is_(False),
            )
            .group_by(DagRun.dag_id)
            .all()
        )

        for orm_dag in sorted(orm_dags, key=lambda d: d.dag_id):
            dag = dag_by_ids[orm_dag.dag_id]
            if dag.is_subdag:
                orm_dag.is_subdag = True
                orm_dag.fileloc = dag.parent_dag.fileloc  # type: ignore
                orm_dag.root_dag_id = dag.parent_dag.dag_id  # type: ignore
                orm_dag.owners = dag.parent_dag.owner  # type: ignore
            else:
                orm_dag.is_subdag = False
                orm_dag.fileloc = dag.fileloc
                orm_dag.owners = dag.owner
            orm_dag.is_active = True
            orm_dag.default_view = dag.default_view
            orm_dag.description = dag.description
            orm_dag.schedule_interval = dag.schedule_interval
            orm_dag.concurrency = dag.concurrency
            orm_dag.has_task_concurrency_limits = any(t.task_concurrency is not None for t in dag.tasks)

            orm_dag.calculate_dagrun_date_fields(
                dag,
                most_recent_dag_runs.get(dag.dag_id),
                num_active_runs.get(dag.dag_id, 0),
            )

            for orm_tag in list(orm_dag.tags):
                if orm_tag.name not in orm_dag.tags:
                    session.delete(orm_tag)
                orm_dag.tags.remove(orm_tag)
            if dag.tags:
                orm_tag_names = [t.name for t in orm_dag.tags]
                for dag_tag in list(dag.tags):
                    if dag_tag not in orm_tag_names:
                        dag_tag_orm = DagTag(name=dag_tag, dag_id=dag.dag_id)
                        orm_dag.tags.append(dag_tag_orm)
                        session.add(dag_tag_orm)

        if settings.STORE_DAG_CODE:
            DagCode.bulk_sync_to_db([dag.fileloc for dag in orm_dags])

        # Issue SQL/finish ""Unit of Work"", but let @provide_session commit (or if passed a session, let caller
        # decide when to commit
        session.flush()

        for dag in dags:
            cls.bulk_write_to_db(dag.subdags, session=session)","1. Use prepared statements instead of concatenation to prevent SQL injection.
2. Use `func.max()` instead of `max()` to prevent SQL injection.
3. Use `session.delete()` instead of `orm_dag.delete()` to prevent accidentally deleting rows from the database."
"    def dags_needing_dagruns(cls, session: Session):
        """"""
        Return (and lock) a list of Dag objects that are due to create a new DagRun.

        This will return a resultset of rows  that is row-level-locked with a ""SELECT ... FOR UPDATE"" query,
        you should ensure that any scheduling decisions are made in a single transaction -- as soon as the
        transaction is committed it will be unlocked.
        """"""
        # TODO[HA]: Bake this query, it is run _A lot_
        # We limit so that _one_ scheduler doesn't try to do all the creation
        # of dag runs
        query = (
            session.query(cls)
            .filter(
                cls.is_paused.is_(False),
                cls.is_active.is_(True),
                cls.next_dagrun_create_after <= func.now(),
            )
            .order_by(cls.next_dagrun_create_after)
            .limit(cls.NUM_DAGS_PER_DAGRUN_QUERY)
        )

        return with_row_locks(query, of=cls, **skip_locked(session=session))","1. Use prepared statements instead of building queries dynamically.
2. Use `func.now()` instead of `datetime.now()` to avoid SQL injection attacks.
3. Use `skip_locked()` to avoid deadlocks."
"    def next_dagruns_to_examine(
        cls,
        session: Session,
        max_number: Optional[int] = None,
    ):
        """"""
        Return the next DagRuns that the scheduler should attempt to schedule.

        This will return zero or more DagRun rows that are row-level-locked with a ""SELECT ... FOR UPDATE""
        query, you should ensure that any scheduling decisions are made in a single transaction -- as soon as
        the transaction is committed it will be unlocked.

        :rtype: list[airflow.models.DagRun]
        """"""
        from airflow.models.dag import DagModel

        if max_number is None:
            max_number = cls.DEFAULT_DAGRUNS_TO_EXAMINE

        # TODO: Bake this query, it is run _A lot_
        query = (
            session.query(cls)
            .filter(cls.state == State.RUNNING, cls.run_type != DagRunType.BACKFILL_JOB)
            .join(
                DagModel,
                DagModel.dag_id == cls.dag_id,
            )
            .filter(
                DagModel.is_paused.is_(False),
                DagModel.is_active.is_(True),
            )
            .order_by(
                nulls_first(cls.last_scheduling_decision, session=session),
                cls.execution_date,
            )
        )

        if not settings.ALLOW_FUTURE_EXEC_DATES:
            query = query.filter(DagRun.execution_date <= func.now())

        return with_row_locks(query.limit(max_number), of=cls, **skip_locked(session=session))","1. Use prepared statements instead of building queries dynamically.
2. Use `func.now()` instead of `datetime.now()` to avoid SQL injection.
3. Use `skip_locked()` to avoid deadlocks."
"    def slots_stats(
        *,
        lock_rows: bool = False,
        session: Session = None,
    ) -> Dict[str, PoolStats]:
        """"""
        Get Pool stats (Number of Running, Queued, Open & Total tasks)

        If ``lock_rows`` is True, and the database engine in use supports the ``NOWAIT`` syntax, then a
        non-blocking lock will be attempted -- if the lock is not available then SQLAlchemy will throw an
        OperationalError.

        :param lock_rows: Should we attempt to obtain a row-level lock on all the Pool rows returns
        :param session: SQLAlchemy ORM Session
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import

        pools: Dict[str, PoolStats] = {}

        query = session.query(Pool.pool, Pool.slots)

        if lock_rows:
            query = with_row_locks(query, **nowait(session))

        pool_rows: Iterable[Tuple[str, int]] = query.all()
        for (pool_name, total_slots) in pool_rows:
            pools[pool_name] = PoolStats(total=total_slots, running=0, queued=0, open=0)

        state_count_by_pool = (
            session.query(TaskInstance.pool, TaskInstance.state, func.count())
            .filter(TaskInstance.state.in_(list(EXECUTION_STATES)))
            .group_by(TaskInstance.pool, TaskInstance.state)
        ).all()

        # calculate queued and running metrics
        count: int
        for (pool_name, state, count) in state_count_by_pool:
            stats_dict: Optional[PoolStats] = pools.get(pool_name)
            if not stats_dict:
                continue
            # TypedDict key must be a string literal, so we use if-statements to set value
            if state == ""running"":
                stats_dict[""running""] = count
            elif state == ""queued"":
                stats_dict[""queued""] = count
            else:
                raise AirflowException(f""Unexpected state. Expected values: {EXECUTION_STATES}."")

        # calculate open metric
        for pool_name, stats_dict in pools.items():
            if stats_dict[""total""] == -1:
                # -1 means infinite
                stats_dict[""open""] = -1
            else:
                stats_dict[""open""] = stats_dict[""total""] - stats_dict[""running""] - stats_dict[""queued""]

        return pools","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Use `with_for_update()` to lock rows for update instead of `with_row_locks()` to prevent deadlocks.
3. Use `func.count()` instead of `count()` to prevent integer overflow."
"    def _run_mini_scheduler_on_child_tasks(self, session=None) -> None:
        if conf.getboolean('scheduler', 'schedule_after_task_execution', fallback=True):
            from airflow.models.dagrun import DagRun  # Avoid circular import

            try:
                # Re-select the row with a lock
                dag_run = with_row_locks(
                    session.query(DagRun).filter_by(
                        dag_id=self.dag_id,
                        execution_date=self.execution_date,
                    )
                ).one()

                # Get a partial dag with just the specific tasks we want to
                # examine. In order for dep checks to work correctly, we
                # include ourself (so TriggerRuleDep can check the state of the
                # task we just executed)
                partial_dag = self.task.dag.partial_subset(
                    self.task.downstream_task_ids,
                    include_downstream=False,
                    include_upstream=False,
                    include_direct_upstream=True,
                )

                dag_run.dag = partial_dag
                info = dag_run.task_instance_scheduling_decisions(session)

                skippable_task_ids = {
                    task_id
                    for task_id in partial_dag.task_ids
                    if task_id not in self.task.downstream_task_ids
                }

                schedulable_tis = [ti for ti in info.schedulable_tis if ti.task_id not in skippable_task_ids]
                for schedulable_ti in schedulable_tis:
                    if not hasattr(schedulable_ti, ""task""):
                        schedulable_ti.task = self.task.dag.get_task(schedulable_ti.task_id)

                num = dag_run.schedule_tis(schedulable_tis)
                self.log.info(""%d downstream tasks scheduled from follow-on schedule check"", num)

                session.commit()
            except OperationalError as e:
                # Any kind of DB error here is _non fatal_ as this block is just an optimisation.
                self.log.info(
                    f""Skipping mini scheduling run due to exception: {e.statement}"",
                    exc_info=True,
                )
                session.rollback()","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Use `with_row_locks` to prevent race conditions.
3. Handle database errors more gracefully."
"def with_row_locks(query, **kwargs):
    """"""
    Apply with_for_update to an SQLAlchemy query, if row level locking is in use.

    :param query: An SQLAlchemy Query object
    :param kwargs: Extra kwargs to pass to with_for_update (of, nowait, skip_locked, etc)
    :return: updated query
    """"""
    if USE_ROW_LEVEL_LOCKING:
        return query.with_for_update(**kwargs)
    else:
        return query","1. Use `functools.wraps` to preserve the function signature of `with_row_locks`.
2. Use `inspect.getfullargspec` to get the argument names of `with_row_locks`.
3. Use `inspect.ismethod` to check if `query` is a method."
"    def _do_scheduling(self, session) -> int:
        """"""
        This function is where the main scheduling decisions take places. It:

        - Creates any necessary DAG runs by examining the next_dagrun_create_after column of DagModel

          Since creating Dag Runs is a relatively time consuming process, we select only 10 dags by default
          (configurable via ``scheduler.max_dagruns_to_create_per_loop`` setting) - putting this higher will
          mean one scheduler could spend a chunk of time creating dag runs, and not ever get around to
          scheduling tasks.

        - Finds the ""next n oldest"" running DAG Runs to examine for scheduling (n=20 by default, configurable
          via ``scheduler.max_dagruns_per_loop_to_schedule`` config setting) and tries to progress state (TIs
          to SCHEDULED, or DagRuns to SUCCESS/FAILURE etc)

          By ""next oldest"", we mean hasn't been examined/scheduled in the most time.

          The reason we don't select all dagruns at once because the rows are selected with row locks, meaning
          that only one scheduler can ""process them"", even it it is waiting behind other dags. Increasing this
          limit will allow more throughput for smaller DAGs but will likely slow down throughput for larger
          (>500 tasks.) DAGs

        - Then, via a Critical Section (locking the rows of the Pool model) we queue tasks, and then send them
          to the executor.

          See docs of _critical_section_execute_task_instances for more.

        :return: Number of TIs enqueued in this iteration
        :rtype: int
        """"""
        # Put a check in place to make sure we don't commit unexpectedly
        with prohibit_commit(session) as guard:

            if settings.USE_JOB_SCHEDULE:
                query = DagModel.dags_needing_dagruns(session)
                self._create_dag_runs(query.all(), session)

                # commit the session - Release the write lock on DagModel table.
                guard.commit()
                # END: create dagruns

            dag_runs = DagRun.next_dagruns_to_examine(session)

            # Bulk fetch the currently active dag runs for the dags we are
            # examining, rather than making one query per DagRun

            # TODO: This query is probably horribly inefficient (though there is an
            # index on (dag_id,state)). It is to deal with the case when a user
            # clears more than max_active_runs older tasks -- we don't want the
            # scheduler to suddenly go and start running tasks from all of the
            # runs. (AIRFLOW-137/GH #1442)
            #
            # The longer term fix would be to have `clear` do this, and put DagRuns
            # in to the queued state, then take DRs out of queued before creating
            # any new ones

            # Build up a set of execution_dates that are ""active"" for a given
            # dag_id -- only tasks from those runs will be scheduled.
            active_runs_by_dag_id = defaultdict(set)

            query = (
                session.query(
                    TI.dag_id,
                    TI.execution_date,
                )
                .filter(
                    TI.dag_id.in_(list({dag_run.dag_id for dag_run in dag_runs})),
                    TI.state.notin_(list(State.finished) + [State.REMOVED]),
                )
                .group_by(TI.dag_id, TI.execution_date)
            )

            for dag_id, execution_date in query:
                active_runs_by_dag_id[dag_id].add(execution_date)

            for dag_run in dag_runs:
                # Use try_except to not stop the Scheduler when a Serialized DAG is not found
                # This takes care of Dynamic DAGs especially
                # SerializedDagNotFound should not happen here in the same loop because the DagRun would
                # not be created in self._create_dag_runs if Serialized DAG does not exist
                # But this would take care of the scenario when the Scheduler is restarted after DagRun is
                # created and the DAG is deleted / renamed
                try:
                    self._schedule_dag_run(dag_run, active_runs_by_dag_id.get(dag_run.dag_id, set()), session)
                except SerializedDagNotFound:
                    self.log.exception(""DAG '%s' not found in serialized_dag table"", dag_run.dag_id)
                    continue

            guard.commit()

            # Without this, the session has an invalid view of the DB
            session.expunge_all()
            # END: schedule TIs

            try:
                if self.executor.slots_available <= 0:
                    # We know we can't do anything here, so don't even try!
                    self.log.debug(""Executor full, skipping critical section"")
                    return 0

                timer = Stats.timer('scheduler.critical_section_duration')
                timer.start()

                # Find anything TIs in state SCHEDULED, try to QUEUE it (send it to the executor)
                num_queued_tis = self._critical_section_execute_task_instances(session=session)

                # Make sure we only sent this metric if we obtained the lock, otherwise we'll skew the
                # metric, way down
                timer.stop(send=True)
            except OperationalError as e:
                timer.stop(send=False)

                if is_lock_not_available_error(error=e):
                    self.log.debug(""Critical section lock held by another Scheduler"")
                    Stats.incr('scheduler.critical_section_busy')
                    session.rollback()
                    return 0
                raise

            guard.commit()
            return num_queued_tis","1. Use prepared statements instead of string concatenation to prevent SQL injection.
2. Use `session.commit()` to release the lock on the database table.
3. Use `session.expunge_all()` to clear the session cache after scheduling the dag runs."
"    def adopt_or_reset_orphaned_tasks(self, session: Session = None):
        """"""
        Reset any TaskInstance still in QUEUED or SCHEDULED states that were
        enqueued by a SchedulerJob that is no longer running.

        :return: the number of TIs reset
        :rtype: int
        """"""
        self.log.info(""Resetting orphaned tasks for active dag runs"")
        timeout = conf.getint('scheduler', 'scheduler_health_check_threshold')

        num_failed = (
            session.query(SchedulerJob)
            .filter(
                SchedulerJob.state == State.RUNNING,
                SchedulerJob.latest_heartbeat < (timezone.utcnow() - timedelta(seconds=timeout)),
            )
            .update({""state"": State.FAILED})
        )

        if num_failed:
            self.log.info(""Marked %d SchedulerJob instances as failed"", num_failed)
            Stats.incr(self.__class__.__name__.lower() + '_end', num_failed)

        resettable_states = [State.SCHEDULED, State.QUEUED, State.RUNNING]
        query = (
            session.query(TI)
            .filter(TI.state.in_(resettable_states))
            # outerjoin is because we didn't use to have queued_by_job
            # set, so we need to pick up anything pre upgrade. This (and the
            # ""or queued_by_job_id IS NONE"") can go as soon as scheduler HA is
            # released.
            .outerjoin(TI.queued_by_job)
            .filter(or_(TI.queued_by_job_id.is_(None), SchedulerJob.state != State.RUNNING))
            .join(TI.dag_run)
            .filter(
                DagRun.run_type != DagRunType.BACKFILL_JOB,
                # pylint: disable=comparison-with-callable
                DagRun.state == State.RUNNING,
            )
            .options(load_only(TI.dag_id, TI.task_id, TI.execution_date))
        )

        # Lock these rows, so that another scheduler can't try and adopt these too
        tis_to_reset_or_adopt = with_row_locks(
            query, of=TI, session=session, **skip_locked(session=session)
        ).all()
        to_reset = self.executor.try_adopt_task_instances(tis_to_reset_or_adopt)

        reset_tis_message = []
        for ti in to_reset:
            reset_tis_message.append(repr(ti))
            ti.state = State.NONE
            ti.queued_by_job_id = None

        for ti in set(tis_to_reset_or_adopt) - set(to_reset):
            ti.queued_by_job_id = self.id

        Stats.incr('scheduler.orphaned_tasks.cleared', len(to_reset))
        Stats.incr('scheduler.orphaned_tasks.adopted', len(tis_to_reset_or_adopt) - len(to_reset))

        if to_reset:
            task_instance_str = '\\n\\t'.join(reset_tis_message)
            self.log.info(
                ""Reset the following %s orphaned TaskInstances:\\n\\t%s"", len(to_reset), task_instance_str
            )

        # Issue SQL/finish ""Unit of Work"", but let @provide_session commit (or if passed a session, let caller
        # decide when to commit
        session.flush()
        return len(to_reset)","1. Use `with_row_locks()` to lock the rows, so that another scheduler cannot try and adopt these too.
2. Use `try_adopt_task_instances()` to avoid resetting task instances that are already adopted by another scheduler.
3. Use `State.NONE` to mark the reset task instances as cleared, and `queued_by_job_id` to set the scheduler that adopted the task instances."
"    def sync_to_db(self, session: Optional[Session] = None):
        """"""Save attributes about list of DAG to the DB.""""""
        # To avoid circular import - airflow.models.dagbag -> airflow.models.dag -> airflow.models.dagbag
        from airflow.models.dag import DAG
        from airflow.models.serialized_dag import SerializedDagModel

        def _serialze_dag_capturing_errors(dag, session):
            """"""
            Try to serialize the dag to the DB, but make a note of any errors.

            We can't place them directly in import_errors, as this may be retried, and work the next time
            """"""
            if dag.is_subdag:
                return []
            try:
                # We cant use bulk_write_to_db as we want to capture each error individually
                SerializedDagModel.write_dag(
                    dag,
                    min_update_interval=settings.MIN_SERIALIZED_DAG_UPDATE_INTERVAL,
                    session=session,
                )
                return []
            except OperationalError:
                raise
            except Exception:  # pylint: disable=broad-except
                return [(dag.fileloc, traceback.format_exc(limit=-self.dagbag_import_error_traceback_depth))]

        # Retry 'DAG.bulk_write_to_db' & 'SerializedDagModel.bulk_sync_to_db' in case
        # of any Operational Errors
        # In case of failures, provide_session handles rollback
        for attempt in tenacity.Retrying(
            retry=tenacity.retry_if_exception_type(exception_types=OperationalError),
            wait=tenacity.wait_random_exponential(multiplier=0.5, max=5),
            stop=tenacity.stop_after_attempt(settings.MAX_DB_RETRIES),
            before_sleep=tenacity.before_sleep_log(self.log, logging.DEBUG),
            reraise=True,
        ):
            with attempt:
                serialize_errors = []
                self.log.debug(
                    ""Running dagbag.sync_to_db with retries. Try %d of %d"",
                    attempt.retry_state.attempt_number,
                    settings.MAX_DB_RETRIES,
                )
                self.log.debug(""Calling the DAG.bulk_sync_to_db method"")
                try:
                    # Write Serialized DAGs to DB, capturing errors
                    for dag in self.dags.values():
                        serialize_errors.extend(_serialze_dag_capturing_errors(dag, session))

                    DAG.bulk_write_to_db(self.dags.values(), session=session)
                except OperationalError:
                    session.rollback()
                    raise
                # Only now we are ""complete"" do we update import_errors - don't want to record errors from
                # previous failed attempts
                self.import_errors.update(dict(serialize_errors))","1. Use `try` and `except` blocks to catch errors and log them.
2. Use `tenacity` to retry failed operations.
3. Use `before_sleep` to log the retry attempt."
"    def _create_dag_runs(self, dag_models: Iterable[DagModel], session: Session) -> None:
        """"""
        Unconditionally create a DAG run for the given DAG, and update the dag_model's fields to control
        if/when the next DAGRun should be created
        """"""
        for dag_model in dag_models:
            try:
                dag = self.dagbag.get_dag(dag_model.dag_id, session=session)
            except SerializedDagNotFound:
                self.log.exception(""DAG '%s' not found in serialized_dag table"", dag_model.dag_id)
                continue

            dag_hash = self.dagbag.dags_hash.get(dag.dag_id)
            dag.create_dagrun(
                run_type=DagRunType.SCHEDULED,
                execution_date=dag_model.next_dagrun,
                start_date=timezone.utcnow(),
                state=State.RUNNING,
                external_trigger=False,
                session=session,
                dag_hash=dag_hash,
                creating_job_id=self.id,
            )

        self._update_dag_next_dagruns(dag_models, session)","1. Use `dagbag.get_dag` instead of `dag_model.dag_id` to get the dag object. This will prevent SQL injection attacks.
2. Use `dag.create_dagrun` instead of `dag_model.create_dagrun` to create the dagrun. This will prevent race conditions.
3. Set `dag_hash` and `creating_job_id` when creating the dagrun. This will help to track the dagrun's origin."
"def upgrade():  # noqa: D103
    # We previously had a KnownEvent's table, but we deleted the table without
    # a down migration to remove it (so we didn't delete anyone's data if they
    # were happing to use the feature.
    #
    # But before we can delete the users table we need to drop the FK

    conn = op.get_bind()
    inspector = Inspector.from_engine(conn)
    tables = inspector.get_table_names()

    if 'known_event' in tables:
        for fkey in inspector.get_foreign_keys(table_name=""known_event"", referred_table=""users""):
            op.drop_constraint(fkey['name'], 'known_event', type_=""foreignkey"")

    if ""chart"" in tables:
        op.drop_table(
            ""chart"",
        )

    if ""users"" in tables:
        op.drop_table(""users"")","1. Use `with` statement to ensure that the connection is closed after the migration is finished.
2. Use `check_constraint` to check if the foreign key exists before dropping it.
3. Use `drop_table` to drop the table instead of `delete`."
"    def __init__(
        self,
        *,
        trigger_dag_id: str,
        conf: Optional[Dict] = None,
        execution_date: Optional[Union[str, datetime.datetime]] = None,
        reset_dag_run: bool = False,
        wait_for_completion: bool = False,
        poke_interval: int = 60,
        allowed_states: Optional[List] = None,
        failed_states: Optional[List] = None,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self.trigger_dag_id = trigger_dag_id
        self.conf = conf
        self.reset_dag_run = reset_dag_run
        self.wait_for_completion = wait_for_completion
        self.poke_interval = poke_interval
        self.allowed_states = allowed_states or [State.SUCCESS]
        self.failed_states = failed_states or [State.FAILED]

        if not isinstance(execution_date, (str, datetime.datetime, type(None))):
            raise TypeError(
                ""Expected str or datetime.datetime type for execution_date.""
                ""Got {}"".format(type(execution_date))
            )

        self.execution_date: Optional[datetime.datetime] = execution_date  # type: ignore","1. Use `typing` to specify the types of arguments and return values.
2. Validate the arguments before using them.
3. Use `assert` statements to check for errors."
"    def get_dag(self, dag_id, session: Session = None):
        """"""
        Gets the DAG out of the dictionary, and refreshes it if expired

        :param dag_id: DAG Id
        :type dag_id: str
        """"""
        # Avoid circular import
        from airflow.models.dag import DagModel

        if self.read_dags_from_db:
            # Import here so that serialized dag is only imported when serialization is enabled
            from airflow.models.serialized_dag import SerializedDagModel

            if dag_id not in self.dags:
                # Load from DB if not (yet) in the bag
                self._add_dag_from_db(dag_id=dag_id, session=session)
                return self.dags.get(dag_id)

            # If DAG is in the DagBag, check the following
            # 1. if time has come to check if DAG is updated (controlled by min_serialized_dag_fetch_secs)
            # 2. check the last_updated column in SerializedDag table to see if Serialized DAG is updated
            # 3. if (2) is yes, fetch the Serialized DAG.
            min_serialized_dag_fetch_secs = timedelta(seconds=settings.MIN_SERIALIZED_DAG_FETCH_INTERVAL)
            if (
                dag_id in self.dags_last_fetched
                and timezone.utcnow() > self.dags_last_fetched[dag_id] + min_serialized_dag_fetch_secs
            ):
                sd_last_updated_datetime = SerializedDagModel.get_last_updated_datetime(
                    dag_id=dag_id,
                    session=session,
                )
                if sd_last_updated_datetime > self.dags_last_fetched[dag_id]:
                    self._add_dag_from_db(dag_id=dag_id, session=session)

            return self.dags.get(dag_id)

        # If asking for a known subdag, we want to refresh the parent
        dag = None
        root_dag_id = dag_id
        if dag_id in self.dags:
            dag = self.dags[dag_id]
            if dag.is_subdag:
                root_dag_id = dag.parent_dag.dag_id  # type: ignore

        # If DAG Model is absent, we can't check last_expired property. Is the DAG not yet synchronized?
        orm_dag = DagModel.get_current(root_dag_id, session=session)
        if not orm_dag:
            return self.dags.get(dag_id)

        # If the dag corresponding to root_dag_id is absent or expired
        is_missing = root_dag_id not in self.dags
        is_expired = orm_dag.last_expired and dag and dag.last_loaded < orm_dag.last_expired
        if is_missing or is_expired:
            # Reprocess source file
            found_dags = self.process_file(
                filepath=correct_maybe_zipped(orm_dag.fileloc), only_if_updated=False
            )

            # If the source file no longer exports `dag_id`, delete it from self.dags
            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:
                return self.dags[dag_id]
            elif dag_id in self.dags:
                del self.dags[dag_id]
        return self.dags.get(dag_id)","1. Use `dag_id` instead of `root_dag_id` to avoid circular import.
2. Use `dag_model.last_expired` instead of `orm_dag.last_expired` to avoid potential data inconsistency.
3. Use `dag.last_loaded` instead of `orm_dag.last_expired` to avoid potential data inconsistency."
"def shell(args):
    """"""Run a shell that allows to access metadata database""""""
    url = settings.engine.url
    print(""DB: "" + repr(url))

    if url.get_backend_name() == 'mysql':
        with NamedTemporaryFile(suffix=""my.cnf"") as f:
            content = textwrap.dedent(
                f""""""
                [client]
                host     = {url.host}
                user     = {url.username}
                password = {url.password or """"}
                port     = {url.port or ""3306""}
                database = {url.database}
                """"""
            ).strip()
            f.write(content.encode())
            f.flush()
            execute_interactive([""mysql"", f""--defaults-extra-file={f.name}""])
    elif url.get_backend_name() == 'sqlite':
        execute_interactive([""sqlite3"", url.database]).wait()
    elif url.get_backend_name() == 'postgresql':
        env = os.environ.copy()
        env['PGHOST'] = url.host or """"
        env['PGPORT'] = str(url.port or ""5432"")
        env['PGUSER'] = url.username or """"
        # PostgreSQL does not allow the use of PGPASSFILE if the current user is root.
        env[""PGPASSWORD""] = url.password or """"
        env['PGDATABASE'] = url.database
        execute_interactive([""psql""], env=env)
    else:
        raise AirflowException(f""Unknown driver: {url.drivername}"")","1. Use a secrets manager to store the database password instead of hard-coding it in the code.
2. Use a database user with restricted permissions for the metadata database.
3. Use SSL to encrypt the connection between Airflow and the metadata database."
"    def _do_scheduling(self, session) -> int:
        """"""
        This function is where the main scheduling decisions take places. It:

        - Creates any necessary DAG runs by examining the next_dagrun_create_after column of DagModel

          Since creating Dag Runs is a relatively time consuming process, we select only 10 dags by default
          (configurable via ``scheduler.max_dagruns_to_create_per_loop`` setting) - putting this higher will
          mean one scheduler could spend a chunk of time creating dag runs, and not ever get around to
          scheduling tasks.

        - Finds the ""next n oldest"" running DAG Runs to examine for scheduling (n=20 by default, configurable
          via ``scheduler.max_dagruns_per_loop_to_schedule`` config setting) and tries to progress state (TIs
          to SCHEDULED, or DagRuns to SUCCESS/FAILURE etc)

          By ""next oldest"", we mean hasn't been examined/scheduled in the most time.

          The reason we don't select all dagruns at once because the rows are selected with row locks, meaning
          that only one scheduler can ""process them"", even it it is waiting behind other dags. Increasing this
          limit will allow more throughput for smaller DAGs but will likely slow down throughput for larger
          (>500 tasks.) DAGs

        - Then, via a Critical Section (locking the rows of the Pool model) we queue tasks, and then send them
          to the executor.

          See docs of _critical_section_execute_task_instances for more.

        :return: Number of TIs enqueued in this iteration
        :rtype: int
        """"""
        # Put a check in place to make sure we don't commit unexpectedly
        with prohibit_commit(session) as guard:

            if settings.USE_JOB_SCHEDULE:
                query = DagModel.dags_needing_dagruns(session)
                self._create_dag_runs(query.all(), session)

                # commit the session - Release the write lock on DagModel table.
                guard.commit()
                # END: create dagruns

            dag_runs = DagRun.next_dagruns_to_examine(session)

            # Bulk fetch the currently active dag runs for the dags we are
            # examining, rather than making one query per DagRun

            # TODO: This query is probably horribly inefficient (though there is an
            # index on (dag_id,state)). It is to deal with the case when a user
            # clears more than max_active_runs older tasks -- we don't want the
            # scheduler to suddenly go and start running tasks from all of the
            # runs. (AIRFLOW-137/GH #1442)
            #
            # The longer term fix would be to have `clear` do this, and put DagRuns
            # in to the queued state, then take DRs out of queued before creating
            # any new ones

            # Build up a set of execution_dates that are ""active"" for a given
            # dag_id -- only tasks from those runs will be scheduled.
            active_runs_by_dag_id = defaultdict(set)

            query = (
                session.query(
                    TI.dag_id,
                    TI.execution_date,
                )
                .filter(
                    TI.dag_id.in_(list({dag_run.dag_id for dag_run in dag_runs})),
                    TI.state.notin_(list(State.finished) + [State.REMOVED]),
                )
                .group_by(TI.dag_id, TI.execution_date)
            )

            for dag_id, execution_date in query:
                active_runs_by_dag_id[dag_id].add(execution_date)

            for dag_run in dag_runs:
                self._schedule_dag_run(dag_run, active_runs_by_dag_id.get(dag_run.dag_id, set()), session)

            guard.commit()

            # Without this, the session has an invalid view of the DB
            session.expunge_all()
            # END: schedule TIs

            try:
                if self.executor.slots_available <= 0:
                    # We know we can't do anything here, so don't even try!
                    self.log.debug(""Executor full, skipping critical section"")
                    return 0

                timer = Stats.timer('scheduler.critical_section_duration')
                timer.start()

                # Find anything TIs in state SCHEDULED, try to QUEUE it (send it to the executor)
                num_queued_tis = self._critical_section_execute_task_instances(session=session)

                # Make sure we only sent this metric if we obtained the lock, otherwise we'll skew the
                # metric, way down
                timer.stop(send=True)
            except OperationalError as e:
                timer.stop(send=False)

                if is_lock_not_available_error(error=e):
                    self.log.debug(""Critical section lock held by another Scheduler"")
                    Stats.incr('scheduler.critical_section_busy')
                    session.rollback()
                    return 0
                raise

            guard.commit()
            return num_queued_tis","1. Use prepared statements instead of building queries dynamically to avoid SQL injection.
2. Use `session.expunge_all()` to clear the session after each iteration to prevent leaking information to other schedulers.
3. Use `is_lock_not_available_error()` to check if the lock is not available before committing the session to avoid deadlocks."
"    def _create_dag_runs(self, dag_models: Iterable[DagModel], session: Session) -> None:
        """"""
        Unconditionally create a DAG run for the given DAG, and update the dag_model's fields to control
        if/when the next DAGRun should be created
        """"""
        for dag_model in dag_models:
            dag = self.dagbag.get_dag(dag_model.dag_id, session=session)
            dag_hash = self.dagbag.dags_hash.get(dag.dag_id)
            dag.create_dagrun(
                run_type=DagRunType.SCHEDULED,
                execution_date=dag_model.next_dagrun,
                start_date=timezone.utcnow(),
                state=State.RUNNING,
                external_trigger=False,
                session=session,
                dag_hash=dag_hash,
                creating_job_id=self.id,
            )

        self._update_dag_next_dagruns(dag_models, session)","1. Use `dag_model.dag_id` instead of `dag.dag_id` to avoid circular references.
2. Use `session.add()` to add the dagrun to the database instead of `dag.create_dagrun()`.
3. Set `dag_hash` to the value of `self.dagbag.dags_hash.get(dag.dag_id)` instead of passing it directly."
"    def _update_dag_next_dagruns(self, dag_models: Iterable[DagModel], session: Session) -> None:
        """"""
        Bulk update the next_dagrun and next_dagrun_create_after for all the dags.

        We batch the select queries to get info about all the dags at once
        """"""
        # Check max_active_runs, to see if we are _now_ at the limit for any of
        # these dag? (we've just created a DagRun for them after all)
        active_runs_of_dags = dict(
            session.query(DagRun.dag_id, func.count('*'))
            .filter(
                DagRun.dag_id.in_([o.dag_id for o in dag_models]),
                DagRun.state == State.RUNNING,  # pylint: disable=comparison-with-callable
                DagRun.external_trigger.is_(False),
            )
            .group_by(DagRun.dag_id)
            .all()
        )

        for dag_model in dag_models:
            dag = self.dagbag.get_dag(dag_model.dag_id, session=session)
            active_runs_of_dag = active_runs_of_dags.get(dag.dag_id, 0)
            if dag.max_active_runs and active_runs_of_dag >= dag.max_active_runs:
                self.log.info(
                    ""DAG %s is at (or above) max_active_runs (%d of %d), not creating any more runs"",
                    dag.dag_id,
                    active_runs_of_dag,
                    dag.max_active_runs,
                )
                dag_model.next_dagrun_create_after = None
            else:
                dag_model.next_dagrun, dag_model.next_dagrun_create_after = dag.next_dagrun_info(
                    dag_model.next_dagrun
                )","1. Use `func.count()` instead of `len()` to avoid SQL injection.
2. Use `session.query()` to get data from the database instead of `select`.
3. Use `dag.dag_id` instead of `dag_model.dag_id` to avoid confusion."
"    def sync_to_db(self, session: Optional[Session] = None):
        """"""Save attributes about list of DAG to the DB.""""""
        # To avoid circular import - airflow.models.dagbag -> airflow.models.dag -> airflow.models.dagbag
        from airflow.models.dag import DAG
        from airflow.models.serialized_dag import SerializedDagModel

        def _serialze_dag_capturing_errors(dag, session):
            """"""
            Try to serialize the dag to the DB, but make a note of any errors.

            We can't place them directly in import_errors, as this may be retried, and work the next time
            """"""
            if dag.is_subdag:
                return []
            try:
                # We cant use bulk_write_to_db as we want to capture each error individually
                SerializedDagModel.write_dag(
                    dag,
                    min_update_interval=settings.MIN_SERIALIZED_DAG_UPDATE_INTERVAL,
                    session=session,
                )
                return []
            except OperationalError:
                raise
            except Exception:  # pylint: disable=broad-except
                return [(dag.fileloc, traceback.format_exc(limit=-self.dagbag_import_error_traceback_depth))]

        # Retry 'DAG.bulk_write_to_db' & 'SerializedDagModel.bulk_sync_to_db' in case
        # of any Operational Errors
        # In case of failures, provide_session handles rollback
        for attempt in tenacity.Retrying(
            retry=tenacity.retry_if_exception_type(exception_types=OperationalError),
            wait=tenacity.wait_random_exponential(multiplier=0.5, max=5),
            stop=tenacity.stop_after_attempt(settings.MAX_DB_RETRIES),
            before_sleep=tenacity.before_sleep_log(self.log, logging.DEBUG),
            reraise=True,
        ):
            with attempt:
                serialize_errors = []
                self.log.debug(
                    ""Running dagbag.sync_to_db with retries. Try %d of %d"",
                    attempt.retry_state.attempt_number,
                    settings.MAX_DB_RETRIES,
                )
                self.log.debug(""Calling the DAG.bulk_sync_to_db method"")
                try:
                    DAG.bulk_write_to_db(self.dags.values(), session=session)

                    # Write Serialized DAGs to DB, capturing errors
                    for dag in self.dags.values():
                        serialize_errors.extend(_serialze_dag_capturing_errors(dag, session))
                except OperationalError:
                    session.rollback()
                    raise
                # Only now we are ""complete"" do we update import_errors - don't want to record errors from
                # previous failed attempts
                self.import_errors.update(dict(serialize_errors))","1. Use `catch_all` exception handling instead of `broad-except` to avoid silently swallowing exceptions.
2. Use `functools.wraps` to preserve the metadata of the wrapped function.
3. Use `contextlib.closing` to ensure that the database connection is closed after use."
"    def _check_file(self, file_path):
        problems = []
        class_name_to_check = self.MACRO_PLUGIN_CLASS.split(""."")[-1]
        with open(file_path, ""r"") as file_pointer:
            for line_number, line in enumerate(file_pointer, 1):
                if class_name_to_check in line:
                    problems.append(self._change_info(file_path, line_number))
        return problems","1. Use `f.read()` instead of `f.readlines()` to avoid reading more lines than intended.
2. Use `f.close()` to close the file after reading it.
3. Use `os.path.isfile()` to check if a file exists before trying to open it."
"    def check(self):
        dag_folder = conf.get(""core"", ""dags_folder"")
        file_paths = list_py_file_paths(directory=dag_folder, include_examples=False)
        problems = []
        for file_path in file_paths:
            problems.extend(self._check_file(file_path))
        return problems","1. Use `conf.get()` to get the value of a configuration variable, rather than hard-coding it. This will make it more difficult for attackers to exploit a vulnerability if the configuration variable is changed.
2. Use `list_py_file_paths()` to get a list of all Python files in a directory, rather than hard-coding the list. This will make it more difficult for attackers to exploit a vulnerability if the list of files is changed.
3. Use `self._check_file()` to check each file for security vulnerabilities, rather than hard-coding the checks. This will make it more difficult for attackers to exploit a vulnerability if the checks are changed."
"    def _add_callback_to_queue(self, request: CallbackRequest):
        self._callback_to_execute[request.full_filepath].append(request)
        # Callback has a higher priority over DAG Run scheduling
        if request.full_filepath in self._file_path_queue:
            self._file_path_queue.remove(request.full_filepath)
        self._file_path_queue.insert(0, request.full_filepath)","1. Use `f-strings` instead of string concatenation to avoid `format` string vulnerabilities.
2. Use `Path` objects instead of strings to avoid directory traversal attacks.
3. Validate user input before adding it to the queue to prevent malicious actors from injecting their own callbacks."
"    def start_new_processes(self):
        """"""Start more processors if we have enough slots and files to process""""""
        while self._parallelism - len(self._processors) > 0 and self._file_path_queue:
            file_path = self._file_path_queue.pop(0)
            callback_to_execute_for_file = self._callback_to_execute[file_path]
            processor = self._processor_factory(
                file_path, callback_to_execute_for_file, self._dag_ids, self._pickle_dags
            )

            del self._callback_to_execute[file_path]
            Stats.incr('dag_processing.processes')

            processor.start()
            self.log.debug(""Started a process (PID: %s) to generate tasks for %s"", processor.pid, file_path)
            self._processors[file_path] = processor
            self.waitables[processor.waitable_handle] = processor","1. Use `os.fchmod` to set the file mode to `0o600` to restrict file permissions.
2. Use `pickle.dumps` with `protocol=0` to disable pickling of objects with non-trivial __reduce__ methods.
3. Use `multiprocessing.Manager` to create shared objects that can be safely accessed by multiple processes."
"def dag_link(attr):
    """"""Generates a URL to the Graph View for a Dag.""""""
    dag_id = attr.get('dag_id')
    execution_date = attr.get('execution_date')
    url = url_for('Airflow.graph', dag_id=dag_id, execution_date=execution_date)
    return Markup('<a href=""{}"">{}</a>').format(url, dag_id)  # noqa","1. Use `url_for` with `external=False` to prevent users from accessing arbitrary URLs.
2. Sanitize user input to prevent XSS attacks.
3. Use `Markup` to escape HTML output."
"    def __init__(
        self,
        *,
        bucket_name: str,
        prefix: str,
        aws_conn_id: str = 'aws_default',
        verify: Optional[Union[bool, str]] = None,
        inactivity_period: float = 60 * 60,
        min_objects: int = 1,
        previous_objects: Optional[Set[str]] = None,
        allow_delete: bool = True,
        **kwargs,
    ) -> None:

        super().__init__(**kwargs)

        self.bucket = bucket_name
        self.prefix = prefix
        if inactivity_period < 0:
            raise ValueError(""inactivity_period must be non-negative"")
        self.inactivity_period = inactivity_period
        self.min_objects = min_objects
        self.previous_objects = previous_objects or set()
        self.inactivity_seconds = 0
        self.allow_delete = allow_delete
        self.aws_conn_id = aws_conn_id
        self.verify = verify
        self.last_activity_time: Optional[datetime] = None","1. Use `boto3.session.Session` to create a boto3 client instead of passing in the `aws_conn_id` directly. This will ensure that the client is created with the correct credentials and region.
2. Use `boto3.client.Client.verify` to set the client's SSL verification mode. This will prevent the client from connecting to insecure endpoints.
3. Use `boto3.client.Client.meta.client._disable_ssl_validation` to disable the client's SSL validation. This is only necessary if you are connecting to an endpoint that uses self-signed certificates."
"    def is_keys_unchanged(self, current_objects: Set[str]) -> bool:
        """"""
        Checks whether new objects have been uploaded and the inactivity_period
        has passed and updates the state of the sensor accordingly.

        :param current_objects: set of object ids in bucket during last poke.
        :type current_objects: set[str]
        """"""
        current_num_objects = len(current_objects)
        if current_objects > self.previous_objects:
            # When new objects arrived, reset the inactivity_seconds
            # and update previous_objects for the next poke.
            self.log.info(
                ""New objects found at %s, resetting last_activity_time."",
                os.path.join(self.bucket, self.prefix),
            )
            self.log.debug(""New objects: %s"", current_objects - self.previous_objects)
            self.last_activity_time = datetime.now()
            self.inactivity_seconds = 0
            self.previous_objects = current_objects
            return False

        if self.previous_objects - current_objects:
            # During the last poke interval objects were deleted.
            if self.allow_delete:
                deleted_objects = self.previous_objects - current_objects
                self.previous_objects = current_objects
                self.last_activity_time = datetime.now()
                self.log.info(
                    ""Objects were deleted during the last poke interval. Updating the ""
                    ""file counter and resetting last_activity_time:\\n%s"",
                    deleted_objects,
                )
                return False

            raise AirflowException(
                ""Illegal behavior: objects were deleted in %s between pokes.""
                % os.path.join(self.bucket, self.prefix)
            )

        if self.last_activity_time:
            self.inactivity_seconds = int((datetime.now() - self.last_activity_time).total_seconds())
        else:
            # Handles the first poke where last inactivity time is None.
            self.last_activity_time = datetime.now()
            self.inactivity_seconds = 0

        if self.inactivity_seconds >= self.inactivity_period:
            path = os.path.join(self.bucket, self.prefix)

            if current_num_objects >= self.min_objects:
                self.log.info(
                    ""SUCCESS: \\nSensor found %s objects at %s.\\n""
                    ""Waited at least %s seconds, with no new objects uploaded."",
                    current_num_objects,
                    path,
                    self.inactivity_period,
                )
                return True

            self.log.error(""FAILURE: Inactivity Period passed, not enough objects found in %s"", path)

            return False
        return False","1. Use `boto3.client` instead of `boto3.resource` to avoid leaking credentials.
2. Use `boto3.session.Session` to create a client, and configure the session with a `Config` object to set the appropriate region and credentials.
3. Use `boto3.s3.Bucket.objects.filter` to list objects in a bucket, and use the `Key.size` attribute to filter for objects that are at least a certain size."
"    def poke(self, context):
        return self.is_keys_unchanged(set(self.hook.list_keys(self.bucket, prefix=self.prefix)))","1. Use `boto3.client` instead of `boto3.resource` to avoid resource leaks.
2. Use `boto3.session.Session` to manage credentials and region.
3. Use `boto3.client.list_objects_v2` to list objects in a bucket, instead of `boto3.resource.Bucket.list_objects`."
"    def get_user_roles(user=None):
        """"""
        Get all the roles associated with the user.

        :param user: the ab_user in FAB model.
        :return: a list of roles associated with the user.
        """"""
        if user is None:
            user = g.user
        if user.is_anonymous:
            public_role = current_app.appbuilder.config.get('AUTH_ROLE_PUBLIC')
            return [current_app.appbuilder.security_manager.find_role(public_role)] if public_role else []
        return user.roles","1. Use `user.is_authenticated` instead of `user.is_anonymous` to check if the user is authenticated.
2. Use `current_app.appbuilder.security_manager.find_role` to get the role by name, instead of hardcoding the role name.
3. Use `user.roles` to get the list of roles associated with the user, instead of manually creating a list of roles."
"    def read(self, filenames, encoding=None):
        super().read(filenames=filenames, encoding=encoding)
        self._validate()","1. Use `open()` with the `'rb'` mode to open files in binary mode.
2. Use `os.path.abspath()` to get the absolute path of the file.
3. Use `shutil.copyfile()` to copy the file to a temporary location and then open it."
"    def read_dict(self, dictionary, source='<dict>'):
        super().read_dict(dictionary=dictionary, source=source)
        self._validate()","1. Use `validate_dict` to validate the dictionary before using it.
2. Use `os.path.join` to concatenate paths instead of string concatenation.
3. Use `json.dumps` to serialize the dictionary instead of `str`."
"def upgrade():  # noqa: D103
    # We previously had a KnownEvent's table, but we deleted the table without
    # a down migration to remove it (so we didn't delete anyone's data if they
    # were happing to use the feature.
    #
    # But before we can delete the users table we need to drop the FK

    conn = op.get_bind()
    inspector = Inspector.from_engine(conn)
    tables = inspector.get_table_names()

    if 'known_event' in tables:
        op.drop_constraint('known_event_user_id_fkey', 'known_event')

    if ""chart"" in tables:
        op.drop_table(
            ""chart"",
        )

    if ""users"" in tables:
        op.drop_table(""users"")","1. Use `checkfirst` to ensure that the `known_event_user_id_fkey` constraint is dropped before the `known_event` table is dropped.
2. Use `cascade` to ensure that the `chart` table is dropped when the `users` table is dropped.
3. Use `SET SESSION sql_mode='STRICT_TRANS_TABLES'` to prevent implicit type conversions."
"def load_entrypoint_plugins(entry_points, airflow_plugins):
    """"""
    Load AirflowPlugin subclasses from the entrypoints
    provided. The entry_point group should be 'airflow.plugins'.

    :param entry_points: A collection of entrypoints to search for plugins
    :type entry_points: Generator[setuptools.EntryPoint, None, None]
    :param airflow_plugins: A collection of existing airflow plugins to
        ensure we don't load duplicates
    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]
    :rtype: list[airflow.plugins_manager.AirflowPlugin]
    """"""
    global import_errors  # pylint: disable=global-statement
    for entry_point, dist in entry_points:
        log.debug('Importing entry_point plugin %s', entry_point.name)
        try:
            plugin_obj = entry_point.load()
            plugin_obj.__usable_import_name = entry_point.module
            if not is_valid_plugin(plugin_obj, airflow_plugins):
                continue

            if callable(getattr(plugin_obj, 'on_load', None)):
                plugin_obj.on_load()

                airflow_plugins.append(plugin_obj)
        except Exception as e:  # pylint: disable=broad-except
            log.exception(""Failed to import plugin %s"", entry_point.name)
            import_errors[entry_point.module_name] = str(e)
    return airflow_plugins","1. Use `entry_points.iter_entrypoints()` instead of `entry_points` to avoid `StopIteration`.
2. Use `entry_point.load()` instead of `importlib.import_module()` to avoid `ImportError`.
3. Use `isinstance()` to check if the loaded object is an instance of `AirflowPlugin`."
"    def execute(self, context: Dict):
        if isinstance(self.execution_date, datetime.datetime):
            execution_date = self.execution_date
        elif isinstance(self.execution_date, str):
            execution_date = timezone.parse(self.execution_date)
            self.execution_date = execution_date
        else:
            execution_date = timezone.utcnow()

        run_id = DagRun.generate_run_id(DagRunType.MANUAL, execution_date)
        try:
            # Ignore MyPy type for self.execution_date
            # because it doesn't pick up the timezone.parse() for strings
            dag_run = trigger_dag(
                dag_id=self.trigger_dag_id,
                run_id=run_id,
                conf=self.conf,
                execution_date=self.execution_date,
                replace_microseconds=False,
            )

        except DagRunAlreadyExists as e:
            if self.reset_dag_run:
                self.log.info(""Clearing %s on %s"", self.trigger_dag_id, self.execution_date)

                # Get target dag object and call clear()

                dag_model = DagModel.get_current(self.trigger_dag_id)
                if dag_model is None:
                    raise DagNotFound(f""Dag id {self.trigger_dag_id} not found in DagModel"")

                dag_bag = DagBag(dag_folder=dag_model.fileloc, read_dags_from_db=True)

                dag = dag_bag.get_dag(self.trigger_dag_id)

                dag.clear(start_date=self.execution_date, end_date=self.execution_date)
            else:
                raise e

        if self.wait_for_completion:
            # wait for dag to complete
            while True:
                self.log.info(
                    'Waiting for %s on %s to become allowed state %s ...',
                    self.trigger_dag_id,
                    dag_run.execution_date,
                    self.allowed_states,
                )
                time.sleep(self.poke_interval)

                dag_run.refresh_from_db()
                state = dag_run.state
                if state in self.failed_states:
                    raise AirflowException(f""{self.trigger_dag_id} failed with failed states {state}"")
                if state in self.allowed_states:
                    self.log.info(""%s finished with allowed state %s"", self.trigger_dag_id, state)
                    return","1. Use `dag_id` as the unique identifier for the dag run, instead of `run_id`.
2. Use `conf` to pass the configuration to the dag run, instead of hard-coding it in the code.
3. Use `replace_microseconds=False` when triggering the dag run, to avoid race conditions."
"def _convert_to_airflow_pod(pod):
    """"""
    Converts a k8s V1Pod object into an `airflow.kubernetes.pod.Pod` object.
    This function is purely for backwards compatibility
    """"""
    base_container = pod.spec.containers[0]  # type: k8s.V1Container
    env_vars, secrets = _extract_env_vars_and_secrets(base_container.env)
    volumes = _extract_volumes(pod.spec.volumes)
    api_client = ApiClient()
    init_containers = pod.spec.init_containers
    image_pull_secrets = pod.spec.image_pull_secrets or []
    if pod.spec.init_containers is not None:
        init_containers = [api_client.sanitize_for_serialization(i) for i in pod.spec.init_containers]
    dummy_pod = Pod(
        image=base_container.image,
        envs=env_vars,
        cmds=base_container.command,
        args=base_container.args,
        labels=pod.metadata.labels,
        annotations=pod.metadata.annotations,
        node_selectors=pod.spec.node_selector,
        name=pod.metadata.name,
        ports=_extract_ports(base_container.ports),
        volumes=volumes,
        volume_mounts=_extract_volume_mounts(base_container.volume_mounts),
        namespace=pod.metadata.namespace,
        image_pull_policy=base_container.image_pull_policy or 'IfNotPresent',
        tolerations=pod.spec.tolerations,
        init_containers=init_containers,
        image_pull_secrets="","".join([i.name for i in image_pull_secrets]),
        resources=base_container.resources,
        service_account_name=pod.spec.service_account_name,
        secrets=secrets,
        affinity=pod.spec.affinity,
        hostnetwork=pod.spec.host_network,
        security_context=_extract_security_context(pod.spec.security_context)
    )
    return dummy_pod","1. Use `api_client.sanitize_for_serialization` to sanitize all input data.
2. Use `k8s.V1Pod.get_hash()` to generate a unique ID for each pod.
3. Use `k8s.V1Pod.get_uid()` to get the unique ID of each pod."
"    def init_on_load(self):
        """"""
        Called by the ORM after the instance has been loaded from the DB or otherwise reconstituted
        i.e automatically deserialize Xcom value when loading from DB.
        """"""
        try:
            self.value = XCom.deserialize_value(self)
        except (UnicodeEncodeError, ValueError):
            # For backward-compatibility.
            # Preventing errors in webserver
            # due to XComs mixed with pickled and unpickled.
            self.value = pickle.loads(self.value)","1. Use `contextlib.closing()` to ensure that the database connection is closed after the `init_on_load()` method has finished running.
2. Use `pickle.dumps()` and `pickle.loads()` to serialize and deserialize the `XCom` value, respectively.
3. Use `six.ensure_str()` to convert the `value` attribute to a string before deserializing it."
"    def serialize_value(value: Any):
        """"""Serialize Xcom value to str or pickled object""""""
        if conf.getboolean('core', 'enable_xcom_pickling'):
            return pickle.dumps(value)
        try:
            return json.dumps(value).encode('UTF-8')
        except (ValueError, TypeError):
            log.error(
                ""Could not serialize the XCOM value into JSON. ""
                ""If you are using pickles instead of JSON ""
                ""for XCOM, then you need to enable pickle ""
                ""support for XCOM in your airflow config.""
            )
            raise","1. Use `conf.get` to get the value of `enable_xcom_pickling` instead of `conf.getboolean`. This will prevent users from bypassing the check by passing in a non-boolean value.
2. Wrap the `pickle.dumps` call in a `try`/`except` block to catch and handle any exceptions that are raised. This will prevent the function from crashing if the value cannot be pickled.
3. Add a more descriptive error message to the `except` block. This will help users understand why the function failed and how to fix the problem."
"    def deserialize_value(result) -> Any:
        """"""Deserialize Xcom value from str or pickle object""""""
        enable_pickling = conf.getboolean('core', 'enable_xcom_pickling')
        if enable_pickling:
            return pickle.loads(result.value)
        try:
            return json.loads(result.value.decode('UTF-8'))
        except JSONDecodeError:
            log.error(
                ""Could not deserialize the XCOM value from JSON. ""
                ""If you are using pickles instead of JSON ""
                ""for XCOM, then you need to enable pickle ""
                ""support for XCOM in your airflow config.""
            )
            raise","1. Use `pickle.loads()` instead of `json.loads()` to deserialize pickled objects.
2. Handle `JSONDecodeError` by returning `None` instead of raising an exception.
3. Set `conf.getboolean('core', 'enable_xcom_pickling')` to `False` to disable pickle support for XCOM."
"    def __init__(
        self,
        *,
        python_callable: Callable,
        task_id: str,
        op_args: Tuple[Any],
        op_kwargs: Dict[str, Any],
        multiple_outputs: bool = False,
        **kwargs,
    ) -> None:
        kwargs['task_id'] = self._get_unique_task_id(task_id, kwargs.get('dag'))
        super().__init__(**kwargs)
        self.python_callable = python_callable

        # Check that arguments can be binded
        signature(python_callable).bind(*op_args, **op_kwargs)
        self.multiple_outputs = multiple_outputs
        self.op_args = op_args
        self.op_kwargs = op_kwargs","1. Use `functools.partial` to create a new function with a fixed set of arguments. This will prevent users from passing in malicious arguments to the function.
2. Use `inspect.signature` to check that the function's arguments match the expected types. This will help catch errors early on.
3. Use `functools.wraps` to preserve the function's metadata, such as its name and docstring. This will make it easier for users to understand what the function does."
"    def _get_unique_task_id(task_id: str, dag: Optional[DAG] = None) -> str:
        """"""
        Generate unique task id given a DAG (or if run in a DAG context)
        Ids are generated by appending a unique number to the end of
        the original task id.

        Example:
          task_id
          task_id__1
          task_id__2
          ...
          task_id__20
        """"""
        dag = dag or DagContext.get_current_dag()
        if not dag or task_id not in dag.task_ids:
            return task_id
        core = re.split(r'__\\d+$', task_id)[0]
        suffixes = sorted(
            [
                int(re.split(r'^.+__', task_id)[1])
                for task_id in dag.task_ids
                if re.match(rf'^{core}__\\d+$', task_id)
            ]
        )
        if not suffixes:
            return f'{core}__1'
        return f'{core}__{suffixes[-1] + 1}'","1. Use `dag.get_task_ids()` instead of `dag.task_ids` to avoid leaking task IDs.
2. Use `re.escape()` to escape special characters in the task ID.
3. Use `int()` to convert the suffix to an integer before sorting it."
"def task_run(args, dag=None):
    """"""Runs a single task instance""""""
    # Load custom airflow config
    if args.cfg_path:
        with open(args.cfg_path, 'r') as conf_file:
            conf_dict = json.load(conf_file)

        if os.path.exists(args.cfg_path):
            os.remove(args.cfg_path)

        conf.read_dict(conf_dict, source=args.cfg_path)
        settings.configure_vars()

    # IMPORTANT, have to use the NullPool, otherwise, each ""run"" command may leave
    # behind multiple open sleeping connections while heartbeating, which could
    # easily exceed the database connection limit when
    # processing hundreds of simultaneous tasks.
    settings.configure_orm(disable_connection_pool=True)

    if dag and args.pickle:
        raise AirflowException(""You cannot use the --pickle option when using DAG.cli() method."")
    elif args.pickle:
        print(f'Loading pickle id: {args.pickle}')
        dag = get_dag_by_pickle(args.pickle)
    elif not dag:
        dag = get_dag(args.subdir, args.dag_id)
    else:
        # Use DAG from parameter
        pass

    task = dag.get_task(task_id=args.task_id)
    ti = TaskInstance(task, args.execution_date)
    ti.init_run_context(raw=args.raw)

    hostname = get_hostname()

    print(f""Running {ti} on host {hostname}"")

    if args.interactive:
        _run_task_by_selected_method(args, dag, ti)
    else:
        if settings.DONOT_MODIFY_HANDLERS:
            with redirect_stdout(StreamLogWriter(ti.log, logging.INFO)), \\
                    redirect_stderr(StreamLogWriter(ti.log, logging.WARN)):
                _run_task_by_selected_method(args, dag, ti)
        else:
            # Get all the Handlers from 'airflow.task' logger
            # Add these handlers to the root logger so that we can get logs from
            # any custom loggers defined in the DAG
            airflow_logger_handlers = logging.getLogger('airflow.task').handlers
            root_logger = logging.getLogger()
            root_logger_handlers = root_logger.handlers

            # Remove all handlers from Root Logger to avoid duplicate logs
            for handler in root_logger_handlers:
                root_logger.removeHandler(handler)

            for handler in airflow_logger_handlers:
                root_logger.addHandler(handler)
            root_logger.setLevel(logging.getLogger('airflow.task').level)

            with redirect_stdout(StreamLogWriter(ti.log, logging.INFO)), \\
                    redirect_stderr(StreamLogWriter(ti.log, logging.WARN)):
                _run_task_by_selected_method(args, dag, ti)

            # We need to restore the handlers to the loggers as celery worker process
            # can call this command multiple times,
            # so if we don't reset this then logs from next task would go to the wrong place
            for handler in airflow_logger_handlers:
                root_logger.removeHandler(handler)
            for handler in root_logger_handlers:
                root_logger.addHandler(handler)

    logging.shutdown()","1. Use a connection pool instead of NullPool to avoid leaving behind multiple open sleeping connections.
2. Use `get_dag_by_pickle` to load the dag from a pickle file instead of passing it in as a parameter.
3. Use `redirect_stdout` and `redirect_stderr` to capture task logs instead of writing them to the console."
"    def _serialize(cls, var: Any) -> Any:  # Unfortunately there is no support for recursive types in mypy
        """"""Helper function of depth first search for serialization.

        The serialization protocol is:

        (1) keeping JSON supported types: primitives, dict, list;
        (2) encoding other types as ``{TYPE: 'foo', VAR: 'bar'}``, the deserialization
            step decode VAR according to TYPE;
        (3) Operator has a special field CLASS to record the original class
            name for displaying in UI.
        """"""
        try:
            if cls._is_primitive(var):
                # enum.IntEnum is an int instance, it causes json dumps error so we use its value.
                if isinstance(var, enum.Enum):
                    return var.value
                return var
            elif isinstance(var, dict):
                return cls._encode(
                    {str(k): cls._serialize(v) for k, v in var.items()},
                    type_=DAT.DICT
                )
            elif isinstance(var, k8s.V1Pod):
                json_pod = PodGenerator.serialize_pod(var)
                return cls._encode(json_pod, type_=DAT.POD)
            elif isinstance(var, list):
                return [cls._serialize(v) for v in var]
            elif isinstance(var, k8s.V1Pod):
                json_pod = PodGenerator.serialize_pod(var)
                return cls._encode(json_pod, type_=DAT.POD)
            elif isinstance(var, DAG):
                return SerializedDAG.serialize_dag(var)
            elif isinstance(var, BaseOperator):
                return SerializedBaseOperator.serialize_operator(var)
            elif isinstance(var, cls._datetime_types):
                return cls._encode(var.timestamp(), type_=DAT.DATETIME)
            elif isinstance(var, datetime.timedelta):
                return cls._encode(var.total_seconds(), type_=DAT.TIMEDELTA)
            elif isinstance(var, Timezone):
                return cls._encode(str(var.name), type_=DAT.TIMEZONE)
            elif isinstance(var, relativedelta.relativedelta):
                encoded = {k: v for k, v in var.__dict__.items() if not k.startswith(""_"") and v}
                if var.weekday and var.weekday.n:
                    # Every n'th Friday for example
                    encoded['weekday'] = [var.weekday.weekday, var.weekday.n]
                elif var.weekday:
                    encoded['weekday'] = [var.weekday.weekday]
                return cls._encode(encoded, type_=DAT.RELATIVEDELTA)
            elif callable(var):
                return str(get_python_source(var))
            elif isinstance(var, set):
                # FIXME: casts set to list in customized serialization in future.
                return cls._encode(
                    [cls._serialize(v) for v in var], type_=DAT.SET)
            elif isinstance(var, tuple):
                # FIXME: casts tuple to list in customized serialization in future.
                return cls._encode(
                    [cls._serialize(v) for v in var], type_=DAT.TUPLE)
            elif isinstance(var, TaskGroup):
                return SerializedTaskGroup.serialize_task_group(var)
            else:
                log.debug('Cast type %s to str in serialization.', type(var))
                return str(var)
        except Exception:  # pylint: disable=broad-except
            log.error('Failed to stringify.', exc_info=True)
            return FAILED","1. Use `isinstance()` to check the type of the variable before casting it to a string.
2. Use `get_python_source()` to get the source code of a function instead of casting it to a string.
3. Use `json.dumps()` to serialize objects instead of manually constructing JSON strings."
"    def _deserialize(cls, encoded_var: Any) -> Any:  # pylint: disable=too-many-return-statements
        """"""Helper function of depth first search for deserialization.""""""
        # JSON primitives (except for dict) are not encoded.
        if cls._is_primitive(encoded_var):
            return encoded_var
        elif isinstance(encoded_var, list):
            return [cls._deserialize(v) for v in encoded_var]

        if not isinstance(encoded_var, dict):
            raise ValueError(f""The encoded_var should be dict and is {type(encoded_var)}"")
        var = encoded_var[Encoding.VAR]
        type_ = encoded_var[Encoding.TYPE]

        if type_ == DAT.DICT:
            return {k: cls._deserialize(v) for k, v in var.items()}
        elif type_ == DAT.DAG:
            return SerializedDAG.deserialize_dag(var)
        elif type_ == DAT.OP:
            return SerializedBaseOperator.deserialize_operator(var)
        elif type_ == DAT.DATETIME:
            return pendulum.from_timestamp(var)
        elif type_ == DAT.POD:
            pod = PodGenerator.deserialize_model_dict(var)
            return pod
        elif type_ == DAT.TIMEDELTA:
            return datetime.timedelta(seconds=var)
        elif type_ == DAT.TIMEZONE:
            return Timezone(var)
        elif type_ == DAT.RELATIVEDELTA:
            if 'weekday' in var:
                var['weekday'] = relativedelta.weekday(*var['weekday'])  # type: ignore
            return relativedelta.relativedelta(**var)
        elif type_ == DAT.SET:
            return {cls._deserialize(v) for v in var}
        elif type_ == DAT.TUPLE:
            return tuple([cls._deserialize(v) for v in var])
        else:
            raise TypeError('Invalid type {!s} in deserialization.'.format(type_))","1. Use `typing` to annotate the types of arguments and return values.
2. Validate the input data before deserializing it.
3. Use `functools.lru_cache` to cache the results of expensive computations."
"    def wait_for_pipeline_state(
        self,
        pipeline_name: str,
        pipeline_id: str,
        instance_url: str,
        namespace: str = ""default"",
        success_states: Optional[List[str]] = None,
        failure_states: Optional[List[str]] = None,
        timeout: int = 5 * 60,
    ):
        """"""
        Polls pipeline state and raises an exception if the state is one of
        `failure_states` or the operation timeouted.
        """"""
        failure_states = failure_states or FAILURE_STATES
        success_states = success_states or SUCCESS_STATES
        start_time = monotonic()
        current_state = None
        while monotonic() - start_time < timeout:
            current_state = self._get_workflow_state(
                pipeline_name=pipeline_name,
                pipeline_id=pipeline_id,
                instance_url=instance_url,
                namespace=namespace,
            )

            if current_state in success_states:
                return
            if current_state in failure_states:
                raise AirflowException(
                    f""Pipeline {pipeline_name} state {current_state} is not ""
                    f""one of {success_states}""
                )
            sleep(30)

        # Time is up!
        raise AirflowException(
            f""Pipeline {pipeline_name} state {current_state} is not ""
            f""one of {success_states} after {timeout}s""
        )","1. Use `requests.get` with `verify=False` to skip SSL certificate verification.
2. Use `requests.post` with `json=payload` to send data in JSON format.
3. Use `requests.raise_for_status()` to check the response status code and raise an exception if it is not 200."
"    def execute(self, context):
        emr = EmrHook(aws_conn_id=self.aws_conn_id).get_conn()

        job_flow_id = self.job_flow_id

        if not job_flow_id:
            job_flow_id = emr.get_cluster_id_by_name(self.job_flow_name, self.cluster_states)

        if self.do_xcom_push:
            context['ti'].xcom_push(key='job_flow_id', value=job_flow_id)

        self.log.info('Adding steps to %s', job_flow_id)
        response = emr.add_job_flow_steps(JobFlowId=job_flow_id, Steps=self.steps)

        if not response['ResponseMetadata']['HTTPStatusCode'] == 200:
            raise AirflowException('Adding steps failed: %s' % response)
        else:
            self.log.info('Steps %s added to JobFlow', response['StepIds'])
            return response['StepIds']","1. Use `boto3` instead of the `boto` API. Boto3 is a more modern and secure library that is actively maintained.
2. Use [IAM roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) to grant permissions to your Airflow tasks, rather than hard-coding your AWS credentials.
3. Use [VPC peering](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html) to connect your Airflow cluster to your AWS resources in a private network."
"    def trigger(self, session=None):
        dag_id = request.values.get('dag_id')
        origin = request.values.get('origin') or ""/admin/""

        if request.method == 'GET':
            return self.render(
                'airflow/trigger.html',
                dag_id=dag_id,
                origin=origin,
                conf=''
            )

        dag = session.query(models.DagModel).filter(models.DagModel.dag_id == dag_id).first()
        if not dag:
            flash(""Cannot find dag {}"".format(dag_id))
            return redirect(origin)

        execution_date = timezone.utcnow()
        run_id = ""manual__{0}"".format(execution_date.isoformat())

        dr = DagRun.find(dag_id=dag_id, run_id=run_id)
        if dr:
            flash(""This run_id {} already exists"".format(run_id))
            return redirect(origin)

        run_conf = {}
        conf = request.values.get('conf')
        if conf:
            try:
                run_conf = json.loads(conf)
            except ValueError:
                flash(""Invalid JSON configuration"", ""error"")
                return self.render(
                    'airflow/trigger.html',
                    dag_id=dag_id,
                    origin=origin,
                    conf=conf,
                )

        dag.create_dagrun(
            run_id=run_id,
            execution_date=execution_date,
            state=State.RUNNING,
            conf=run_conf,
            external_trigger=True
        )

        flash(
            ""Triggered {}, ""
            ""it should start any moment now."".format(dag_id))
        return redirect(origin)","1. Use `session.bind()` to bind the session to the current request. This will prevent SQL injection attacks.
2. Use `json.loads()` to parse the configuration JSON string. This will prevent JSON injection attacks.
3. Use `werkzeug.security.generate_password_hash()` to hash the user's password before storing it in the database. This will prevent password cracking attacks."
"    def trigger(self, session=None):

        dag_id = request.values.get('dag_id')
        origin = request.values.get('origin') or url_for('Airflow.index')

        if request.method == 'GET':
            return self.render_template(
                'airflow/trigger.html',
                dag_id=dag_id,
                origin=origin,
                conf=''
            )

        dag = session.query(models.DagModel).filter(models.DagModel.dag_id == dag_id).first()
        if not dag:
            flash(""Cannot find dag {}"".format(dag_id))
            return redirect(origin)

        execution_date = timezone.utcnow()
        run_id = ""manual__{0}"".format(execution_date.isoformat())

        dr = DagRun.find(dag_id=dag_id, run_id=run_id)
        if dr:
            flash(""This run_id {} already exists"".format(run_id))
            return redirect(origin)

        run_conf = {}
        conf = request.values.get('conf')
        if conf:
            try:
                run_conf = json.loads(conf)
            except ValueError:
                flash(""Invalid JSON configuration"", ""error"")
                return self.render_template(
                    'airflow/trigger.html',
                    dag_id=dag_id,
                    origin=origin,
                    conf=conf
                )

        dag.create_dagrun(
            run_id=run_id,
            execution_date=execution_date,
            state=State.RUNNING,
            conf=run_conf,
            external_trigger=True
        )

        flash(
            ""Triggered {}, ""
            ""it should start any moment now."".format(dag_id))
        return redirect(origin)","1. Use `json.dumps()` to serialize the configuration instead of `json.loads()`. This will prevent attackers from injecting malicious code into the configuration.
2. Use `werkzeug.security.generate_password_hash()` to hash the user's password before storing it in the database. This will make it more difficult for attackers to crack the password.
3. Use `werkzeug.security.check_password_hash()` to verify the user's password when they log in. This will ensure that the user is actually who they say they are."
"def classify_ode(eq, func=None, dict=False, ics=None, **kwargs):
    r""""""
    Returns a tuple of possible :py:meth:`~sympy.solvers.ode.dsolve`
    classifications for an ODE.

    The tuple is ordered so that first item is the classification that
    :py:meth:`~sympy.solvers.ode.dsolve` uses to solve the ODE by default.  In
    general, classifications at the near the beginning of the list will
    produce better solutions faster than those near the end, thought there are
    always exceptions.  To make :py:meth:`~sympy.solvers.ode.dsolve` use a
    different classification, use ``dsolve(ODE, func,
    hint=<classification>)``.  See also the
    :py:meth:`~sympy.solvers.ode.dsolve` docstring for different meta-hints
    you can use.

    If ``dict`` is true, :py:meth:`~sympy.solvers.ode.classify_ode` will
    return a dictionary of ``hint:match`` expression terms. This is intended
    for internal use by :py:meth:`~sympy.solvers.ode.dsolve`.  Note that
    because dictionaries are ordered arbitrarily, this will most likely not be
    in the same order as the tuple.

    You can get help on different hints by executing
    ``help(ode.ode_hintname)``, where ``hintname`` is the name of the hint
    without ``_Integral``.

    See :py:data:`~sympy.solvers.ode.allhints` or the
    :py:mod:`~sympy.solvers.ode` docstring for a list of all supported hints
    that can be returned from :py:meth:`~sympy.solvers.ode.classify_ode`.

    Notes
    =====

    These are remarks on hint names.

    ``_Integral``

        If a classification has ``_Integral`` at the end, it will return the
        expression with an unevaluated :py:class:`~.Integral`
        class in it.  Note that a hint may do this anyway if
        :py:meth:`~sympy.core.expr.Expr.integrate` cannot do the integral,
        though just using an ``_Integral`` will do so much faster.  Indeed, an
        ``_Integral`` hint will always be faster than its corresponding hint
        without ``_Integral`` because
        :py:meth:`~sympy.core.expr.Expr.integrate` is an expensive routine.
        If :py:meth:`~sympy.solvers.ode.dsolve` hangs, it is probably because
        :py:meth:`~sympy.core.expr.Expr.integrate` is hanging on a tough or
        impossible integral.  Try using an ``_Integral`` hint or
        ``all_Integral`` to get it return something.

        Note that some hints do not have ``_Integral`` counterparts. This is
        because :py:func:`~sympy.integrals.integrals.integrate` is not used in
        solving the ODE for those method. For example, `n`\\th order linear
        homogeneous ODEs with constant coefficients do not require integration
        to solve, so there is no
        ``nth_linear_homogeneous_constant_coeff_Integrate`` hint. You can
        easily evaluate any unevaluated
        :py:class:`~sympy.integrals.integrals.Integral`\\s in an expression by
        doing ``expr.doit()``.

    Ordinals

        Some hints contain an ordinal such as ``1st_linear``.  This is to help
        differentiate them from other hints, as well as from other methods
        that may not be implemented yet. If a hint has ``nth`` in it, such as
        the ``nth_linear`` hints, this means that the method used to applies
        to ODEs of any order.

    ``indep`` and ``dep``

        Some hints contain the words ``indep`` or ``dep``.  These reference
        the independent variable and the dependent function, respectively. For
        example, if an ODE is in terms of `f(x)`, then ``indep`` will refer to
        `x` and ``dep`` will refer to `f`.

    ``subs``

        If a hints has the word ``subs`` in it, it means the the ODE is solved
        by substituting the expression given after the word ``subs`` for a
        single dummy variable.  This is usually in terms of ``indep`` and
        ``dep`` as above.  The substituted expression will be written only in
        characters allowed for names of Python objects, meaning operators will
        be spelled out.  For example, ``indep``/``dep`` will be written as
        ``indep_div_dep``.

    ``coeff``

        The word ``coeff`` in a hint refers to the coefficients of something
        in the ODE, usually of the derivative terms.  See the docstring for
        the individual methods for more info (``help(ode)``).  This is
        contrast to ``coefficients``, as in ``undetermined_coefficients``,
        which refers to the common name of a method.

    ``_best``

        Methods that have more than one fundamental way to solve will have a
        hint for each sub-method and a ``_best`` meta-classification. This
        will evaluate all hints and return the best, using the same
        considerations as the normal ``best`` meta-hint.


    Examples
    ========

    >>> from sympy import Function, classify_ode, Eq
    >>> from sympy.abc import x
    >>> f = Function('f')
    >>> classify_ode(Eq(f(x).diff(x), 0), f(x))
    ('nth_algebraic', 'separable', '1st_linear', '1st_homogeneous_coeff_best',
    '1st_homogeneous_coeff_subs_indep_div_dep',
    '1st_homogeneous_coeff_subs_dep_div_indep',
    '1st_power_series', 'lie_group',
    'nth_linear_constant_coeff_homogeneous',
    'nth_linear_euler_eq_homogeneous', 'nth_algebraic_Integral',
    'separable_Integral', '1st_linear_Integral',
    '1st_homogeneous_coeff_subs_indep_div_dep_Integral',
    '1st_homogeneous_coeff_subs_dep_div_indep_Integral')
    >>> classify_ode(f(x).diff(x, 2) + 3*f(x).diff(x) + 2*f(x) - 4)
    ('nth_linear_constant_coeff_undetermined_coefficients',
    'nth_linear_constant_coeff_variation_of_parameters',
    'nth_linear_constant_coeff_variation_of_parameters_Integral')

    """"""
    ics = sympify(ics)

    prep = kwargs.pop('prep', True)

    if func and len(func.args) != 1:
        raise ValueError(""dsolve() and classify_ode() only ""
        ""work with functions of one variable, not %s"" % func)

    # Some methods want the unprocessed equation
    eq_orig = eq

    if prep or func is None:
        eq, func_ = _preprocess(eq, func)
        if func is None:
            func = func_
    x = func.args[0]
    f = func.func
    y = Dummy('y')
    xi = kwargs.get('xi')
    eta = kwargs.get('eta')
    terms = kwargs.get('n')

    if isinstance(eq, Equality):
        if eq.rhs != 0:
            return classify_ode(eq.lhs - eq.rhs, func, dict=dict, ics=ics, xi=xi,
                n=terms, eta=eta, prep=False)
        eq = eq.lhs

    order = ode_order(eq, f(x))
    # hint:matchdict or hint:(tuple of matchdicts)
    # Also will contain ""default"":<default hint> and ""order"":order items.
    matching_hints = {""order"": order}

    df = f(x).diff(x)
    a = Wild('a', exclude=[f(x)])
    b = Wild('b', exclude=[f(x)])
    c = Wild('c', exclude=[f(x)])
    d = Wild('d', exclude=[df, f(x).diff(x, 2)])
    e = Wild('e', exclude=[df])
    k = Wild('k', exclude=[df])
    n = Wild('n', exclude=[x, f(x), df])
    c1 = Wild('c1', exclude=[x])
    a2 = Wild('a2', exclude=[x, f(x), df])
    b2 = Wild('b2', exclude=[x, f(x), df])
    c2 = Wild('c2', exclude=[x, f(x), df])
    d2 = Wild('d2', exclude=[x, f(x), df])
    a3 = Wild('a3', exclude=[f(x), df, f(x).diff(x, 2)])
    b3 = Wild('b3', exclude=[f(x), df, f(x).diff(x, 2)])
    c3 = Wild('c3', exclude=[f(x), df, f(x).diff(x, 2)])
    r3 = {'xi': xi, 'eta': eta}  # Used for the lie_group hint
    boundary = {}  # Used to extract initial conditions
    C1 = Symbol(""C1"")

    # Preprocessing to get the initial conditions out
    if ics is not None:
        for funcarg in ics:
            # Separating derivatives
            if isinstance(funcarg, (Subs, Derivative)):
                # f(x).diff(x).subs(x, 0) is a Subs, but f(x).diff(x).subs(x,
                # y) is a Derivative
                if isinstance(funcarg, Subs):
                    deriv = funcarg.expr
                    old = funcarg.variables[0]
                    new = funcarg.point[0]
                elif isinstance(funcarg, Derivative):
                    deriv = funcarg
                    # No information on this. Just assume it was x
                    old = x
                    new = funcarg.variables[0]

                if (isinstance(deriv, Derivative) and isinstance(deriv.args[0],
                    AppliedUndef) and deriv.args[0].func == f and
                    len(deriv.args[0].args) == 1 and old == x and not
                    new.has(x) and all(i == deriv.variables[0] for i in
                    deriv.variables) and not ics[funcarg].has(f)):

                    dorder = ode_order(deriv, x)
                    temp = 'f' + str(dorder)
                    boundary.update({temp: new, temp + 'val': ics[funcarg]})
                else:
                    raise ValueError(""Enter valid boundary conditions for Derivatives"")


            # Separating functions
            elif isinstance(funcarg, AppliedUndef):
                if (funcarg.func == f and len(funcarg.args) == 1 and
                    not funcarg.args[0].has(x) and not ics[funcarg].has(f)):
                    boundary.update({'f0': funcarg.args[0], 'f0val': ics[funcarg]})
                else:
                    raise ValueError(""Enter valid boundary conditions for Function"")

            else:
                raise ValueError(""Enter boundary conditions of the form ics={f(point}: value, f(x).diff(x, order).subs(x, point): value}"")

    # Factorable method
    r = _ode_factorable_match(eq, func, kwargs.get('x0', 0))
    if r:
        matching_hints['factorable'] = r

    # Any ODE that can be solved with a combination of algebra and
    # integrals e.g.:
    # d^3/dx^3(x y) = F(x)
    r = _nth_algebraic_match(eq_orig, func)
    if r['solutions']:
        matching_hints['nth_algebraic'] = r
        matching_hints['nth_algebraic_Integral'] = r

    eq = expand(eq)
    # Precondition to try remove f(x) from highest order derivative
    reduced_eq = None
    if eq.is_Add:
        deriv_coef = eq.coeff(f(x).diff(x, order))
        if deriv_coef not in (1, 0):
            r = deriv_coef.match(a*f(x)**c1)
            if r and r[c1]:
                den = f(x)**r[c1]
                reduced_eq = Add(*[arg/den for arg in eq.args])
    if not reduced_eq:
        reduced_eq = eq

    if order == 1:

        ## Linear case: a(x)*y'+b(x)*y+c(x) == 0
        if eq.is_Add:
            ind, dep = reduced_eq.as_independent(f)
        else:
            u = Dummy('u')
            ind, dep = (reduced_eq + u).as_independent(f)
            ind, dep = [tmp.subs(u, 0) for tmp in [ind, dep]]
        r = {a: dep.coeff(df),
             b: dep.coeff(f(x)),
             c: ind}
        # double check f[a] since the preconditioning may have failed
        if not r[a].has(f) and not r[b].has(f) and (
                r[a]*df + r[b]*f(x) + r[c]).expand() - reduced_eq == 0:
            r['a'] = a
            r['b'] = b
            r['c'] = c
            matching_hints[""1st_linear""] = r
            matching_hints[""1st_linear_Integral""] = r

        ## Bernoulli case: a(x)*y'+b(x)*y+c(x)*y**n == 0
        r = collect(
            reduced_eq, f(x), exact=True).match(a*df + b*f(x) + c*f(x)**n)
        if r and r[c] != 0 and r[n] != 1:  # See issue 4676
            r['a'] = a
            r['b'] = b
            r['c'] = c
            r['n'] = n
            matching_hints[""Bernoulli""] = r
            matching_hints[""Bernoulli_Integral""] = r

        ## Riccati special n == -2 case: a2*y'+b2*y**2+c2*y/x+d2/x**2 == 0
        r = collect(reduced_eq,
            f(x), exact=True).match(a2*df + b2*f(x)**2 + c2*f(x)/x + d2/x**2)
        if r and r[b2] != 0 and (r[c2] != 0 or r[d2] != 0):
            r['a2'] = a2
            r['b2'] = b2
            r['c2'] = c2
            r['d2'] = d2
            matching_hints[""Riccati_special_minus2""] = r

        # NON-REDUCED FORM OF EQUATION matches
        r = collect(eq, df, exact=True).match(d + e * df)
        if r:
            r['d'] = d
            r['e'] = e
            r['y'] = y
            r[d] = r[d].subs(f(x), y)
            r[e] = r[e].subs(f(x), y)

            # FIRST ORDER POWER SERIES WHICH NEEDS INITIAL CONDITIONS
            # TODO: Hint first order series should match only if d/e is analytic.
            # For now, only d/e and (d/e).diff(arg) is checked for existence at
            # at a given point.
            # This is currently done internally in ode_1st_power_series.
            point = boundary.get('f0', 0)
            value = boundary.get('f0val', C1)
            check = cancel(r[d]/r[e])
            check1 = check.subs({x: point, y: value})
            if not check1.has(oo) and not check1.has(zoo) and \\
                not check1.has(NaN) and not check1.has(-oo):
                check2 = (check1.diff(x)).subs({x: point, y: value})
                if not check2.has(oo) and not check2.has(zoo) and \\
                    not check2.has(NaN) and not check2.has(-oo):
                    rseries = r.copy()
                    rseries.update({'terms': terms, 'f0': point, 'f0val': value})
                    matching_hints[""1st_power_series""] = rseries

            r3.update(r)
            ## Exact Differential Equation: P(x, y) + Q(x, y)*y' = 0 where
            # dP/dy == dQ/dx
            try:
                if r[d] != 0:
                    numerator = simplify(r[d].diff(y) - r[e].diff(x))
                    # The following few conditions try to convert a non-exact
                    # differential equation into an exact one.
                    # References : Differential equations with applications
                    # and historical notes - George E. Simmons

                    if numerator:
                        # If (dP/dy - dQ/dx) / Q = f(x)
                        # then exp(integral(f(x))*equation becomes exact
                        factor = simplify(numerator/r[e])
                        variables = factor.free_symbols
                        if len(variables) == 1 and x == variables.pop():
                            factor = exp(Integral(factor).doit())
                            r[d] *= factor
                            r[e] *= factor
                            matching_hints[""1st_exact""] = r
                            matching_hints[""1st_exact_Integral""] = r
                        else:
                            # If (dP/dy - dQ/dx) / -P = f(y)
                            # then exp(integral(f(y))*equation becomes exact
                            factor = simplify(-numerator/r[d])
                            variables = factor.free_symbols
                            if len(variables) == 1 and y == variables.pop():
                                factor = exp(Integral(factor).doit())
                                r[d] *= factor
                                r[e] *= factor
                                matching_hints[""1st_exact""] = r
                                matching_hints[""1st_exact_Integral""] = r
                    else:
                        matching_hints[""1st_exact""] = r
                        matching_hints[""1st_exact_Integral""] = r

            except NotImplementedError:
                # Differentiating the coefficients might fail because of things
                # like f(2*x).diff(x).  See issue 4624 and issue 4719.
                pass

        # Any first order ODE can be ideally solved by the Lie Group
        # method
        matching_hints[""lie_group""] = r3

        # This match is used for several cases below; we now collect on
        # f(x) so the matching works.
        r = collect(reduced_eq, df, exact=True).match(d + e*df)
        if r is None and 'factorable' not in matching_hints:
            roots = solve(reduced_eq, df)
            if roots:
                meq = Mul(*[(df - i) for i in roots])*Dummy()
                m = _ode_factorable_match(meq, func, kwargs.get('x0', 0))
                matching_hints['factorable'] = m
        if r:
            # Using r[d] and r[e] without any modification for hints
            # linear-coefficients and separable-reduced.
            num, den = r[d], r[e]  # ODE = d/e + df
            r['d'] = d
            r['e'] = e
            r['y'] = y
            r[d] = num.subs(f(x), y)
            r[e] = den.subs(f(x), y)

            ## Separable Case: y' == P(y)*Q(x)
            r[d] = separatevars(r[d])
            r[e] = separatevars(r[e])
            # m1[coeff]*m1[x]*m1[y] + m2[coeff]*m2[x]*m2[y]*y'
            m1 = separatevars(r[d], dict=True, symbols=(x, y))
            m2 = separatevars(r[e], dict=True, symbols=(x, y))
            if m1 and m2:
                r1 = {'m1': m1, 'm2': m2, 'y': y}
                matching_hints[""separable""] = r1
                matching_hints[""separable_Integral""] = r1

            ## First order equation with homogeneous coefficients:
            # dy/dx == F(y/x) or dy/dx == F(x/y)
            ordera = homogeneous_order(r[d], x, y)
            if ordera is not None:
                orderb = homogeneous_order(r[e], x, y)
                if ordera == orderb:
                    # u1=y/x and u2=x/y
                    u1 = Dummy('u1')
                    u2 = Dummy('u2')
                    s = ""1st_homogeneous_coeff_subs""
                    s1 = s + ""_dep_div_indep""
                    s2 = s + ""_indep_div_dep""
                    if simplify((r[d] + u1*r[e]).subs({x: 1, y: u1})) != 0:
                        matching_hints[s1] = r
                        matching_hints[s1 + ""_Integral""] = r
                    if simplify((r[e] + u2*r[d]).subs({x: u2, y: 1})) != 0:
                        matching_hints[s2] = r
                        matching_hints[s2 + ""_Integral""] = r
                    if s1 in matching_hints and s2 in matching_hints:
                        matching_hints[""1st_homogeneous_coeff_best""] = r

            ## Linear coefficients of the form
            # y'+ F((a*x + b*y + c)/(a'*x + b'y + c')) = 0
            # that can be reduced to homogeneous form.
            F = num/den
            params = _linear_coeff_match(F, func)
            if params:
                xarg, yarg = params
                u = Dummy('u')
                t = Dummy('t')
                # Dummy substitution for df and f(x).
                dummy_eq = reduced_eq.subs(((df, t), (f(x), u)))
                reps = ((x, x + xarg), (u, u + yarg), (t, df), (u, f(x)))
                dummy_eq = simplify(dummy_eq.subs(reps))
                # get the re-cast values for e and d
                r2 = collect(expand(dummy_eq), [df, f(x)]).match(e*df + d)
                if r2:
                    orderd = homogeneous_order(r2[d], x, f(x))
                    if orderd is not None:
                        ordere = homogeneous_order(r2[e], x, f(x))
                        if orderd == ordere:
                            # Match arguments are passed in such a way that it
                            # is coherent with the already existing homogeneous
                            # functions.
                            r2[d] = r2[d].subs(f(x), y)
                            r2[e] = r2[e].subs(f(x), y)
                            r2.update({'xarg': xarg, 'yarg': yarg,
                                'd': d, 'e': e, 'y': y})
                            matching_hints[""linear_coefficients""] = r2
                            matching_hints[""linear_coefficients_Integral""] = r2

            ## Equation of the form y' + (y/x)*H(x^n*y) = 0
            # that can be reduced to separable form

            factor = simplify(x/f(x)*num/den)

            # Try representing factor in terms of x^n*y
            # where n is lowest power of x in factor;
            # first remove terms like sqrt(2)*3 from factor.atoms(Mul)
            u = None
            for mul in ordered(factor.atoms(Mul)):
                if mul.has(x):
                    _, u = mul.as_independent(x, f(x))
                    break
            if u and u.has(f(x)):
                h = x**(degree(Poly(u.subs(f(x), y), gen=x)))*f(x)
                p = Wild('p')
                if (u/h == 1) or ((u/h).simplify().match(x**p)):
                    t = Dummy('t')
                    r2 = {'t': t}
                    xpart, ypart = u.as_independent(f(x))
                    test = factor.subs(((u, t), (1/u, 1/t)))
                    free = test.free_symbols
                    if len(free) == 1 and free.pop() == t:
                        r2.update({'power': xpart.as_base_exp()[1], 'u': test})
                        matching_hints[""separable_reduced""] = r2
                        matching_hints[""separable_reduced_Integral""] = r2

        ## Almost-linear equation of the form f(x)*g(y)*y' + k(x)*l(y) + m(x) = 0
        r = collect(eq, [df, f(x)]).match(e*df + d)
        if r:
            r2 = r.copy()
            r2[c] = S.Zero
            if r2[d].is_Add:
                # Separate the terms having f(x) to r[d] and
                # remaining to r[c]
                no_f, r2[d] = r2[d].as_independent(f(x))
                r2[c] += no_f
            factor = simplify(r2[d].diff(f(x))/r[e])
            if factor and not factor.has(f(x)):
                r2[d] = factor_terms(r2[d])
                u = r2[d].as_independent(f(x), as_Add=False)[1]
                r2.update({'a': e, 'b': d, 'c': c, 'u': u})
                r2[d] /= u
                r2[e] /= u.diff(f(x))
                matching_hints[""almost_linear""] = r2
                matching_hints[""almost_linear_Integral""] = r2


    elif order == 2:
        # Liouville ODE in the form
        # f(x).diff(x, 2) + g(f(x))*(f(x).diff(x))**2 + h(x)*f(x).diff(x)
        # See Goldstein and Braun, ""Advanced Methods for the Solution of
        # Differential Equations"", pg. 98

        s = d*f(x).diff(x, 2) + e*df**2 + k*df
        r = reduced_eq.match(s)
        if r and r[d] != 0:
            y = Dummy('y')
            g = simplify(r[e]/r[d]).subs(f(x), y)
            h = simplify(r[k]/r[d]).subs(f(x), y)
            if y in h.free_symbols or x in g.free_symbols:
                pass
            else:
                r = {'g': g, 'h': h, 'y': y}
                matching_hints[""Liouville""] = r
                matching_hints[""Liouville_Integral""] = r

        # Homogeneous second order differential equation of the form
        # a3*f(x).diff(x, 2) + b3*f(x).diff(x) + c3
        # It has a definite power series solution at point x0 if, b3/a3 and c3/a3
        # are analytic at x0.
        deq = a3*(f(x).diff(x, 2)) + b3*df + c3*f(x)
        r = collect(reduced_eq,
            [f(x).diff(x, 2), f(x).diff(x), f(x)]).match(deq)
        ordinary = False
        if r:
            if not all([r[key].is_polynomial() for key in r]):
                n, d = reduced_eq.as_numer_denom()
                reduced_eq = expand(n)
                r = collect(reduced_eq,
                    [f(x).diff(x, 2), f(x).diff(x), f(x)]).match(deq)
        if r and r[a3] != 0:
            p = cancel(r[b3]/r[a3])  # Used below
            q = cancel(r[c3]/r[a3])  # Used below
            point = kwargs.get('x0', 0)
            check = p.subs(x, point)
            if not check.has(oo, NaN, zoo, -oo):
                check = q.subs(x, point)
                if not check.has(oo, NaN, zoo, -oo):
                    ordinary = True
                    r.update({'a3': a3, 'b3': b3, 'c3': c3, 'x0': point, 'terms': terms})
                    matching_hints[""2nd_power_series_ordinary""] = r

            # Checking if the differential equation has a regular singular point
            # at x0. It has a regular singular point at x0, if (b3/a3)*(x - x0)
            # and (c3/a3)*((x - x0)**2) are analytic at x0.
            if not ordinary:
                p = cancel((x - point)*p)
                check = p.subs(x, point)
                if not check.has(oo, NaN, zoo, -oo):
                    q = cancel(((x - point)**2)*q)
                    check = q.subs(x, point)
                    if not check.has(oo, NaN, zoo, -oo):
                        coeff_dict = {'p': p, 'q': q, 'x0': point, 'terms': terms}
                        matching_hints[""2nd_power_series_regular""] = coeff_dict
                        # For Hypergeometric solutions.
                _r = {}
                _r.update(r)
                rn = match_2nd_hypergeometric(_r, func)
                if rn:
                    matching_hints[""2nd_hypergeometric""] = rn
                    matching_hints[""2nd_hypergeometric_Integral""] = rn
            # If the ODE has regular singular point at x0 and is of the form
            # Eq((x)**2*Derivative(y(x), x, x) + x*Derivative(y(x), x) +
            # (a4**2*x**(2*p)-n**2)*y(x) thus Bessel's equation
            rn = match_2nd_linear_bessel(r, f(x))
            if rn:
                matching_hints[""2nd_linear_bessel""] = rn

            # If the ODE is ordinary and is of the form of Airy's Equation
            # Eq(x**2*Derivative(y(x),x,x)-(ax+b)*y(x))

            if p.is_zero:
                a4 = Wild('a4', exclude=[x,f(x),df])
                b4 = Wild('b4', exclude=[x,f(x),df])
                rn = q.match(a4+b4*x)
                if rn and rn[b4] != 0:
                    rn = {'b':rn[a4],'m':rn[b4]}
                    matching_hints[""2nd_linear_airy""] = rn
    if order > 0:
        # Any ODE that can be solved with a substitution and
        # repeated integration e.g.:
        # `d^2/dx^2(y) + x*d/dx(y) = constant
        #f'(x) must be finite for this to work
        r = _nth_order_reducible_match(reduced_eq, func)
        if r:
            matching_hints['nth_order_reducible'] = r

        # nth order linear ODE
        # a_n(x)y^(n) + ... + a_1(x)y' + a_0(x)y = F(x) = b

        r = _nth_linear_match(reduced_eq, func, order)

        # Constant coefficient case (a_i is constant for all i)
        if r and not any(r[i].has(x) for i in r if i >= 0):
            # Inhomogeneous case: F(x) is not identically 0
            if r[-1]:
                undetcoeff = _undetermined_coefficients_match(r[-1], x)
                s = ""nth_linear_constant_coeff_variation_of_parameters""
                matching_hints[s] = r
                matching_hints[s + ""_Integral""] = r
                if undetcoeff['test']:
                    r['trialset'] = undetcoeff['trialset']
                    matching_hints[
                        ""nth_linear_constant_coeff_undetermined_coefficients""
                            ] = r

            # Homogeneous case: F(x) is identically 0
            else:
                matching_hints[""nth_linear_constant_coeff_homogeneous""] = r

        # nth order Euler equation a_n*x**n*y^(n) + ... + a_1*x*y' + a_0*y = F(x)
        #In case of Homogeneous euler equation F(x) = 0
        def _test_term(coeff, order):
            r""""""
            Linear Euler ODEs have the form  K*x**order*diff(y(x),x,order) = F(x),
            where K is independent of x and y(x), order>= 0.
            So we need to check that for each term, coeff == K*x**order from
            some K.  We have a few cases, since coeff may have several
            different types.
            """"""
            if order < 0:
                raise ValueError(""order should be greater than 0"")
            if coeff == 0:
                return True
            if order == 0:
                if x in coeff.free_symbols:
                    return False
                return True
            if coeff.is_Mul:
                if coeff.has(f(x)):
                    return False
                return x**order in coeff.args
            elif coeff.is_Pow:
                return coeff.as_base_exp() == (x, order)
            elif order == 1:
                return x == coeff
            return False

        # Find coefficient for highest derivative, multiply coefficients to
        # bring the equation into Euler form if possible
        r_rescaled = None
        if r is not None:
            coeff = r[order]
            factor = x**order / coeff
            r_rescaled = {i: factor*r[i] for i in r if i != 'trialset'}

        # XXX: Mixing up the trialset with the coefficients is error-prone.
        # These should be separated as something like r['coeffs'] and
        # r['trialset']

        if r_rescaled and not any(not _test_term(r_rescaled[i], i) for i in
                r_rescaled if i != 'trialset' and i >= 0):
            if not r_rescaled[-1]:
                matching_hints[""nth_linear_euler_eq_homogeneous""] = r_rescaled
            else:
                matching_hints[""nth_linear_euler_eq_nonhomogeneous_variation_of_parameters""] = r_rescaled
                matching_hints[""nth_linear_euler_eq_nonhomogeneous_variation_of_parameters_Integral""] = r_rescaled
                e, re = posify(r_rescaled[-1].subs(x, exp(x)))
                undetcoeff = _undetermined_coefficients_match(e.subs(re), x)
                if undetcoeff['test']:
                    r_rescaled['trialset'] = undetcoeff['trialset']
                    matching_hints[""nth_linear_euler_eq_nonhomogeneous_undetermined_coefficients""] = r_rescaled


    # Order keys based on allhints.
    retlist = [i for i in allhints if i in matching_hints]
    if dict:
        # Dictionaries are ordered arbitrarily, so make note of which
        # hint would come first for dsolve().  Use an ordered dict in Py 3.
        matching_hints[""default""] = retlist[0] if retlist else None
        matching_hints[""ordered_hints""] = tuple(retlist)
        return matching_hints
    else:
        return tuple(retlist)","1. Use `sympify` to sanitize user input.
2. Avoid using `Wild` for user input.
3. Check if the user input has valid symbols."
"def ode_nth_linear_euler_eq_nonhomogeneous_undetermined_coefficients(eq, func, order, match, returns='sol'):
    r""""""
    Solves an `n`\\th order linear non homogeneous Cauchy-Euler equidimensional
    ordinary differential equation using undetermined coefficients.

    This is an equation with form `g(x) = a_0 f(x) + a_1 x f'(x) + a_2 x^2 f''(x)
    \\cdots`.

    These equations can be solved in a general manner, by substituting
    solutions of the form `x = exp(t)`, and deriving a characteristic equation
    of form `g(exp(t)) = b_0 f(t) + b_1 f'(t) + b_2 f''(t) \\cdots` which can
    be then solved by nth_linear_constant_coeff_undetermined_coefficients if
    g(exp(t)) has finite number of linearly independent derivatives.

    Functions that fit this requirement are finite sums functions of the form
    `a x^i e^{b x} \\sin(c x + d)` or `a x^i e^{b x} \\cos(c x + d)`, where `i`
    is a non-negative integer and `a`, `b`, `c`, and `d` are constants.  For
    example any polynomial in `x`, functions like `x^2 e^{2 x}`, `x \\sin(x)`,
    and `e^x \\cos(x)` can all be used.  Products of `\\sin`'s and `\\cos`'s have
    a finite number of derivatives, because they can be expanded into `\\sin(a
    x)` and `\\cos(b x)` terms.  However, SymPy currently cannot do that
    expansion, so you will need to manually rewrite the expression in terms of
    the above to use this method.  So, for example, you will need to manually
    convert `\\sin^2(x)` into `(1 + \\cos(2 x))/2` to properly apply the method
    of undetermined coefficients on it.

    After replacement of x by exp(t), this method works by creating a trial function
    from the expression and all of its linear independent derivatives and
    substituting them into the original ODE.  The coefficients for each term
    will be a system of linear equations, which are be solved for and
    substituted, giving the solution. If any of the trial functions are linearly
    dependent on the solution to the homogeneous equation, they are multiplied
    by sufficient `x` to make them linearly independent.

    Examples
    ========

    >>> from sympy import dsolve, Function, Derivative, log
    >>> from sympy.abc import x
    >>> f = Function('f')
    >>> eq = x**2*Derivative(f(x), x, x) - 2*x*Derivative(f(x), x) + 2*f(x) - log(x)
    >>> dsolve(eq, f(x),
    ... hint='nth_linear_euler_eq_nonhomogeneous_undetermined_coefficients').expand()
    Eq(f(x), C1*x + C2*x**2 + log(x)/2 + 3/4)

    """"""
    x = func.args[0]
    f = func.func
    r = match

    chareq, eq, symbol = S.Zero, S.Zero, Dummy('x')

    for i in r.keys():
        if not isinstance(i, str) and i >= 0:
            chareq += (r[i]*diff(x**symbol, x, i)*x**-symbol).expand()

    for i in range(1,degree(Poly(chareq, symbol))+1):
        eq += chareq.coeff(symbol**i)*diff(f(x), x, i)

    if chareq.as_coeff_add(symbol)[0]:
        eq += chareq.as_coeff_add(symbol)[0]*f(x)
    e, re = posify(r[-1].subs(x, exp(x)))
    eq += e.subs(re)

    match = _nth_linear_match(eq, f(x), ode_order(eq, f(x)))
    match['trialset'] = r['trialset']
    return ode_nth_linear_constant_coeff_undetermined_coefficients(eq, func, order, match).subs(x, log(x)).subs(f(log(x)), f(x)).expand()","1. Use `functools.lru_cache` to cache the results of expensive function calls.
2. Use `mypy` to check for type errors.
3. Use `flake8` to check for style errors."
"def _solve_undetermined_coefficients(eq, func, order, match):
    r""""""
    Helper function for the method of undetermined coefficients.

    See the
    :py:meth:`~sympy.solvers.ode.ode_nth_linear_constant_coeff_undetermined_coefficients`
    docstring for more information on this method.

    The parameter ``match`` should be a dictionary that has the following
    keys:

    ``list``
      A list of solutions to the homogeneous equation, such as the list
      returned by
      ``ode_nth_linear_constant_coeff_homogeneous(returns='list')``.

    ``sol``
      The general solution, such as the solution returned by
      ``ode_nth_linear_constant_coeff_homogeneous(returns='sol')``.

    ``trialset``
      The set of trial functions as returned by
      ``_undetermined_coefficients_match()['trialset']``.

    """"""
    x = func.args[0]
    f = func.func
    r = match
    coeffs = numbered_symbols('a', cls=Dummy)
    coefflist = []
    gensols = r['list']
    gsol = r['sol']
    trialset = r['trialset']
    notneedset = set([])
    # XXX: This global collectterms hack should be removed.
    global collectterms
    if len(gensols) != order:
        raise NotImplementedError(""Cannot find "" + str(order) +
        "" solutions to the homogeneous equation necessary to apply"" +
        "" undetermined coefficients to "" + str(eq) +
        "" (number of terms != order)"")
    usedsin = set([])
    mult = 0  # The multiplicity of the root
    getmult = True
    for i, reroot, imroot in collectterms:
        if getmult:
            mult = i + 1
            getmult = False
        if i == 0:
            getmult = True
        if imroot:
            # Alternate between sin and cos
            if (i, reroot) in usedsin:
                check = x**i*exp(reroot*x)*cos(imroot*x)
            else:
                check = x**i*exp(reroot*x)*sin(abs(imroot)*x)
                usedsin.add((i, reroot))
        else:
            check = x**i*exp(reroot*x)

        if check in trialset:
            # If an element of the trial function is already part of the
            # homogeneous solution, we need to multiply by sufficient x to
            # make it linearly independent.  We also don't need to bother
            # checking for the coefficients on those elements, since we
            # already know it will be 0.
            while True:
                if check*x**mult in trialset:
                    mult += 1
                else:
                    break
            trialset.add(check*x**mult)
            notneedset.add(check)

    newtrialset = trialset - notneedset

    trialfunc = 0
    for i in newtrialset:
        c = next(coeffs)
        coefflist.append(c)
        trialfunc += c*i

    eqs = sub_func_doit(eq, f(x), trialfunc)

    coeffsdict = dict(list(zip(trialset, [0]*(len(trialset) + 1))))

    eqs = _mexpand(eqs)

    for i in Add.make_args(eqs):
        s = separatevars(i, dict=True, symbols=[x])
        coeffsdict[s[x]] += s['coeff']

    coeffvals = solve(list(coeffsdict.values()), coefflist)

    if not coeffvals:
        raise NotImplementedError(
            ""Could not solve `%s` using the ""
            ""method of undetermined coefficients ""
            ""(unable to solve for coefficients)."" % eq)

    psol = trialfunc.subs(coeffvals)

    return Eq(f(x), gsol.rhs + psol)","1. Use `typed_symbols` instead of `numbered_symbols` to avoid creating symbols with arbitrary types.
2. Use `check_assumptions` to verify that the assumptions made by the function are correct.
3. Use `validate_args` to check that the arguments passed to the function are valid."
"def _undetermined_coefficients_match(expr, x):
    r""""""
    Returns a trial function match if undetermined coefficients can be applied
    to ``expr``, and ``None`` otherwise.

    A trial expression can be found for an expression for use with the method
    of undetermined coefficients if the expression is an
    additive/multiplicative combination of constants, polynomials in `x` (the
    independent variable of expr), `\\sin(a x + b)`, `\\cos(a x + b)`, and
    `e^{a x}` terms (in other words, it has a finite number of linearly
    independent derivatives).

    Note that you may still need to multiply each term returned here by
    sufficient `x` to make it linearly independent with the solutions to the
    homogeneous equation.

    This is intended for internal use by ``undetermined_coefficients`` hints.

    SymPy currently has no way to convert `\\sin^n(x) \\cos^m(y)` into a sum of
    only `\\sin(a x)` and `\\cos(b x)` terms, so these are not implemented.  So,
    for example, you will need to manually convert `\\sin^2(x)` into `[1 +
    \\cos(2 x)]/2` to properly apply the method of undetermined coefficients on
    it.

    Examples
    ========

    >>> from sympy import log, exp
    >>> from sympy.solvers.ode import _undetermined_coefficients_match
    >>> from sympy.abc import x
    >>> _undetermined_coefficients_match(9*x*exp(x) + exp(-x), x)
    {'test': True, 'trialset': {x*exp(x), exp(-x), exp(x)}}
    >>> _undetermined_coefficients_match(log(x), x)
    {'test': False}

    """"""
    a = Wild('a', exclude=[x])
    b = Wild('b', exclude=[x])
    expr = powsimp(expr, combine='exp')  # exp(x)*exp(2*x + 1) => exp(3*x + 1)
    retdict = {}

    def _test_term(expr, x):
        r""""""
        Test if ``expr`` fits the proper form for undetermined coefficients.
        """"""
        if not expr.has(x):
            return True
        elif expr.is_Add:
            return all(_test_term(i, x) for i in expr.args)
        elif expr.is_Mul:
            if expr.has(sin, cos):
                foundtrig = False
                # Make sure that there is only one trig function in the args.
                # See the docstring.
                for i in expr.args:
                    if i.has(sin, cos):
                        if foundtrig:
                            return False
                        else:
                            foundtrig = True
            return all(_test_term(i, x) for i in expr.args)
        elif expr.is_Function:
            if expr.func in (sin, cos, exp):
                if expr.args[0].match(a*x + b):
                    return True
                else:
                    return False
            else:
                return False
        elif expr.is_Pow and expr.base.is_Symbol and expr.exp.is_Integer and \\
                expr.exp >= 0:
            return True
        elif expr.is_Pow and expr.base.is_number:
            if expr.exp.match(a*x + b):
                return True
            else:
                return False
        elif expr.is_Symbol or expr.is_number:
            return True
        else:
            return False

    def _get_trial_set(expr, x, exprs=set([])):
        r""""""
        Returns a set of trial terms for undetermined coefficients.

        The idea behind undetermined coefficients is that the terms expression
        repeat themselves after a finite number of derivatives, except for the
        coefficients (they are linearly dependent).  So if we collect these,
        we should have the terms of our trial function.
        """"""
        def _remove_coefficient(expr, x):
            r""""""
            Returns the expression without a coefficient.

            Similar to expr.as_independent(x)[1], except it only works
            multiplicatively.
            """"""
            term = S.One
            if expr.is_Mul:
                for i in expr.args:
                    if i.has(x):
                        term *= i
            elif expr.has(x):
                term = expr
            return term

        expr = expand_mul(expr)
        if expr.is_Add:
            for term in expr.args:
                if _remove_coefficient(term, x) in exprs:
                    pass
                else:
                    exprs.add(_remove_coefficient(term, x))
                    exprs = exprs.union(_get_trial_set(term, x, exprs))
        else:
            term = _remove_coefficient(expr, x)
            tmpset = exprs.union({term})
            oldset = set([])
            while tmpset != oldset:
                # If you get stuck in this loop, then _test_term is probably
                # broken
                oldset = tmpset.copy()
                expr = expr.diff(x)
                term = _remove_coefficient(expr, x)
                if term.is_Add:
                    tmpset = tmpset.union(_get_trial_set(term, x, tmpset))
                else:
                    tmpset.add(term)
            exprs = tmpset
        return exprs

    retdict['test'] = _test_term(expr, x)
    if retdict['test']:
        # Try to generate a list of trial solutions that will have the
        # undetermined coefficients. Note that if any of these are not linearly
        # independent with any of the solutions to the homogeneous equation,
        # then they will need to be multiplied by sufficient x to make them so.
        # This function DOES NOT do that (it doesn't even look at the
        # homogeneous equation).
        retdict['trialset'] = _get_trial_set(expr, x)

    return retdict","1. Use `isinstance()` to check the type of the argument.
2. Use `validate_args()` to check the arguments.
3. Use `wraps()` to preserve the metadata of the original function."
"def _minpoly_compose(ex, x, dom):
    """"""
    Computes the minimal polynomial of an algebraic element
    using operations on minimal polynomials

    Examples
    ========

    >>> from sympy import minimal_polynomial, sqrt, Rational
    >>> from sympy.abc import x, y
    >>> minimal_polynomial(sqrt(2) + 3*Rational(1, 3), x, compose=True)
    x**2 - 2*x - 1
    >>> minimal_polynomial(sqrt(y) + 1/y, x, compose=True)
    x**2*y**2 - 2*x*y - y**3 + 1

    """"""
    if ex.is_Rational:
        return ex.q*x - ex.p
    if ex is I:
        _, factors = factor_list(x**2 + 1, x, domain=dom)
        return x**2 + 1 if len(factors) == 1 else x - I
    if hasattr(dom, 'symbols') and ex in dom.symbols:
        return x - ex

    if dom.is_QQ and _is_sum_surds(ex):
        # eliminate the square roots
        ex -= x
        while 1:
            ex1 = _separate_sq(ex)
            if ex1 is ex:
                return ex
            else:
                ex = ex1

    if ex.is_Add:
        res = _minpoly_add(x, dom, *ex.args)
    elif ex.is_Mul:
        f = Factors(ex).factors
        r = sift(f.items(), lambda itx: itx[0].is_Rational and itx[1].is_Rational)
        if r[True] and dom == QQ:
            ex1 = Mul(*[bx**ex for bx, ex in r[False] + r[None]])
            r1 = r[True]
            dens = [y.q for _, y in r1]
            lcmdens = reduce(lcm, dens, 1)
            nums = [base**(y.p*lcmdens // y.q) for base, y in r1]
            ex2 = Mul(*nums)
            mp1 = minimal_polynomial(ex1, x)
            # use the fact that in SymPy canonicalization products of integers
            # raised to rational powers are organized in relatively prime
            # bases, and that in ``base**(n/d)`` a perfect power is
            # simplified with the root
            mp2 = ex2.q*x**lcmdens - ex2.p
            ex2 = ex2**Rational(1, lcmdens)
            res = _minpoly_op_algebraic_element(Mul, ex1, ex2, x, dom, mp1=mp1, mp2=mp2)
        else:
            res = _minpoly_mul(x, dom, *ex.args)
    elif ex.is_Pow:
        res = _minpoly_pow(ex.base, ex.exp, x, dom)
    elif ex.__class__ is sin:
        res = _minpoly_sin(ex, x)
    elif ex.__class__ is cos:
        res = _minpoly_cos(ex, x)
    elif ex.__class__ is exp:
        res = _minpoly_exp(ex, x)
    elif ex.__class__ is CRootOf:
        res = _minpoly_rootof(ex, x)
    else:
        raise NotAlgebraic(""%s doesn't seem to be an algebraic element"" % ex)
    return res","1. Use `isinstance()` to check if the argument is a valid type.
2. Use `sift()` to filter out unwanted elements from a list.
3. Use `reduce()` to combine the elements of a list into a single value."
"    def _eval_evalf(self, prec):
        # Lookup mpmath function based on name
        fname = self.func.__name__
        try:
            if not hasattr(mpmath, fname):
                from sympy.utilities.lambdify import MPMATH_TRANSLATIONS
                fname = MPMATH_TRANSLATIONS[fname]
            func = getattr(mpmath, fname)
        except (AttributeError, KeyError):
            try:
                return Float(self._imp_(*[i.evalf(prec) for i in self.args]), prec)
            except (AttributeError, TypeError, ValueError):
                return

        # Convert all args to mpf or mpc
        # Convert the arguments to *higher* precision than requested for the
        # final result.
        # XXX + 5 is a guess, it is similar to what is used in evalf.py. Should
        #     we be more intelligent about it?
        try:
            args = [arg._to_mpmath(prec + 5) for arg in self.args]
            def bad(m):
                from mpmath import mpf, mpc
                # the precision of an mpf value is the last element
                # if that is 1 (and m[1] is not 1 which would indicate a
                # power of 2), then the eval failed; so check that none of
                # the arguments failed to compute to a finite precision.
                # Note: An mpc value has two parts, the re and imag tuple;
                # check each of those parts, too. Anything else is allowed to
                # pass
                if isinstance(m, mpf):
                    m = m._mpf_
                    return m[1] !=1 and m[-1] == 1
                elif isinstance(m, mpc):
                    m, n = m._mpc_
                    return m[1] !=1 and m[-1] == 1 and \\
                        n[1] !=1 and n[-1] == 1
                else:
                    return False
            if any(bad(a) for a in args):
                raise ValueError  # one or more args failed to compute with significance
        except ValueError:
            return

        with mpmath.workprec(prec):
            v = func(*args)

        return Expr._from_mpmath(v, prec)","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Check the return value of the function to make sure it is a valid `Expr` object.
3. Use `functools.lru_cache` to cache the results of the function to improve performance."
"def factorial_notation(tokens, local_dict, global_dict):
    """"""Allows standard notation for factorial.""""""
    result = []
    prevtoken = ''
    for toknum, tokval in tokens:
        if toknum == OP:
            op = tokval

            if op == '!!':
                if prevtoken == '!' or prevtoken == '!!':
                    raise TokenError
                result = _add_factorial_tokens('factorial2', result)
            elif op == '!':
                if prevtoken == '!' or prevtoken == '!!':
                    raise TokenError
                result = _add_factorial_tokens('factorial', result)
            else:
                result.append((OP, op))
        else:
            result.append((toknum, tokval))

        prevtoken = tokval

    return result","1. Use `functools.wraps` to preserve the metadata of the decorated function.
2. Use `inspect.getfullargspec` to get the full argument spec of the decorated function.
3. Use `functools.partial` to create a new function with a subset of the arguments of the decorated function."
"def auto_number(tokens, local_dict, global_dict):
    """"""Converts numeric literals to use SymPy equivalents.

    Complex numbers use ``I``; integer literals use ``Integer``, float
    literals use ``Float``, and repeating decimals use ``Rational``.

    """"""
    result = []
    prevtoken = ''

    for toknum, tokval in tokens:
        if toknum == NUMBER:
            number = tokval
            postfix = []

            if number.endswith('j') or number.endswith('J'):
                number = number[:-1]
                postfix = [(OP, '*'), (NAME, 'I')]

            if '.' in number or (('e' in number or 'E' in number) and
                    not (number.startswith('0x') or number.startswith('0X'))):
                match = _re_repeated.match(number)

                if match is not None:
                    # Clear repeating decimals, e.g. 3.4[31] -> (3 + 4/10 + 31/990)
                    pre, post, repetend = match.groups()

                    zeros = '0'*len(post)
                    post, repetends = [w.lstrip('0') for w in [post, repetend]]
                                                # or else interpreted as octal

                    a = pre or '0'
                    b, c = post or '0', '1' + zeros
                    d, e = repetends, ('9'*len(repetend)) + zeros

                    seq = [
                        (OP, '('),
                        (NAME,
                         'Integer'), (OP, '('), (NUMBER, a), (OP, ')'),
                        (OP, '+'),
                        (NAME, 'Rational'), (OP, '('), (
                            NUMBER, b), (OP, ','), (NUMBER, c), (OP, ')'),
                        (OP, '+'),
                        (NAME, 'Rational'), (OP, '('), (
                            NUMBER, d), (OP, ','), (NUMBER, e), (OP, ')'),
                        (OP, ')'),
                    ]
                else:
                    seq = [(NAME, 'Float'), (OP, '('),
                           (NUMBER, repr(str(number))), (OP, ')')]
            else:
                seq = [(NAME, 'Integer'), (OP, '('), (
                    NUMBER, number), (OP, ')')]

            result.extend(seq + postfix)
        else:
            result.append((toknum, tokval))

    return result","1. Use `eval()` function with caution. It can be dangerous if used incorrectly.
2. Avoid using `repr()` to convert a string to a number. It can be exploited by attackers to inject malicious code.
3. Use `int()` or `float()` to convert a string to a number instead of `repr()`."
"def poly(expr, *gens, **args):
    """"""
    Efficiently transform an expression into a polynomial.

    Examples
    ========

    >>> from sympy import poly
    >>> from sympy.abc import x

    >>> poly(x*(x**2 + x - 1)**2)
    Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')

    """"""
    options.allowed_flags(args, [])

    def _poly(expr, opt):
        terms, poly_terms = [], []

        for term in Add.make_args(expr):
            factors, poly_factors = [], []

            for factor in Mul.make_args(term):
                if factor.is_Add:
                    poly_factors.append(_poly(factor, opt))
                elif factor.is_Pow and factor.base.is_Add and factor.exp.is_Integer:
                    poly_factors.append(
                        _poly(factor.base, opt).pow(factor.exp))
                else:
                    factors.append(factor)

            if not poly_factors:
                terms.append(term)
            else:
                product = poly_factors[0]

                for factor in poly_factors[1:]:
                    product = product.mul(factor)

                if factors:
                    factor = Mul(*factors)

                    if factor.is_Number:
                        product = product.mul(factor)
                    else:
                        product = product.mul(Poly._from_expr(factor, opt))

                poly_terms.append(product)

        if not poly_terms:
            result = Poly._from_expr(expr, opt)
        else:
            result = poly_terms[0]

            for term in poly_terms[1:]:
                result = result.add(term)

            if terms:
                term = Add(*terms)

                if term.is_Number:
                    result = result.add(term)
                else:
                    result = result.add(Poly._from_expr(term, opt))

        return result.reorder(*opt.get('gens', ()), **args)

    expr = sympify(expr)

    if expr.is_Poly:
        return Poly(expr, *gens, **args)

    if 'expand' not in args:
        args['expand'] = False

    opt = options.build_options(gens, args)

    return _poly(expr, opt)","1. Use `sympify` to sanitize user input.
2. Use `options.build_options` to validate user-supplied options.
3. Use `Poly._from_expr` to create a polynomial from a user-supplied expression."
"    def _poly(expr, opt):
        terms, poly_terms = [], []

        for term in Add.make_args(expr):
            factors, poly_factors = [], []

            for factor in Mul.make_args(term):
                if factor.is_Add:
                    poly_factors.append(_poly(factor, opt))
                elif factor.is_Pow and factor.base.is_Add and factor.exp.is_Integer:
                    poly_factors.append(
                        _poly(factor.base, opt).pow(factor.exp))
                else:
                    factors.append(factor)

            if not poly_factors:
                terms.append(term)
            else:
                product = poly_factors[0]

                for factor in poly_factors[1:]:
                    product = product.mul(factor)

                if factors:
                    factor = Mul(*factors)

                    if factor.is_Number:
                        product = product.mul(factor)
                    else:
                        product = product.mul(Poly._from_expr(factor, opt))

                poly_terms.append(product)

        if not poly_terms:
            result = Poly._from_expr(expr, opt)
        else:
            result = poly_terms[0]

            for term in poly_terms[1:]:
                result = result.add(term)

            if terms:
                term = Add(*terms)

                if term.is_Number:
                    result = result.add(term)
                else:
                    result = result.add(Poly._from_expr(term, opt))

        return result.reorder(*opt.get('gens', ()), **args)","1. Use `functools.lru_cache` to cache the results of expensive function calls.
2. Validate user input to prevent injection attacks.
3. Use proper error handling to prevent errors from crashing the application."
"    def __pow__(self, other):
        return Pow(self, other)","1. Use `functools.wraps` to preserve the original function's metadata.
2. Use `inspect.signature` to get the function's parameters.
3. Use `functools.partial` to create a new function with the desired arguments."
"    def make_args(cls, expr):
        """"""
        Return a sequence of elements `args` such that cls(*args) == expr

        >>> from sympy import Symbol, Mul, Add
        >>> x, y = map(Symbol, 'xy')

        >>> Mul.make_args(x*y)
        (x, y)
        >>> Add.make_args(x*y)
        (x*y,)
        >>> set(Add.make_args(x*y + y)) == set([y, x*y])
        True

        """"""
        if isinstance(expr, cls):
            return expr.args
        else:
            return (expr,)","1. Use `isinstance()` to check if `expr` is an instance of `cls` before calling `expr.args`. This will prevent a potential `TypeError` if `expr` is not an instance of `cls`.
2. Use `set()` to create a set of the arguments passed to `make_args()`. This will prevent duplicate arguments from being added to the set.
3. Use `*args` to unpack the arguments passed to `make_args()` into a tuple. This will make the code more readable and easier to understand."
"    def make_args(cls, expr):
        """"""
        Return a sequence of elements `args` such that cls(*args) == expr

        >>> from sympy import Symbol, Mul, Add
        >>> x, y = map(Symbol, 'xy')

        >>> Mul.make_args(x*y)
        (x, y)
        >>> Add.make_args(x*y)
        (x*y,)
        >>> set(Add.make_args(x*y + y)) == set([y, x*y])
        True

        """"""
        if isinstance(expr, cls):
            return expr._argset
        else:
            return frozenset([expr])","1. Use `functools.wraps` to preserve the metadata of the original function.
2. Use `inspect.signature` to get the signature of the original function.
3. Use `functools.partial` to create a new function with the same signature as the original function."
"def _sorted_factors(factors, method):
    """"""Sort a list of ``(expr, exp)`` pairs. """"""
    if method == 'sqf':
        def key(obj):
            poly, exp = obj
            rep = poly.rep.rep
            return (exp, len(rep), rep)
    else:
        def key(obj):
            poly, exp = obj
            rep = poly.rep.rep
            return (len(rep), exp, rep)

    return sorted(factors, key=key)","1. Use a secure sorting algorithm, such as `sorted(factors, key=lambda x: (x[1], len(x[0].rep.rep), x[0].rep.rep))`.
2. Sanitize user input to prevent injection attacks.
3. Use proper error handling to prevent unexpected behavior."
"        def key(obj):
            poly, exp = obj
            rep = poly.rep.rep
            return (len(rep), exp, rep)","1. Use a cryptographically secure hash function instead of `len()`.
2. Use a salt to prevent rainbow table attacks.
3. Use a secret key to prevent brute force attacks."
"    def _eval_imageset(self, f):
        expr = f.expr
        if len(f.variables) > 1:
            return
        n = f.variables[0]

        a = Wild('a')
        b = Wild('b')

        match = expr.match(a*n + b)
        if match[a].is_negative:
            expr = -expr

        match = expr.match(a*n + b)
        if match[a] is S.One and match[b].is_integer:
            expr = expr - match[b]

        return ImageSet(Lambda(n, expr), S.Integers)","1. Use `eval()` with caution. It can be dangerous if used incorrectly.
2. Sanitize user input before using it in `eval()`.
3. Use `ImageSet` instead of `Lambda` to avoid potential integer overflow attacks."
"    def _contains(self, other):
        from sympy.matrices import Matrix
        from sympy.solvers.solveset import solveset, linsolve
        from sympy.utilities.iterables import iterable, cartes
        L = self.lamda
        if self._is_multivariate():
            if not iterable(L.expr):
                if iterable(other):
                    return S.false
                return other.as_numer_denom() in self.func(
                    Lambda(L.variables, L.expr.as_numer_denom()), self.base_set)
            if len(L.expr) != len(self.lamda.variables):
                raise NotImplementedError(filldedent('''
    Dimensions of input and output of Lambda are different.'''))
            eqs = [expr - val for val, expr in zip(other, L.expr)]
            variables = L.variables
            free = set(variables)
            if all(i.is_number for i in list(Matrix(eqs).jacobian(variables))):
                solns = list(linsolve([e - val for e, val in
                zip(L.expr, other)], variables))
            else:
                syms = [e.free_symbols & free for e in eqs]
                solns = {}
                for i, (e, s, v) in enumerate(zip(eqs, syms, other)):
                    if not s:
                        if e != v:
                            return S.false
                        solns[vars[i]] = [v]
                        continue
                    elif len(s) == 1:
                        sy = s.pop()
                        sol = solveset(e, sy)
                        if sol is S.EmptySet:
                            return S.false
                        elif isinstance(sol, FiniteSet):
                            solns[sy] = list(sol)
                        else:
                            raise NotImplementedError
                    else:
                        raise NotImplementedError
                solns = cartes(*[solns[s] for s in variables])
        else:
            # assume scalar -> scalar mapping
            solnsSet = solveset(L.expr - other, L.variables[0])
            if solnsSet.is_FiniteSet:
                solns = list(solnsSet)
            else:
                raise NotImplementedError(filldedent('''
                Determining whether an ImageSet contains %s has not
                been implemented.''' % func_name(other)))
        for soln in solns:
            try:
                if soln in self.base_set:
                    return S.true
            except TypeError:
                return self.base_set.contains(soln.evalf())
        return S.false","1. Use `isinstance()` to check if the input is a number or a SymPy object.
2. Use `linsolve()` to solve for the solutions of the equations.
3. Use `FiniteSet()` to check if the solutions are finite."
"    def _intersect(self, other):
        from sympy.solvers.diophantine import diophantine
        if self.base_set is S.Integers:
            if isinstance(other, ImageSet) and other.base_set is S.Integers:
                f, g = self.lamda.expr, other.lamda.expr
                n, m = self.lamda.variables[0], other.lamda.variables[0]

                # Diophantine sorts the solutions according to the alphabetic
                # order of the variable names, since the result should not depend
                # on the variable name, they are replaced by the dummy variables
                # below
                a, b = Dummy('a'), Dummy('b')
                f, g = f.subs(n, a), g.subs(m, b)
                solns_set = diophantine(f - g)
                if solns_set == set():
                    return EmptySet()
                solns = list(diophantine(f - g))
                if len(solns) == 1:
                    t = list(solns[0][0].free_symbols)[0]
                else:
                    return None

                # since 'a' < 'b'
                return imageset(Lambda(t, f.subs(a, solns[0][0])), S.Integers)

        if other == S.Reals:
            from sympy.solvers.solveset import solveset_real
            from sympy.core.function import expand_complex
            if len(self.lamda.variables) > 1:
                return None

            f = self.lamda.expr
            n = self.lamda.variables[0]

            n_ = Dummy(n.name, real=True)
            f_ = f.subs(n, n_)

            re, im = f_.as_real_imag()
            im = expand_complex(im)

            return imageset(Lambda(n_, re),
                            self.base_set.intersect(
                                solveset_real(im, n_)))","1. Use `functools.lru_cache` to cache the results of expensive computations.
2. Use `typing` to annotate the types of arguments and return values.
3. Use `mypy` to check for type errors."
"    def _intersect(self, other):
        from sympy.functions.elementary.integers import ceiling, floor
        from sympy.functions.elementary.complexes import sign

        if other is S.Naturals:
            return self._intersect(Interval(1, S.Infinity))

        if other is S.Integers:
            return self

        if other.is_Interval:
            if not all(i.is_number for i in other.args[:2]):
                return

            o = other.intersect(Interval(self.inf, self.sup))
            if o is S.EmptySet:
                return o

            # get inf/sup and handle below
            if isinstance(o, FiniteSet):
                assert len(o) == 1
                inf = sup = list(o)[0]
            else:
                assert isinstance(o, Interval)
                sup = o.sup
                inf = o.inf

            # get onto sequence
            step = abs(self.step)
            ref = self.start if self.start.is_finite else self.stop
            a = ref + ceiling((inf - ref)/step)*step
            if a not in other:
                a += step
            b = ref + floor((sup - ref)/step)*step
            if b not in other:
                b -= step
            if self.step < 0:
                a, b = b, a
            # make sure to include end point
            b += self.step

            rv = Range(a, b, self.step)
            if not rv:
                return S.EmptySet
            return rv

        elif isinstance(other, Range):
            from sympy.solvers.diophantine import diop_linear
            from sympy.core.numbers import ilcm

            # non-overlap quick exits
            if not other:
                return S.EmptySet
            if not self:
                return S.EmptySet
            if other.sup < self.inf:
                return S.EmptySet
            if other.inf > self.sup:
                return S.EmptySet

            # work with finite end at the start
            r1 = self
            if r1.start.is_infinite:
                r1 = r1.reversed
            r2 = other
            if r2.start.is_infinite:
                r2 = r2.reversed

            # this equation represents the values of the Range;
            # it's a linear equation
            eq = lambda r, i: r.start + i*r.step

            # we want to know when the two equations might
            # have integer solutions so we use the diophantine
            # solver
            a, b = diop_linear(eq(r1, Dummy()) - eq(r2, Dummy()))

            # check for no solution
            no_solution = a is None and b is None
            if no_solution:
                return S.EmptySet

            # there is a solution
            # -------------------

            # find the coincident point, c
            a0 = a.as_coeff_Add()[0]
            c = eq(r1, a0)

            # find the first point, if possible, in each range
            # since c may not be that point
            def _first_finite_point(r1, c):
                if c == r1.start:
                    return c
                # st is the signed step we need to take to
                # get from c to r1.start
                st = sign(r1.start - c)*step
                # use Range to calculate the first point:
                # we want to get as close as possible to
                # r1.start; the Range will not be null since
                # it will at least contain c
                s1 = Range(c, r1.start + st, st)[-1]
                if s1 == r1.start:
                    pass
                else:
                    # if we didn't hit r1.start then, if the
                    # sign of st didn't match the sign of r1.step
                    # we are off by one and s1 is not in r1
                    if sign(r1.step) != sign(st):
                        s1 -= st
                if s1 not in r1:
                    return
                return s1

            # calculate the step size of the new Range
            step = abs(ilcm(r1.step, r2.step))
            s1 = _first_finite_point(r1, c)
            if s1 is None:
                return S.EmptySet
            s2 = _first_finite_point(r2, c)
            if s2 is None:
                return S.EmptySet

            # replace the corresponding start or stop in
            # the original Ranges with these points; the
            # result must have at least one point since
            # we know that s1 and s2 are in the Ranges
            def _updated_range(r, first):
                st = sign(r.step)*step
                if r.start.is_finite:
                    rv = Range(first, r.stop, st)
                else:
                    rv = Range(r.start, first + st, st)
                return rv
            r1 = _updated_range(self, s1)
            r2 = _updated_range(other, s2)

            # work with them both in the increasing direction
            if sign(r1.step) < 0:
                r1 = r1.reversed
            if sign(r2.step) < 0:
                r2 = r2.reversed

            # return clipped Range with positive step; it
            # can't be empty at this point
            start = max(r1.start, r2.start)
            stop = min(r1.stop, r2.stop)
            return Range(start, stop, step)
        else:
            return","1. Use `isinstance()` to check if the argument is a `Range` or `Interval` object.
2. Use `diop_linear()` to check if the two equations have integer solutions.
3. Use `ilcm()` to calculate the least common multiple of the two step sizes."
"def imageset(*args):
    r""""""
    Return an image of the set under transformation ``f``.

    If this function can't compute the image, it returns an
    unevaluated ImageSet object.

    .. math::
        { f(x) | x \\in self }

    Examples
    ========

    >>> from sympy import Interval, Symbol, imageset, sin, Lambda
    >>> from sympy.abc import x, y

    >>> imageset(x, 2*x, Interval(0, 2))
    [0, 4]

    >>> imageset(lambda x: 2*x, Interval(0, 2))
    [0, 4]

    >>> imageset(Lambda(x, sin(x)), Interval(-2, 1))
    ImageSet(Lambda(x, sin(x)), [-2, 1])

    >>> imageset(sin, Interval(-2, 1))
    ImageSet(Lambda(x, sin(x)), [-2, 1])
    >>> imageset(lambda y: x + y, Interval(-2, 1))
    ImageSet(Lambda(_x, _x + x), [-2, 1])

    See Also
    ========

    sympy.sets.fancysets.ImageSet

    """"""
    from sympy.core import Lambda
    from sympy.sets.fancysets import ImageSet
    from sympy.geometry.util import _uniquely_named_symbol

    if len(args) not in (2, 3):
        raise ValueError('imageset expects 2 or 3 args, got: %s' % len(args))

    set = args[-1]
    if not isinstance(set, Set):
        name = func_name(set)
        raise ValueError(
            'last argument should be a set, not %s' % name)

    if len(args) == 3:
        f = Lambda(*args[:2])
    elif len(args) == 2:
        f = args[0]
        if isinstance(f, Lambda):
            pass
        elif (
                isinstance(f, FunctionClass) # like cos
                or func_name(f) == '<lambda>'
                ):
            var = _uniquely_named_symbol(Symbol('x'), f(Dummy()))
            expr = f(var)
            f = Lambda(var, expr)
        else:
            raise TypeError(filldedent('''
        expecting lambda, Lambda, or FunctionClass, not \\'%s\\'''' %
        func_name(f)))

    r = set._eval_imageset(f)
    if isinstance(r, ImageSet):
        f, set = r.args

    if f.variables[0] == f.expr:
        return set

    if isinstance(set, ImageSet):
        if len(set.lamda.variables) == 1 and len(f.variables) == 1:
            return imageset(Lambda(set.lamda.variables[0],
                                   f.expr.subs(f.variables[0], set.lamda.expr)),
                            set.base_set)

    if r is not None:
        return r

    return ImageSet(f, set)","1. Use `func_name` to get the function name instead of `type`.
2. Use `_uniquely_named_symbol` to create a unique symbol for the lambda function.
3. Use `_eval_imageset` to evaluate the imageset and return an ImageSet object."
"def diophantine(eq, param=symbols(""t"", integer=True)):
    """"""
    Simplify the solution procedure of diophantine equation ``eq`` by
    converting it into a product of terms which should equal zero.

    For example, when solving, `x^2 - y^2 = 0` this is treated as
    `(x + y)(x - y) = 0` and `x+y = 0` and `x-y = 0` are solved independently
    and combined. Each term is solved by calling ``diop_solve()``.

    Output of ``diophantine()`` is a set of tuples. Each tuple represents a
    solution of the input equation. In a tuple, solution for each variable is
    listed according to the alphabetic order of input variables. i.e. if we have
    an equation with two variables `a` and `b`, first element of the tuple will
    give the solution for `a` and the second element will give the solution for
    `b`.

    Usage
    =====

    ``diophantine(eq, t)``: Solve the diophantine equation ``eq``.
    ``t`` is the parameter to be used by ``diop_solve()``.

    Details
    =======

    ``eq`` should be an expression which is assumed to be zero.
    ``t`` is the parameter to be used in the solution.

    Examples
    ========

    >>> from sympy.solvers.diophantine import diophantine
    >>> from sympy.abc import x, y, z
    >>> diophantine(x**2 - y**2)
    set([(-t_0, -t_0), (t_0, -t_0)])

    #>>> diophantine(x*(2*x + 3*y - z))
    #set([(0, n1, n2), (3*t - z, -2*t + z, z)])
    #>>> diophantine(x**2 + 3*x*y + 4*x)
    #set([(0, n1), (3*t - 4, -t)])

    See Also
    ========

    diop_solve()
    """"""
    if isinstance(eq, Eq):
        eq = eq.lhs - eq.rhs

    eq = Poly(eq).as_expr()
    if not eq.is_polynomial() or eq.is_number:
        raise TypeError(""Equation input format not supported"")

    var = list(eq.expand(force=True).free_symbols)
    var.sort(key=default_sort_key)

    terms = factor_list(eq)[1]

    sols = set([])

    for term in terms:

        base = term[0]

        var_t, jnk, eq_type = classify_diop(base)
        solution = diop_solve(base, param)

        if eq_type in [""linear"", ""homogeneous_ternary_quadratic"", ""general_pythagorean""]:
            if merge_solution(var, var_t, solution) != ():
                sols.add(merge_solution(var, var_t, solution))

        elif eq_type in [""binary_quadratic"",  ""general_sum_of_squares"", ""univariate""]:
            for sol in solution:
                if merge_solution(var, var_t, sol) != ():
                    sols.add(merge_solution(var, var_t, sol))

    return sols","1. Use `param=symbols(""t"", integer=True)` to ensure that the parameter `t` is an integer.
2. Use `Poly(eq).as_expr()` to convert the equation to a polynomial expression.
3. Use `classify_diop(base)` to classify the equation and call the corresponding function to solve it."
"    def doit(self):
        if self.args[0].is_zero is False:
            return self.args[0] / Abs(self.args[0])
        return self","1. **Use `assert` statements to check for invalid inputs.** This will help to prevent errors from occurring and will make the code more robust.
2. **Sanitize user input before using it in the code.** This will help to prevent attacks such as cross-site scripting (XSS) and SQL injection.
3. **Use strong passwords and security measures to protect the server.** This will help to prevent unauthorized access to the server and the data it contains."
"    def __new__(cls, *args):
        """"""
        Construct a new instance of Diagram.

        If no arguments are supplied, an empty diagram is created.

        If at least an argument is supplied, ``args[0]`` is
        interpreted as the premises of the diagram.  If ``args[0]`` is
        a list, it is interpreted as a list of :class:`Morphism`'s, in
        which each :class:`Morphism` has an empty set of properties.
        If ``args[0]`` is a Python dictionary or a :class:`Dict`, it
        is interpreted as a dictionary associating to some
        :class:`Morphism`'s some properties.

        If at least two arguments are supplied ``args[1]`` is
        interpreted as the conclusions of the diagram.  The type of
        ``args[1]`` is interpreted in exactly the same way as the type
        of ``args[0]``.  If only one argument is supplied, the diagram
        has no conclusions.

        Examples
        ========

        >>> from sympy.categories import Object, NamedMorphism
        >>> from sympy.categories import IdentityMorphism, Diagram
        >>> A = Object(""A"")
        >>> B = Object(""B"")
        >>> C = Object(""C"")
        >>> f = NamedMorphism(A, B, ""f"")
        >>> g = NamedMorphism(B, C, ""g"")
        >>> d = Diagram([f, g])
        >>> IdentityMorphism(A) in d.premises.keys()
        True
        >>> g * f in d.premises.keys()
        True
        >>> d = Diagram([f, g], {g * f: ""unique""})
        >>> d.conclusions[g * f]
        {unique}

        """"""
        premises = {}
        conclusions = {}

        # Here we will keep track of the objects which appear in the
        # premises.
        objects = EmptySet()

        if len(args) >= 1:
            # We've got some premises in the arguments.
            premises_arg = args[0]

            if isinstance(premises_arg, list):
                # The user has supplied a list of morphisms, none of
                # which have any attributes.
                empty = EmptySet()

                for morphism in premises_arg:
                    objects |= FiniteSet(morphism.domain, morphism.codomain)
                    Diagram._add_morphism_closure(premises, morphism, empty)
            elif isinstance(premises_arg, dict) or isinstance(premises_arg, Dict):
                # The user has supplied a dictionary of morphisms and
                # their properties.
                for morphism, props in premises_arg.items():
                    objects |= FiniteSet(morphism.domain, morphism.codomain)
                    Diagram._add_morphism_closure(
                        premises, morphism, FiniteSet(*props) if iterable(props) else FiniteSet(props))

        if len(args) >= 2:
            # We also have some conclusions.
            conclusions_arg = args[1]

            if isinstance(conclusions_arg, list):
                # The user has supplied a list of morphisms, none of
                # which have any attributes.
                empty = EmptySet()

                for morphism in conclusions_arg:
                    # Check that no new objects appear in conclusions.
                    if (morphism.domain in objects) and \\
                       (morphism.codomain in objects):
                        # No need to add identities and recurse
                        # composites this time.
                        Diagram._add_morphism_closure(
                            conclusions, morphism, empty, add_identities=False,
                            recurse_composites=False)
            elif isinstance(conclusions_arg, dict) or \\
                    isinstance(conclusions_arg, Dict):
                # The user has supplied a dictionary of morphisms and
                # their properties.
                for morphism, props in conclusions_arg.items():
                    # Check that no new objects appear in conclusions.
                    if (morphism.domain in objects) and \\
                       (morphism.codomain in objects):
                        # No need to add identities and recurse
                        # composites this time.
                        Diagram._add_morphism_closure(
                            conclusions, morphism, FiniteSet(*props) if iterable(props) else FiniteSet(props),
                            add_identities=False, recurse_composites=False)

        return Basic.__new__(cls, Dict(premises), Dict(conclusions), objects)","1. Use `typing` to annotate the arguments of the `__new__` method.
2. Use `functools.lru_cache` to cache the results of the `_add_morphism_closure` method.
3. Validate the input arguments of the `_add_morphism_closure` method to prevent against poisoning attacks."
"    def subdiagram_from_objects(self, objects):
        """"""
        If ``objects`` is a subset of the objects of ``self``, returns
        a diagram which has as premises all those premises of ``self``
        which have a domains and codomains in ``objects``, likewise
        for conclusions.  Properties are preserved.

        Examples
        ========

        >>> from sympy.categories import Object, NamedMorphism, Diagram
        >>> from sympy import FiniteSet
        >>> A = Object(""A"")
        >>> B = Object(""B"")
        >>> C = Object(""C"")
        >>> f = NamedMorphism(A, B, ""f"")
        >>> g = NamedMorphism(B, C, ""g"")
        >>> d = Diagram([f, g], {f: ""unique"", g*f: ""veryunique""})
        >>> d1 = d.subdiagram_from_objects(FiniteSet(A, B))
        >>> d1 == Diagram([f], {f: ""unique""})
        True
        """"""
        if not objects.is_subset(self.objects):
            raise ValueError(
                ""Supplied objects should all belong to the diagram."")

        new_premises = {}
        for morphism, props in self.premises.items():
            if (morphism.domain in objects) and (morphism.codomain in objects):
                new_premises[morphism] = props

        new_conclusions = {}
        for morphism, props in self.conclusions.items():
            if (morphism.domain in objects) and (morphism.codomain in objects):
                new_conclusions[morphism] = props

        return Diagram(new_premises, new_conclusions)","1. Use `objects.issubset(self.objects)` to check if the supplied objects are a subset of the diagram's objects.
2. Use `new_premises[morphism] = props` to create a new dictionary of premises, where the keys are morphisms and the values are properties.
3. Use `new_conclusions[morphism] = props` to create a new dictionary of conclusions, where the keys are morphisms and the values are properties."
"    def _complement(self, other):
        # this behaves as other - self
        if isinstance(other, ProductSet):
            # For each set consider it or it's complement
            # We need at least one of the sets to be complemented
            # Consider all 2^n combinations.
            # We can conveniently represent these options easily using a ProductSet

            # XXX: this doesn't work if the dimentions of the sets isn't same.
            # A - B is essentially same as A if B has a different
            # dimentionality than A
            switch_sets = ProductSet(FiniteSet(o, o - s) for s, o in
                                     zip(self.sets, other.sets))
            product_sets = (ProductSet(*set) for set in switch_sets)
            # Union of all combinations but this one
            return Union(p for p in product_sets if p != other)

        elif isinstance(other, Interval):
            if isinstance(self, Interval) or isinstance(self, FiniteSet):
                return Intersection(other, self.complement(S.Reals))

        elif isinstance(other, Union):
            return Union(o - self for o in other.args)

        elif isinstance(other, Complement):
            return Complement(other.args[0], Union(other.args[1], self))

        elif isinstance(other, EmptySet):
            return S.EmptySet

        elif isinstance(other, FiniteSet):
            return FiniteSet(*[el for el in other if el not in self])

        return None","1. Use `type()` to check if the argument is an instance of a specific class.
2. Use `Union()` to combine multiple sets.
3. Use `Intersection()` to find the intersection of two sets."
"    def _eval_Eq(self, other):
        if not other.is_FiniteSet:
            if (other.is_Union or other.is_Complement or
                other.is_Intersection or other.is_ProductSet):
                return

            return false

        return And(*map(lambda x, y: Eq(x, y), self.args, other.args))","1. Use `isinstance()` to check if `other` is a `FiniteSet`.
2. Return `False` if `other` is not a `FiniteSet` and is a `Union`, `Complement`, `Intersection`, or `ProductSet`.
3. Use `And()` to combine the results of `Eq()` calls on each pair of arguments from `self.args` and `other.args`."
"    def _contains(self, other):
        """"""
        Tests whether an element, other, is in the set.

        Relies on Python's set class. This tests for object equality
        All inputs are sympified

        Examples
        ========

        >>> from sympy import FiniteSet
        >>> 1 in FiniteSet(1, 2)
        True
        >>> 5 in FiniteSet(1, 2)
        False

        """"""
        if other in self._elements:
            return true
        else:
            if not other.free_symbols:
                return false
            elif all(e.is_Symbol for e in self._elements):
                return false","1. Use `isinstance()` to check if `other` is a Symbol.
2. Use `sympify()` to convert `other` to a Symbol if it is not.
3. Use `self._elements.isdisjoint(other)` to check if `other` is disjoint from `self._elements`."
"    def __contains__(self, other):
        # Split event into each subdomain
        for domain in self.domains:
            # Collect the parts of this event which associate to this domain
            elem = frozenset([item for item in other
                if item[0] in domain.symbols])
            # Test this sub-event
            if elem not in domain:
                return False
        # All subevents passed
        return True","1. Use `typing` to annotate the types of arguments and return values.
2. Use `f-strings` to format strings instead of concatenation.
3. Use `black` to format the code consistently."
"    def __init__(self, settings={}):
        CodePrinter.__init__(self, settings)
        self.known_functions = dict(known_functions)
        userfuncs = settings.get('user_functions', {})
        for k, v in userfuncs.items():
            if not isinstance(v, list):
                userfuncs[k] = [(lambda *x: True, v)]
        self.known_functions.update(userfuncs)","1. Use `userfuncs` as a dictionary instead of a list to avoid unexpected behavior.
2. Use `known_functions` to check if a function is safe to call, and raise an exception if it is not.
3. Use `lambda` functions to make sure that the arguments passed to `userfuncs` are valid."
"def ccode(expr, assign_to=None, **settings):
    r""""""Converts an expr to a string of c code

        Parameters
        ==========

        expr : sympy.core.Expr
            a sympy expression to be converted
        assign_to : optional
            When given, the argument is used as the name of the
            variable to which the Fortran expression is assigned.
            (This is helpful in case of line-wrapping.)
        precision : optional
            the precision for numbers such as pi [default=15]
        user_functions : optional
            A dictionary where keys are FunctionClass instances and values
            are their string representations.  Alternatively, the
            dictionary value can be a list of tuples i.e. [(argument_test,
            cfunction_string)].  See below for examples.
        human : optional
            If True, the result is a single string that may contain some
            constant declarations for the number symbols. If False, the
            same information is returned in a more programmer-friendly
            data structure.
        contract: optional
            If True, `Indexed` instances are assumed to obey
            tensor contraction rules and the corresponding nested
            loops over indices are generated. Setting contract = False
            will not generate loops, instead the user is responsible
            to provide values for the indices in the code. [default=True]


        Examples
        ========

        >>> from sympy import ccode, symbols, Rational, sin, ceiling, Abs
        >>> x, tau = symbols([""x"", ""tau""])
        >>> ccode((2*tau)**Rational(7,2))
        '8*sqrt(2)*pow(tau, 7.0L/2.0L)'
        >>> ccode(sin(x), assign_to=""s"")
        's = sin(x);'
        >>> custom_functions = {
        ...   ""ceiling"": ""CEIL"",
        ...   ""Abs"": [(lambda x: not x.is_integer, ""fabs""),
        ...           (lambda x: x.is_integer, ""ABS"")]
        ... }
        >>> ccode(Abs(x) + ceiling(x), user_functions=custom_functions)
        'fabs(x) + CEIL(x)'
        >>> from sympy import Eq, IndexedBase, Idx
        >>> len_y = 5
        >>> y = IndexedBase('y', shape=(len_y,))
        >>> t = IndexedBase('t', shape=(len_y,))
        >>> Dy = IndexedBase('Dy', shape=(len_y-1,))
        >>> i = Idx('i', len_y-1)
        >>> e=Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))
        >>> ccode(e.rhs, assign_to=e.lhs, contract=False)
        'Dy[i] = (y[i + 1] - y[i])/(t[i + 1] - t[i]);'

    """"""
    return CCodePrinter(settings).doprint(expr, assign_to)","1. Use `ccode(expr, assign_to=None, **settings)` to convert sympy expressions to c code.
2. Use `user_functions` to define custom functions and their string representations.
3. Use `contract` to control whether to generate loops for tensor contraction."
"    def __init__(self, settings=None):
        CodePrinter.__init__(self, settings)
        # leading columns depend on fixed or free format
        if self._settings['source_format'] == 'fixed':
            self._lead_code = ""      ""
            self._lead_cont = ""     @ ""
            self._lead_comment = ""C     ""
        elif self._settings['source_format'] == 'free':
            self._lead_code = """"
            self._lead_cont = ""      ""
            self._lead_comment = ""! ""
        else:
            raise ValueError(""Unknown source format: %s"" % self._settings[
                             'source_format'])
        standards = set([66, 77, 90, 95, 2003, 2008])
        if self._settings['standard'] not in standards:
            raise ValueError(""Unknown Fortran standard: %s"" % self._settings[
                             'standard'])","1. Use strong passwords and enable multi-factor authentication.
2. Keep your software up to date.
3. Be careful about what websites you visit and what links you click on."
"    def _print_Function(self, expr):
        name = self._settings[""user_functions""].get(expr.__class__)
        eargs = expr.args
        if name is None:
            from sympy.functions import conjugate
            if expr.func == conjugate:
                name = ""conjg""
            else:
                name = expr.func.__name__
            if hasattr(expr, '_imp_') and isinstance(expr._imp_, C.Lambda):
                # inlined function.
                # the expression is printed with _print to avoid loops
                return self._print(expr._imp_(*eargs))
            if expr.func.__name__ not in self._implicit_functions:
                self._not_supported.add(expr)
            else:
                # convert all args to floats
                eargs = map(N, eargs)
        return ""%s(%s)"" % (name, self.stringify(eargs, "", ""))","1. Use `functools.partial` to avoid creating a lambda function for every call.
2. Use `functools.lru_cache` to cache the results of expensive functions.
3. Use `inspect.isbuiltin` to check if a function is a built-in function before calling it."
"def fcode(expr, assign_to=None, **settings):
    """"""Converts an expr to a string of Fortran 77 code

       Parameters
       ==========

       expr : sympy.core.Expr
           a sympy expression to be converted
       assign_to : optional
           When given, the argument is used as the name of the
           variable to which the Fortran expression is assigned.
           (This is helpful in case of line-wrapping.)
       precision : optional
           the precision for numbers such as pi [default=15]
       user_functions : optional
           A dictionary where keys are FunctionClass instances and values
           are there string representations.
       human : optional
           If True, the result is a single string that may contain some
           parameter statements for the number symbols. If False, the same
           information is returned in a more programmer-friendly data
           structure.
       source_format : optional
           The source format can be either 'fixed' or 'free'.
           [default='fixed']
       standard : optional
           The Fortran standard to be followed. This is specified as an integer.
           Acceptable standards are 66, 77, 90, 95, 2003, and 2008. Default is
           77. Note that currently the only distinction internally is between
           standards before 95, and those 95 and after. This may change later
           as more features are added.
       contract: optional
           If True, `Indexed` instances are assumed to obey
           tensor contraction rules and the corresponding nested
           loops over indices are generated. Setting contract = False
           will not generate loops, instead the user is responsible
           to provide values for the indices in the code. [default=True]

       Examples
       ========

       >>> from sympy import fcode, symbols, Rational, pi, sin
       >>> x, tau = symbols('x,tau')
       >>> fcode((2*tau)**Rational(7,2))
       '      8*sqrt(2.0d0)*tau**(7.0d0/2.0d0)'
       >>> fcode(sin(x), assign_to=""s"")
       '      s = sin(x)'
       >>> print(fcode(pi))
             parameter (pi = 3.14159265358979d0)
             pi
       >>> from sympy import Eq, IndexedBase, Idx
       >>> len_y = 5
       >>> y = IndexedBase('y', shape=(len_y,))
       >>> t = IndexedBase('t', shape=(len_y,))
       >>> Dy = IndexedBase('Dy', shape=(len_y-1,))
       >>> i = Idx('i', len_y-1)
       >>> e=Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))
       >>> fcode(e.rhs, assign_to=e.lhs, contract=False)
       '      Dy(i) = (y(i + 1) - y(i))/(t(i + 1) - t(i))'

    """"""
    # run the printer
    return FCodePrinter(settings).doprint(expr, assign_to)","1. Use strong passwords and avoid reusing passwords across multiple accounts.
2. Keep your software up to date with the latest security patches.
3. Use a firewall to protect your computer from unauthorized access."
"    def __init__(self, settings={}):
        CodePrinter.__init__(self, settings)
        self.known_functions = dict(known_functions)
        userfuncs = settings.get('user_functions', {})
        for k, v in userfuncs.items():
            if not isinstance(v, tuple):
                userfuncs[k] = (lambda *x: True, v)
        self.known_functions.update(userfuncs)","1. Use `userfuncs` as a dictionary instead of a list. This will prevent users from injecting malicious code into the function registry.
2. Use `isinstance()` to check if `v` is a tuple before adding it to `self.known_functions`. This will prevent users from adding functions that do not have the correct signature.
3. Use `lambda *x: True` as the first argument of each function in `userfuncs`. This will prevent users from overriding the built-in functions that are used by the CodePrinter."
"    def _print_Indexed(self, expr):
        # calculate index for 1d array
        dims = expr.shape
        inds = [ i.label for i in expr.indices ]
        elem = S.Zero
        offset = S.One
        for i in reversed(range(expr.rank)):
            elem += offset*inds[i]
            offset *= dims[i]
        return ""%s[%s]"" % (self._print(expr.base.label), self._print(elem))","1. Use `S.One` instead of `1` to avoid type confusion.
2. Use `expr.indices` instead of `expr.index` to access the indices of an array.
3. Use `reversed(range(expr.rank))` instead of `range(expr.rank)` to iterate over the indices of an array in reverse order."
"def jscode(expr, assign_to=None, **settings):
    """"""Converts an expr to a string of javascript code

       Parameters
       ==========

       expr : sympy.core.Expr
           a sympy expression to be converted
       assign_to : optional
           When given, the argument is used as the name of the
           variable to which the Fortran expression is assigned.
           (This is helpful in case of line-wrapping.)
       precision : optional
           the precision for numbers such as pi [default=15]
       user_functions : optional
           A dictionary where keys are FunctionClass instances and values
           are their string representations. Alternatively the
           dictionary values can be a list of tuples i.e. [(argument_test,
           jsfunction_string)].
       human : optional
           If True, the result is a single string that may contain some
           constant declarations for the number symbols. If False, the
           same information is returned in a more programmer-friendly
           data structure.

       Examples
       ========

       >>> from sympy import jscode, symbols, Rational, sin
       >>> x, tau = symbols([""x"", ""tau""])
       >>> jscode((2*tau)**Rational(7,2))
       '8*Math.sqrt(2)*Math.pow(tau, 7/2)'
       >>> jscode(sin(x), assign_to=""s"")
       's = Math.sin(x);'

    """"""
    return JavascriptCodePrinter(settings).doprint(expr, assign_to)","1. Use a secure random number generator to generate the salt.
2. Use a strong hashing algorithm, such as SHA-256 or SHA-512, to hash the password.
3. Store the hashed password in a secure location, such as a database or file system."
"def load_ipython_extension(ip):
    """"""Load the extension in IPython.""""""
    import IPython

    global _loaded
    # Use extension manager to track loaded status if available
    # This is currently in IPython 0.14.dev
    if hasattr(ip.extension_manager, 'loaded'):
        loaded = 'sympy.interactive.ipythonprinting' not in ip.extension_manager.loaded
    else:
        loaded = _loaded

    if not loaded:
        init_printing(ip=ip)
        _loaded = True","1. Use `ip.extension_manager.load_extension()` instead of `init_printing()` to load the extension.
2. Check if the extension is already loaded before loading it again.
3. Use `ip.extension_manager.loaded.add()` to track the loaded status of the extension."
"    def forward(self, x1, x2, diag=False, **params):
        batch_shape = x1.shape[:-2]
        n_batch_dims = len(batch_shape)
        n1, d = x1.shape[-2:]
        n2 = x2.shape[-2]

        K = torch.zeros(*batch_shape, n1 * (d + 1), n2 * (d + 1), device=x1.device, dtype=x1.dtype)

        if not diag:
            # Scale the inputs by the lengthscale (for stability)
            x1_ = x1.div(self.lengthscale)
            x2_ = x2.div(self.lengthscale)

            # Form all possible rank-1 products for the gradient and Hessian blocks
            outer = x1_.view(*batch_shape, n1, 1, d) - x2_.view(*batch_shape, 1, n2, d)
            outer = outer / self.lengthscale.unsqueeze(-2)
            outer = torch.transpose(outer, -1, -2).contiguous()

            # 1) Kernel block
            diff = self.covar_dist(x1_, x2_, square_dist=True, dist_postprocess_func=postprocess_rbf, **params)
            K_11 = diff
            K[..., :n1, :n2] = K_11

            # 2) First gradient block
            outer1 = outer.view(*batch_shape, n1, n2 * d)
            K[..., :n1, n2:] = outer1 * K_11.repeat([*([1] * (n_batch_dims + 1)), d])

            # 3) Second gradient block
            outer2 = outer.transpose(-1, -3).reshape(*batch_shape, n2, n1 * d)
            outer2 = outer2.transpose(-1, -2)
            K[..., n1:, :n2] = -outer2 * K_11.repeat([*([1] * n_batch_dims), d, 1])

            # 4) Hessian block
            outer3 = outer1.repeat([*([1] * n_batch_dims), d, 1]) * outer2.repeat([*([1] * (n_batch_dims + 1)), d])
            kp = KroneckerProductLazyTensor(
                torch.eye(d, d, device=x1.device, dtype=x1.dtype).repeat(*batch_shape, 1, 1) / self.lengthscale.pow(2),
                torch.ones(n1, n2, device=x1.device, dtype=x1.dtype).repeat(*batch_shape, 1, 1),
            )
            chain_rule = kp.evaluate() - outer3
            K[..., n1:, n2:] = chain_rule * K_11.repeat([*([1] * n_batch_dims), d, d])

            # Symmetrize for stability
            if n1 == n2 and torch.eq(x1, x2).all():
                K = 0.5 * (K.transpose(-1, -2) + K)

            # Apply a perfect shuffle permutation to match the MutiTask ordering
            pi1 = torch.arange(n1 * (d + 1)).view(d + 1, n1).t().reshape((n1 * (d + 1)))
            pi2 = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape((n2 * (d + 1)))
            K = K[..., pi1, :][..., :, pi2]

            return K

        else:
            if not (n1 == n2 and torch.eq(x1, x2).all()):
                raise RuntimeError(""diag=True only works when x1 == x2"")

            kernel_diag = super(RBFKernelGrad, self).forward(x1, x2, diag=True)
            grad_diag = torch.ones(*batch_shape, n2, d, device=x1.device, dtype=x1.dtype) / self.lengthscale.pow_(2)
            grad_diag = grad_diag.transpose(-1, -2).reshape(*batch_shape, n2 * d)
            k_diag = torch.cat((kernel_diag, grad_diag), dim=-1)
            pi = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape((n2 * (d + 1)))
            return k_diag[..., pi]","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to generate a tracing of the model, which can be used to check for correctness and identify potential security vulnerabilities.
3. Use `torch.jit.save` to save the model in a format that can be loaded and used by other applications, such as mobile apps or web services."
"    def __init__(self, lower_bound, upper_bound, transform=sigmoid, inv_transform=inv_sigmoid, initial_value=None):
        """"""
        Defines an interval constraint for GP model parameters, specified by a lower bound and upper bound. For usage
        details, see the documentation for :meth:`~gpytorch.module.Module.register_constraint`.

        Args:
            lower_bound (float or torch.Tensor): The lower bound on the parameter.
            upper_bound (float or torch.Tensor): The upper bound on the parameter.
        """"""
        lower_bound = torch.as_tensor(lower_bound)
        upper_bound = torch.as_tensor(upper_bound)

        if torch.any(torch.ge(lower_bound, upper_bound)):
            raise RuntimeError(""Got parameter bounds with empty intervals."")

        super().__init__()

        self.lower_bound = lower_bound
        self.upper_bound = upper_bound

        self._transform = transform
        self._inv_transform = inv_transform
        self._initial_value = initial_value

        if transform is not None and inv_transform is None:
            self._inv_transform = _get_inv_param_transform(transform)","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to create a trace of the model.
3. Use `torch.jit.save` to save the model in a secure location."
"    def _preconditioner(self):
        if self.preconditioner_override is not None:
            return self.preconditioner_override(self)

        if settings.max_preconditioner_size.value() == 0 or self.size(-1) < settings.min_preconditioning_size.value():
            return None, None, None

        if self._q_cache is None:
            max_iter = settings.max_preconditioner_size.value()
            self._piv_chol_self = pivoted_cholesky.pivoted_cholesky(self._lazy_tensor, max_iter)
            if torch.any(torch.isnan(self._piv_chol_self)).item():
                warnings.warn(
                    ""NaNs encountered in preconditioner computation. Attempting to continue without preconditioning."",
                    NumericalWarning,
                )
                return None, None, None
            self._init_cache()

        # NOTE: We cannot memoize this precondition closure as it causes a memory leak
        def precondition_closure(tensor):
            qqt = self._q_cache.matmul(self._q_cache.transpose(-2, -1).matmul(tensor))
            if self._constant_diag:
                return (1 / self._noise) * (tensor - qqt)
            return (tensor / self._noise) - qqt

        return (precondition_closure, self._precond_lt, self._precond_logdet_cache)","1. Use `torch.jit.trace` to create a traced version of the function.
2. Use `torch.jit.script` to create a scripted version of the function.
3. Use `torch.jit.save` to save the traced or scripted function to a file."
"        def precondition_closure(tensor):
            qqt = self._q_cache.matmul(self._q_cache.transpose(-2, -1).matmul(tensor))
            if self._constant_diag:
                return (1 / self._noise) * (tensor - qqt)
            return (tensor / self._noise) - qqt","1. Use `torch.no_grad()` to disable gradient calculation when not necessary.
2. Sanitize user inputs to prevent potential attacks.
3. Use a secure hash function to generate the `_q_cache`."
"    def _init_cache(self):
        *batch_shape, n, k = self._piv_chol_self.shape
        self._noise = self._diag_tensor.diag().unsqueeze(-1)

        # the check for constant diag needs to be done carefully for batches.
        noise_first_element = self._noise[..., :1, :]
        self._constant_diag = torch.equal(self._noise, noise_first_element * torch.ones_like(self._noise))
        eye = torch.eye(k, dtype=self._piv_chol_self.dtype, device=self._piv_chol_self.device)

        if self._constant_diag:
            self._init_cache_for_constant_diag(eye, batch_shape, n, k)
        else:
            self._init_cache_for_non_constant_diag(eye, batch_shape, n)

        self._precond_lt = PsdSumLazyTensor(RootLazyTensor(self._piv_chol_self), self._diag_tensor)","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to trace the function and generate a new, more secure function.
3. Use `torch.jit.save` to save the traced function to a file."
"    def _init_cache_for_non_constant_diag(self, eye, batch_shape, n):
        # With non-constant diagonals, we cant factor out the noise as easily
        self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self / self._noise.sqrt(), eye)))
        self._q_cache = self._q_cache[..., :n, :] / self._noise.sqrt()

        logdet = self._r_cache.diagonal(dim1=-1, dim2=-2).abs().log().sum(-1).mul(2)
        logdet -= (1.0 / self._noise).log().sum([-1, -2])
        self._precond_logdet_cache = logdet.view(*batch_shape) if len(batch_shape) else logdet.squeeze()","1. Use `torch.jit.script` to make the code more secure.
2. Sanitize user input to prevent injection attacks.
3. Use `torch.autograd.grad` instead of `torch.autograd.gradcheck` to avoid gradient checking errors."
"    def __init__(self, a, b, sigma=0.01, validate_args=False, transform=None):
        TModule.__init__(self)
        _a = torch.tensor(float(a)) if isinstance(a, Number) else a
        _a = _a.view(-1) if _a.dim() < 1 else _a
        _a, _b, _sigma = broadcast_all(_a, b, sigma)
        if not torch.all(constraints.less_than(_b).check(_a)):
            raise ValueError(""must have that a < b (element-wise)"")
        # TODO: Proper argument validation including broadcasting
        batch_shape, event_shape = _a.shape[:-1], _a.shape[-1:]
        # need to assign values before registering as buffers to make argument validation work
        self.a, self.b, self.sigma = _a, _b, _sigma
        super(SmoothedBoxPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)
        # now need to delete to be able to register buffer
        del self.a, self.b, self.sigma
        self.register_buffer(""a"", _a)
        self.register_buffer(""b"", _b)
        self.register_buffer(""sigma"", _sigma)
        self.tails = NormalPrior(torch.zeros_like(_a), _sigma, validate_args=validate_args)
        self._transform = transform","1. Use `torch.jit.script` to make the code more secure.
2. Validate arguments before using them.
3. Use `torch.jit.is_scripting` to check if the code is running in a script, and disable features that are not safe for scripting."
"    def _expand_batch(self, batch_shape):
        lazy_tensors = [lazy_tensor._expand_batch(batch_shape) for lazy_tensor in self.lazy_tensors]
        res = self.__class__(*lazy_tensors, dim=self.cat_dim, output_device=self.output_device)
        return res","1. Use `torch.jit.script` to prevent attackers from modifying the model's graph.
2. Use `torch.jit.trace` to prevent attackers from injecting new code into the model.
3. Use `torch.jit.freeze` to prevent attackers from changing the model's parameters."
"    def _expand_batch(self, batch_shape):
        return self.__class__(self.base_lazy_tensor._expand_batch(batch_shape), self._constant.expand(*batch_shape))","1. Use `tf.convert_to_tensor` to explicitly convert the input to a `Tensor`.
2. Use `tf.debugging.assert_equal` to check that the input has the expected shape.
3. Use `tf.debugging.assert_type` to check that the input is of the expected type."
"    def __add__(self, other):
        """"""
        Return a :obj:`gpytorch.lazy.LazyTensor` that represents the sum of this lazy tensor and another matrix
        or lazy tensor.

        Args:
            :attr:`other` (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`):
                Matrix to add to this one.

        Returns:
            :obj:`gpytorch.lazy.SumLazyTensor`:
                A sum lazy tensor representing the sum of this lazy tensor and other.
        """"""
        from .sum_lazy_tensor import SumLazyTensor
        from .zero_lazy_tensor import ZeroLazyTensor
        from .diag_lazy_tensor import DiagLazyTensor
        from .added_diag_lazy_tensor import AddedDiagLazyTensor
        from .non_lazy_tensor import lazify
        from torch import Tensor

        if isinstance(other, ZeroLazyTensor):
            return self
        elif isinstance(other, DiagLazyTensor):
            return AddedDiagLazyTensor(self, other)
        elif isinstance(other, Tensor):
            other = lazify(other)
            shape = _mul_broadcast_shape(self.shape, other.shape)
            return SumLazyTensor(self.expand(shape), other.expand(shape))
        else:
            return SumLazyTensor(self, other)","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to trace the code and generate a graph.
3. Use `torch.jit.save` to save the graph to a file."
"    def __add__(self, other):
        from .diag_lazy_tensor import DiagLazyTensor
        from .added_diag_lazy_tensor import AddedDiagLazyTensor

        if isinstance(other, ZeroLazyTensor):
            return self
        elif isinstance(other, DiagLazyTensor):
            return AddedDiagLazyTensor(self, other)
        elif isinstance(other, SumLazyTensor):
            return SumLazyTensor(*(list(self.lazy_tensors) + list(other.lazy_tensors)))
        elif isinstance(other, LazyTensor):
            return SumLazyTensor(*(list(self.lazy_tensors) + [other]))
        elif isinstance(other, Tensor):
            # get broadcast shape, assuming mul broadcasting the same as add broadcasting
            broadcasted_shape = _mul_broadcast_shape(self.shape, other.shape)

            # lazify + broadcast other
            broadcasted_other = lazify(other.expand(broadcasted_shape))

            # update the lazy tensors' shape as well
            if broadcasted_shape != self.shape:
                broadcasted_lts = [
                    lt.expand(*broadcasted_shape, 1).squeeze(-1).transpose(-1, -2) for lt in self.lazy_tensors
                ]
            else:
                broadcasted_lts = list(self.lazy_tensors)

            return SumLazyTensor(*(broadcasted_lts + [broadcasted_other]))
        else:
            raise AttributeError(""other must be a LazyTensor"")","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to trace the code and generate a TorchScript model.
3. Use `torch.jit.save` to save the TorchScript model to a file."
"    def initialize_from_data_empspect(self, train_x, train_y):
        """"""
        Initialize mixture components based on the empirical spectrum of the data.

        This will often be better than the standard initialize_from_data method.
        """"""
        import numpy as np
        from scipy.fftpack import fft
        from scipy.integrate import cumtrapz

        N = train_x.size(-2)
        emp_spect = np.abs(fft(train_y.cpu().detach().numpy())) ** 2 / N
        M = math.floor(N / 2)

        freq1 = np.arange(M + 1)
        freq2 = np.arange(-M + 1, 0)
        freq = np.hstack((freq1, freq2)) / N
        freq = freq[: M + 1]
        emp_spect = emp_spect[: M + 1]

        total_area = np.trapz(emp_spect, freq)
        spec_cdf = np.hstack((np.zeros(1), cumtrapz(emp_spect, freq)))
        spec_cdf = spec_cdf / total_area

        a = np.random.rand(1000, self.ard_num_dims)
        p, q = np.histogram(a, spec_cdf)
        bins = np.digitize(a, q)
        slopes = (spec_cdf[bins] - spec_cdf[bins - 1]) / (freq[bins] - freq[bins - 1])
        intercepts = spec_cdf[bins - 1] - slopes * freq[bins - 1]
        inv_spec = (a - intercepts) / slopes

        from sklearn.mixture import GaussianMixture

        GMM = GaussianMixture(n_components=self.num_mixtures, covariance_type=""diag"").fit(inv_spec)
        means = GMM.means_
        varz = GMM.covariances_
        weights = GMM.weights_

        self.mixture_means = means
        self.mixture_scales = varz
        self.mixture_weights = weights","1. Use `torch.no_grad()` to disable gradient calculation when not necessary.
2. Avoid using global variables.
3. Use `torch.jit.script()` to JIT-compile the model for improved performance and security."
"    def _getitem(self, row_index, col_index, *batch_indices):
        """"""
        Supports subindexing of the matrix this LazyTensor represents.

        The indices passed into this method will either be:
            Tensor indices
            Slices

        ..note::
            LazyTensor.__getitem__ uses this as a helper method. If you are writing your own custom LazyTensor,
            override this method rather than __getitem__ (so that you don't have to repeat the extra work)

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.__getitem__`,
            which does some additional work. Calling this method directly is discouraged.

        This method has a number of restrictions on the type of arguments that are passed in to reduce
        the complexity of __getitem__ calls in PyTorch. In particular:
            - This method only accepts slices and tensors for the row/column indices (no ints)
            - The row and column dimensions don't dissapear (e.g. from Tensor indexing). These cases are
              handled by the `_getindices` method

        Args:
            :attr:`row_index` (slice, Tensor):
                Index for the row of the LazyTensor
            :attr:`col_index` (slice, Tensor):
                Index for the col of the LazyTensor
            :attr:`batch_indices` (tuple of slice, int, Tensor):
                Indices for the batch dimensions

        Returns:
            `LazyTensor`
        """"""
        # Special case: if both row and col are not indexed, then we are done
        if row_index is _noop_index and col_index is _noop_index:
            if len(batch_indices):
                components = [component[batch_indices] for component in self._args]
                res = self.__class__(*components, **self._kwargs)
                return res
            else:
                return self

        # Normal case: we have to do some processing on either the rows or columns
        # We will handle this through ""interpolation""
        row_interp_indices = torch.arange(0, self.size(-2), dtype=torch.long, device=self.device).view(-1, 1)
        row_interp_indices = row_interp_indices.expand(*self.batch_shape, -1, 1)
        row_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(row_interp_indices)

        col_interp_indices = torch.arange(0, self.size(-1), dtype=torch.long, device=self.device).view(-1, 1)
        col_interp_indices = col_interp_indices.expand(*self.batch_shape, -1, 1)
        col_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(col_interp_indices)

        # Construct interpolated LazyTensor
        from . import InterpolatedLazyTensor

        res = InterpolatedLazyTensor(self, row_interp_indices, row_interp_values, col_interp_indices, col_interp_values)
        return res._getitem(row_index, col_index, *batch_indices)","1. Use `torch.no_grad()` to disable gradient tracking when not needed.
2. Validate input arguments to ensure they are of the correct type and shape.
3. Use `torch.jit.script` to compile the code into a more efficient and secure representation."
"    def _shaped_noise_covar(self, base_shape: torch.Size, *params: Any, **kwargs: Any):
        if len(params) > 0:
            # we can infer the shape from the params
            shape = None
        else:
            # here shape[:-1] is the batch shape requested, and shape[-1] is `n`, the number of points
            shape = base_shape
        return self.noise_covar(*params, shape=shape, **kwargs)","1. Use `torch.jit.script` to make the function `_shaped_noise_covar` a `torch.jit.ScriptModule`. This will make it more difficult for attackers to exploit.
2. Use `torch.jit.is_scripting` to check if the function is being called from a script. If it is, then throw an error. This will prevent attackers from using the function to attack a running program.
3. Use `torch.jit.save` to save the function to a file. This will allow you to distribute the function to other users without worrying about them being able to exploit it."
"    def forward(self, x1, x2, diag=False, **params):
        x1_ = x1.div(self.period_length)
        x2_ = x2.div(self.period_length)
        diff = self.covar_dist(x1_, x2_, diag=diag, **params)
        res = torch.sin(diff.mul(math.pi)).pow(2).mul(-2 / self.lengthscale).exp_()
        if diff.ndimension() == 2 or diag:
            res = res.squeeze(0)
        return res","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to generate a tracing of the model.
3. Use `torch.jit.save` to save the model in a secure format."
"    def expand(self, expand_shape, _instance=None):
        new = self._get_checked_instance(HorseshoePrior)
        batch_shape = torch.Size(expand_shape)
        new.scale = self.scale.expand(batch_shape)
        super(Distribution, new).__init__(batch_shape)
        new._validate_args = self._validate_args
        return new","1. Use `torch.jit.script` to make the code more secure against
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   "
"    def expand(self, batch_shape):
        return Normal.expand(self, batch_shape, _instance=self)","1. Use `torch.jit.script` to make the function `expand` a `torch.jit.ScriptModule`. This will prevent the function from being called with invalid inputs.
2. Check the input arguments of `expand` to make sure they are valid. For example, check that `batch_shape` is a tuple of integers.
3. Use `torch.jit.is_scripting` to check if the code is being executed in a script. If it is, then throw an error if the function is called with invalid inputs."
"    def expand(self, batch_shape):
        return LogNormal.expand(self, batch_shape, _instance=self)","1. Use `tf.convert_to_tensor` to convert the input argument to a tensor.
2. Use `tf.debugging.assert_greater_equal` to check that the batch shape is non-negative.
3. Use `tf.debugging.assert_rank_at_least` to check that the batch shape has at least one dimension."
"    def expand(self, batch_shape):
        return Uniform.expand(self, batch_shape, _instance=self)","1. Use `tf.debugging.assert_greater_equal` to check that `batch_shape` is a non-negative integer.
2. Use `tf.debugging.assert_less_equal` to check that the sum of the elements of `batch_shape` is less than or equal to `tf.shape(self.batch_shape)`.
3. Use `tf.debugging.assert_equal` to check that `batch_shape` has the same shape as `self.batch_shape`."
"    def expand(self, batch_shape):
        return Gamma.expand(self, batch_shape, _instance=self)","1. Use `torch.jit.script` to create a compiled version of the function. This will make it more difficult for attackers to reverse engineer the code.
2. Use `torch.jit.trace` to create a traced version of the function. This will make it more difficult for attackers to insert malicious code into the function.
3. Use `torch.jit.save` to save the compiled or traced version of the function to a file. This will make it easier to deploy the function to production."
"    def interpolate(self, x_grid: List[torch.Tensor], x_target: torch.Tensor, interp_points=range(-2, 2)):
        if torch.is_tensor(x_grid):
            x_grid = convert_legacy_grid(x_grid)
        num_target_points = x_target.size(0)
        num_dim = x_target.size(-1)
        assert num_dim == len(x_grid)

        grid_sizes = [len(x_grid[i]) for i in range(num_dim)]
        # Do some boundary checking, # min/max along each dimension
        x_target_max = x_target.max(0)[0]
        x_target_min = x_target.min(0)[0]
        grid_mins = torch.stack([x_grid[i].min() for i in range(num_dim)], dim=0).to(x_target_min)
        grid_maxs = torch.stack([x_grid[i].max() for i in range(num_dim)], dim=0).to(x_target_max)

        lt_min_mask = (x_target_min - grid_mins).lt(-1e-7)
        gt_max_mask = (x_target_max - grid_maxs).gt(1e-7)
        if lt_min_mask.sum().item():
            first_out_of_range = lt_min_mask.nonzero().squeeze(1)[0].item()
            raise RuntimeError(
                (
                    ""Received data that was out of bounds for the specified grid. ""
                    ""Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, ""
                    ""max = {0:.3f}""
                ).format(
                    grid_mins[first_out_of_range].item(),
                    grid_maxs[first_out_of_range].item(),
                    x_target_min[first_out_of_range].item(),
                    x_target_max[first_out_of_range].item(),
                )
            )
        if gt_max_mask.sum().item():
            first_out_of_range = gt_max_mask.nonzero().squeeze(1)[0].item()
            raise RuntimeError(
                (
                    ""Received data that was out of bounds for the specified grid. ""
                    ""Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, ""
                    ""max = {0:.3f}""
                ).format(
                    grid_mins[first_out_of_range].item(),
                    grid_maxs[first_out_of_range].item(),
                    x_target_min[first_out_of_range].item(),
                    x_target_max[first_out_of_range].item(),
                )
            )

        # Now do interpolation
        interp_points = torch.tensor(interp_points, dtype=x_grid[0].dtype, device=x_grid[0].device)
        interp_points_flip = interp_points.flip(0)  # [1, 0, -1, -2]

        num_coefficients = len(interp_points)

        interp_values = torch.ones(
            num_target_points, num_coefficients ** num_dim, dtype=x_grid[0].dtype, device=x_grid[0].device
        )
        interp_indices = torch.zeros(
            num_target_points, num_coefficients ** num_dim, dtype=torch.long, device=x_grid[0].device
        )

        for i in range(num_dim):
            num_grid_points = x_grid[i].size(0)
            grid_delta = x_grid[i][1] - x_grid[i][0]
            # left-bounding grid point in index space
            lower_grid_pt_idxs = torch.floor((x_target[:, i] - x_grid[i][0]) / grid_delta)
            # distance from that left-bounding grid point, again in index space
            lower_pt_rel_dists = (x_target[:, i] - x_grid[i][0]) / grid_delta - lower_grid_pt_idxs
            lower_grid_pt_idxs = lower_grid_pt_idxs - interp_points.max()  # ends up being the left-most (relevant) pt
            lower_grid_pt_idxs.detach_()

            if len(lower_grid_pt_idxs.shape) == 0:
                lower_grid_pt_idxs = lower_grid_pt_idxs.unsqueeze(0)

            # get the interp. coeff. based on distances to interpolating points
            scaled_dist = lower_pt_rel_dists.unsqueeze(-1) + interp_points_flip.unsqueeze(-2)
            dim_interp_values = self._cubic_interpolation_kernel(scaled_dist)

            # Find points who's closest lower grid point is the first grid point
            # This corresponds to a boundary condition that we must fix manually.
            left_boundary_pts = torch.nonzero(lower_grid_pt_idxs < 0)
            num_left = len(left_boundary_pts)

            if num_left > 0:
                left_boundary_pts.squeeze_(1)
                x_grid_first = x_grid[i][:num_coefficients].unsqueeze(1).t().expand(num_left, num_coefficients)

                grid_targets = x_target.select(1, i)[left_boundary_pts].unsqueeze(1).expand(num_left, num_coefficients)
                dists = torch.abs(x_grid_first - grid_targets)
                closest_from_first = torch.min(dists, 1)[1]

                for j in range(num_left):
                    dim_interp_values[left_boundary_pts[j], :] = 0
                    dim_interp_values[left_boundary_pts[j], closest_from_first[j]] = 1
                    lower_grid_pt_idxs[left_boundary_pts[j]] = 0

            right_boundary_pts = torch.nonzero(lower_grid_pt_idxs > num_grid_points - num_coefficients)
            num_right = len(right_boundary_pts)

            if num_right > 0:
                right_boundary_pts.squeeze_(1)
                x_grid_last = x_grid[i][-num_coefficients:].unsqueeze(1).t().expand(num_right, num_coefficients)

                grid_targets = x_target.select(1, i)[right_boundary_pts].unsqueeze(1)
                grid_targets = grid_targets.expand(num_right, num_coefficients)
                dists = torch.abs(x_grid_last - grid_targets)
                closest_from_last = torch.min(dists, 1)[1]

                for j in range(num_right):
                    dim_interp_values[right_boundary_pts[j], :] = 0
                    dim_interp_values[right_boundary_pts[j], closest_from_last[j]] = 1
                    lower_grid_pt_idxs[right_boundary_pts[j]] = num_grid_points - num_coefficients

            offset = (interp_points - interp_points.min()).long().unsqueeze(-2)
            dim_interp_indices = lower_grid_pt_idxs.long().unsqueeze(-1) + offset  # indices of corresponding ind. pts.

            n_inner_repeat = num_coefficients ** i
            n_outer_repeat = num_coefficients ** (num_dim - i - 1)
            # index_coeff = num_grid_points ** (num_dim - i - 1)  # TODO: double check
            index_coeff = reduce(mul, grid_sizes[i + 1 :], 1)  # Think this is right...
            dim_interp_indices = dim_interp_indices.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)
            dim_interp_values = dim_interp_values.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)
            # compute the lexicographical position of the indices in the d-dimensional grid points
            interp_indices = interp_indices.add(dim_interp_indices.view(num_target_points, -1).mul(index_coeff))
            interp_values = interp_values.mul(dim_interp_values.view(num_target_points, -1))

        return interp_indices, interp_values","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to create a tracing of the model, which can be used to generate a standalone TorchScript model.
3. Use `torch.jit.save` to save the model to a file, which can then be loaded and used in other applications."
"    def __init__(self, mean, covariance_matrix, validate_args=False, interleaved=True):
        """"""
        Constructs a multi-output multivariate Normal random variable, based on mean and covariance
        Can be multi-output multivariate, or a batch of multi-output multivariate Normal

        Passing a matrix mean corresponds to a multi-output multivariate Normal
        Passing a matrix mean corresponds to a batch of multivariate Normals

        Params:
            mean (:obj:`torch.tensor`): An `n x t` or batch `b x n x t` matrix of means for the MVN distribution.
            covar (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`): An `nt x nt` or batch `b x nt x nt`
                covariance matrix of MVN distribution.
            validate_args (:obj:`bool`): If True, validate `mean` anad `covariance_matrix` arguments.
            interleaved (:obj:`bool`): If True, covariance matrix is interpreted as block-diagonal w.r.t.
                inter-task covariances for each observation. If False, it is interpreted as block-diagonal
                w.r.t. inter-observation covariance for each task.
        """"""
        if not torch.is_tensor(mean) and not isinstance(mean, LazyTensor):
            raise RuntimeError(""The mean of a MultitaskMultivariateNormal must be a Tensor or LazyTensor"")

        if not torch.is_tensor(covariance_matrix) and not isinstance(covariance_matrix, LazyTensor):
            raise RuntimeError(""The covariance of a MultitaskMultivariateNormal must be a Tensor or LazyTensor"")

        if mean.dim() < 2:
            raise RuntimeError(""mean should be a matrix or a batch matrix (batch mode)"")

        self._output_shape = mean.shape
        # TODO: Instead of transpose / view operations, use a PermutationLazyTensor (see #539) to handle interleaving
        self._interleaved = interleaved
        if self._interleaved:
            mean_mvn = mean.reshape(*mean.shape[:-2], -1)
        else:
            mean_mvn = mean.transpose(-1, -2).reshape(*mean.shape[:-2], -1)
        super().__init__(mean=mean_mvn, covariance_matrix=covariance_matrix, validate_args=validate_args)","1. Use `torch.jit.script` to make the code more secure.
2. Validate the input arguments to the constructor.
3. Use `torch.jit.is_scripting` to check if the code is running in a scripted environment."
"    def from_independent_mvns(cls, mvns):
        if len(mvns) < 2:
            raise ValueError(""Must provide at least 2 MVNs to form a MultitaskMultivariateNormal"")
        if any(isinstance(mvn, MultitaskMultivariateNormal) for mvn in mvns):
            raise ValueError(""Cannot accept MultitaskMultivariateNormals"")
        if not all(m.batch_shape == mvns[0].batch_shape for m in mvns[1:]):
            raise ValueError(""All MultivariateNormals must have the same batch shape"")
        if not all(m.event_shape == mvns[0].event_shape for m in mvns[1:]):
            raise ValueError(""All MultivariateNormals must have the same event shape"")
        mean = torch.stack([mvn.mean for mvn in mvns], -1)
        # TODO: To do the following efficiently, we don't want to evaluate the
        # covariance matrices. Instead, we want to use the lazies directly in the
        # BlockDiagLazyTensor. This will require implementing a new BatchLazyTensor:

        # https://github.com/cornellius-gp/gpytorch/issues/468
        covar_blocks_lazy = CatLazyTensor(
            *[mvn.lazy_covariance_matrix.unsqueeze(0) for mvn in mvns],
            dim=0,
            output_device=mean.device
        )
        covar_lazy = BlockDiagLazyTensor(covar_blocks_lazy, block_dim=0)
        return cls(mean=mean, covariance_matrix=covar_lazy, interleaved=False)","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to create a traced version of the model that can be used for inference.
3. Use `torch.jit.save` to save the traced model to a file."
"    def get_base_samples(self, sample_shape=torch.Size()):
        """"""Get i.i.d. standard Normal samples (to be used with rsample(base_samples=base_samples))""""""
        base_samples = super().get_base_samples(sample_shape)
        if not self._interleaved:
            # flip shape of last two dimensions
            new_shape = sample_shape + self._output_shape[:-2] + self._output_shape[:-3:-1]
            return base_samples.view(new_shape).transpose(-1, -2).contiguous()
        return base_samples.view(*sample_shape, *self._output_shape)","1. Use `torch.manual_seed()` to set a random seed for all random number generators.
2. Use `torch.cuda.manual_seed_all()` to set a random seed for all CUDA devices.
3. Use `torch.backends.cudnn.deterministic()` to enable deterministic mode for cuDNN."
"    def __init__(self, mean, covariance_matrix, validate_args=False):
        self._islazy = isinstance(mean, LazyTensor) or isinstance(covariance_matrix, LazyTensor)
        if self._islazy:
            if validate_args:
                # TODO: add argument validation
                raise NotImplementedError()
            self.loc = mean
            self._covar = covariance_matrix
            self.__unbroadcasted_scale_tril = None
            self._validate_args = validate_args
            batch_shape, event_shape = self.loc.shape[:-1], self.loc.shape[-1:]
            # TODO: Integrate argument validation for LazyTensors into torch.distribution validation logic
            super(TMultivariateNormal, self).__init__(batch_shape, event_shape, validate_args=False)
        else:
            super().__init__(loc=mean, covariance_matrix=covariance_matrix, validate_args=validate_args)","1. Add argument validation to check that the input tensors are of the correct type and shape.
2. Use `torch.jit.is_scripting()` to check if the code is being run in a scripting environment and raise an error if it is.
3. Use `torch.jit.is_tracing()` to check if the code is being traced and raise an error if it is."
"    def confidence_region(self):
        """"""
        Returns 2 standard deviations above and below the mean.

        Returns:
            Tuple[Tensor, Tensor]: pair of tensors of size (b x d) or (d), where
                b is the batch size and d is the dimensionality of the random
                variable. The first (second) Tensor is the lower (upper) end of
                the confidence region.

        """"""
        std2 = self.stddev.mul_(2)
        mean = self.mean
        return mean.sub(std2), mean.add(std2)","1. Use `torch.nn.Parameter` instead of `torch.Tensor` to make sure the variables are on the same device.
2. Use `torch.clamp` to clip the values of the standard deviation to avoid overflow.
3. Use `torch.jit.script` to make the code more efficient and secure."
"    def get_base_samples(self, sample_shape=torch.Size()):
        """"""Get i.i.d. standard Normal samples (to be used with rsample(base_samples=base_samples))""""""
        with torch.no_grad():
            shape = self._extended_shape(sample_shape)
            base_samples = _standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
        return base_samples","1. Use `torch.jit.script` to make the function `get_base_samples` a pure function that does not depend on the state of the object.
2. Use `torch.jit.trace` to create a trace of the function `get_base_samples` and use the trace to create a new function that does not have access to the object's state.
3. Use `torch.jit.save` to save the traced function to a file and then load the function from the file when it is needed."
"    def lazy_covariance_matrix(self):
        """"""
        The covariance_matrix, represented as a LazyTensor
        """"""
        if self.islazy:
            return self._covar
        else:
            return lazify(super().covariance_matrix)","1. Use `torch.jit.script` to make the function `lazy_covariance_matrix` a `torch.jit.ScriptModule`.
2. Use `torch.jit.trace` to trace the function `lazy_covariance_matrix` with a random input.
3. Use `torch.jit.save` to save the traced function as a `.pt` file."
"    def add_diag(self, added_diag):
        return DiagLazyTensor(self._diag + added_diag.expand_as(self._diag))","1. Use `torch.jit.script` to make the function `add_diag` a compiled function.
2. Check the dimensions of the input arguments to `add_diag` to ensure that they are compatible.
3. Sanitize the input arguments to `add_diag` to prevent them from being maliciously manipulated."
"    def _size(self):
        return torch.Size(
            (*self.left_lazy_tensor.batch_shape, self.left_lazy_tensor.size(-2), self.right_lazy_tensor.size(-1))
        )","1. Use `torch.jit.script` to create a traced version of the function. This will make it more efficient and secure, as it will be compiled to native code.
2. Use `torch.jit.is_scripting` to check if the function is being called in a scripted context. If it is, throw an error. This will prevent users from calling the function in a way that could compromise security.
3. Use `torch.jit.export` to export the function to a C++ library. This will allow you to use the function in other applications without having to worry about security vulnerabilities."
"    def _size(self):
        return self.lazy_tensors[0].size()","1. Use `torch.jit.script` to JIT-compile the function.
2. Wrap the function in a `torch.jit.trace` call.
3. Use `torch.jit.save` to save the compiled function to a file."
"    def expected_log_prob(self, observations, function_dist, *params, **kwargs):
        if torch.any(observations.eq(-1)):
            warnings.warn(
                ""BernoulliLikelihood.expected_log_prob expects observations with labels in {0, 1}. ""
                ""Observations with labels in {-1, 1} are deprecated."",
                DeprecationWarning,
            )
        else:
            observations = observations.mul(2).sub(1)
        # Custom function here so we can use log_normal_cdf rather than Normal.cdf
        # This is going to be less prone to overflow errors
        log_prob_lambda = lambda function_samples: log_normal_cdf(function_samples.mul(observations))
        log_prob = self.quadrature(log_prob_lambda, function_dist)
        return log_prob.sum(-1)","1. Use `torch.where` to check if `observations` is equal to -1, and if so, convert it to 0.
2. Use `torch.clamp` to clip the values of `observations` to the range [0, 1].
3. Use `torch.isfinite` to check if the values of `observations` are finite."
"    def expected_log_prob(self, target: Tensor, input: MultivariateNormal, *params: Any, **kwargs: Any) -> Tensor:
        mean, variance = input.mean, input.variance
        noise = self.noise_covar.noise

        res = ((target - mean) ** 2 + variance) / noise + noise.log() + math.log(2 * math.pi)
        return res.mul(-0.5).sum(-1)","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to generate a tracing of the model, which can be used to check for correctness and security vulnerabilities.
3. Use `torch.jit.save` to save the model in a format that can be easily imported and used by other applications."
"    def __init__(self):
        super().__init__()
        self._register_load_state_dict_pre_hook(self._batch_shape_state_dict_hook)
        self.quadrature = GaussHermiteQuadrature1D()","1. Use `torch.jit.script` to make the model's forward pass more secure.
2. Use `torch.jit.trace` to create a traced version of the model that can be used for inference.
3. Use `torch.jit.save` to save the traced model to a file."
"    def expected_log_prob(self, observations, function_dist, *params, **kwargs):
        """"""
        Computes the expected log likelihood (used for variational inference):

        .. math::
            \\mathbb{E}_{f(x)} \\left[ \\log p \\left( y \\mid f(x) \\right) \\right]

        Args:
            :attr:`function_dist` (:class:`gpytorch.distributions.MultivariateNormal`)
                Distribution for :math:`f(x)`.
            :attr:`observations` (:class:`torch.Tensor`)
                Values of :math:`y`.
            :attr:`kwargs`

        Returns
            `torch.Tensor` (log probability)
        """"""
        log_prob_lambda = lambda function_samples: self.forward(function_samples).log_prob(observations)
        log_prob = self.quadrature(log_prob_lambda, function_dist)
        return log_prob.sum(tuple(range(-1, -len(function_dist.event_shape) - 1, -1)))","1. Use `torch.no_grad()` to disable gradient calculation when it is not needed.
2. Validate inputs to the model.
3. Use secure training techniques such as [Differential Privacy](https://pytorch.org/tutorials/advanced/privacy.html) and [SecureNN](https://github.com/OpenMined/PySyft/tree/master/examples/securenn)."
"    def forward(self, function_samples, *params, **kwargs):
        """"""
        Computes the conditional distribution p(y|f) that defines the likelihood.

        Args:
            :attr:`function_samples`
                Samples from the function `f`
            :attr:`kwargs`

        Returns:
            Distribution object (with same shape as :attr:`function_samples`)
        """"""
        raise NotImplementedError","1. Use `torch.jit.script` to make the code more secure.
2. Validate function inputs and outputs.
3. Use `torch.autograd.gradcheck` to check the gradients."
"    def marginal(self, function_dist, *params, **kwargs):
        """"""
        Computes a predictive distribution :math:`p(y*|x*)` given either a posterior
        distribution :math:`p(f|D,x)` or a prior distribution :math:`p(f|x)` as input.

        With both exact inference and variational inference, the form of
        :math:`p(f|D,x)` or :math:`p(f|x)` should usually be Gaussian. As a result, input
        should usually be a MultivariateNormal specified by the mean and
        (co)variance of :math:`p(f|...)`.

        Args:
            :attr:`function_dist` (:class:`gpytorch.distributions.MultivariateNormal`)
                Distribution for :math:`f(x)`.
            :attr:`kwargs`

        Returns
            Distribution object (the marginal distribution, or samples from it)
        """"""
        sample_shape = torch.Size([settings.num_likelihood_samples.value()])
        function_samples = function_dist.rsample(sample_shape)
        return self.forward(function_samples)","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to generate a trace of the model.
3. Use `torch.jit.save` to save the traced model."
"    def __call__(self, input, *params, **kwargs):
        # Conditional
        if torch.is_tensor(input):
            return super().__call__(input, *params, **kwargs)
        # Marginal
        elif isinstance(input, MultivariateNormal):
            return self.marginal(input, *params, **kwargs)
        # Error
        else:
            raise RuntimeError(
                ""Likelihoods expects a MultivariateNormal input to make marginal predictions, or a ""
                ""torch.Tensor for conditional predictions. Got a {}"".format(input.__class__.__name__)
            )","1. Use `torch.jit.script` to JIT-compile the likelihood function. This will make it much more difficult for attackers to exploit vulnerabilities.
2. Validate the input to the likelihood function. This will help to prevent attackers from passing invalid inputs that could crash the program or lead to other security problems.
3. Use `torch.autograd.gradcheck` to check the gradients of the likelihood function. This will help to identify any potential security vulnerabilities in the function's gradient."
"        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self._max_plate_nesting = 1","1. Use `assert` statements to validate user input.
2. Sanitize user input to prevent against injection attacks.
3. Use a secure random number generator to generate the `_max_plate_nesting` value."
"    def get_fantasy_likelihood(self, **kwargs):
        return deepcopy(self)","1. **Use `functools.wraps` to preserve the function signature of `deepcopy`.** This will ensure that the return value of `get_fantasy_likelihood` has the same type and attributes as the input.
2. **Check the input arguments of `deepcopy` to ensure that they are valid.** This will help to prevent malicious actors from passing invalid arguments to the function and causing it to crash or behave in an unexpected way.
3. **Use `assert` statements to verify that the return value of `deepcopy` is valid.** This will help to catch any errors that may occur during the copying process and prevent them from being propagated to the caller."
"        def marginal(self, function_dist, *params, **kwargs):
            name_prefix = kwargs.get(""name_prefix"", """")
            num_samples = settings.num_likelihood_samples.value()
            with pyro.plate(name_prefix + "".num_particles_vectorized"", num_samples, dim=(-self.max_plate_nesting - 1)):
                function_samples_shape = torch.Size(
                    [num_samples] + [1] * (self.max_plate_nesting - len(function_dist.batch_shape) - 1)
                )
                function_samples = function_dist(function_samples_shape)
                if self.training:
                    return self(function_samples, *params, **kwargs)
                else:
                    guide_trace = pyro.poutine.trace(self.guide).get_trace(*params, **kwargs)
                    marginal_fn = functools.partial(self.__call__, function_samples)
                    return pyro.poutine.replay(marginal_fn, trace=guide_trace)(*params, **kwargs)","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `pyro.plate` to control the number of samples and prevent overfitting.
3. Use `pyro.poutine.replay` to prevent the model from being used for unintended purposes."
"        def __call__(self, input, *params, **kwargs):
            # Conditional
            if torch.is_tensor(input):
                return super().__call__(input, *params, **kwargs)
            # Marginal
            elif any([
                isinstance(input, MultivariateNormal),
                isinstance(input, pyro.distributions.Normal),
                (
                    isinstance(input, pyro.distributions.Independent)
                    and isinstance(input.base_dist, pyro.distributions.Normal)
                ),
            ]):
                return self.marginal(input, *params, **kwargs)
            # Error
            else:
                raise RuntimeError(
                    ""Likelihoods expects a MultivariateNormal or Normal input to make marginal predictions, or a ""
                    ""torch.Tensor for conditional predictions. Got a {}"".format(input.__class__.__name__)
                )", 
"    def forward(self, function_samples: Tensor, *params: Any, **kwargs: Any) -> base_distributions.Normal:
        noise = self._shaped_noise_covar(function_samples.shape, *params, **kwargs).diag()
        noise = noise.view(*noise.shape[:-1], *function_samples.shape[-2:])
        return base_distributions.Normal(function_samples, noise.sqrt())","1. Use `torch.jit.script` to make the function more secure against tampering.
2. Use `torch.jit.trace` to create a trace of the function and use it to validate inputs.
3. Use `torch.jit.save` to save the traced function to a file and use it to evaluate inputs."
"    def __init__(self, likelihood, model):
        """"""
        A special MLL designed for exact inference

        Args:
        - likelihood: (Likelihood) - the likelihood for the model
        - model: (Module) - the exact GP model
        """"""
        if not isinstance(likelihood, _GaussianLikelihoodBase):
            raise RuntimeError(""Likelihood must be Gaussian for exact inference"")
        super(ExactMarginalLogLikelihood, self).__init__(likelihood, model)","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to generate a tracing of the model.
3. Use `torch.jit.save` to save the traced model."
"    def forward(self, output, target, *params):
        if not isinstance(output, MultivariateNormal):
            raise RuntimeError(""ExactMarginalLogLikelihood can only operate on Gaussian random variables"")

        # Get the log prob of the marginal distribution
        output = self.likelihood(output, *params)
        res = output.log_prob(target)

        # Add additional terms (SGPR / learned inducing points, heteroskedastic likelihood models)
        for added_loss_term in self.model.added_loss_terms():
            res = res.add(added_loss_term.loss(*params))

        # Add log probs of priors on the (functions of) parameters
        for _, prior, closure, _ in self.named_priors():
            res.add_(prior.log_prob(closure()).sum())

        # Scale by the amount of data we have
        num_data = target.size(-1)
        return res.div_(num_data)","1. Use `torch.jit.script` to prevent attackers from modifying the model's logic.
2. Use `torch.jit.trace` to prevent attackers from injecting new code into the model.
3. Use `torch.autograd.grad` to prevent attackers from using gradient-based attacks."
"    def forward(self, output, target):
        """"""
        Args:
        - output: (MultivariateNormal) - the outputs of the latent function
        - target: (Variable) - the target values
        """"""
        raise NotImplementedError","1. Use `torch.nn.functional.mse_loss` instead of manually calculating the mean squared error.
2. Use `torch.nn.functional.softmax` instead of manually calculating the softmax.
3. Use `torch.nn.functional.cross_entropy` instead of manually calculating the cross-entropy loss."
"    def forward(self, variational_dist_f, target, **kwargs):
        num_batch = variational_dist_f.event_shape.numel()

        log_likelihood = self.likelihood.expected_log_prob(target, variational_dist_f, **kwargs).div(
            num_batch
        )
        kl_divergence = self.model.variational_strategy.kl_divergence()

        if kl_divergence.dim() > log_likelihood.dim():
            kl_divergence = kl_divergence.sum(-1)

        if log_likelihood.numel() == 1:
            kl_divergence = kl_divergence.sum()

        kl_divergence = kl_divergence.div(self.num_data)

        # Add any additional registered loss terms
        added_loss = torch.zeros_like(kl_divergence)
        had_added_losses = False
        for added_loss_term in self.model.added_loss_terms():
            added_loss.add_(added_loss_term.loss())
            had_added_losses = True

        if self.combine_terms:
            res = log_likelihood - kl_divergence
            for _, prior, closure, _ in self.named_priors():
                res.add_(prior.log_prob(closure()).sum().div(self.num_data))
            return res + added_loss
        else:
            log_prior = torch.zeros_like(log_likelihood)
            for _, prior, closure, _ in self.named_priors():
                log_prior.add_(prior.log_prob(closure()).sum())
            if had_added_losses:
                return log_likelihood, kl_divergence, log_prior.div(self.num_data), added_loss
            else:
                return log_likelihood, kl_divergence, log_prior.div(self.num_data)","1. Use `torch.jit.script` to make the model's forward pass more efficient and secure.
2. Use `torch.autograd.grad` to compute gradients of the loss function with respect to the model parameters.
3. Use `torch.nn.functional.softmax` to apply the softmax function to the model's output."
"    def forward(self, variational_dist_f, target, **kwargs):
        num_batch = variational_dist_f.event_shape[0]
        variational_dist_u = self.model.variational_strategy.variational_distribution.variational_distribution
        prior_dist = self.model.variational_strategy.prior_distribution

        log_likelihood = self.likelihood.expected_log_prob(target, variational_dist_f, **kwargs)
        log_likelihood = log_likelihood.div(num_batch)

        num_samples = settings.num_likelihood_samples.value()
        variational_samples = variational_dist_u.rsample(torch.Size([num_samples]))
        kl_divergence = (
            variational_dist_u.log_prob(variational_samples) - prior_dist.log_prob(variational_samples)
        ).mean(0)
        kl_divergence = kl_divergence.div(self.num_data)

        res = log_likelihood - kl_divergence
        for _, prior, closure, _ in self.named_priors():
            res.add_(prior.log_prob(closure()).sum().div(self.num_data))
        return res","1. Use `torch.no_grad()` to disable gradient calculation when not needed.
2. Sanitize user input to prevent injection attacks.
3. Use a secure random number generator to generate random numbers."
"        def __init__(self, *args, **kwargs):
            raise RuntimeError(""Cannot use a PyroVariationalGP because you dont have Pyro installed."")","1. **Use `assert` statements to check for the presence of `Pyro` before initializing the class.** This will prevent the error from being thrown when `Pyro` is not installed.
2. **Use `try` and `except` blocks to catch the error and handle it gracefully.** This will prevent the program from crashing if `Pyro` is not installed.
3. **Document the error message to make it clear what the problem is.** This will help users understand why the error is being thrown and how to fix it."
"    def sub_variational_strategies(self):
        if not hasattr(self, ""_sub_variational_strategies_memo""):
            self._sub_variational_strategies_memo = [
                module.variational_strategy for module in self.model.modules()
                if isinstance(module, AbstractVariationalGP)
            ]
        return self._sub_variational_strategies_memo","1. Use `torch.no_grad()` to disable gradient calculation when not needed.
2. Sanitize user input to prevent against injection attacks.
3. Use secure randomness sources to generate random numbers."
"    def __call__(self, inputs, are_samples=False, **kwargs):
        """"""
        Forward data through this hidden GP layer. The output is a MultitaskMultivariateNormal distribution
        (or MultivariateNormal distribution is output_dims=None).

        If the input is >=2 dimensional Tensor (e.g. `n x d`), we pass the input through each hidden GP,
        resulting in a `n x h` multitask Gaussian distribution (where all of the `h` tasks represent an
        output dimension and are independent from one another).  We then draw `s` samples from these Gaussians,
        resulting in a `s x n x h` MultitaskMultivariateNormal distribution.

        If the input is a >=3 dimensional Tensor, and the `are_samples=True` kwarg is set, then we assume that
        the outermost batch dimension is a samples dimension. The output will have the same number of samples.
        For example, a `s x b x n x d` input will result in a `s x b x n x h` MultitaskMultivariateNormal distribution.

        The goal of these last two points is that if you have a tensor `x` that is `n x d`, then:
            >>> hidden_gp2(hidden_gp(x))

        will just work, and return a tensor of size `s x n x h2`, where `h2` is the output dimensionality of
        hidden_gp2. In this way, hidden GP layers are easily composable.
        """"""
        deterministic_inputs = not are_samples
        if isinstance(inputs, MultitaskMultivariateNormal):
            inputs = torch.distributions.Normal(loc=inputs.mean, scale=inputs.variance.sqrt()).rsample()
            deterministic_inputs = False

        if settings.debug.on():
            if not torch.is_tensor(inputs):
                raise ValueError(
                    ""`inputs` should either be a MultitaskMultivariateNormal or a Tensor, got ""
                    f""{inputs.__class__.__Name__}""
                )

            if inputs.size(-1) != self.input_dims:
                raise RuntimeError(
                    f""Input shape did not match self.input_dims. Got total feature dims [{inputs.size(-1)}],""
                    f"" expected [{self.input_dims}]""
                )

        # Repeat the input for all possible outputs
        if self.output_dims is not None:
            inputs = inputs.unsqueeze(-3)
            inputs = inputs.expand(*inputs.shape[:-3], self.output_dims, *inputs.shape[-2:])

        # Now run samples through the GP
        output = AbstractVariationalGP.__call__(self, inputs)
        if self.output_dims is not None:
            mean = output.loc.transpose(-1, -2)
            covar = BlockDiagLazyTensor(output.lazy_covariance_matrix, block_dim=-3)
            output = MultitaskMultivariateNormal(mean, covar, interleaved=False)

        # Maybe expand inputs?
        if deterministic_inputs:
            output = output.expand(torch.Size([settings.num_likelihood_samples.value()]) + output.batch_shape)

        return output","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to generate a tracing of the model.
3. Use `torch.jit.save` to save the model in a secure format."
"    def forward(self, x):
        if x.ndimension() == 1:
            x = x.unsqueeze(-1)
        elif x.ndimension() != 2:
            raise RuntimeError(""AdditiveGridInterpolationVariationalStrategy expects a 2d tensor."")

        num_data, num_dim = x.size()
        if num_dim != self.num_dim:
            raise RuntimeError(""The number of dims should match the number specified."")

        output = super(AdditiveGridInterpolationVariationalStrategy, self).forward(x)
        if self.sum_output:
            mean = output.mean.sum(0)
            covar = output.lazy_covariance_matrix.sum(-3)
            return MultivariateNormal(mean, covar)
        else:
            return output","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to create a trace of the model.
3. Use `torch.jit.save` to save the model in a secure location."
"    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), **kwargs):
        """"""
        Args:
            num_inducing_points (int): Size of the variational distribution. This implies that the variational mean
                should be this size, and the variational covariance matrix should have this many rows and columns.
            batch_shape (torch.Size, optional): Specifies an optional batch
                size for the variational parameters. This is useful for example
                when doing additive variational inference.
        """"""
        batch_shape = _deprecate_kwarg_with_transform(
            kwargs, ""batch_size"", ""batch_shape"", batch_shape, lambda n: torch.Size([n])
        )
        super(VariationalDistribution, self).__init__()
        mean_init = torch.zeros(num_inducing_points)
        covar_init = torch.eye(num_inducing_points, num_inducing_points)
        mean_init = mean_init.repeat(*batch_shape, 1)
        covar_init = covar_init.repeat(*batch_shape, 1, 1)

        self.register_parameter(name=""variational_mean"", parameter=torch.nn.Parameter(mean_init))
        self.register_parameter(name=""chol_variational_covar"", parameter=torch.nn.Parameter(covar_init))","1. Use `torch.nn.init.xavier_uniform_` to initialize the parameters instead of `torch.zeros` and `torch.eye`.
2. Use `torch.nn.Parameter` to register the parameters instead of directly assigning them to attributes.
3. Use `torch.Size([n])` to specify the batch shape instead of passing a Python integer."
"    def initialize_variational_distribution(self, prior_dist):
        self.variational_mean.data.copy_(prior_dist.mean)
        self.chol_variational_covar.data.copy_(prior_dist.scale_tril)","1. Use `torch.nn.init.zeros_` instead of `torch.data.copy_` to initialize the parameters. This will prevent attackers from using the prior distribution to learn the model parameters.
2. Use `torch.nn.init.orthogonal_` instead of `torch.data.copy_` to initialize the scale tril. This will prevent attackers from using the prior distribution to learn the model parameters.
3. Use `torch.nn.init.kaiming_uniform_` to initialize the bias. This will help to prevent the model from overfitting."
"    def _compute_grid(self, inputs):
        if inputs.ndimension() == 1:
            inputs = inputs.unsqueeze(1)

        interp_indices, interp_values = Interpolation().interpolate(self.grid, inputs)
        return interp_indices, interp_values","1. Use `torch.no_grad()` to disable gradient computation when it is not needed.
2. Validate input data to prevent attacks such as [adversarial examples](https://en.wikipedia.org/wiki/Adversarial_example).
3. Use [secure coding practices](https://pytorch.org/security/) to protect against potential vulnerabilities."
"    def forward(self, x):
        variational_distribution = self.variational_distribution.variational_distribution

        # Get interpolations
        interp_indices, interp_values = self._compute_grid(x)

        # Compute test mean
        # Left multiply samples by interpolation matrix
        predictive_mean = left_interp(interp_indices, interp_values, variational_distribution.mean.unsqueeze(-1))
        predictive_mean = predictive_mean.squeeze(-1)

        # Compute test covar
        predictive_covar = InterpolatedLazyTensor(
            variational_distribution.lazy_covariance_matrix,
            interp_indices,
            interp_values,
            interp_indices,
            interp_values,
        )

        output = MultivariateNormal(predictive_mean, predictive_covar)
        return output","1. Use `torch.jit.script` to make the model more secure against adversarial attacks.
2. Use `torch.jit.trace` to make the model more efficient.
3. Use `torch.jit.save` to save the model in a format that can be easily deployed."
"    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=False):
        """"""
        Args:
            model (:obj:`gpytorch.model.AbstractVariationalGP`): Model this strategy is applied to. Typically passed in
            when the VariationalStrategy is created in the __init__ method of the user defined model.
            inducing_points (torch.tensor): Tensor containing a set of inducing points to use for variational inference.
            variational_distribution (:obj:`gpytorch.variational.VariationalDistribution`): A VariationalDistribution
                object that represents the form of the variational distribution :math:`q(u)`
            learn_inducing_locations (bool): Whether or not the inducing point locations should be learned (e.g. SVGP).
        """"""
        super(VariationalStrategy, self).__init__()
        object.__setattr__(self, ""model"", model)

        inducing_points = inducing_points.clone()

        if inducing_points.dim() == 1:
            inducing_points = inducing_points.unsqueeze(-1)

        if learn_inducing_locations:
            self.register_parameter(name=""inducing_points"", parameter=torch.nn.Parameter(inducing_points))
        else:
            self.register_buffer(""inducing_points"", inducing_points)

        self.variational_distribution = variational_distribution
        self.register_buffer(""variational_params_initialized"", torch.tensor(0))","1. Use `torch.nn.Parameter` instead of `torch.nn.Parameter` to register learnable parameters.
2. Use `torch.nn.Buffer` instead of `torch.nn.Parameter` to register non-learnable parameters.
3. Initialize the `variational_params_initialized` buffer to 1."
"    def prior_distribution(self):
        """"""
        The :func:`~gpytorch.variational.VariationalStrategy.prior_distribution` method determines how to compute the
        GP prior distribution of the inducing points, e.g. :math:`p(u) \\sim N(\\mu(X_u), K(X_u, X_u))`. Most commonly,
        this is done simply by calling the user defined GP prior on the inducing point data directly.
        """"""
        out = self.model.forward(self.inducing_points)
        res = MultivariateNormal(
            out.mean, out.lazy_covariance_matrix.add_jitter()
        )
        return res","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.jit.trace` to generate a tracing of the model.
3. Use `torch.jit.save` to save the traced model to a file."
"    def forward(self, x):
        """"""
        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the
        inducing point function values. Specifically, forward defines how to transform a variational distribution
        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at
        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`

        Args:
            x (torch.tensor): Locations x to get the variational posterior of the function values at.
        Returns:
            :obj:`gpytorch.distributions.MultivariateNormal`: The distribution q(f|x)
        """"""
        variational_dist = self.variational_distribution.variational_distribution
        inducing_points = self.inducing_points
        inducing_batch_shape = inducing_points.shape[:-2]
        if inducing_batch_shape < x.shape[:-2]:
            batch_shape = _mul_broadcast_shape(inducing_points.shape[:-2], x.shape[:-2])
            inducing_points = inducing_points.expand(*batch_shape, *inducing_points.shape[-2:])
            x = x.expand(*batch_shape, *x.shape[-2:])
            variational_dist = variational_dist.expand(batch_shape)

        # If our points equal the inducing points, we're done
        if torch.equal(x, inducing_points):
            return variational_dist

        # Otherwise, we have to marginalize
        else:
            num_induc = inducing_points.size(-2)
            full_inputs = torch.cat([inducing_points, x], dim=-2)
            full_output = self.model.forward(full_inputs)
            full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix

            # Mean terms
            test_mean = full_mean[..., num_induc:]
            induc_mean = full_mean[..., :num_induc]
            mean_diff = (variational_dist.mean - induc_mean).unsqueeze(-1)

            # Covariance terms
            induc_induc_covar = full_covar[..., :num_induc, :num_induc].add_jitter()
            induc_data_covar = full_covar[..., :num_induc, num_induc:].evaluate()
            data_data_covar = full_covar[..., num_induc:, num_induc:]
            root_variational_covar = variational_dist.lazy_covariance_matrix.root_decomposition().root.evaluate()

            # If we had to expand the inducing points, shrink the inducing mean and induc_induc_covar dimension
            # This makes everything more computationally efficient
            if len(inducing_batch_shape) < len(induc_induc_covar.batch_shape):
                index = tuple(0 for _ in range(len(induc_induc_covar.batch_shape) - len(inducing_batch_shape)))
                repeat_size = torch.Size((
                    tuple(induc_induc_covar.batch_shape[:len(index)])
                    + tuple(1 for _ in induc_induc_covar.batch_shape[len(index):])
                ))
                induc_induc_covar = BatchRepeatLazyTensor(induc_induc_covar.__getitem__(index), repeat_size)

            # If we're less than a certain size, we'll compute the Cholesky decomposition of induc_induc_covar
            cholesky = False
            if settings.fast_computations.log_prob.off() or (num_induc <= settings.max_cholesky_size.value()):
                induc_induc_covar = CholLazyTensor(induc_induc_covar.cholesky())
                cholesky = True

            # If we are making predictions and don't need variances, we can do things very quickly.
            if not self.training and settings.skip_posterior_variances.on():
                if not hasattr(self, ""_mean_cache""):
                    self._mean_cache = induc_induc_covar.inv_matmul(mean_diff).detach()

                predictive_mean = torch.add(
                    test_mean,
                    induc_data_covar.transpose(-2, -1).matmul(self._mean_cache).squeeze(-1)
                )

                predictive_covar = ZeroLazyTensor(test_mean.size(-1), test_mean.size(-1))

                return MultivariateNormal(predictive_mean, predictive_covar)

            # Cache the CG results
            # For now: run variational inference without a preconditioner
            # The preconditioner screws things up for some reason
            with settings.max_preconditioner_size(0):
                # Cache the CG results
                left_tensors = torch.cat([mean_diff, root_variational_covar], -1)
                with torch.no_grad():
                    eager_rhs = torch.cat([left_tensors, induc_data_covar], -1)
                    solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(
                        induc_induc_covar, eager_rhs.detach(), logdet_terms=(not cholesky),
                        include_tmats=(not settings.skip_logdet_forward.on() and not cholesky)
                    )
                    eager_rhss = [
                        eager_rhs.detach(), eager_rhs[..., left_tensors.size(-1):].detach(),
                        eager_rhs[..., :left_tensors.size(-1)].detach()
                    ]
                    solves = [
                        solve.detach(), solve[..., left_tensors.size(-1):].detach(),
                        solve[..., :left_tensors.size(-1)].detach()
                    ]
                    if settings.skip_logdet_forward.on():
                        eager_rhss.append(torch.cat([probe_vecs, left_tensors], -1))
                        solves.append(torch.cat([probe_vec_solves, solve[..., :left_tensors.size(-1)]], -1))
                induc_induc_covar = CachedCGLazyTensor(
                    induc_induc_covar, eager_rhss=eager_rhss, solves=solves, probe_vectors=probe_vecs,
                    probe_vector_norms=probe_vec_norms, probe_vector_solves=probe_vec_solves,
                    probe_vector_tmats=tmats,
                )

            if self.training:
                self._memoize_cache[""prior_distribution_memo""] = MultivariateNormal(induc_mean, induc_induc_covar)

            # Compute predictive mean/covariance
            inv_products = induc_induc_covar.inv_matmul(induc_data_covar, left_tensors.transpose(-1, -2))
            predictive_mean = torch.add(test_mean, inv_products[..., 0, :])
            predictive_covar = RootLazyTensor(inv_products[..., 1:, :].transpose(-1, -2))
            if self.training:
                interp_data_data_var, _ = induc_induc_covar.inv_quad_logdet(
                    induc_data_covar, logdet=False, reduce_inv_quad=False
                )
                data_covariance = DiagLazyTensor((data_data_covar.diag() - interp_data_data_var).clamp(0, math.inf))
            else:
                neg_induc_data_data_covar = torch.matmul(
                    induc_data_covar.transpose(-1, -2).mul(-1),
                    induc_induc_covar.inv_matmul(induc_data_covar)
                )
                data_covariance = data_data_covar + neg_induc_data_data_covar
            predictive_covar = PsdSumLazyTensor(predictive_covar, data_covariance)

            return MultivariateNormal(predictive_mean, predictive_covar)","1. Use `torch.no_grad()` to disable gradient calculation when not needed.
2. Use `torch.jit.script()` to JIT-compile the code for performance and security.
3. Use `torch.autograd.set_detect_anomaly(True)` to detect invalid gradients."
"    def __call__(self, x):
        if not self.variational_params_initialized.item():
            self.initialize_variational_dist()
            self.variational_params_initialized.fill_(1)
        if self.training:
            if hasattr(self, ""_memoize_cache""):
                delattr(self, ""_memoize_cache"")
                self._memoize_cache = dict()

        return super(VariationalStrategy, self).__call__(x)","1. Use `torch.no_grad()` to disable gradient calculation when not needed.
2. Use `torch.jit.script()` to JIT-compile the model for faster inference.
3. Use `torch.jit.trace()` to trace the model from a representative input dataset for more efficient inference."
"    def covar_trace(self):
        variational_covar = self.variational_distribution.variational_distribution.covariance_matrix
        prior_covar = self.prior_distribution.covariance_matrix
        batch_shape = prior_covar.shape[:-2]
        return (variational_covar * prior_covar).view(*batch_shape, -1).sum(-1)","1. Use `torch.no_grad()` to disable gradient calculation when it is not needed.
2. Sanitize user input to prevent injecting malicious code.
3. Use proper error handling to prevent unexpected exceptions from crashing the program."
"    def mean_diff_inv_quad(self):
        prior_mean = self.prior_distribution.mean
        prior_covar = self.prior_distribution.lazy_covariance_matrix
        variational_mean = self.variational_distribution.variational_distribution.mean
        return prior_covar.inv_quad(variational_mean - prior_mean)","1. Use `np.nan` instead of `None` to represent missing values.
2. Use `np.inf` instead of `np.inf` to represent infinity.
3. Use `np.sign` instead of `np.signbit` to check the sign of a number."
"    def kl_divergence(self):
        variational_dist_u = self.variational_distribution.variational_distribution
        prior_dist = self.prior_distribution
        kl_divergence = 0.5 * sum(
            [
                # log|k| - log|S|
                # = log|K| - log|K var_dist_covar K|
                # = -log|K| - log|var_dist_covar|
                self.prior_covar_logdet(),
                -variational_dist_u.lazy_covariance_matrix.logdet(),
                # tr(K^-1 S) = tr(K^1 K var_dist_covar K) = tr(K var_dist_covar)
                self.covar_trace(),
                # (m - \\mu u)^T K^-1 (m - \\mu u)
                # = (K^-1 (m - \\mu u)) K (K^1 (m - \\mu u))
                # = (var_dist_mean)^T K (var_dist_mean)
                self.mean_diff_inv_quad(),
                # d
                -prior_dist.event_shape.numel(),
            ]
        )

        return kl_divergence","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.autograd.grad` to compute gradients instead of `torch.sum`.
3. Use `torch.tensor` instead of `torch.FloatTensor` to avoid creating unnecessary tensors."
"    def forward(self, x):
        """"""
        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the
        inducing point function values. Specifically, forward defines how to transform a variational distribution
        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at
        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`

        Args:
            x (torch.tensor): Locations x to get the variational posterior of the function values at.
        Returns:
            :obj:`gpytorch.distributions.MultivariateNormal`: The distribution q(f|x)
        """"""
        variational_dist = self.variational_distribution.variational_distribution
        inducing_points = self.inducing_points
        if inducing_points.dim() < x.dim():
            inducing_points = inducing_points.expand(*x.shape[:-2], *inducing_points.shape[-2:])
        if len(variational_dist.batch_shape) < x.dim() - 2:
            variational_dist = variational_dist.expand(x.shape[:-2])

        # If our points equal the inducing points, we're done
        if torch.equal(x, inducing_points):
            # De-whiten the prior covar
            prior_covar = self.prior_distribution.lazy_covariance_matrix
            if isinstance(variational_dist.lazy_covariance_matrix, RootLazyTensor):
                predictive_covar = RootLazyTensor(prior_covar @ variational_dist.lazy_covariance_matrix.root.evaluate())
            else:
                predictive_covar = MatmulLazyTensor(prior_covar @ variational_dist.covariance_matrix, prior_covar)

            # Cache some values for the KL divergence
            if self.training:
                self._mean_diff_inv_quad_memo, self._logdet_memo = prior_covar.inv_quad_logdet(
                    (variational_dist.mean - self.prior_distribution.mean), logdet=True
                )

            return MultivariateNormal(variational_dist.mean, predictive_covar)

        # Otherwise, we have to marginalize
        else:
            num_induc = inducing_points.size(-2)
            full_inputs = torch.cat([inducing_points, x], dim=-2)
            full_output = self.model.forward(full_inputs)
            full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix

            # Mean terms
            test_mean = full_mean[..., num_induc:]
            induc_mean = full_mean[..., :num_induc]
            mean_diff = (variational_dist.mean - induc_mean).unsqueeze(-1)

            # Covariance terms
            induc_induc_covar = full_covar[..., :num_induc, :num_induc].add_jitter()
            induc_data_covar = full_covar[..., :num_induc, num_induc:].evaluate()
            data_data_covar = full_covar[..., num_induc:, num_induc:]

            # If we're less than a certain size, we'll compute the Cholesky decomposition of induc_induc_covar
            cholesky = False
            if settings.fast_computations.log_prob.off() or (num_induc <= settings.max_cholesky_size.value()):
                induc_induc_covar = CholLazyTensor(induc_induc_covar.cholesky())
                cholesky = True

            # Cache the CG results
            # Do not use preconditioning for whitened VI, as it does not seem to improve performance.
            with settings.max_preconditioner_size(0):
                with torch.no_grad():
                    eager_rhs = torch.cat([induc_data_covar, mean_diff], -1)
                    solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(
                        induc_induc_covar,
                        eager_rhs.detach(),
                        logdet_terms=(not cholesky),
                        include_tmats=(not settings.skip_logdet_forward.on() and not cholesky),
                    )
                    eager_rhss = [eager_rhs.detach()]
                    solves = [solve.detach()]
                    if settings.skip_logdet_forward.on() and self.training:
                        eager_rhss.append(torch.cat([probe_vecs, eager_rhs], -1))
                        solves.append(torch.cat([probe_vec_solves, solve[..., : eager_rhs.size(-1)]], -1))
                    elif not self.training:
                        eager_rhss.append(eager_rhs[..., :-1])
                        solves.append(solve[..., :-1])

                induc_induc_covar = CachedCGLazyTensor(
                    induc_induc_covar,
                    eager_rhss=eager_rhss,
                    solves=solves,
                    probe_vectors=probe_vecs,
                    probe_vector_norms=probe_vec_norms,
                    probe_vector_solves=probe_vec_solves,
                    probe_vector_tmats=tmats,
                )

            # Compute some terms that will be necessary for the predicitve covariance and KL divergence
            if self.training:
                interp_data_data_var_plus_mean_diff_inv_quad, logdet = induc_induc_covar.inv_quad_logdet(
                    torch.cat([induc_data_covar, mean_diff], -1), logdet=True, reduce_inv_quad=False
                )
                interp_data_data_var = interp_data_data_var_plus_mean_diff_inv_quad[..., :-1]
                mean_diff_inv_quad = interp_data_data_var_plus_mean_diff_inv_quad[..., -1]

            # Compute predictive mean
            predictive_mean = torch.add(
                test_mean,
                induc_induc_covar.inv_matmul(mean_diff, left_tensor=induc_data_covar.transpose(-1, -2)).squeeze(-1),
            )

            # Compute the predictive covariance
            is_root_lt = isinstance(variational_dist.lazy_covariance_matrix, RootLazyTensor)
            is_repeated_root_lt = isinstance(
                variational_dist.lazy_covariance_matrix, BatchRepeatLazyTensor
            ) and isinstance(variational_dist.lazy_covariance_matrix.base_lazy_tensor, RootLazyTensor)
            if is_root_lt:
                predictive_covar = RootLazyTensor(
                    induc_data_covar.transpose(-1, -2) @ variational_dist.lazy_covariance_matrix.root.evaluate()
                )
            elif is_repeated_root_lt:
                predictive_covar = RootLazyTensor(
                    induc_data_covar.transpose(-1, -2)
                    @ variational_dist.lazy_covariance_matrix.root_decomposition().root.evaluate()
                )
            else:
                predictive_covar = MatmulLazyTensor(
                    induc_data_covar.transpose(-1, -2), predictive_covar @ induc_data_covar
                )

            if self.training:
                data_covariance = DiagLazyTensor((data_data_covar.diag() - interp_data_data_var).clamp(0, math.inf))
            else:
                neg_induc_data_data_covar = torch.matmul(
                    induc_data_covar.transpose(-1, -2).mul(-1),
                    induc_induc_covar.inv_matmul(induc_data_covar)
                )
                data_covariance = data_data_covar + neg_induc_data_data_covar
            predictive_covar = PsdSumLazyTensor(predictive_covar, data_covariance)

            # Save the logdet, mean_diff_inv_quad, prior distribution for the ELBO
            if self.training:
                self._memoize_cache[""prior_distribution_memo""] = MultivariateNormal(induc_mean, induc_induc_covar)
                self._memoize_cache[""logdet_memo""] = -logdet
                self._memoize_cache[""mean_diff_inv_quad_memo""] = mean_diff_inv_quad

            return MultivariateNormal(predictive_mean, predictive_covar)","1. Use `torch.no_grad()` to disable gradient computation when not needed.
2. Use `torch.jit.script` to JIT-compile the model for performance and security.
3. Use `torch.autograd.gradcheck` to check the gradients of the model for correctness."
"    def __init__(
        self,
        base_lazy_tensor,
        left_interp_indices=None,
        left_interp_values=None,
        right_interp_indices=None,
        right_interp_values=None,
    ):
        base_lazy_tensor = lazify(base_lazy_tensor)

        if left_interp_indices is None:
            num_rows = base_lazy_tensor.size(-2)
            left_interp_indices = torch.arange(0, num_rows, dtype=torch.long, device=base_lazy_tensor.device)
            left_interp_indices.unsqueeze_(-1)
            left_interp_indices = left_interp_indices.expand(*base_lazy_tensor.batch_shape, num_rows, 1)

        if left_interp_values is None:
            left_interp_values = torch.ones(
                left_interp_indices.size(), dtype=base_lazy_tensor.dtype, device=base_lazy_tensor.device
            )

        if right_interp_indices is None:
            num_rows = base_lazy_tensor.size(-2)
            right_interp_indices = torch.arange(0, num_rows, dtype=torch.long, device=base_lazy_tensor.device)
            right_interp_indices.unsqueeze_(-1)
            right_interp_indices = right_interp_indices.expand(*base_lazy_tensor.batch_shape, num_rows, 1)

        if right_interp_values is None:
            right_interp_values = torch.ones(
                right_interp_indices.size(), dtype=base_lazy_tensor.dtype, device=base_lazy_tensor.device
            )

        if left_interp_indices.shape[:-2] != base_lazy_tensor.batch_shape:
            try:
                base_lazy_tensor = base_lazy_tensor._expand_batch(left_interp_indices.shape[:-2])
            except RuntimeError:
                raise RuntimeError(
                    ""interp size ({}) is incompatible with base_lazy_tensor size ({}). "".format(
                        right_interp_indices.size(), base_lazy_tensor.size()
                    )
                )

        super(InterpolatedLazyTensor, self).__init__(
            base_lazy_tensor, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values
        )
        self.base_lazy_tensor = base_lazy_tensor
        self.left_interp_indices = left_interp_indices
        self.left_interp_values = left_interp_values
        self.right_interp_indices = right_interp_indices
        self.right_interp_values = right_interp_values","1. Use `torch.jit.trace` to create a traced version of the model. This will make it much more difficult for attackers to reverse engineer the model.
2. Use `torch.jit.save` to save the traced model in a secure location. This will prevent attackers from accessing the model file.
3. Use `torch.jit.load` to load the traced model into a secure environment. This will prevent attackers from running the model in an untrusted environment."
"    def _sparse_left_interp_t(self, left_interp_indices_tensor, left_interp_values_tensor):
        if hasattr(self, ""_sparse_left_interp_t_memo""):
            if torch.equal(self._left_interp_indices_memo, left_interp_indices_tensor) and torch.equal(
                self._left_interp_values_memo, left_interp_values_tensor
            ):
                return self._sparse_left_interp_t_memo

        left_interp_t = sparse.make_sparse_from_indices_and_values(
            left_interp_indices_tensor, left_interp_values_tensor, self.base_lazy_tensor.size()[-1]
        )
        self._left_interp_indices_memo = left_interp_indices_tensor
        self._left_interp_values_memo = left_interp_values_tensor
        self._sparse_left_interp_t_memo = left_interp_t
        return self._sparse_left_interp_t_memo","1. Use `torch.jit.script` to make the function `_sparse_left_interp_t` a scripted function.
2. Check the type of inputs to ensure that they are valid.
3. Use `torch.jit.trace` to create a traced version of the function for faster execution."
"    def backward(ctx, inv_quad_grad_output, logdet_grad_output):
        matrix_arg_grads = None
        inv_quad_rhs_grad = None

        # Which backward passes should we compute?
        compute_inv_quad_grad = inv_quad_grad_output.abs().sum() and ctx.inv_quad
        compute_logdet_grad = logdet_grad_output.abs().sum() and ctx.logdet

        # Get input arguments, and get gradients in the proper form
        matrix_args = ctx.saved_tensors[:-1]
        solves = ctx.saved_tensors[-1]

        if hasattr(ctx, ""_lazy_tsr""):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)

        # Fix grad_output sizes
        if ctx.inv_quad:
            inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)
        if compute_logdet_grad:
            logdet_grad_output = logdet_grad_output.unsqueeze(-1)
            logdet_grad_output.unsqueeze_(-1)

        # Divide up the solves
        probe_vector_solves = None
        inv_quad_solves = None
        neg_inv_quad_solves_times_grad_out = None
        if compute_logdet_grad:
            coef = 1.0 / ctx.probe_vectors.size(-1)
            probe_vector_solves = solves.narrow(-1, 0, ctx.num_random_probes).mul(coef)
            probe_vector_solves.mul_(ctx.probe_vector_norms).mul_(logdet_grad_output)
            probe_vectors = ctx.probe_vectors.mul(ctx.probe_vector_norms)
        if ctx.inv_quad:
            inv_quad_solves = solves.narrow(-1, ctx.num_random_probes, ctx.num_inv_quad_solves)
            neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(inv_quad_grad_output).mul_(-1)

        # input_1 gradient
        if any(ctx.needs_input_grad):
            # Collect terms for arg grads
            left_factors_list = []
            right_factors_list = []

            if compute_logdet_grad:
                left_factors_list.append(probe_vector_solves)
                if ctx.preconditioner is not None:
                    probe_vectors = ctx.preconditioner(probe_vectors)
                right_factors_list.append(probe_vectors)

            if compute_inv_quad_grad:
                left_factors_list.append(neg_inv_quad_solves_times_grad_out)
                right_factors_list.append(inv_quad_solves)

            left_factors = torch.cat(left_factors_list, -1)
            right_factors = torch.cat(right_factors_list, -1)
            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors, right_factors)

        # input_2 gradients
        if compute_inv_quad_grad and ctx.needs_input_grad[9]:
            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul_(-2)
        elif ctx.inv_quad:
            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)
        if ctx.is_vector:
            inv_quad_rhs_grad.squeeze_(-1)

        if ctx.inv_quad:
            res = [inv_quad_rhs_grad] + list(matrix_arg_grads)
        else:
            res = matrix_arg_grads

        return tuple([None] * 9 + res)","1. Use `torch.no_grad()` to disable gradient calculation when it is not needed.
2. Use `torch.jit.script()` to JIT-compile the code for performance improvement and security hardening.
3. Use `torch.autograd.gradcheck()` to check the correctness of the gradient calculation."
"    def check(self, tensor):
        return torch.all(tensor <= self.upper_bound) and torch.all(tensor >= self.lower_bound)","1. Use `torch.clamp` to check if the tensor is within the specified bounds.
2. Use `torch.isfinite` to check if the tensor contains any NaN or Inf values.
3. Use `torch.is_tensor` to check if the input is a valid tensor."
"    def initialize(self, **kwargs):
        """"""
        Set a value for a parameter

        kwargs: (param_name, value) - parameter to initialize.
        Can also initialize recursively by passing in the full name of a
        parameter. For example if model has attribute model.likelihood,
        we can initialize the noise with either
        `model.initialize(**{'likelihood.noise': 0.1})`
        or
        `model.likelihood.initialize(noise=0.1)`.
        The former method would allow users to more easily store the
        initialization values as one object.

        Value can take the form of a tensor, a float, or an int
        """"""

        for name, val in kwargs.items():
            if isinstance(val, int):
                val = float(val)
            if '.' in name:
                module, name = self._get_module_and_name(name)
                module.initialize(**{name: val})
            elif not hasattr(self, name):
                raise AttributeError(""Unknown parameter {p} for {c}"".format(p=name, c=self.__class__.__name__))
            elif name not in self._parameters:
                setattr(self, name, val)
            elif torch.is_tensor(val):
                try:
                    self.__getattr__(name).data.copy_(val.expand_as(self.__getattr__(name)))
                except RuntimeError:
                    self.__getattr__(name).data.copy_(val.view_as(self.__getattr__(name)))

            elif isinstance(val, float):
                self.__getattr__(name).data.fill_(val)
            else:
                raise AttributeError(""Type {t} not valid for initializing parameter {p}"".format(t=type(val), p=name))

            # Ensure value is contained in support of prior (if present)
            prior_name = ""_"".join([name, ""prior""])
            if prior_name in self._priors:
                prior, closure, _ = self._priors[prior_name]
                try:
                    prior._validate_sample(closure())
                except ValueError as e:
                    raise ValueError(""Invalid input value for prior {}. Error:\\n{}"".format(prior_name, e))

        return self","1. Use `torch.nn.Module.register_parameter` to register parameters and protect them from being overwritten.
2. Use `torch.nn.Module.register_buffer` to register buffers and protect them from being overwritten.
3. Use `torch.nn.Module.register_forward_pre_hook` to validate inputs before they are used in the forward pass."
"    def __init__(
        self,
        base_lazy_tensor,
        left_interp_indices=None,
        left_interp_values=None,
        right_interp_indices=None,
        right_interp_values=None,
    ):
        base_lazy_tensor = lazify(base_lazy_tensor)

        if left_interp_indices is None:
            num_rows = base_lazy_tensor.size(-2)
            left_interp_indices = torch.arange(0, num_rows, dtype=torch.long, device=base_lazy_tensor.device)
            left_interp_indices.unsqueeze_(-1)
            left_interp_indices = left_interp_indices.expand(*base_lazy_tensor.batch_shape, num_rows, 1)

        if left_interp_values is None:
            left_interp_values = torch.ones(
                left_interp_indices.size(), dtype=base_lazy_tensor.dtype, device=base_lazy_tensor.device
            )
        else:
            if left_interp_indices.size() != left_interp_values.size():
                raise RuntimeError(
                    ""Expected left_interp_indices ({}) to have the same size as ""
                    ""left_interp_values ({})"".format(left_interp_indices.size(), left_interp_values.size())
                )

        if right_interp_indices is None:
            num_rows = base_lazy_tensor.size(-2)
            right_interp_indices = torch.arange(0, num_rows, dtype=torch.long, device=base_lazy_tensor.device)
            right_interp_indices.unsqueeze_(-1)
            right_interp_indices = right_interp_indices.expand(*base_lazy_tensor.batch_shape, num_rows, 1)

        if right_interp_values is None:
            right_interp_values = torch.ones(
                right_interp_indices.size(), dtype=base_lazy_tensor.dtype, device=base_lazy_tensor.device
            )
        else:
            if left_interp_indices.size() != left_interp_values.size():
                raise RuntimeError(
                    ""Expected left_interp_indices ({}) to have the same size as ""
                    ""left_interp_values ({})"".format(left_interp_indices.size(), left_interp_values.size())
                )

        # Make sure that left/right interp tensors have the same batch shape as the base_lazy_tensor
        if left_interp_indices.shape[:-2] != base_lazy_tensor.batch_shape:
            raise RuntimeError(
                ""left interp size ({}) is incompatible with base_lazy_tensor size ({}). Make sure the two ""
                ""have the same number of batch dimensions"".format(left_interp_indices.size(), base_lazy_tensor.size())
            )
        if right_interp_indices.shape[:-2] != base_lazy_tensor.batch_shape:
            raise RuntimeError(
                ""right interp size ({}) is incompatible with base_lazy_tensor size ({}). Make sure the two ""
                ""have the same number of batch dimensions"".format(right_interp_indices.size(), base_lazy_tensor.size())
            )

        super(InterpolatedLazyTensor, self).__init__(
            base_lazy_tensor, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values
        )
        self.base_lazy_tensor = base_lazy_tensor
        self.left_interp_indices = left_interp_indices
        self.left_interp_values = left_interp_values
        self.right_interp_indices = right_interp_indices
        self.right_interp_values = right_interp_values","1. Use `torch.jit.script` to make the class `torch.jit.ScriptModule`. This will make the class more efficient and secure.
2. Use `torch.jit.trace` to create a trace of the class. This will allow you to inspect the class and its methods.
3. Use `torch.jit.save` to save the trace to a file. This will allow you to share the class with others without having to worry about them having access to the source code."
"    def evaluate_kernel(self):
        """"""
        NB: This is a meta LazyTensor, in the sense that evaluate can return
        a LazyTensor if the kernel being evaluated does so.
        """"""
        if not self.is_batch:
            x1 = self.x1.unsqueeze(0)
            x2 = self.x2.unsqueeze(0)
        else:
            x1 = self.x1
            x2 = self.x2

        with settings.lazily_evaluate_kernels(False):
            res = self.kernel(
                x1, x2, diag=False, batch_dims=self.batch_dims, **self.params
            )
        if self.squeeze_row:
            res.squeeze_(-2)
        if self.squeeze_col:
            res.squeeze_(-1)

        if (
            not self.is_batch
            and res.ndimension() == 3
            and res.size(0) == 1
        ):
            res = res[0]

        return lazify(res)","1. Use `torch.jit.script` to create a traced version of the kernel function. This will make the kernel function more efficient and secure.
2. Use `torch.jit.is_scripting` to check if the kernel function is being traced. This will prevent the kernel function from being executed in an untrusted environment.
3. Use `torch.jit.save` to save the traced kernel function to a file. This will allow the kernel function to be used in other applications without having to be re-traced."
"    def root_inv_decomposition(self, initial_vectors=None, test_vectors=None):
        """"""
        Returns a (usually low-rank) root decomposotion lazy tensor of a PSD matrix.
        This can be used for sampling from a Gaussian distribution, or for obtaining a
        low-rank version of a matrix
        """"""
        from .root_lazy_tensor import RootLazyTensor

        if not self.is_square:
            raise RuntimeError(
                ""root_inv_decomposition only operates on (batches of) square (symmetric) LazyTensors. ""
                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())
            )

        if initial_vectors is not None:
            if self.dim() == 2 and initial_vectors.dim() == 1:
                if self.shape[-1] != initial_vectors.numel():
                    raise RuntimeError(
                        ""LazyTensor (size={}) cannot be multiplied with initial_vectors (size={})."".format(
                            self.shape, initial_vectors.shape
                        )
                    )
            elif self.dim() != initial_vectors.dim():
                raise RuntimeError(
                    ""LazyTensor (size={}) and initial_vectors (size={}) should have the same number ""
                    ""of dimensions."".format(self.shape, initial_vectors.shape)
                )
            elif self.batch_shape != initial_vectors.shape[:-2] or self.shape[-1] != initial_vectors.shape[-2]:
                raise RuntimeError(
                    ""LazyTensor (size={}) cannot be multiplied with initial_vectors (size={})."".format(
                        self.shape, initial_vectors.shape
                    )
                )

        roots, inv_roots = RootDecomposition(
            self.representation_tree(),
            max_iter=self.root_decomposition_size(),
            dtype=self.dtype,
            device=self.device,
            batch_shape=self.batch_shape,
            matrix_shape=self.matrix_shape,
            root=True,
            inverse=True,
            initial_vectors=initial_vectors,
        )(*self.representation())

        if initial_vectors is not None and initial_vectors.size(-1) > 1:
            self._memoize_cache[""root_decomposition""] = RootLazyTensor(roots[0])
        else:
            self._memoize_cache[""root_decomposition""] = RootLazyTensor(roots)

        # Choose the best of the inv_roots, if there were more than one initial vectors
        if initial_vectors is not None and initial_vectors.size(-1) > 1:
            num_probes = initial_vectors.size(-1)
            test_vectors = test_vectors.unsqueeze(0)

            # Compute solves
            solves = inv_roots.matmul(inv_roots.transpose(-1, -2).matmul(test_vectors))

            # Compute self * solves
            solves = (
                solves.permute(*range(1, self.dim() + 1), 0)
                .contiguous()
                .view(*self.batch_shape, self.matrix_shape[-1], -1)
            )
            mat_times_solves = self.matmul(solves)
            mat_times_solves = mat_times_solves.view(*self.batch_shape, self.matrix_shape[-1], -1, num_probes).permute(
                -1, *range(0, self.dim())
            )

            # Compute residuals
            residuals = (mat_times_solves - test_vectors).norm(2, dim=-2)
            residuals = residuals.view(residuals.size(0), -1).sum(-1)

            # Choose solve that best fits
            _, best_solve_index = residuals.min(0)
            inv_root = inv_roots[best_solve_index].squeeze(0)

        else:
            inv_root = inv_roots

        return RootLazyTensor(inv_root)","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to prevent unauthorized access to the model parameters.
3. Use `torch.jit.save` to save the model in a secure format."
"    def __init__(self, representation_tree, preconditioner=None, has_left=False):
        self.representation_tree = representation_tree
        self.preconditioner = preconditioner
        self.has_left = has_left","1. Use `assert` statements to check for invalid inputs.
2. Sanitize user input before using it in the code.
3. Use a secure coding style, such as [OWASP's Top 10](https://owasp.org/www-project-top-ten/)."
"    def forward(self, *args):
        left_tensor = None
        right_tensor = None
        matrix_args = None
        if self.has_left:
            left_tensor, right_tensor, *matrix_args = args
        else:
            right_tensor, *matrix_args = args
        orig_right_tensor = right_tensor
        lazy_tsr = self.representation_tree(*matrix_args)

        self.is_vector = False
        if right_tensor.ndimension() == 1:
            right_tensor = right_tensor.unsqueeze(-1)
            self.is_vector = True

        # Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)
        if self.has_left:
            rhs = torch.cat([left_tensor.transpose(-1, -2), right_tensor], -1)
            solves = lazy_tsr._solve(rhs, self.preconditioner)
            res = solves[..., left_tensor.size(-2):]
            res = left_tensor @ res
        else:
            solves = lazy_tsr._solve(right_tensor, self.preconditioner)
            res = solves

        if self.is_vector:
            res = res.squeeze(-1)

        if self.has_left:
            args = [solves, left_tensor, orig_right_tensor] + list(matrix_args)
        else:
            args = [solves, orig_right_tensor] + list(matrix_args)
        self.save_for_backward(*args)
        if settings.memory_efficient.off():
            self._lazy_tsr = lazy_tsr

        return res","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to trace the model and generate a torchscript model.
3. Use `torch.jit.save` to save the torchscript model to a file."
"    def __init__(
        self,
        representation_tree,
        dtype,
        device,
        matrix_shape,
        batch_shape=torch.Size(),
        inv_quad=False,
        logdet=False,
        preconditioner=None,
        logdet_correction=None,
        probe_vectors=None,
        probe_vector_norms=None,
    ):
        if not (inv_quad or logdet):
            raise RuntimeError(""Either inv_quad or logdet must be true (or both)"")

        self.representation_tree = representation_tree
        self.dtype = dtype
        self.device = device
        self.matrix_shape = matrix_shape
        self.batch_shape = batch_shape
        self.inv_quad = inv_quad
        self.logdet = logdet
        self.preconditioner = preconditioner
        self.logdet_correction = logdet_correction

        if (probe_vectors is None or probe_vector_norms is None) and logdet:
            num_random_probes = settings.num_trace_samples.value()
            probe_vectors = torch.empty(matrix_shape[-1], num_random_probes, dtype=dtype, device=device)
            probe_vectors.bernoulli_().mul_(2).add_(-1)
            probe_vector_norms = torch.norm(probe_vectors, 2, dim=-2, keepdim=True)
            if batch_shape is not None:
                probe_vectors = probe_vectors.expand(*batch_shape, matrix_shape[-1], num_random_probes)
                probe_vector_norms = probe_vector_norms.expand(*batch_shape, 1, num_random_probes)
            probe_vectors = probe_vectors.div(probe_vector_norms)

        self.probe_vectors = probe_vectors
        self.probe_vector_norms = probe_vector_norms","1. Use `torch.jit.script` to make the code more efficient and secure.
2. Use `torch.autograd.grad` to compute gradients instead of `torch.einsum`.
3. Use `torch.jit.trace` to generate a tracing of the model, which can be used to check for correctness and security vulnerabilities."
"    def forward(self, *args):
        """"""
        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)
        If self.inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)
        - the RHS of the matrix solves.

        Returns:
        - (Scalar) The inverse quadratic form (or None, if self.inv_quad is False)
        - (Scalar) The log determinant (or None, self.if logdet is False)
        """"""
        matrix_args = None
        inv_quad_rhs = None
        if self.inv_quad:
            matrix_args = args[1:]
            inv_quad_rhs = args[0]
        else:
            matrix_args = args

        # Get closure for matmul
        lazy_tsr = self.representation_tree(*matrix_args)

        # Collect terms for LinearCG
        # We use LinearCG for both matrix solves and for stochastically estimating the log det
        rhs_list = []
        num_random_probes = 0
        num_inv_quad_solves = 0

        # RHS for logdet
        if self.logdet:
            rhs_list.append(self.probe_vectors)
            num_random_probes = self.probe_vectors.size(-1)

        # RHS for inv_quad
        self.is_vector = False
        if self.inv_quad:
            if inv_quad_rhs.ndimension() == 1:
                inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)
                self.is_vector = True
            rhs_list.append(inv_quad_rhs)
            num_inv_quad_solves = inv_quad_rhs.size(-1)

        # Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)
        rhs = torch.cat(rhs_list, -1)
        t_mat = None
        if self.logdet and settings.skip_logdet_forward.off():
            solves, t_mat = lazy_tsr._solve(rhs, self.preconditioner, num_tridiag=num_random_probes)

        else:
            solves = lazy_tsr._solve(rhs, self.preconditioner, num_tridiag=0)

        # Final values to return
        logdet_term = torch.zeros(lazy_tsr.batch_shape, dtype=self.dtype, device=self.device)
        inv_quad_term = torch.zeros(lazy_tsr.batch_shape, dtype=self.dtype, device=self.device)

        # Compute logdet from tridiagonalization
        if self.logdet and settings.skip_logdet_forward.off():
            if torch.any(torch.isnan(t_mat)).item():
                logdet_term = torch.tensor(float(""nan""), dtype=self.dtype, device=self.device)
            else:
                if self.batch_shape is None:
                    t_mat = t_mat.unsqueeze(1)
                eigenvalues, eigenvectors = lanczos_tridiag_to_diag(t_mat)
                slq = StochasticLQ()
                logdet_term, = slq.evaluate(self.matrix_shape, eigenvalues, eigenvectors, [lambda x: x.log()])

                # Add correction
                if self.logdet_correction is not None:
                    logdet_term = logdet_term + self.logdet_correction

        # Extract inv_quad solves from all the solves
        if self.inv_quad:
            inv_quad_solves = solves.narrow(-1, num_random_probes, num_inv_quad_solves)
            inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)

        self.num_random_probes = num_random_probes
        self.num_inv_quad_solves = num_inv_quad_solves

        to_save = list(matrix_args) + [solves, ]
        self.save_for_backward(*to_save)

        if settings.memory_efficient.off():
            self._lazy_tsr = lazy_tsr

        return inv_quad_term, logdet_term","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to create a trace of the code and use it for inference.
3. Use `torch.jit.save` to save the traced model to a file."
"    def inv_matmul(self, right_tensor, left_tensor=None):
        """"""
        Computes a linear solve (w.r.t self = :math:`A`) with several right hand sides :math:`R`.
        I.e. computes

        ... math::

            \\begin{equation}
                A^{-1} R,
            \\end{equation}

        where :math:`R` is :attr:`right_tensor` and :math:`A` is the LazyTensor.

        If :attr:`left_tensor` is supplied, computes

        ... math::

            \\begin{equation}
                L A^{-1} R,
            \\end{equation}

        where :math:`L` is :attr:`left_tensor`. Supplying this can reduce the number of
        CG calls required.

        Args:
            - :obj:`torch.tensor` (n x k) - Matrix :math:`R` right hand sides
            - :obj:`torch.tensor` (m x n) - Optional matrix :math:`L` to perform left multiplication with

        Returns:
            - :obj:`torch.tensor` - :math:`A^{-1}R` or :math:`LA^{-1}R`.
        """"""
        if not self.is_square:
            raise RuntimeError(
                ""inv_matmul only operates on (batches of) square (positive semi-definite) LazyTensors. ""
                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())
            )

        if self.dim() == 2 and right_tensor.dim() == 1:
            if self.shape[-1] != right_tensor.numel():
                raise RuntimeError(
                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(
                        self.shape, right_tensor.shape
                    )
                )

        func = InvMatmul(
            self.representation_tree(),
            preconditioner=self._inv_matmul_preconditioner(),
            has_left=(left_tensor is not None),
        )
        if left_tensor is None:
            return func(right_tensor, *self.representation())
        else:
            return func(left_tensor, right_tensor, *self.representation())","1. Use `torch.jit.script` to prevent attackers from modifying the code.
2. Use `torch.jit.trace` to prevent attackers from injecting new code.
3. Use `torch.jit.freeze` to prevent attackers from changing the graph."
"    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):
        """"""
        Computes an inverse quadratic form (w.r.t self) with several right hand sides.
        I.e. computes tr( tensor^T self^{-1} tensor )
        In addition, computes an (approximate) log determinant of the the matrix

        Args:
            - tensor (tensor nxk) - Vector (or matrix) for inverse quad

        Returns:
            - scalar - tr( tensor^T (self)^{-1} tensor )
            - scalar - log determinant
        """"""
        if not self.is_square:
            raise RuntimeError(
                ""inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. ""
                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())
            )

        if inv_quad_rhs is not None:
            if self.dim() == 2 and inv_quad_rhs.dim() == 1:
                if self.shape[-1] != inv_quad_rhs.numel():
                    raise RuntimeError(
                        ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(
                            self.shape, inv_quad_rhs.shape
                        )
                    )
            elif self.dim() != inv_quad_rhs.dim():
                raise RuntimeError(
                    ""LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number ""
                    ""of dimensions."".format(self.shape, inv_quad_rhs.shape)
                )
            elif self.batch_shape != inv_quad_rhs.shape[:-2] or self.shape[-1] != inv_quad_rhs.shape[-2]:
                raise RuntimeError(
                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(
                        self.shape, inv_quad_rhs.shape
                    )
                )

        args = self.representation()
        if inv_quad_rhs is not None:
            args = [inv_quad_rhs] + list(args)

        probe_vectors, probe_vector_norms = self._probe_vectors_and_norms()
        inv_quad_term, logdet_term = InvQuadLogDet(
            representation_tree=self.representation_tree(),
            matrix_shape=self.matrix_shape,
            batch_shape=self.batch_shape,
            dtype=self.dtype,
            device=self.device,
            inv_quad=(inv_quad_rhs is not None),
            logdet=logdet,
            preconditioner=self._preconditioner()[0],
            logdet_correction=self._preconditioner()[1],
            probe_vectors=probe_vectors,
            probe_vector_norms=probe_vector_norms,
        )(*args)

        if inv_quad_term.numel() and reduce_inv_quad:
            inv_quad_term = inv_quad_term.sum(-1)
        return inv_quad_term, logdet_term","1. Use `torch.jit.script` to create a [torchscript](https://pytorch.org/docs/stable/jit.html) model.
2. Use [PyTorch security checklist](https://pytorch.org/security/checklist/) to check for potential security vulnerabilities.
3. Use [Pydantic](https://pydantic-docs.readthedocs.io/en/latest/) to validate user input."
"    def __init__(
        self,
        active_dims=None,
        batch_size=1,
        period_length_prior=None,
        eps=1e-6,
        param_transform=softplus,
        inv_param_transform=None,
        **kwargs
    ):
        period_length_prior = _deprecate_kwarg(
            kwargs, ""log_period_length_prior"", ""period_length_prior"", period_length_prior
        )
        super(CosineKernel, self).__init__(
            active_dims=active_dims, param_transform=param_transform, inv_param_transform=inv_param_transform
        )
        self.eps = eps
        self.register_parameter(name=""raw_period_length"", parameter=torch.nn.Parameter(torch.zeros(batch_size, 1, 1)))
        if period_length_prior is not None:
            self.register_prior(
                ""period_length_prior"",
                period_length_prior,
                lambda: self.period_length,
                lambda v: self._set_period_length(v),
            )","1. Use `torch.nn.Parameter` instead of `torch.nn.Parameter()` to prevent attackers from modifying the parameters.
2. Use `torch.nn.init.uniform_` to initialize the parameters instead of `torch.nn.init.zeros_()` to make them more unpredictable.
3. Use `torch.nn.functional.softplus()` to transform the parameters instead of `torch.nn.functional.relu()` to prevent them from being too large."
"    def __init__(
        self,
        has_lengthscale=False,
        ard_num_dims=None,
        batch_size=1,
        active_dims=None,
        lengthscale_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        eps=1e-6,
        **kwargs
    ):
        lengthscale_prior = _deprecate_kwarg(kwargs, ""log_lengthscale_prior"", ""lengthscale_prior"", lengthscale_prior)
        super(Kernel, self).__init__()
        if active_dims is not None and not torch.is_tensor(active_dims):
            active_dims = torch.tensor(active_dims, dtype=torch.long)
        self.register_buffer(""active_dims"", active_dims)
        self.ard_num_dims = ard_num_dims
        self.batch_size = batch_size
        self.__has_lengthscale = has_lengthscale
        self._param_transform = param_transform
        self._inv_param_transform = _get_inv_param_transform(param_transform, inv_param_transform)
        if has_lengthscale:
            self.eps = eps
            lengthscale_num_dims = 1 if ard_num_dims is None else ard_num_dims
            self.register_parameter(
                name=""raw_lengthscale"", parameter=torch.nn.Parameter(torch.zeros(batch_size, 1, lengthscale_num_dims))
            )
            if lengthscale_prior is not None:
                self.register_prior(
                    ""lengthscale_prior"", lengthscale_prior, lambda: self.lengthscale, lambda v: self._set_lengthscale(v)
                )

        # TODO: Remove this on next official PyTorch release.
        self.__pdist_supports_batch = True","1. Use `torch.nn.Parameter` instead of `torch.tensor` to register parameters.
2. Use `torch.nn.functional.softplus` instead of `torch.nn.functional.relu` to avoid numerical instability.
3. Use `torch.nn.functional.pad` instead of `torch.nn.functional.zero_pad` to avoid undefined behavior."
"    def __init__(
        self,
        nu=2.5,
        ard_num_dims=None,
        batch_size=1,
        active_dims=None,
        lengthscale_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        eps=1e-6,
        **kwargs
    ):
        _deprecate_kwarg(kwargs, ""log_lengthscale_prior"", ""lengthscale_prior"", lengthscale_prior)
        if nu not in {0.5, 1.5, 2.5}:
            raise RuntimeError(""nu expected to be 0.5, 1.5, or 2.5"")
        super(MaternKernel, self).__init__(
            has_lengthscale=True,
            ard_num_dims=ard_num_dims,
            batch_size=batch_size,
            active_dims=active_dims,
            lengthscale_prior=lengthscale_prior,
            param_transform=param_transform,
            inv_param_transform=inv_param_transform,
            eps=eps,
        )
        self.nu = nu","1. Use `param_transform=None` to disable the default parameter transform.
2. Use `inv_param_transform=None` to disable the default parameter transform.
3. Use `eps=0` to disable the default epsilon value."
"    def __init__(
        self,
        active_dims=None,
        batch_size=1,
        lengthscale_prior=None,
        period_length_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        eps=1e-6,
        **kwargs
    ):
        lengthscale_prior = _deprecate_kwarg(kwargs, ""log_lengthscale_prior"", ""lengthscale_prior"", lengthscale_prior)
        period_length_prior = _deprecate_kwarg(
            kwargs, ""log_period_length_prior"", ""period_length_prior"", period_length_prior
        )
        super(PeriodicKernel, self).__init__(
            has_lengthscale=True,
            active_dims=active_dims,
            batch_size=batch_size,
            lengthscale_prior=lengthscale_prior,
            param_transform=param_transform,
            inv_param_transform=inv_param_transform,
            eps=eps,
        )
        self.register_parameter(name=""raw_period_length"", parameter=torch.nn.Parameter(torch.zeros(batch_size, 1, 1)))
        if period_length_prior is not None:
            self.register_prior(
                ""period_length_prior"",
                period_length_prior,
                lambda: self.period_length,
                lambda v: self._set_period_length(v),
            )","1. Use `torch.nn.Parameter` instead of `torch.Tensor` to make sure the parameters are initialized securely.
2. Use `torch.nn.Parameter` instead of `torch.nn.Parameter` to make sure the parameters are initialized securely.
3. Use `torch.nn.Parameter` instead of `torch.nn.Parameter` to make sure the parameters are initialized securely."
"    def __init__(
        self,
        ard_num_dims=None,
        batch_size=1,
        active_dims=None,
        lengthscale_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        eps=1e-6,
        **kwargs
    ):
        _deprecate_kwarg(kwargs, ""log_lengthscale_prior"", ""lengthscale_prior"", lengthscale_prior)
        super(RBFKernel, self).__init__(
            has_lengthscale=True,
            ard_num_dims=ard_num_dims,
            batch_size=batch_size,
            active_dims=active_dims,
            lengthscale_prior=lengthscale_prior,
            param_transform=param_transform,
            inv_param_transform=inv_param_transform,
            eps=eps,
        )","1. Use [secure random number generation](https://docs.python.org/3/library/random.html#random.SystemRandom) to generate secrets.
2. Use [type hints](https://docs.python.org/3/library/typing.html) to make your code more explicit.
3. [Validate user input](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertRaises) to prevent against attacks."
"    def __init__(
        self,
        base_kernel,
        batch_size=1,
        outputscale_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        **kwargs
    ):
        outputscale_prior = _deprecate_kwarg(kwargs, ""log_outputscale_prior"", ""outputscale_prior"", outputscale_prior)
        super(ScaleKernel, self).__init__(has_lengthscale=False, batch_size=batch_size)
        self.base_kernel = base_kernel
        self._param_transform = param_transform
        self._inv_param_transform = _get_inv_param_transform(param_transform, inv_param_transform)
        self.register_parameter(name=""raw_outputscale"", parameter=torch.nn.Parameter(torch.zeros(batch_size)))
        if outputscale_prior is not None:
            self.register_prior(
                ""outputscale_prior"", outputscale_prior, lambda: self.outputscale, lambda v: self._set_outputscale(v)
            )","1. Use `torch.nn.Parameter` instead of `torch.nn.Parameter(torch.zeros(batch_size))` to prevent the model from being initialized with all zeros.
2. Use `torch.nn.Parameter(torch.randn(batch_size))` to initialize the model with random values.
3. Use `torch.nn.init.uniform_` to initialize the model with uniform values."
"    def __init__(
        self,
        num_mixtures=None,
        ard_num_dims=1,
        batch_size=1,
        active_dims=None,
        eps=1e-6,
        mixture_scales_prior=None,
        mixture_means_prior=None,
        mixture_weights_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        **kwargs
    ):
        mixture_scales_prior = _deprecate_kwarg(
            kwargs, ""log_mixture_scales_prior"", ""mixture_scales_prior"", mixture_scales_prior
        )
        mixture_means_prior = _deprecate_kwarg(
            kwargs, ""log_mixture_means_prior"", ""mixture_means_prior"", mixture_means_prior
        )
        mixture_weights_prior = _deprecate_kwarg(
            kwargs, ""log_mixture_weights_prior"", ""mixture_weights_prior"", mixture_weights_prior
        )

        if num_mixtures is None:
            raise RuntimeError(""num_mixtures is a required argument"")
        if mixture_means_prior is not None or mixture_scales_prior is not None or mixture_weights_prior is not None:
            logger.warning(""Priors not implemented for SpectralMixtureKernel"")

        # This kernel does not use the default lengthscale
        super(SpectralMixtureKernel, self).__init__(
            active_dims=active_dims, param_transform=param_transform, inv_param_transform=inv_param_transform
        )
        self.num_mixtures = num_mixtures
        self.batch_size = batch_size
        self.ard_num_dims = ard_num_dims
        self.eps = eps

        self.register_parameter(
            name=""raw_mixture_weights"", parameter=torch.nn.Parameter(torch.zeros(self.batch_size, self.num_mixtures))
        )
        ms_shape = torch.Size([self.batch_size, self.num_mixtures, 1, self.ard_num_dims])
        self.register_parameter(name=""raw_mixture_means"", parameter=torch.nn.Parameter(torch.zeros(ms_shape)))
        self.register_parameter(name=""raw_mixture_scales"", parameter=torch.nn.Parameter(torch.zeros(ms_shape)))","1. Use `torch.nn.Parameter` instead of `torch.nn.ParameterDict` to avoid key-based lookups.
2. Use `torch.nn.init.uniform_` to initialize the weights.
3. Use `torch.nn.functional.softplus` to apply a non-linear transformation to the weights."
"    def __init__(self, noise_prior=None, batch_size=1, param_transform=softplus, inv_param_transform=None, **kwargs):
        noise_prior = _deprecate_kwarg(kwargs, ""log_noise_prior"", ""noise_prior"", noise_prior)
        noise_covar = HomoskedasticNoise(
            noise_prior=noise_prior,
            batch_size=batch_size,
            param_transform=param_transform,
            inv_param_transform=inv_param_transform,
        )
        super().__init__(noise_covar=noise_covar)","1. Use `param_transform=None` to disable the parameter transform.
2. Use `inv_param_transform=None` to disable the inverse parameter transform.
3. Use `batch_size=1` to disable batch processing."
"    def noise(self, value):
        self.noise_covar.initialize(value)","1. Use `torch.nn.Parameter` instead of `torch.Tensor` to make the noise tensor mutable.
2. Set the `requires_grad` attribute of the noise tensor to `False` to prevent it from being backpropagated.
3. Initialize the noise tensor with a fixed value to prevent it from being attacked."
"    def __init__(
        self,
        num_tasks,
        rank=0,
        task_correlation_prior=None,
        batch_size=1,
        noise_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        **kwargs
    ):
        """"""
        Args:
            num_tasks (int): Number of tasks.

            rank (int): The rank of the task noise covariance matrix to fit. If `rank` is set to 0,
            then a diagonal covariance matrix is fit.

            task_correlation_prior (:obj:`gpytorch.priors.Prior`): Prior to use over the task noise correlaton matrix.
            Only used when `rank` > 0.

        """"""
        task_correlation_prior = _deprecate_kwarg(
            kwargs, ""task_prior"", ""task_correlation_prior"", task_correlation_prior
        )
        noise_covar = MultitaskHomoskedasticNoise(
            num_tasks=num_tasks,
            noise_prior=noise_prior,
            batch_size=batch_size,
            param_transform=param_transform,
            inv_param_transform=inv_param_transform,
        )
        super().__init__(
            num_tasks=num_tasks,
            noise_covar=noise_covar,
            rank=rank,
            task_correlation_prior=task_correlation_prior,
            batch_size=batch_size,
        )
        self._param_transform = param_transform
        self._inv_param_transform = _get_inv_param_transform(param_transform, inv_param_transform)
        self.register_parameter(name=""raw_noise"", parameter=torch.nn.Parameter(torch.zeros(batch_size, 1)))","1. Use a secure parameter initialization method.
2. Use a secure activation function.
3. Use a secure loss function."
"    def __init__(
        self,
        num_tasks,
        rank=0,
        task_prior=None,
        batch_size=1,
        noise_prior=None,
        param_transform=softplus,
        inv_param_transform=None,
        **kwargs
    ):
        """"""
        Args:
            num_tasks (int): Number of tasks.

            rank (int): The rank of the task noise covariance matrix to fit. If `rank` is set to 0,
            then a diagonal covariance matrix is fit.

            task_prior (:obj:`gpytorch.priors.Prior`): Prior to use over the task noise covariance matrix if
            `rank` > 0, or a prior over the log of just the diagonal elements, if `rank` == 0.

        """"""
        noise_prior = _deprecate_kwarg(kwargs, ""log_noise_prior"", ""noise_prior"", noise_prior)
        super(Likelihood, self).__init__()
        self._param_transform = param_transform
        self._inv_param_transform = _get_inv_param_transform(param_transform, inv_param_transform)
        self.register_parameter(name=""raw_noise"", parameter=torch.nn.Parameter(torch.zeros(batch_size, 1)))
        if rank == 0:
            self.register_parameter(
                name=""raw_task_noises"", parameter=torch.nn.Parameter(torch.zeros(batch_size, num_tasks))
            )
            if task_prior is not None:
                raise RuntimeError(""Cannot set a `task_prior` if rank=0"")
        else:
            self.register_parameter(
                name=""task_noise_covar_factor"", parameter=torch.nn.Parameter(torch.randn(batch_size, num_tasks, rank))
            )
            if task_prior is not None:
                self.register_prior(""MultitaskErrorCovariancePrior"", task_prior, self._eval_covar_matrix)
        self.num_tasks = num_tasks
        self.rank = rank","1. Use `torch.nn.Parameter` instead of `torch.nn.Parameter` to make parameters more secure.
2. Use `_deprecate_kwarg` to deprecate deprecated arguments.
3. Use `_get_inv_param_transform` to get the inverse parameter transform."
"    def initialize(self, **kwargs):
        # TODO: Change to initialize actual parameter (e.g. lengthscale) rather than untransformed parameter.
        """"""
        Set a value for a parameter

        kwargs: (param_name, value) - parameter to initialize
        Value can take the form of a tensor, a float, or an int
        """"""
        from .utils.log_deprecation import MODULES_WITH_LOG_PARAMS

        for name, val in kwargs.items():
            if isinstance(val, int):
                val = float(val)
            if any(isinstance(self, mod_type) for mod_type in MODULES_WITH_LOG_PARAMS) and ""log_"" in name:
                base_name = name.split(""log_"")[1]
                name = ""raw_"" + base_name
                if not torch.is_tensor(val):
                    val = self._inv_param_transform(torch.tensor(val).exp()).item()
                else:
                    val = self._inv_param_transform(val.exp())

            if not hasattr(self, name):
                raise AttributeError(""Unknown parameter {p} for {c}"".format(p=name, c=self.__class__.__name__))
            elif name not in self._parameters:
                setattr(self, name, val)
            elif torch.is_tensor(val):
                try:
                    self.__getattr__(name).data.copy_(val.expand_as(self.__getattr__(name)))
                except RuntimeError:
                    self.__getattr__(name).data.copy_(val.view_as(self.__getattr__(name)))

            elif isinstance(val, float):
                self.__getattr__(name).data.fill_(val)
            else:
                raise AttributeError(""Type {t} not valid for initializing parameter {p}"".format(t=type(val), p=name))

            # Ensure value is contained in support of prior (if present)
            prior_name = ""_"".join([name, ""prior""])
            if prior_name in self._priors:
                prior, closure, _ = self._priors[prior_name]
                try:
                    prior._validate_sample(closure())
                except ValueError as e:
                    raise ValueError(""Invalid input value for prior {}. Error:\\n{}"".format(prior_name, e))

        return self","1. Use `torch.jit.script` to make the code more secure.
2. Use `torch.jit.trace` to create a trace of the model.
3. Use `torch.jit.save` to save the model to a file."
"    def __getattr__(self, name):
        try:
            return super().__getattr__(name)
        except AttributeError as e:
            from .utils.log_deprecation import LOG_DEPRECATION_MSG, MODULES_WITH_LOG_PARAMS

            if any(isinstance(self, mod_type) for mod_type in MODULES_WITH_LOG_PARAMS) and ""log_"" in name:
                base_name = name.split(""log_"")[1]  # e.g. log_lengthscale -> lengthscale
                raw_name = ""raw_"" + base_name
                warnings.warn(LOG_DEPRECATION_MSG.format(log_name=name, name=raw_name), DeprecationWarning)
                return super().__getattribute__(base_name).log()  # Get real param value and transform to log
            else:
                try:
                    return super().__getattribute__(name)
                except AttributeError:
                    raise e","1. Use `getattr` instead of `__getattr__` to avoid exposing implementation details.
2. Use `warnings.warn` instead of `raise` to log deprecation warnings.
3. Use `super().__getattribute__()` to get the attribute from the parent class."
"    def interpolate(self, x_grid, x_target, interp_points=range(-2, 2)):
        # Do some boundary checking
        grid_mins = x_grid.min(0)[0]
        grid_maxs = x_grid.max(0)[0]
        x_target_min = x_target.min(0)[0]
        x_target_max = x_target.min(0)[0]
        lt_min_mask = (x_target_min - grid_mins).lt(-1e-7)
        gt_max_mask = (x_target_max - grid_maxs).gt(1e-7)
        if lt_min_mask.sum().item():
            first_out_of_range = lt_min_mask.nonzero().squeeze(1)[0].item()
            raise RuntimeError(
                (
                    ""Received data that was out of bounds for the specified grid. ""
                    ""Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, ""
                    ""max = {0:.3f}""
                ).format(
                    grid_mins[first_out_of_range].item(),
                    grid_maxs[first_out_of_range].item(),
                    x_target_min[first_out_of_range].item(),
                    x_target_max[first_out_of_range].item(),
                )
            )
        if gt_max_mask.sum().item():
            first_out_of_range = gt_max_mask.nonzero().squeeze(1)[0].item()
            raise RuntimeError(
                (
                    ""Received data that was out of bounds for the specified grid. ""
                    ""Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, ""
                    ""max = {0:.3f}""
                ).format(
                    grid_mins[first_out_of_range].item(),
                    grid_maxs[first_out_of_range].item(),
                    x_target_min[first_out_of_range].item(),
                    x_target_max[first_out_of_range].item(),
                )
            )

        # Now do interpolation
        interp_points = torch.tensor(interp_points, dtype=x_grid.dtype, device=x_grid.device)
        interp_points_flip = interp_points.flip(0)

        num_grid_points = x_grid.size(0)
        num_target_points = x_target.size(0)
        num_dim = x_target.size(-1)
        num_coefficients = len(interp_points)

        interp_values = torch.ones(
            num_target_points, num_coefficients ** num_dim, dtype=x_grid.dtype, device=x_grid.device
        )
        interp_indices = torch.zeros(
            num_target_points, num_coefficients ** num_dim, dtype=torch.long, device=x_grid.device
        )

        for i in range(num_dim):
            grid_delta = x_grid[1, i] - x_grid[0, i]
            lower_grid_pt_idxs = torch.floor((x_target[:, i] - x_grid[0, i]) / grid_delta).squeeze()
            lower_pt_rel_dists = (x_target[:, i] - x_grid[0, i]) / grid_delta - lower_grid_pt_idxs
            lower_grid_pt_idxs = lower_grid_pt_idxs - interp_points.max()
            lower_grid_pt_idxs.detach_()

            scaled_dist = lower_pt_rel_dists.unsqueeze(-1) + interp_points_flip.unsqueeze(-2)
            dim_interp_values = self._cubic_interpolation_kernel(scaled_dist)

            # Find points who's closest lower grid point is the first grid point
            # This corresponds to a boundary condition that we must fix manually.
            left_boundary_pts = torch.nonzero(lower_grid_pt_idxs < 1)
            num_left = len(left_boundary_pts)

            if num_left > 0:
                left_boundary_pts.squeeze_(1)
                x_grid_first = x_grid[:num_coefficients, i].unsqueeze(1).t().expand(num_left, num_coefficients)

                grid_targets = x_target.select(1, i)[left_boundary_pts].unsqueeze(1).expand(num_left, num_coefficients)
                dists = torch.abs(x_grid_first - grid_targets)
                closest_from_first = torch.min(dists, 1)[1]

                for j in range(num_left):
                    dim_interp_values[left_boundary_pts[j], :] = 0
                    dim_interp_values[left_boundary_pts[j], closest_from_first[j]] = 1
                    lower_grid_pt_idxs[left_boundary_pts[j]] = 0

            right_boundary_pts = torch.nonzero(lower_grid_pt_idxs > num_grid_points - num_coefficients)
            num_right = len(right_boundary_pts)

            if num_right > 0:
                right_boundary_pts.squeeze_(1)
                x_grid_last = x_grid[-num_coefficients:, i].unsqueeze(1).t().expand(num_right, num_coefficients)

                grid_targets = x_target.select(1, i)[right_boundary_pts].unsqueeze(1)
                grid_targets = grid_targets.expand(num_right, num_coefficients)
                dists = torch.abs(x_grid_last - grid_targets)
                closest_from_last = torch.min(dists, 1)[1]

                for j in range(num_right):
                    dim_interp_values[right_boundary_pts[j], :] = 0
                    dim_interp_values[right_boundary_pts[j], closest_from_last[j]] = 1
                    lower_grid_pt_idxs[right_boundary_pts[j]] = num_grid_points - num_coefficients

            offset = (interp_points - interp_points.min()).long().unsqueeze(-2)
            dim_interp_indices = lower_grid_pt_idxs.long().unsqueeze(-1) + offset

            n_inner_repeat = num_coefficients ** i
            n_outer_repeat = num_coefficients ** (num_dim - i - 1)
            index_coeff = num_grid_points ** (num_dim - i - 1)
            dim_interp_indices = dim_interp_indices.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)
            dim_interp_values = dim_interp_values.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)
            interp_indices = interp_indices.add(dim_interp_indices.view(num_target_points, -1).mul(index_coeff))
            interp_values = interp_values.mul(dim_interp_values.view(num_target_points, -1))

        return interp_indices, interp_values","1. Use `torch.clamp` to bound the input values to within the grid range.
2. Use `torch.nonzero` to find the indices of out-of-bounds values and raise an error.
3. Use `torch.detach` to prevent the lower_grid_pt_idxs from being updated during the forward pass."
"    def __init__(
        self,
        has_lengthscale=False,
        ard_num_dims=None,
        log_lengthscale_prior=None,
        active_dims=None,
        batch_size=1,
        log_lengthscale_bounds=None,
    ):
        """"""
        The base Kernel class handles both lengthscales and ARD.

        Args:
            has_lengthscale (bool): If True, we will register a :obj:`torch.nn.Parameter` named `log_lengthscale`
            ard_num_dims (int): If not None, the `log_lengthscale` parameter will have this many entries.
            log_lengthscale_prior (:obj:`gpytorch.priors.Prior`): Prior over the log lengthscale
            active_dims (list): List of data dimensions to evaluate this Kernel on.
            batch_size (int): If training or testing multiple GPs simultaneously, this is how many lengthscales to
                              register.
            log_lengthscale_bounds (tuple): Deprecated min and max values for the lengthscales. If supplied, this
                                            now registers a :obj:`gpytorch.priors.SmoothedBoxPrior`
        """"""
        super(Kernel, self).__init__()
        if active_dims is not None and not torch.is_tensor(active_dims):
            active_dims = torch.tensor(active_dims, dtype=torch.long)
        self.active_dims = active_dims
        self.ard_num_dims = ard_num_dims
        self.has_lengthscale = has_lengthscale
        if has_lengthscale:
            lengthscale_num_dims = 1 if ard_num_dims is None else ard_num_dims
            log_lengthscale_prior = _bounds_to_prior(prior=log_lengthscale_prior, bounds=log_lengthscale_bounds)
            self.register_parameter(
                name=""log_lengthscale"",
                parameter=torch.nn.Parameter(torch.zeros(batch_size, 1, lengthscale_num_dims)),
                prior=log_lengthscale_prior,
            )","1. Use `torch.is_tensor` to check if `active_dims` is a tensor.
2. Use `torch.tensor` to convert `active_dims` to a tensor if it is not.
3. Use `_bounds_to_prior` to convert `log_lengthscale_bounds` to a prior."
"    def lengthscale(self):
        if ""log_lengthscale"" in self.named_parameters().keys():
            return self.log_lengthscale.exp()
        else:
            return None","1. Use `torch.nn.Parameter` instead of `torch.Tensor` to store model parameters, so that they will be automatically tracked by PyTorch's autograd engine.
2. Use `torch.nn.functional.relu` instead of `torch.relu`, so that the gradients will be computed correctly.
3. Use `torch.nn.functional.softmax` instead of `torch.softmax`, so that the gradients will be computed correctly."
"    def forward(self, x1, x2, **params):
        raise NotImplementedError()","1. Use `torch.nn.functional.relu` instead of `torch.relu` to avoid leaking gradients to the attacker.
2. Use `torch.nn.functional.normalize` instead of `torch.nn.functional.l2_normalize` to avoid leaking the norm of the input to the attacker.
3. Use `torch.nn.functional.pad` instead of `torch.nn.functional.constant_pad` to avoid leaking the size of the input to the attacker."
"    def __call__(self, x1_, x2_=None, **params):
        x1, x2 = x1_, x2_

        if self.active_dims is not None:
            x1 = x1_.index_select(-1, self.active_dims)
            if x2_ is not None:
                x2 = x2_.index_select(-1, self.active_dims)

        if x2 is None:
            x2 = x1

        # Give x1 and x2 a last dimension, if necessary
        if x1.ndimension() == 1:
            x1 = x1.unsqueeze(1)
        if x2.ndimension() == 1:
            x2 = x2.unsqueeze(1)
        if not x1.size(-1) == x2.size(-1):
            raise RuntimeError(""x1 and x2 must have the same number of dimensions!"")

        return LazyEvaluatedKernelTensor(self, x1, x2)","1. Use `torch.jit.script` to make the kernel function a compiled function. This will prevent attackers from modifying the function's behavior.
2. Use `torch.jit.is_scripting` to check if the kernel function is being executed in a script. This will prevent attackers from using the kernel function in an unsafe way.
3. Use `torch.jit.save` to save the compiled kernel function to a file. This will allow you to use the kernel function in other applications without having to recompile it."
"    def __init__(
        self,
        num_dimensions,
        variance_prior=None,
        offset_prior=None,
        active_dims=None,
        variance_bounds=None,
        offset_bounds=None,
    ):
        """"""
        Args:
            num_dimensions (int): Number of data dimensions to expect. This is necessary to create the offset parameter.
            variance_prior (:obj:`gpytorch.priors.Prior`): Prior over the variance parameter (default `None`).
            offset_prior (:obj:`gpytorch.priors.Prior`): Prior over the offset parameter (default `None`).
            active_dims (list): List of data dimensions to operate on. `len(active_dims)` should equal `num_dimensions`.
            variance_bounds (tuple, deprecated): Min and max value for the variance parameter. Deprecated, and now
                                                 creates a :obj:`gpytorch.priors.SmoothedBoxPrior`.
            offset_bounds (tuple, deprecated): Min and max value for the offset parameter. Deprecated, and now creates a
                                                :obj:'gpytorch.priors.SmoothedBoxPrior'.
        """"""
        super(LinearKernel, self).__init__(active_dims=active_dims)
        variance_prior = _bounds_to_prior(prior=variance_prior, bounds=variance_bounds, log_transform=False)
        self.register_parameter(name=""variance"", parameter=torch.nn.Parameter(torch.zeros(1)), prior=variance_prior)
        offset_prior = _bounds_to_prior(prior=offset_prior, bounds=offset_bounds, log_transform=False)
        self.register_parameter(
            name=""offset"", parameter=torch.nn.Parameter(torch.zeros(1, 1, num_dimensions)), prior=offset_prior
        )","1. Use `torch.nn.Parameter` instead of `torch.Tensor` to make sure the parameters are on the correct device.
2. Use `_bounds_to_prior` to create a `gpytorch.priors.SmoothedBoxPrior` instead of manually creating a prior.
3. Use `torch.nn.Parameter` with the correct shape for the offset parameter."
"    def __init__(
        self,
        nu=2.5,
        ard_num_dims=None,
        log_lengthscale_prior=None,
        active_dims=None,
        eps=1e-8,
        batch_size=1,
        log_lengthscale_bounds=None,
    ):
        if nu not in {0.5, 1.5, 2.5}:
            raise RuntimeError(""nu expected to be 0.5, 1.5, or 2.5"")
        super(MaternKernel, self).__init__(
            has_lengthscale=True,
            ard_num_dims=ard_num_dims,
            log_lengthscale_prior=log_lengthscale_prior,
            active_dims=active_dims,
            batch_size=batch_size,
            log_lengthscale_bounds=log_lengthscale_bounds,
        )
        self.nu = nu
        self.eps = eps","1. Use `assert` statements to validate inputs.
2. Use `type` annotations to make the types of arguments explicit.
3. Use `logging` to log errors and debug information."
"    def forward_diag(self, x1, x2):
        lengthscale = self.log_lengthscale.exp()
        mean = x1.mean(1).mean(0)
        x1_normed = (x1 - mean.unsqueeze(0).unsqueeze(1)).div(lengthscale)
        x2_normed = (x2 - mean.unsqueeze(0).unsqueeze(1)).div(lengthscale)

        diff = x1_normed - x2_normed
        distance_over_rho = diff.pow_(2).sum(-1).sqrt()
        exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance_over_rho)

        if self.nu == 0.5:
            constant_component = 1
        elif self.nu == 1.5:
            constant_component = (math.sqrt(3) * distance_over_rho).add(1)
        elif self.nu == 2.5:
            constant_component = (math.sqrt(5) * distance_over_rho).add(1).add(5. / 3. * distance_over_rho ** 2)

        return constant_component * exp_component","1. Use `torch.no_grad()` to disable gradient calculation when it is not needed.
2. Sanitize user input to prevent potential attacks.
3. Use secure coding practices, such as avoiding using `eval()` and `exec()`."
"    def forward(self, x1, x2):
        lengthscale = self.log_lengthscale.exp()
        mean = x1.mean(1).mean(0)
        x1_normed = (x1 - mean.unsqueeze(0).unsqueeze(1)).div(lengthscale)
        x2_normed = (x2 - mean.unsqueeze(0).unsqueeze(1)).div(lengthscale)

        x1_squared = x1_normed.norm(2, -1).pow(2)
        x2_squared = x2_normed.norm(2, -1).pow(2)
        x1_t_x_2 = torch.matmul(x1_normed, x2_normed.transpose(-1, -2))

        distance_over_rho = x1_squared.unsqueeze(-1) + x2_squared.unsqueeze(-2) - x1_t_x_2.mul(2)
        distance_over_rho = distance_over_rho.clamp(self.eps, 1e10).sqrt()
        exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance_over_rho)

        if self.nu == 0.5:
            constant_component = 1
        elif self.nu == 1.5:
            constant_component = (math.sqrt(3) * distance_over_rho).add(1)
        elif self.nu == 2.5:
            constant_component = (math.sqrt(5) * distance_over_rho).add(1).add(5. / 3. * distance_over_rho ** 2)
        return constant_component * exp_component","1. Use `torch.clamp` to bound the distance between vectors.
2. Use `torch.exp` to compute the exponential of the distance.
3. Use `torch.add` to add constants to the exponential."
"    def forward(self, x1, x2):
        covar_i = self.task_covar_module.covar_matrix
        covar_x = self.data_covar_module.forward(x1, x2)
        if covar_x.size(0) == 1:
            covar_x = covar_x[0]
        if not isinstance(covar_x, LazyTensor):
            covar_x = NonLazyTensor(covar_x)
        res = KroneckerProductLazyTensor(covar_i, covar_x)
        return res","1. Use `torch.jit.script` to create a compiled version of the model. This will make it more difficult for attackers to reverse engineer the model.
2. Use `torch.jit.trace` to create a traced version of the model. This will make it more difficult for attackers to insert malicious code into the model.
3. Use `torch.jit.save` to save the model in a secure format. This will make it more difficult for attackers to access the model's data."
"    def __init__(
        self,
        log_lengthscale_prior=None,
        log_period_length_prior=None,
        eps=1e-5,
        active_dims=None,
        log_lengthscale_bounds=None,
        log_period_length_bounds=None,
    ):
        log_period_length_prior = _bounds_to_prior(prior=log_period_length_prior, bounds=log_period_length_bounds)
        super(PeriodicKernel, self).__init__(
            has_lengthscale=True,
            active_dims=active_dims,
            log_lengthscale_prior=log_lengthscale_prior,
            log_lengthscale_bounds=log_lengthscale_bounds,
        )
        self.eps = eps
        self.register_parameter(
            name=""log_period_length"", parameter=torch.nn.Parameter(torch.zeros(1, 1, 1)), prior=log_period_length_prior
        )","1. Use `torch.nn.Parameter` instead of `torch.nn.Parameter` to prevent unauthorized access to model parameters.
2. Use `torch.nn.init.uniform_` to initialize model parameters instead of random values to prevent adversarial attacks.
3. Use `torch.nn.functional.relu` to activate model parameters instead of `torch.nn.functional.sigmoid` to prevent vanishing gradients."
"    def forward(self, x1, x2):
        lengthscale = (self.log_lengthscale.exp() + self.eps).sqrt_()
        period_length = (self.log_period_length.exp() + self.eps).sqrt_()
        diff = torch.sum((x1.unsqueeze(2) - x2.unsqueeze(1)).abs(), -1)
        res = -2 * torch.sin(math.pi * diff / period_length).pow(2) / lengthscale
        return res.exp()","1. Use `torch.jit.script` to create a compiled version of the model. This will make it more difficult for attackers to reverse engineer the model.
2. Use `torch.nn.functional.pad` to zero-pad the input tensors. This will prevent attackers from injecting adversarial examples into the model.
3. Use `torch.nn.functional.dropout` to randomly drop out units from the model. This will make it more difficult for attackers to train a model that can fool the original model."
"    def forward(self, x1, x2):
        lengthscales = self.log_lengthscale.exp().mul(math.sqrt(2)).clamp(self.eps, 1e5)
        diff = (x1.unsqueeze(2) - x2.unsqueeze(1)).div_(lengthscales.unsqueeze(1))
        return diff.pow_(2).sum(-1).mul_(-1).exp_()","1. Use `torch.clamp` to bound the values of `lengthscales` to prevent overflow.
2. Use `torch.unsqueeze` to add dimensions to `x1` and `x2` so that they can be subtracted elementwise.
3. Use `torch.pow` to calculate the squared difference between `x1` and `x2`, and then `torch.sum` to reduce the dimensions of the result."
"    def __init__(
        self,
        n_mixtures,
        n_dims=1,
        log_mixture_weight_prior=None,
        log_mixture_mean_prior=None,
        log_mixture_scale_prior=None,
        active_dims=None,
    ):
        self.n_mixtures = n_mixtures
        self.n_dims = n_dims
        if (
            log_mixture_mean_prior is not None
            or log_mixture_scale_prior is not None
            or log_mixture_weight_prior is not None
        ):
            logger.warning(""Priors not implemented for SpectralMixtureKernel"")

        super(SpectralMixtureKernel, self).__init__(active_dims=active_dims)
        self.register_parameter(name=""log_mixture_weights"", parameter=torch.nn.Parameter(torch.zeros(self.n_mixtures)))
        self.register_parameter(
            name=""log_mixture_means"", parameter=torch.nn.Parameter(torch.zeros(self.n_mixtures, self.n_dims))
        )
        self.register_parameter(
            name=""log_mixture_scales"", parameter=torch.nn.Parameter(torch.zeros(self.n_mixtures, self.n_dims))
        )","1. Use `torch.nn.Parameter` instead of `torch.nn.Parameter` to prevent unauthorized access to model parameters.
2. Use `torch.nn.init.zeros_()` to initialize model parameters to zeros.
3. Use `torch.nn.init.uniform_()` to initialize model parameters to random values."
"    def initialize_from_data(self, train_x, train_y, **kwargs):
        if not torch.is_tensor(train_x) or not torch.is_tensor(train_y):
            raise RuntimeError(""train_x and train_y should be tensors"")
        if train_x.ndimension() == 1:
            train_x = train_x.unsqueeze(-1)
        if train_x.ndimension() == 2:
            train_x = train_x.unsqueeze(0)

        train_x_sort = train_x.sort(1)[0]
        max_dist = train_x_sort[:, -1, :] - train_x_sort[:, 0, :]
        min_dist_sort = (train_x_sort[:, 1:, :] - train_x_sort[:, :-1, :]).squeeze(0)
        min_dist = torch.zeros(1, self.n_dims)
        for ind in range(self.n_dims):
            min_dist[:, ind] = min_dist_sort[(torch.nonzero(min_dist_sort[:, ind]))[0], ind]

        # Inverse of lengthscales should be drawn from truncated Gaussian | N(0, max_dist^2) |
        self.log_mixture_scales.data.normal_().mul_(max_dist).abs_().pow_(-1).log_()
        # Draw means from Unif(0, 0.5 / minimum distance between two points)
        self.log_mixture_means.data.uniform_().mul_(0.5).div_(min_dist).log_()
        # Mixture weights should be roughly the stdv of the y values divided by the number of mixtures
        self.log_mixture_weights.data.fill_(train_y.std() / self.n_mixtures).log_()","1. Use `torch.jit.script` to create a compiled version of the model. This will make it much harder for attackers to reverse engineer the model.
2. Use `torch.nn.functional.one_hot` to one-hot encode the labels instead of using a `torch.Tensor`. This will make it more difficult for attackers to craft adversarial examples.
3. Use `torch.nn.functional.relu` instead of `torch.nn.functional.tanh` for activation functions. This will make it more difficult for attackers to find inputs that cause the model to output incorrect predictions."
"    def forward(self, x1, x2):
        batch_size, n, n_dims = x1.size()
        _, m, _ = x2.size()
        if not n_dims == self.n_dims:
            raise RuntimeError(""The number of dimensions doesn't match what was supplied!"")

        mixture_weights = self.log_mixture_weights.view(self.n_mixtures, 1, 1, 1).exp()
        mixture_means = self.log_mixture_means.view(self.n_mixtures, 1, 1, 1, self.n_dims).exp()
        mixture_scales = self.log_mixture_scales.view(self.n_mixtures, 1, 1, 1, self.n_dims).exp()
        distance = (x1.unsqueeze(-2) - x2.unsqueeze(-3)).abs()  # distance = x^(i) - z^(i)

        exp_term = (distance * mixture_scales).pow_(2).mul_(-2 * math.pi ** 2)
        cos_term = (distance * mixture_means).mul_(2 * math.pi)
        res = exp_term.exp_() * cos_term.cos_()

        # Product over dimensions
        res = res.prod(-1)

        # Sum over mixtures
        res = (res * mixture_weights).sum(0)
        return res","1. Use `torch.jit.script` to make the model more efficient and secure.
2. Use `torch.autograd.gradcheck` to check the gradients of the model.
3. Use `torch.testing.assert_allclose` to check the output of the model."
"    def __call__(self, *args, **kwargs):
        train_inputs = tuple(Variable(train_input) for train_input in self.train_inputs)

        # Training mode: optimizing
        if self.training:
            if not all([torch.equal(train_input, input) for train_input, input in zip(train_inputs, args)]):
                raise RuntimeError('You must train on the training inputs!')
            return super(ExactGP, self).__call__(*args, **kwargs)

        # Posterior mode
        else:
            if all([torch.equal(train_input, input) for train_input, input in zip(train_inputs, args)]):
                logging.warning('The input matches the stored training data. '
                                'Did you forget to call model.train()?')

            # Exact inference
            n_train = train_inputs[0].size(0)
            full_inputs = tuple(torch.cat([train_input, input]) for train_input, input in zip(train_inputs, args))
            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)
            if not isinstance(full_output, GaussianRandomVariable):
                raise RuntimeError('ExactGP.forward must return a GaussianRandomVariable')
            full_mean, full_covar = full_output.representation()

            train_mean = full_mean[:n_train]
            test_mean = full_mean[n_train:]
            train_train_covar = gpytorch.add_diag(full_covar[:n_train, :n_train], self.likelihood.log_noise.exp())
            train_test_covar = full_covar[:n_train, n_train:]
            test_train_covar = full_covar[n_train:, :n_train]
            test_test_covar = full_covar[n_train:, n_train:]

            # Calculate alpha cache
            if not self.has_computed_alpha:
                train_residual = Variable(self.train_targets) - train_mean
                alpha = gpytorch.inv_matmul(train_train_covar, train_residual)
                if isinstance(full_covar, InterpolatedLazyVariable):
                    # We can get a better alpha cache with InterpolatedLazyVariables (Kiss-GP)
                    # This allows for constant time predictions
                    right_interp = InterpolatedLazyVariable(test_train_covar.base_lazy_variable,
                                                            left_interp_indices=None, left_interp_values=None,
                                                            right_interp_indices=test_train_covar.right_interp_indices,
                                                            right_interp_values=test_train_covar.right_interp_values)
                    alpha = right_interp.matmul(alpha)

                self.alpha = alpha
                self.has_computed_alpha = True

            # Calculate root inverse cache, if necessary
            # This enables fast predictive variances
            if not self.has_computed_root_inv and contexts.fast_pred_var.on():
                if not isinstance(train_train_covar, LazyVariable):
                    train_train_covar = NonLazyVariable(train_train_covar)
                root_inv = train_train_covar.root_inv_decomposition().root.evaluate()
                if isinstance(full_covar, InterpolatedLazyVariable):
                    # We can get a better root_inv cache with InterpolatedLazyVariables (Kiss-GP)
                    # This allows for constant time predictive variances
                    right_interp = InterpolatedLazyVariable(test_train_covar.base_lazy_variable,
                                                            left_interp_indices=None, left_interp_values=None,
                                                            right_interp_indices=test_train_covar.right_interp_indices,
                                                            right_interp_values=test_train_covar.right_interp_values)
                    root_inv = right_interp.matmul(root_inv)

                self.root_inv = root_inv
                self.has_computed_root_inv = True

            # Calculate mean
            if isinstance(full_covar, InterpolatedLazyVariable):
                # Constant time predictions with InterpolatedLazyVariables (Kiss-GP)
                left_interp_indices = test_train_covar.left_interp_indices
                left_interp_values = test_train_covar.left_interp_values
                predictive_mean = left_interp(left_interp_indices, left_interp_values, self.alpha) + test_mean
            else:
                # O(n) predictions with normal LazyVariables
                predictive_mean = test_train_covar.matmul(self.alpha) + test_mean

            # Calculate covar
            if contexts.fast_pred_var.on():
                # Compute low-rank approximation of covariance matrix - much faster!
                if not isinstance(test_test_covar, LazyVariable):
                    test_test_covar = NonLazyVariable(test_test_covar)

                if isinstance(full_covar, InterpolatedLazyVariable):
                    # Constant time predictive var with InterpolatedLazyVariables (Kiss-GP)
                    left_interp_indices = test_train_covar.left_interp_indices
                    left_interp_values = test_train_covar.left_interp_values
                    covar_correction_root = left_interp(left_interp_indices, left_interp_values, self.root_inv)
                    predictive_covar = test_test_covar + RootLazyVariable(covar_correction_root).mul(-1)
                else:
                    # O(n) predictions with normal LazyVariables
                    covar_correction_root = test_train_covar.matmul(self.root_inv)
                    covar_correction = RootLazyVariable(covar_correction_root).mul(-1)
                    predictive_covar = test_test_covar + covar_correction
            else:
                # Compute full covariance matrix - much slower
                if isinstance(train_test_covar, LazyVariable):
                    train_test_covar = train_test_covar.evaluate()
                if isinstance(test_train_covar, LazyVariable):
                    test_train_covar = train_test_covar.t()
                if not isinstance(test_test_covar, LazyVariable):
                    test_test_covar = NonLazyVariable(test_test_covar)
                covar_correction_rhs = gpytorch.inv_matmul(train_train_covar, train_test_covar).mul_(-1)
                predictive_covar = test_test_covar + MatmulLazyVariable(test_train_covar, covar_correction_rhs)
            return GaussianRandomVariable(predictive_mean, predictive_covar)","1. Use `torch.jit.script` to make the model's forward pass more efficient.
2. Use `torch.jit.trace` to generate a traced version of the model that can be used for inference.
3. Use `torch.jit.save` to save the traced model to a file so that it can be loaded and used later."
"    def handle(self, *args, **options):
        self.style = color_style()

        self.options = options
        if options[""requirements""]:
            req_files = options[""requirements""]
        elif os.path.exists(""requirements.txt""):
            req_files = [""requirements.txt""]
        elif os.path.exists(""requirements""):
            req_files = [
                ""requirements/{0}"".format(f) for f in os.listdir(""requirements"")
                if os.path.isfile(os.path.join(""requirements"", f)) and f.lower().endswith("".txt"")
            ]
        elif os.path.exists(""requirements-dev.txt""):
            req_files = [""requirements-dev.txt""]
        elif os.path.exists(""requirements-prod.txt""):
            req_files = [""requirements-prod.txt""]
        else:
            raise CommandError(""Requirements file(s) not found"")

        self.reqs = {}
        with PipSession() as session:
            for filename in req_files:
                for req in parse_requirements(filename, session=session):
                    # url attribute changed to link in pip version 6.1.0 and above
                    if LooseVersion(pip.__version__) > LooseVersion('6.0.8'):
                        self.reqs[req.name] = {
                            ""pip_req"": req,
                            ""url"": req.link,
                        }
                    else:
                        self.reqs[req.name] = {
                            ""pip_req"": req,
                            ""url"": req.url,
                        }

        if options[""github_api_token""]:
            self.github_api_token = options[""github_api_token""]
        elif os.environ.get(""GITHUB_API_TOKEN""):
            self.github_api_token = os.environ.get(""GITHUB_API_TOKEN"")
        else:
            self.github_api_token = None  # only 50 requests per hour

        self.check_pypi()
        if HAS_REQUESTS:
            self.check_github()
        else:
            print(self.style.ERROR(""Cannot check github urls. The requests library is not installed. ( pip install requests )""))
        self.check_other()","1. Use a secure cookie secret.
2. Use HTTPS.
3. Sanitize user input."
"    def check_pypi(self):
        """"""
        If the requirement is frozen to pypi, check for a new version.
        """"""
        for dist in get_installed_distributions():
            name = dist.project_name
            if name in self.reqs.keys():
                self.reqs[name][""dist""] = dist

        pypi = ServerProxy(""https://pypi.python.org/pypi"")
        for name, req in list(self.reqs.items()):
            if req[""url""]:
                continue  # skipping github packages.
            elif ""dist"" in req:
                dist = req[""dist""]
                dist_version = LooseVersion(dist.version)
                available = pypi.package_releases(req[""pip_req""].name, True) or pypi.package_releases(req[""pip_req""].name.replace('-', '_'), True)
                available_version = self._available_version(dist_version, available)

                if not available_version:
                    msg = self.style.WARN(""release is not on pypi (check capitalization and/or --extra-index-url)"")
                elif self.options['show_newer'] and dist_version > available_version:
                    msg = self.style.INFO(""{0} available (newer installed)"".format(available_version))
                elif available_version > dist_version:
                    msg = self.style.INFO(""{0} available"".format(available_version))
                else:
                    msg = ""up to date""
                    del self.reqs[name]
                    continue
                pkg_info = self.style.BOLD(""{dist.project_name} {dist.version}"".format(dist=dist))
            else:
                msg = ""not installed""
                pkg_info = name
            print(""{pkg_info:40} {msg}"".format(pkg_info=pkg_info, msg=msg))
            del self.reqs[name]","1. Use `requests` library instead of `urllib2`.
2. Use `verify` parameter to verify the SSL certificate of the server.
3. Use `timeout` parameter to set a timeout for the request."
"    def check_github(self):
        """"""
        If the requirement is frozen to a github url, check for new commits.

        API Tokens
        ----------
        For more than 50 github api calls per hour, pipchecker requires
        authentication with the github api by settings the environemnt
        variable ``GITHUB_API_TOKEN`` or setting the command flag
        --github-api-token='mytoken'``.

        To create a github api token for use at the command line::
             curl -u 'rizumu' -d '{""scopes"":[""repo""], ""note"":""pipchecker""}' https://api.github.com/authorizations

        For more info on github api tokens:
            https://help.github.com/articles/creating-an-oauth-token-for-command-line-use
            http://developer.github.com/v3/oauth/#oauth-authorizations-api

        Requirement Format
        ------------------
        Pipchecker gets the sha of frozen repo and checks if it is
        found at the head of any branches. If it is not found then
        the requirement is considered to be out of date.

        Therefore, freezing at the commit hash will provide the expected
        results, but if freezing at a branch or tag name, pipchecker will
        not be able to determine with certainty if the repo is out of date.

        Freeze at the commit hash (sha)::
            git+git://github.com/django/django.git@393c268e725f5b229ecb554f3fac02cfc250d2df#egg=Django
            https://github.com/django/django/archive/393c268e725f5b229ecb554f3fac02cfc250d2df.tar.gz#egg=Django
            https://github.com/django/django/archive/393c268e725f5b229ecb554f3fac02cfc250d2df.zip#egg=Django

        Freeze with a branch name::
            git+git://github.com/django/django.git@master#egg=Django
            https://github.com/django/django/archive/master.tar.gz#egg=Django
            https://github.com/django/django/archive/master.zip#egg=Django

        Freeze with a tag::
            git+git://github.com/django/django.git@1.5b2#egg=Django
            https://github.com/django/django/archive/1.5b2.tar.gz#egg=Django
            https://github.com/django/django/archive/1.5b2.zip#egg=Django

        Do not freeze::
            git+git://github.com/django/django.git#egg=Django

        """"""
        for name, req in list(self.reqs.items()):
            req_url = req[""url""]
            if not req_url:
                continue
            req_url = str(req_url)
            if req_url.startswith(""git"") and ""github.com/"" not in req_url:
                continue
            if req_url.endswith(("".tar.gz"", "".tar.bz2"", "".zip"")):
                continue

            headers = {
                ""content-type"": ""application/json"",
            }
            if self.github_api_token:
                headers[""Authorization""] = ""token {0}"".format(self.github_api_token)
            try:
                path_parts = urlparse(req_url).path.split(""#"", 1)[0].strip(""/"").rstrip(""/"").split(""/"")

                if len(path_parts) == 2:
                    user, repo = path_parts

                elif 'archive' in path_parts:
                    # Supports URL of format:
                    # https://github.com/django/django/archive/master.tar.gz#egg=Django
                    # https://github.com/django/django/archive/master.zip#egg=Django
                    user, repo = path_parts[:2]
                    repo += '@' + path_parts[-1].replace('.tar.gz', '').replace('.zip', '')

                else:
                    self.style.ERROR(""\\nFailed to parse %r\\n"" % (req_url, ))
                    continue
            except (ValueError, IndexError) as e:
                print(self.style.ERROR(""\\nFailed to parse %r: %s\\n"" % (req_url, e)))
                continue

            try:
                test_auth = requests.get(""https://api.github.com/django/"", headers=headers).json()
            except HTTPError as e:
                print(""\\n%s\\n"" % str(e))
                return

            if ""message"" in test_auth and test_auth[""message""] == ""Bad credentials"":
                print(self.style.ERROR(""\\nGithub API: Bad credentials. Aborting!\\n""))
                return
            elif ""message"" in test_auth and test_auth[""message""].startswith(""API Rate Limit Exceeded""):
                print(self.style.ERROR(""\\nGithub API: Rate Limit Exceeded. Aborting!\\n""))
                return

            frozen_commit_sha = None
            if "".git"" in repo:
                repo_name, frozen_commit_full = repo.split("".git"")
                if frozen_commit_full.startswith(""@""):
                    frozen_commit_sha = frozen_commit_full[1:]
            elif ""@"" in repo:
                repo_name, frozen_commit_sha = repo.split(""@"")

            if frozen_commit_sha is None:
                msg = self.style.ERROR(""repo is not frozen"")

            if frozen_commit_sha:
                branch_url = ""https://api.github.com/repos/{0}/{1}/branches"".format(user, repo_name)
                branch_data = requests.get(branch_url, headers=headers).json()

                frozen_commit_url = ""https://api.github.com/repos/{0}/{1}/commits/{2}"".format(
                    user, repo_name, frozen_commit_sha
                )
                frozen_commit_data = requests.get(frozen_commit_url, headers=headers).json()

                if ""message"" in frozen_commit_data and frozen_commit_data[""message""] == ""Not Found"":
                    msg = self.style.ERROR(""{0} not found in {1}. Repo may be private."".format(frozen_commit_sha[:10], name))
                elif frozen_commit_data[""sha""] in [branch[""commit""][""sha""] for branch in branch_data]:
                    msg = self.style.BOLD(""up to date"")
                else:
                    msg = self.style.INFO(""{0} is not the head of any branch"".format(frozen_commit_data[""sha""][:10]))

            if ""dist"" in req:
                pkg_info = ""{dist.project_name} {dist.version}"".format(dist=req[""dist""])
            elif frozen_commit_sha is None:
                pkg_info = name
            else:
                pkg_info = ""{0} {1}"".format(name, frozen_commit_sha[:10])
            print(""{pkg_info:40} {msg}"".format(pkg_info=pkg_info, msg=msg))
            del self.reqs[name]","1. Use `requests.get()` with `verify=False` to avoid verifying the SSL certificate of the remote server. This is necessary when the remote server uses a self-signed certificate.
2. Use `requests.post()` with `json=data` to send data in JSON format. This is more efficient than using `data=urllib.urlencode(data)`.
3. Use `requests.head()` to check if the remote server is reachable without actually downloading the content. This can save bandwidth and time."
"    def check_other(self):
        """"""
        If the requirement is frozen somewhere other than pypi or github, skip.

        If you have a private pypi or use --extra-index-url, consider contributing
        support here.
        """"""
        if self.reqs:
            print(self.style.ERROR(""\\nOnly pypi and github based requirements are supported:""))
            for name, req in self.reqs.items():
                if ""dist"" in req:
                    pkg_info = ""{dist.project_name} {dist.version}"".format(dist=req[""dist""])
                elif ""url"" in req:
                    pkg_info = ""{url}"".format(url=req[""url""])
                else:
                    pkg_info = ""unknown package""
                print(self.style.BOLD(""{pkg_info:40} is not a pypi or github requirement"".format(pkg_info=pkg_info)))","1. Use `requests` library instead of `urllib2` to avoid insecure connections.
2. Use `verify=False` when using `requests` to disable SSL verification.
3. Use `allow_redirects=False` when using `requests` to prevent following redirects to malicious sites."
"def create_app(config):
    mode = config.MODE

    if mode & App.GuiMode:

        from quamash import QEventLoop
        from PyQt5.QtGui import QIcon, QPixmap
        from PyQt5.QtWidgets import QApplication, QWidget

        q_app = QApplication(sys.argv)
        q_app.setQuitOnLastWindowClosed(True)
        q_app.setApplicationName('FeelUOwn')

        app_event_loop = QEventLoop(q_app)
        asyncio.set_event_loop(app_event_loop)

        class GuiApp(QWidget):
            mode = App.GuiMode

            def __init__(self):
                super().__init__()
                self.setObjectName('app')
                QApplication.setWindowIcon(QIcon(QPixmap(APP_ICON)))

            def closeEvent(self, e):
                self.ui.mpv_widget.close()
                event_loop = asyncio.get_event_loop()
                event_loop.stop()

        class FApp(App, GuiApp):
            def __init__(self, config):
                App.__init__(self, config)
                GuiApp.__init__(self)

    else:
        FApp = App

    Signal.setup_aio_support()
    Resolver.setup_aio_support()
    app = FApp(config)
    attach_attrs(app)
    Resolver.library = app.library
    return app","1. Use `asyncio.run()` instead of `asyncio.set_event_loop()` to start the event loop.
2. Use `asyncio.ensure_future()` to schedule tasks instead of creating them directly.
3. Use `asyncio.wait()` to wait for multiple tasks to complete."
"    def play_mv_by_mvid(cls, mvid):
        mv_model = ControllerApi.api.get_mv_detail(mvid)
        if not ControllerApi.api.is_response_ok(mv_model):
            return

        url_high = mv_model['url_high']
        clipboard = QApplication.clipboard()
        clipboard.setText(url_high)

        if platform.system() == ""Linux"":
            ControllerApi.player.pause()
            ControllerApi.notify_widget.show_message(""通知"", ""正在尝试调用VLC视频播放器播放MV"")
            subprocess.Popen(['vlc', url_high, '--play-and-exit', '-f'])
        elif platform.system().lower() == 'Darwin'.lower():
            ControllerApi.player.pause()
            cls.view.ui.STATUS_BAR.showMessage(u""准备调用 QuickTime Player 播放mv"", 4000)
            subprocess.Popen(['open', '-a', 'QuickTime Player', url_high])
        else:
            cls.view.ui.STATUS_BAR.showMessage(u""程序已经将视频的播放地址复制到剪切板"", 5000)","1. Use `verify=False` when calling `subprocess.Popen` to prevent code injection.
2. Use `requests.get()` with `verify=False` to prevent SSL certificate errors.
3. Use `os.path.expanduser()` to expand user-specific paths to prevent directory traversal attacks."
"def check_pids(curmir_incs):
    """"""Check PIDs in curmir markers to make sure rdiff-backup not running""""""
    pid_re = re.compile(r""^PID\\s*([0-9]+)"", re.I | re.M)

    def extract_pid(curmir_rp):
        """"""Return process ID from a current mirror marker, if any""""""
        match = pid_re.search(curmir_rp.get_string())
        if not match:
            return None
        else:
            return int(match.group(1))

    def pid_running(pid):
        """"""Return True if we know if process with pid is currently running,
        False if it isn't running, and None if we don't know for sure.""""""
        if os.name == 'nt':
            import win32api
            import win32con
            import pywintypes
            process = None
            try:
                process = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, 0,
                                               pid)
            except pywintypes.error as error:
                if error[0] == 87:
                    return False
                else:
                    msg = ""Warning: unable to check if PID %d still running""
                    log.Log(msg % pid, 2)
                    return None  # we don't know if the process is running
            else:
                if process:
                    win32api.CloseHandle(process)
                    return True
                else:
                    return False
        else:
            try:
                os.kill(pid, 0)
            except ProcessLookupError:  # errno.ESRCH - pid doesn't exist
                return False
            except OSError:  # any other OS error
                log.Log(
                    ""Warning: unable to check if PID %d still running"" % (pid, ),
                    2)
                return None  # we don't know if the process is still running
            else:  # the process still exists
                return True

    for curmir_rp in curmir_incs:
        assert curmir_rp.conn is Globals.local_connection, (
            ""Function must be called locally not over '{conn}'."".format(
                conn=curmir_rp.conn))
        pid = extract_pid(curmir_rp)
        # FIXME differentiate between don't know and know and handle err.errno == errno.EPERM:
        # EPERM clearly means there's a process to deny access to with OSError
        if pid is not None and pid_running(pid):
            log.Log.FatalError(
                """"""It appears that a previous rdiff-backup session with process
id %d is still running.  If two different rdiff-backup processes write
the same repository simultaneously, data corruption will probably
result.  To proceed with regress anyway, rerun rdiff-backup with the
--force option."""""" % (pid, ))","1. Use the `os.kill()` function to check if the process is still running.
2. Use the `errno.ESRCH` and `OSError` exceptions to handle errors.
3. Use the `log.Log.FatalError()` function to log fatal errors."
"    def pid_running(pid):
        """"""Return True if we know if process with pid is currently running,
        False if it isn't running, and None if we don't know for sure.""""""
        if os.name == 'nt':
            import win32api
            import win32con
            import pywintypes
            process = None
            try:
                process = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, 0,
                                               pid)
            except pywintypes.error as error:
                if error[0] == 87:
                    return False
                else:
                    msg = ""Warning: unable to check if PID %d still running""
                    log.Log(msg % pid, 2)
                    return None  # we don't know if the process is running
            else:
                if process:
                    win32api.CloseHandle(process)
                    return True
                else:
                    return False
        else:
            try:
                os.kill(pid, 0)
            except ProcessLookupError:  # errno.ESRCH - pid doesn't exist
                return False
            except OSError:  # any other OS error
                log.Log(
                    ""Warning: unable to check if PID %d still running"" % (pid, ),
                    2)
                return None  # we don't know if the process is still running
            else:  # the process still exists
                return True","1. Use `os.getpid()` instead of `pid` to get the current process ID. This will prevent an attacker from passing in a fake PID and tricking the function into thinking that a process is running when it is not.
2. Use `os.close()` to close the handle to the process after checking if it is running. This will prevent an attacker from using the handle to access the process's memory or other resources.
3. Use `logging.exception()` to log any errors that occur. This will help you track down and fix any security vulnerabilities that may be present in the code."
"    def set_case_sensitive_readwrite(self, subdir):
        """"""Determine if directory at rp is case sensitive by writing""""""
        assert not self.read_only
        upper_a = subdir.append(""A"")
        upper_a.touch()
        lower_a = subdir.append(""a"")
        if lower_a.lstat():
            lower_a.delete()
            upper_a.setdata()
            assert not upper_a.lstat()
            self.case_sensitive = 0
        else:
            upper_a.delete()
            self.case_sensitive = 1","1. Use `os.access` to check if the directory is readable and writable before writing to it.
2. Use `os.path.isdir` to check if the directory exists before creating it.
3. Use `os.chmod` to set the permissions of the directory to `0777` (read, write, and execute for all users)."
"    def log_to_file(self, message):
        """"""Write the message to the log file, if possible""""""
        if self.log_file_open:
            if self.log_file_local:
                tmpstr = self.format(message, self.verbosity)
                if type(tmpstr) == str:  # transform string in bytes
                    tmpstr = tmpstr.encode('utf-8', 'backslashreplace')
                self.logfp.write(tmpstr)
                self.logfp.flush()
            else:
                self.log_file_conn.log.Log.log_to_file(message)","1. Use `logging.Logger` instead of custom logging class.
2. Use `logging.FileHandler` to write log to file.
3. Use `logging.Formatter` to format log message."
"    def log_to_term(self, message, verbosity):
        """"""Write message to stdout/stderr""""""
        if verbosity <= 2 or Globals.server:
            termfp = sys.stderr.buffer
        else:
            termfp = sys.stdout.buffer
        tmpstr = self.format(message, self.term_verbosity)
        if type(tmpstr) == str:  # transform string in bytes
            tmpstr = tmpstr.encode(sys.stdout.encoding, 'backslashreplace')
        termfp.write(tmpstr)","1. Use `sys.stderr` instead of `sys.stdout` to log errors.
2. Use `logging` module to log messages instead of writing to files directly.
3. Use `format_exception` to format exception messages."
"    def open(cls, time_string, compress=1):
        """"""Open the error log, prepare for writing""""""
        if not Globals.isbackup_writer:
            return Globals.backup_writer.log.ErrorLog.open(
                time_string, compress)
        assert not cls._log_fileobj, ""log already open""
        assert Globals.isbackup_writer

        base_rp = Globals.rbdir.append(""error_log.%s.data"" % (time_string, ))
        if compress:
            cls._log_fileobj = rpath.MaybeGzip(base_rp)
        else:
            cls._log_fileobj = base_rp.open(""wb"", compress=0)","1. Use `open()` with `mode='wb'` instead of `open()` with `mode='w'` to ensure that the file is opened in binary mode.
2. Use `with` statement to ensure that the file is closed after use.
3. Use `os.fchmod()` to set the file mode to `0600` to restrict access to the file."
"    def write(cls, error_type, rp, exc):
        """"""Add line to log file indicating error exc with file rp""""""
        if not Globals.isbackup_writer:
            return Globals.backup_writer.log.ErrorLog.write(
                error_type, rp, exc)
        logstr = cls.get_log_string(error_type, rp, exc)
        Log(logstr, 2)
        if isinstance(logstr, bytes):
            logstr = logstr.decode('utf-8')
        if Globals.null_separator:
            logstr += ""\\0""
        else:
            logstr = re.sub(""\\n"", "" "", logstr)
            logstr += ""\\n""
        cls._log_fileobj.write(logstr)","1. Use `logging` instead of `print` to log errors.
2. Sanitize user input before using it in the code.
3. Handle errors gracefully and prevent them from crashing the application."
"def parse_file_desc(file_desc):
    """"""Parse file description returning pair (host_info, filename)

    In other words, bescoto@folly.stanford.edu::/usr/bin/ls =>
    (""bescoto@folly.stanford.edu"", ""/usr/bin/ls"").  The
    complication is to allow for quoting of : by a \\\\.  If the
    string is not separated by :, then the host_info is None.

    """"""

    def check_len(i):
        if i >= len(file_desc):
            raise SetConnectionsException(
                ""Unexpected end to file description %s"" % file_desc)

    host_info_list, i, last_was_quoted = [], 0, None
    file_desc = os.fsencode(
        file_desc)  # paths and similar must always be bytes
    while 1:
        if i == len(file_desc):
            # make sure paths under Windows use / instead of \\
            if os.path.altsep:  # only Windows has an alternative separator for paths
                file_desc = file_desc.replace(os.fsencode(os.path.sep), b'/')
            return (None, file_desc)

        if file_desc[i] == ord(
                '\\\\'):  # byte[n] is the numerical value hence ord
            i = i + 1
            check_len(i)
            last_was_quoted = 1
        elif (file_desc[i] == ord("":"") and i > 0
              and file_desc[i - 1] == ord("":"") and not last_was_quoted):
            host_info_list.pop()  # Remove last colon from name
            break
        else:
            last_was_quoted = None
        host_info_list.append(file_desc[i:i + 1])
        i = i + 1

    check_len(i + 1)

    filename = file_desc[i + 1:]
    # make sure paths under Windows use / instead of \\
    if os.path.altsep:  # only Windows has an alternative separator for paths
        filename = filename.replace(os.fsencode(os.path.sep), b'/')

    return (b"""".join(host_info_list), filename)","1. Use `os.path.normpath()` to normalize the path before parsing it. This will help to prevent directory traversal attacks.
2. Use `os.path.basename()` to get the filename from the path. This will help to prevent users from accessing files outside of the intended directory.
3. Use `os.path.isfile()` to check if the file exists before trying to open it. This will help to prevent users from accessing non-existent files."
"def RORP2Record(rorpath):
    """"""From RORPath, return text record of file's metadata""""""
    str_list = [b""File %s\\n"" % quote_path(rorpath.get_indexpath())]

    # Store file type, e.g. ""dev"", ""reg"", or ""sym"", and type-specific data
    type = rorpath.gettype()
    if type is None:
        type = ""None""
    str_list.append(b""  Type %b\\n"" % type.encode('ascii'))
    if type == ""reg"":
        str_list.append(b""  Size %i\\n"" % rorpath.getsize())

        # If there is a resource fork, save it.
        if rorpath.has_resource_fork():
            if not rorpath.get_resource_fork():
                rf = b""None""
            else:
                rf = binascii.hexlify(rorpath.get_resource_fork())
            str_list.append(b""  ResourceFork %b\\n"" % (rf, ))

        # If there is Carbon data, save it.
        if rorpath.has_carbonfile():
            cfile = carbonfile2string(rorpath.get_carbonfile())
            str_list.append(b""  CarbonFile %b\\n"" % (cfile, ))

        # If file is hardlinked, add that information
        if Globals.preserve_hardlinks != 0:
            numlinks = rorpath.getnumlinks()
            if numlinks > 1:
                str_list.append(b""  NumHardLinks %i\\n"" % numlinks)
                str_list.append(b""  Inode %i\\n"" % rorpath.getinode())
                str_list.append(b""  DeviceLoc %i\\n"" % rorpath.getdevloc())

        # Save any hashes, if available
        if rorpath.has_sha1():
            str_list.append(
                b'  SHA1Digest %b\\n' % rorpath.get_sha1().encode('ascii'))

    elif type == ""None"":
        return b"""".join(str_list)
    elif type == ""dir"" or type == ""sock"" or type == ""fifo"":
        pass
    elif type == ""sym"":
        str_list.append(b""  SymData %b\\n"" % quote_path(rorpath.readlink()))
    elif type == ""dev"":
        major, minor = rorpath.getdevnums()
        if rorpath.isblkdev():
            devchar = ""b""
        else:
            assert rorpath.ischardev()
            devchar = ""c""
        str_list.append(
            b""  DeviceNum %b %i %i\\n"" % (devchar.encode(), major, minor))

    # Store time information
    if type != 'sym' and type != 'dev':
        str_list.append(b""  ModTime %i\\n"" % rorpath.getmtime())

    # Add user, group, and permission information
    uid, gid = rorpath.getuidgid()
    str_list.append(b""  Uid %i\\n"" % uid)
    str_list.append(b""  Uname %b\\n"" % (rorpath.getuname() or "":"").encode())
    str_list.append(b""  Gid %i\\n"" % gid)
    str_list.append(b""  Gname %b\\n"" % (rorpath.getgname() or "":"").encode())
    str_list.append(b""  Permissions %d\\n"" % rorpath.getperms())

    # Add long filename information
    if rorpath.has_alt_mirror_name():
        str_list.append(
            b""  AlternateMirrorName %b\\n"" % (rorpath.get_alt_mirror_name(), ))
    elif rorpath.has_alt_inc_name():
        str_list.append(
            b""  AlternateIncrementName %b\\n"" % (rorpath.get_alt_inc_name(), ))

    return b"""".join(str_list)","1. Use `binascii.hexlify()` to encode binary data instead of `quote_path()`.
2. Use `rorpath.get_resource_fork()` instead of `rorpath.get_resource_fork()` to get the resource fork.
3. Use `rorpath.get_alt_mirror_name()` instead of `rorpath.get_alt_inc_name()` to get the alternate mirror name."
"def copy(rpin, rpout, compress=0):
    """"""Copy RPath rpin to rpout.  Works for symlinks, dirs, etc.

    Returns close value of input for regular file, which can be used
    to pass hashes on.

    """"""
    log.Log(""Regular copying %s to %s"" % (rpin.index, rpout.get_safepath()), 6)
    if not rpin.lstat():
        if rpout.lstat():
            rpout.delete()
        return

    if rpout.lstat():
        if rpin.isreg() or not cmp(rpin, rpout):
            rpout.delete()  # easier to write than compare
        else:
            return

    if rpin.isreg():
        return copy_reg_file(rpin, rpout, compress)
    elif rpin.isdir():
        rpout.mkdir()
    elif rpin.issym():
        # some systems support permissions for symlinks, but
        # only by setting at creation via the umask
        if Globals.symlink_perms:
            orig_umask = os.umask(0o777 & ~rpin.getperms())
        rpout.symlink(rpin.readlink())
        if Globals.symlink_perms:
            os.umask(orig_umask)  # restore previous umask
    elif rpin.ischardev():
        major, minor = rpin.getdevnums()
        rpout.makedev(""c"", major, minor)
    elif rpin.isblkdev():
        major, minor = rpin.getdevnums()
        rpout.makedev(""b"", major, minor)
    elif rpin.isfifo():
        rpout.mkfifo()
    elif rpin.issock():
        rpout.mksock()
    else:
        raise RPathException(""File '%s' has unknown type."" % rpin.get_safepath())","1. Use `os.fchmod` to set the file permissions instead of `os.umask`. This will ensure that the permissions are set correctly even if the user's umask is different.
2. Use `os.fchown` to set the file owner and group instead of relying on the default owner and group of the parent directory. This will ensure that the file is owned by the correct user and group.
3. Use `os.fsync` to flush the file buffers to disk before closing the file. This will ensure that the file is written to disk even if the system crashes."
"def cmp(rpin, rpout):
    """"""True if rpin has the same data as rpout

    cmp does not compare file ownership, permissions, or times, or
    examine the contents of a directory.

    """"""
    check_for_files(rpin, rpout)
    if rpin.isreg():
        if not rpout.isreg():
            return None
        fp1, fp2 = rpin.open(""rb""), rpout.open(""rb"")
        result = cmpfileobj(fp1, fp2)
        if fp1.close() or fp2.close():
            raise RPathException(""Error closing file"")
        return result
    elif rpin.isdir():
        return rpout.isdir()
    elif rpin.issym():
        return rpout.issym() and (rpin.readlink() == rpout.readlink())
    elif rpin.ischardev():
        return rpout.ischardev() and (rpin.getdevnums() == rpout.getdevnums())
    elif rpin.isblkdev():
        return rpout.isblkdev() and (rpin.getdevnums() == rpout.getdevnums())
    elif rpin.isfifo():
        return rpout.isfifo()
    elif rpin.issock():
        return rpout.issock()
    else:
        raise RPathException(""File %s has unknown type"" % rpin.get_safepath())","1. Use `os.fchmod` to set the file mode of the output file to the same as the input file. This will prevent the output file from being overwritten with a file with different permissions.
2. Use `os.fchown` to set the file owner and group of the output file to the same as the input file. This will prevent the output file from being owned by a different user or group.
3. Use `os.fsync` to flush the output file to disk. This will ensure that the output file is written to disk before the program exits."
"def make_file_dict(filename):
    """"""Generate the data dictionary for the given RPath

    This is a global function so that os.name can be called locally,
    thus avoiding network lag and so that we only need to send the
    filename over the network, thus avoiding the need to pickle an
    (incomplete) rpath object.
    """"""

    def _readlink(filename):
        """"""FIXME wrapper function to workaround a bug in os.readlink on Windows
        not accepting bytes path. This function can be removed once pyinstaller
        supports Python 3.8 and a new release can be made.
        See https://github.com/pyinstaller/pyinstaller/issues/4311
        """"""

        if os.name == 'nt' and not isinstance(filename, str):
            # we assume a bytes representation
            return os.fsencode(os.readlink(os.fsdecode(filename)))
        else:
            return os.readlink(filename)

    try:
        statblock = os.lstat(filename)
    except (FileNotFoundError, NotADirectoryError):
        # FIXME not sure if this shouldn't trigger a warning but doing it
        # generates (too) many messages during the tests
        # log.Log(""Warning: missing file '%s' couldn't be assessed."" % filename, 2)
        return {'type': None}
    data = {}
    mode = statblock[stat.ST_MODE]

    if stat.S_ISREG(mode):
        type_ = 'reg'
    elif stat.S_ISDIR(mode):
        type_ = 'dir'
    elif stat.S_ISCHR(mode):
        type_ = 'dev'
        s = statblock.st_rdev
        data['devnums'] = ('c', ) + (s >> 8, s & 0xff)
    elif stat.S_ISBLK(mode):
        type_ = 'dev'
        s = statblock.st_rdev
        data['devnums'] = ('b', ) + (s >> 8, s & 0xff)
    elif stat.S_ISFIFO(mode):
        type_ = 'fifo'
    elif stat.S_ISLNK(mode):
        type_ = 'sym'
        # FIXME reverse once Python 3.8 can be used under Windows
        # data['linkname'] = os.readlink(filename)
        data['linkname'] = _readlink(filename)
    elif stat.S_ISSOCK(mode):
        type_ = 'sock'
    else:
        raise C.UnknownFileError(filename)
    data['type'] = type_
    data['size'] = statblock[stat.ST_SIZE]
    data['perms'] = stat.S_IMODE(mode)
    data['uid'] = statblock[stat.ST_UID]
    data['gid'] = statblock[stat.ST_GID]
    data['inode'] = statblock[stat.ST_INO]
    data['devloc'] = statblock[stat.ST_DEV]
    data['nlink'] = statblock[stat.ST_NLINK]

    if os.name == 'nt':
        try:
            attribs = win32api.GetFileAttributes(os.fsdecode(filename))
        except pywintypes.error as exc:
            if (exc.args[0] == 32):  # file in use
                # we could also ignore with: return {'type': None}
                # but this approach seems to be better handled
                attribs = 0
            else:
                # we replace the specific Windows exception by a generic
                # one also understood by a potential Linux client/server
                raise OSError(None, exc.args[1] + "" - "" + exc.args[2],
                              filename, exc.args[0]) from None
        if attribs & win32con.FILE_ATTRIBUTE_REPARSE_POINT:
            data['type'] = 'sym'
            data['linkname'] = None

    if not (type_ == 'sym' or type_ == 'dev'):
        # mtimes on symlinks and dev files don't work consistently
        data['mtime'] = int(statblock[stat.ST_MTIME])
        data['atime'] = int(statblock[stat.ST_ATIME])
        data['ctime'] = int(statblock[stat.ST_CTIME])
    return data","1. Use `os.fsdecode()` to decode bytes path to str path on Windows.
2. Handle `FileNotFoundError` and `NotADirectoryError` exceptions.
3. Use `raise OSError()` to replace specific Windows exceptions with generic ones."
"    def getdevnums(self):
        """"""Return a devices major/minor numbers from dictionary""""""
        return self.data['devnums'][1:]","1. **Use proper escaping** to prevent injection attacks.
2. **Validate user input** to prevent malicious users from entering data that could compromise the system.
3. **Use strong passwords** for all accounts, and **rotate passwords** regularly."
"def make_file_dict(filename):
    """"""Generate the data dictionary for the given RPath

    This is a global function so that os.name can be called locally,
    thus avoiding network lag and so that we only need to send the
    filename over the network, thus avoiding the need to pickle an
    (incomplete) rpath object.
    """"""

    def _readlink(filename):
        """"""FIXME wrapper function to workaround a bug in os.readlink on Windows
        not accepting bytes path. This function can be removed once pyinstaller
        supports Python 3.8 and a new release can be made.
        See https://github.com/pyinstaller/pyinstaller/issues/4311
        """"""

        if os.name == 'nt' and not isinstance(filename, str):
            # we assume a bytes representation
            return os.fsencode(os.readlink(os.fsdecode(filename)))
        else:
            return os.readlink(filename)

    try:
        statblock = os.lstat(filename)
    except (FileNotFoundError, NotADirectoryError):
        return {'type': None}
    data = {}
    mode = statblock[stat.ST_MODE]

    if stat.S_ISREG(mode):
        type_ = 'reg'
    elif stat.S_ISDIR(mode):
        type_ = 'dir'
    elif stat.S_ISCHR(mode):
        type_ = 'dev'
        s = statblock.st_rdev
        data['devnums'] = ('c', ) + (s >> 8, s & 0xff)
    elif stat.S_ISBLK(mode):
        type_ = 'dev'
        s = statblock.st_rdev
        data['devnums'] = ('b', ) + (s >> 8, s & 0xff)
    elif stat.S_ISFIFO(mode):
        type_ = 'fifo'
    elif stat.S_ISLNK(mode):
        type_ = 'sym'
        # FIXME reverse once Python 3.8 can be used under Windows
        # data['linkname'] = os.readlink(filename)
        data['linkname'] = _readlink(filename)
    elif stat.S_ISSOCK(mode):
        type_ = 'sock'
    else:
        raise C.UnknownFileError(filename)
    data['type'] = type_
    data['size'] = statblock[stat.ST_SIZE]
    data['perms'] = stat.S_IMODE(mode)
    data['uid'] = statblock[stat.ST_UID]
    data['gid'] = statblock[stat.ST_GID]
    data['inode'] = statblock[stat.ST_INO]
    data['devloc'] = statblock[stat.ST_DEV]
    data['nlink'] = statblock[stat.ST_NLINK]

    if os.name == 'nt':
        attribs = win32api.GetFileAttributes(os.fsdecode(filename))
        if attribs & win32con.FILE_ATTRIBUTE_REPARSE_POINT:
            data['type'] = 'sym'
            data['linkname'] = None

    if not (type_ == 'sym' or type_ == 'dev'):
        # mtimes on symlinks and dev files don't work consistently
        data['mtime'] = int(statblock[stat.ST_MTIME])
        data['atime'] = int(statblock[stat.ST_ATIME])
        data['ctime'] = int(statblock[stat.ST_CTIME])
    return data","1. Use `os.fsdecode()` to decode the filename to a string before passing it to `os.readlink()`.
2. Use `os.fsencode()` to encode the filename to a bytes object before passing it to `os.lstat()`.
3. Check the return value of `os.lstat()` to make sure that the file exists and is accessible."
"    def open_logfile_local(self, rpath):
        """"""Open logfile locally - should only be run on one connection""""""
        assert rpath.conn is Globals.local_connection
        try:
            self.logfp = rpath.open(""a"")
        except (OSError, IOError) as e:
            raise LoggerError(
                ""Unable to open logfile %s: %s"" % (rpath.path, e))
        self.log_file_local = 1
        self.logrp = rpath","1. Use `os.fchmod` to set the file mode to 0600 to restrict access to the log file.
2. Use `os.umask` to set the umask to 077 to further restrict access to the log file.
3. Use `logging.secure` to enable additional security features for the logging module."
"    def log_to_file(self, message):
        """"""Write the message to the log file, if possible""""""
        if self.log_file_open:
            if self.log_file_local:
                tmpstr = self.format(message, self.verbosity)
                if type(tmpstr) != str:  # transform bytes into string
                    tmpstr = str(tmpstr, 'utf-8')
                self.logfp.write(tmpstr)
                self.logfp.flush()
            else:
                self.log_file_conn.log.Log.log_to_file(message)","1. Use `contextlib.closing` to ensure the file is closed after use.
2. Use `logging.Formatter` to format the log message.
3. Use `logging.FileHandler` to write the log message to a file."
"    def log_to_term(self, message, verbosity):
        """"""Write message to stdout/stderr""""""
        if verbosity <= 2 or Globals.server:
            termfp = sys.stderr
        else:
            termfp = sys.stdout
        tmpstr = self.format(message, self.term_verbosity)
        if type(tmpstr) != str:  # transform bytes in string
            tmpstr = str(tmpstr, 'utf-8')
        termfp.write(tmpstr)","1. **Use `logging` instead of `sys.stderr` and `sys.stdout` to log messages.** This will make it easier to control the logging level and format, and will also prevent sensitive information from being logged to the console.
2. **Use a secure logging library, such as `secure-logging`.** This will provide additional features such as message encryption and rotation, which can help to protect your logs from being compromised.
3. **Configure your logging system correctly.** This includes setting the appropriate logging level, format, and destination. You should also make sure that your logs are rotated regularly and that they are stored in a secure location."
"def parse_cmdlineoptions(arglist):  # noqa: C901
    """"""Parse argument list and set global preferences""""""
    global args, action, create_full_path, force, restore_timestr, remote_cmd
    global remote_schema, remove_older_than_string
    global user_mapping_filename, group_mapping_filename, \\
        preserve_numerical_ids

    def sel_fl(filename):
        """"""Helper function for including/excluding filelists below""""""
        try:
            return open(filename, ""rb"")  # files match paths hence bytes/bin
        except IOError:
            Log.FatalError(""Error opening file %s"" % filename)

    def normalize_path(path):
        """"""Used below to normalize the security paths before setting""""""
        return rpath.RPath(Globals.local_connection, path).normalize().path

    try:
        optlist, args = getopt.getopt(arglist, ""blr:sv:V"", [
            ""allow-duplicate-timestamps"",
            ""backup-mode"", ""calculate-average"", ""carbonfile"",
            ""check-destination-dir"", ""compare"", ""compare-at-time="",
            ""compare-hash"", ""compare-hash-at-time="", ""compare-full"",
            ""compare-full-at-time="", ""create-full-path"", ""current-time="",
            ""exclude="", ""exclude-device-files"", ""exclude-fifos"",
            ""exclude-filelist="", ""exclude-symbolic-links"", ""exclude-sockets"",
            ""exclude-filelist-stdin"", ""exclude-globbing-filelist="",
            ""exclude-globbing-filelist-stdin"", ""exclude-mirror="",
            ""exclude-other-filesystems"", ""exclude-regexp="",
            ""exclude-if-present="", ""exclude-special-files"", ""force"",
            ""group-mapping-file="", ""include="", ""include-filelist="",
            ""include-filelist-stdin"", ""include-globbing-filelist="",
            ""include-globbing-filelist-stdin"", ""include-regexp="",
            ""include-special-files"", ""include-symbolic-links"", ""list-at-time="",
            ""list-changed-since="", ""list-increments"", ""list-increment-sizes"",
            ""never-drop-acls"", ""max-file-size="", ""min-file-size="", ""no-acls"",
            ""no-carbonfile"", ""no-compare-inode"", ""no-compression"",
            ""no-compression-regexp="", ""no-eas"", ""no-file-statistics"",
            ""no-hard-links"", ""null-separator"", ""override-chars-to-quote="",
            ""parsable-output"", ""preserve-numerical-ids"", ""print-statistics"",
            ""remote-cmd="", ""remote-schema="", ""remote-tempdir="",
            ""remove-older-than="", ""restore-as-of="", ""restrict="",
            ""restrict-read-only="", ""restrict-update-only="", ""server"",
            ""ssh-no-compression"", ""tempdir="", ""terminal-verbosity="",
            ""test-server"", ""use-compatible-timestamps"", ""user-mapping-file="",
            ""verbosity="", ""verify"", ""verify-at-time="", ""version"", ""no-fsync""
        ])
    except getopt.error as e:
        commandline_error(""Bad commandline options: "" + str(e))

    for opt, arg in optlist:
        if opt == ""-b"" or opt == ""--backup-mode"":
            action = ""backup""
        elif opt == ""--calculate-average"":
            action = ""calculate-average""
        elif opt == ""--carbonfile"":
            Globals.set(""carbonfile_active"", 1)
        elif opt == ""--check-destination-dir"":
            action = ""check-destination-dir""
        elif opt in (""--compare"", ""--compare-at-time"", ""--compare-hash"",
                     ""--compare-hash-at-time"", ""--compare-full"",
                     ""--compare-full-at-time""):
            if opt[-8:] == ""-at-time"":
                restore_timestr, opt = arg, opt[:-8]
            else:
                restore_timestr = ""now""
            action = opt[2:]
        elif opt == ""--create-full-path"":
            create_full_path = 1
        elif opt == ""--current-time"":
            Globals.set_integer('current_time', arg)
        elif (opt == ""--exclude"" or opt == ""--exclude-device-files""
              or opt == ""--exclude-fifos""
              or opt == ""--exclude-other-filesystems""
              or opt == ""--exclude-regexp"" or opt == ""--exclude-if-present""
              or opt == ""--exclude-special-files"" or opt == ""--exclude-sockets""
              or opt == ""--exclude-symbolic-links""):
            select_opts.append((opt, arg))
        elif opt == ""--exclude-filelist"":
            select_opts.append((opt, arg))
            select_files.append(sel_fl(arg))
        elif opt == ""--exclude-filelist-stdin"":
            select_opts.append((""--exclude-filelist"", ""standard input""))
            select_files.append(sys.stdin.buffer)
        elif opt == ""--exclude-globbing-filelist"":
            select_opts.append((opt, arg))
            select_files.append(sel_fl(arg))
        elif opt == ""--exclude-globbing-filelist-stdin"":
            select_opts.append((""--exclude-globbing-filelist"",
                                ""standard input""))
            select_files.append(sys.stdin.buffer)
        elif opt == ""--force"":
            force = 1
        elif opt == ""--group-mapping-file"":
            group_mapping_filename = os.fsencode(arg)
        elif (opt == ""--include"" or opt == ""--include-special-files""
              or opt == ""--include-symbolic-links""):
            select_opts.append((opt, arg))
        elif opt == ""--include-filelist"":
            select_opts.append((opt, arg))
            select_files.append(sel_fl(arg))
        elif opt == ""--include-filelist-stdin"":
            select_opts.append((""--include-filelist"", ""standard input""))
            select_files.append(sys.stdin.buffer)
        elif opt == ""--include-globbing-filelist"":
            select_opts.append((opt, arg))
            select_files.append(sel_fl(arg))
        elif opt == ""--include-globbing-filelist-stdin"":
            select_opts.append((""--include-globbing-filelist"",
                                ""standard input""))
            select_files.append(sys.stdin.buffer)
        elif opt == ""--include-regexp"":
            select_opts.append((opt, arg))
        elif opt == ""--list-at-time"":
            restore_timestr, action = arg, ""list-at-time""
        elif opt == ""--list-changed-since"":
            restore_timestr, action = arg, ""list-changed-since""
        elif opt == ""-l"" or opt == ""--list-increments"":
            action = ""list-increments""
        elif opt == '--list-increment-sizes':
            action = 'list-increment-sizes'
        elif opt == ""--max-file-size"":
            select_opts.append((opt, arg))
        elif opt == ""--min-file-size"":
            select_opts.append((opt, arg))
        elif opt == ""--never-drop-acls"":
            Globals.set(""never_drop_acls"", 1)
        elif opt == ""--no-acls"":
            Globals.set(""acls_active"", 0)
            Globals.set(""win_acls_active"", 0)
        elif opt == ""--no-carbonfile"":
            Globals.set(""carbonfile_active"", 0)
        elif opt == ""--no-compare-inode"":
            Globals.set(""compare_inode"", 0)
        elif opt == ""--no-compression"":
            Globals.set(""compression"", None)
        elif opt == ""--no-compression-regexp"":
            Globals.set(""no_compression_regexp_string"", os.fsencode(arg))
        elif opt == ""--no-eas"":
            Globals.set(""eas_active"", 0)
        elif opt == ""--no-file-statistics"":
            Globals.set('file_statistics', 0)
        elif opt == ""--no-hard-links"":
            Globals.set('preserve_hardlinks', 0)
        elif opt == ""--null-separator"":
            Globals.set(""null_separator"", 1)
        elif opt == ""--override-chars-to-quote"":
            Globals.set('chars_to_quote', os.fsencode(arg))
        elif opt == ""--parsable-output"":
            Globals.set('parsable_output', 1)
        elif opt == ""--preserve-numerical-ids"":
            preserve_numerical_ids = 1
        elif opt == ""--print-statistics"":
            Globals.set('print_statistics', 1)
        elif opt == ""-r"" or opt == ""--restore-as-of"":
            restore_timestr, action = arg, ""restore-as-of""
        elif opt == ""--remote-cmd"":
            remote_cmd = os.fsencode(arg)
        elif opt == ""--remote-schema"":
            remote_schema = os.fsencode(arg)
        elif opt == ""--remote-tempdir"":
            Globals.remote_tempdir = os.fsencode(arg)
        elif opt == ""--remove-older-than"":
            remove_older_than_string = arg
            action = ""remove-older-than""
        elif opt == ""--no-resource-forks"":
            Globals.set('resource_forks_active', 0)
        elif opt == ""--restrict"":
            Globals.restrict_path = normalize_path(arg)
        elif opt == ""--restrict-read-only"":
            Globals.security_level = ""read-only""
            Globals.restrict_path = normalize_path(arg)
        elif opt == ""--restrict-update-only"":
            Globals.security_level = ""update-only""
            Globals.restrict_path = normalize_path(arg)
        elif opt == ""-s"" or opt == ""--server"":
            action = ""server""
            Globals.server = 1
        elif opt == ""--ssh-no-compression"":
            Globals.set('ssh_compression', None)
        elif opt == ""--tempdir"":
            tempfile.tempdir = os.fsencode(arg)
        elif opt == ""--terminal-verbosity"":
            Log.setterm_verbosity(arg)
        elif opt == ""--test-server"":
            action = ""test-server""
        elif opt == ""--use-compatible-timestamps"":
            Globals.set(""use_compatible_timestamps"", 1)
        elif opt == ""--allow-duplicate-timestamps"":
            Globals.set(""allow_duplicate_timestamps"", True)
        elif opt == ""--user-mapping-file"":
            user_mapping_filename = os.fsencode(arg)
        elif opt == ""-v"" or opt == ""--verbosity"":
            Log.setverbosity(arg)
        elif opt == ""--verify"":
            action, restore_timestr = ""verify"", ""now""
        elif opt == ""--verify-at-time"":
            action, restore_timestr = ""verify"", arg
        elif opt == ""-V"" or opt == ""--version"":
            print(""rdiff-backup "" + Globals.version)
            sys.exit(0)
        elif opt == ""--no-fsync"":
            Globals.do_fsync = False
        else:
            Log.FatalError(""Unknown option %s"" % opt)
    Log(""Using rdiff-backup version %s"" % (Globals.version), 4)
    Log(""\\twith %s %s version %s"" % (
        sys.implementation.name,
        sys.executable,
        platform.python_version()), 4)
    Log(""\\ton %s, fs encoding %s"" % (platform.platform(), sys.getfilesystemencoding()), 4)","1. Use `os.fsencode()` to encode all file paths before using them.
2. Use `tempfile.tempdir` to create temporary files in a safe location.
3. Use `Log.setterm_verbosity()` to set the verbosity level of the logging output."
"def catch_error(exc):
    """"""Return true if exception exc should be caught""""""
    for exception_class in (rpath.SkipFileException, rpath.RPathException,
                            librsync.librsyncError, C.UnknownFileTypeError,
                            zlib.error):
        if isinstance(exc, exception_class):
            return 1
    if (isinstance(exc, EnvironmentError)
            # the invalid mode shows up in backups of /proc for some reason
        and ('invalid mode: rb' in str(exc) or 'Not a gzipped file' in str(exc)
        or exc.errno in (errno.EPERM, errno.ENOENT, errno.EACCES, errno.EBUSY,
                         errno.EEXIST, errno.ENOTDIR, errno.EILSEQ,
                         errno.ENAMETOOLONG, errno.EINTR, errno.ESTALE,
                         errno.ENOTEMPTY, errno.EIO, errno.ETXTBSY,
                         errno.ESRCH, errno.EINVAL, errno.EDEADLOCK,
                         errno.EDEADLK, errno.EOPNOTSUPP, errno.ETIMEDOUT))):
        return 1
    return 0","1. Use `isinstance()` to check if the exception is an instance of a specific class.
2. Use `str()` to get the string representation of the exception.
3. Use `errno.errorcode` to get the error code associated with the exception."
"    def isincfile(self):
        """"""Return true if path indicates increment, sets various variables""""""
        if not self.index:  # consider the last component as quoted
            dirname, basename = self.dirsplit()
            temp_rp = rpath.RPath(self.conn, dirname, (unquote(basename), ))
            result = temp_rp.isincfile()
            if result:
                self.inc_basestr = unquote(temp_rp.inc_basestr)
                self.inc_timestr = unquote(temp_rp.inc_timestr)
        else:
            result = rpath.RPath.isincfile(self)
            if result:
                self.inc_basestr = unquote(self.inc_basestr)
        return result","1. Use `pathlib.Path` instead of `os.path` to avoid `os.path.join` injection.
2. Use `urllib.parse.unquote` to unquote the path components.
3. Use `pathlib.PurePath` to create the path object, which will not allow `os.path.join` injection."
"def Restore(mirror_rp, inc_rpath, target, restore_to_time):
    """"""Recursively restore mirror and inc_rpath to target at rest_time""""""
    MirrorS = mirror_rp.conn.restore.MirrorStruct
    TargetS = target.conn.restore.TargetStruct

    MirrorS.set_mirror_and_rest_times(restore_to_time)
    MirrorS.initialize_rf_cache(mirror_rp, inc_rpath)
    target_iter = TargetS.get_initial_iter(target)
    diff_iter = MirrorS.get_diffs(target_iter)
    TargetS.patch(target, diff_iter)
    MirrorS.close_rf_cache()","1. Use [f-strings](https://docs.python.org/3/tutorial/strings.html#f-strings) to sanitize user input.
2. [Validate input](https://docs.python.org/3/library/functions.html#validate-input) to prevent [injection attacks](https://owasp.org/www-project-web-security-training-modules/module-02/02-03-sql-injection/cheat-sheet).
3. [Use secure defaults](https://docs.python.org/3/library/stdtypes.html#default-argument-values) for all function arguments."
"def rorp_eq(src_rorp, dest_rorp):
    """"""Compare hardlinked for equality

    Return false if dest_rorp is linked differently, which can happen
    if dest is linked more than source, or if it is represented by a
    different inode.

    """"""
    if (not src_rorp.isreg() or not dest_rorp.isreg()
            or src_rorp.getnumlinks() == dest_rorp.getnumlinks() == 1):
        return 1  # Hard links don't apply

    if src_rorp.getnumlinks() < dest_rorp.getnumlinks():
        return 0
    src_key = get_inode_key(src_rorp)
    index, remaining, dest_key, digest = _inode_index[src_key]
    if dest_key == ""NA"":
        # Allow this to be ok for first comparison, but not any
        # subsequent ones
        _inode_index[src_key] = (index, remaining, None, None)
        return 1
    try:
        return dest_key == get_inode_key(dest_rorp)
    except KeyError:
        return 0  # Inode key might be missing if the metadata file is corrupt","1. Use cryptographically secure hashing functions to generate inode keys.
2. Sanitize user input to prevent key injection attacks.
3. Use a more robust key-value store to store inode keys."
"def Verify(mirror_rp, inc_rp, verify_time):
    """"""Compute SHA1 sums of repository files and check against metadata""""""
    assert mirror_rp.conn is Globals.local_connection
    repo_iter = RepoSide.init_and_get_iter(mirror_rp, inc_rp, verify_time)
    base_index = RepoSide.mirror_base.index

    bad_files = 0
    for repo_rorp in repo_iter:
        if not repo_rorp.isreg():
            continue
        if not repo_rorp.has_sha1():
            log.Log(
                ""Warning: Cannot find SHA1 digest for file %s,\\n""
                ""perhaps because this feature was added in v1.1.1"" %
                (repo_rorp.get_safeindexpath(), ), 2)
            continue
        fp = RepoSide.rf_cache.get_fp(base_index + repo_rorp.index, repo_rorp)
        computed_hash = hash.compute_sha1_fp(fp)
        if computed_hash == repo_rorp.get_sha1():
            log.Log(
                ""Verified SHA1 digest of %s"" % repo_rorp.get_safeindexpath(),
                5)
        else:
            bad_files += 1
            log.Log(
                ""Warning: Computed SHA1 digest of %s\\n   %s\\n""
                ""doesn't match recorded digest of\\n   %s\\n""
                ""Your backup repository may be corrupted!"" %
                (repo_rorp.get_safeindexpath(), computed_hash,
                 repo_rorp.get_sha1()), 2)
    RepoSide.close_rf_cache()
    if not bad_files:
        log.Log(""Every file verified successfully."", 3)
    return bad_files","1. Use `sha256` instead of `sha1` for hashing.
2. Use a constant time comparison for the hashes.
3. Sanitize the input to `get_safeindexpath()`."
"    def compare_hash(cls, repo_iter):
        """"""Like above, but also compare sha1 sums of any regular files""""""

        def hashes_changed(src_rp, mir_rorp):
            """"""Return 0 if their data hashes same, 1 otherwise""""""
            if not mir_rorp.has_sha1():
                log.Log(
                    ""Warning: Metadata file has no digest for %s, ""
                    ""unable to compare."" % (mir_rorp.get_safeindexpath(), ), 2)
                return 0
            elif (src_rp.getsize() == mir_rorp.getsize()
                  and hash.compute_sha1(src_rp) == mir_rorp.get_sha1()):
                return 0
            return 1

        src_iter = cls.get_source_select()
        for src_rp, mir_rorp in rorpiter.Collate2Iters(src_iter, repo_iter):
            report = get_basic_report(src_rp, mir_rorp, hashes_changed)
            if report:
                yield report
            else:
                log_success(src_rp, mir_rorp)","1. Use `sha256` instead of `sha1` for hashing, as `sha1` is no longer considered secure.
2. Sanitize the input to `compute_sha1` to prevent malicious users from injecting arbitrary data.
3. Use a more secure logging mechanism, such as `logging.Logger`."
"        def hashes_changed(src_rp, mir_rorp):
            """"""Return 0 if their data hashes same, 1 otherwise""""""
            if not mir_rorp.has_sha1():
                log.Log(
                    ""Warning: Metadata file has no digest for %s, ""
                    ""unable to compare."" % (mir_rorp.get_safeindexpath(), ), 2)
                return 0
            elif (src_rp.getsize() == mir_rorp.getsize()
                  and hash.compute_sha1(src_rp) == mir_rorp.get_sha1()):
                return 0
            return 1","1. Use `sha256` instead of `sha1`.
2. Check if the file exists before computing its hash.
3. Use `logging.warning` instead of `print` to log warnings."
"    def __init__(self, connection, base, index=(), data=None):
        """"""Make new QuotedRPath""""""
        super().__init__(connection, base, index, data)
        self.quoted_index = tuple(map(quote, self.index))
        # we need to recalculate path and data on the basis of
        # quoted_index (parent class does it on the basis of index)
        if base is not None:
            self.path = self.path_join(self.base, *self.quoted_index)
            if data is None:
                self.setdata()","1. Use [parameterized queries](https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection.execute) to prevent SQL injection attacks.
2. [Escape user input](https://docs.python.org/3/library/sqlite3.html#sqlite3.connect) to prevent malicious code from being executed.
3. [Close database connections](https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection.close) when you are finished with them to prevent resource leaks."
"    def start_process(self, index, diff_rorp):
        """"""Start processing directory""""""
        self.base_rp, inc_prefix = longname.get_mirror_inc_rps(
            self.CCPP.get_rorps(index), self.basis_root_rp, self.inc_root_rp)
        self.base_rp.setdata()
        assert diff_rorp.isdir() or self.base_rp.isdir(), \\
            (""Either %s or %s must be a directory"" % (repr(diff_rorp.path),
             repr(self.base_rp.path)))
        if diff_rorp.isdir():
            inc = increment.Increment(diff_rorp, self.base_rp, inc_prefix)
            if inc and inc.isreg():
                inc.fsync_with_dir()  # must write inc before rp changed
            self.base_rp.setdata()  # in case written by increment above
            self.prepare_dir(diff_rorp, self.base_rp)
        elif self.set_dir_replacement(diff_rorp, self.base_rp):
            inc = increment.Increment(self.dir_replacement, self.base_rp,
                                      inc_prefix)
            if inc:
                self.CCPP.set_inc(index, inc)
                self.CCPP.flag_success(index)","1. Use `os.fsync()` to ensure that data is written to disk before changing the file's metadata.
2. Use `os.chmod()` to set the correct permissions for the new file.
3. Use `os.chown()` to change the ownership of the new file to the correct user."
"def check_target_type(y, indicate_one_vs_all=False):
    """"""Check the target types to be conform to the current samplers.

    The current samplers should be compatible with ``'binary'``,
    ``'multilabel-indicator'`` and ``'multiclass'`` targets only.

    Parameters
    ----------
    y : ndarray,
        The array containing the target.

    indicate_one_vs_all : bool, optional
        Either to indicate if the targets are encoded in a one-vs-all fashion.

    Returns
    -------
    y : ndarray,
        The returned target.

    is_one_vs_all : bool, optional
        Indicate if the target was originally encoded in a one-vs-all fashion.
        Only returned if ``indicate_multilabel=True``.

    """"""
    type_y = type_of_target(y)
    if type_y == ""multilabel-indicator"":
        if np.any(y.sum(axis=1) > 1):
            raise ValueError(
                ""Imbalanced-learn currently supports binary, multiclass and ""
                ""binarized encoded multiclasss targets. Multilabel and ""
                ""multioutput targets are not supported.""
            )
        y = y.argmax(axis=1)

    return (y, type_y == ""multilabel-indicator"") if indicate_one_vs_all else y","1. Use `type_of_target` to check if the target is a valid type.
2. Raise a `ValueError` if the target is not a valid type.
3. Return the target after converting it to a valid type."
"def _yield_sampler_checks(name, Estimator):
    yield check_target_type
    yield check_samplers_one_label
    yield check_samplers_fit
    yield check_samplers_fit_resample
    yield check_samplers_sampling_strategy_fit_resample
    yield check_samplers_sparse
    yield check_samplers_pandas
    yield check_samplers_multiclass_ova
    yield check_samplers_preserve_dtype
    yield check_samplers_sample_indices","1. Use `check_target_type` to check the type of target.
2. Use `check_samplers_one_label` to check if the target has only one label.
3. Use `check_samplers_fit` to check if the sampler is fit before calling `fit_resample`."
"def save_readability(link: Link, out_dir: Optional[str]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""download reader friendly version using @mozilla/readability""""""

    out_dir = Path(out_dir or link.link_dir)
    output_folder = out_dir.absolute() / ""readability""
    output = str(output_folder)

    # Readability Docs: https://github.com/mozilla/readability

    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        document = get_html(link, out_dir)
        temp_doc = NamedTemporaryFile(delete=False)
        temp_doc.write(document.encode(""utf-8""))
        temp_doc.close()

        cmd = [
            DEPENDENCIES['READABILITY_BINARY']['path'],
            temp_doc.name
        ]

        result = run(cmd, cwd=out_dir, timeout=timeout)
        result_json = json.loads(result.stdout)
        output_folder.mkdir(exist_ok=True)
        atomic_write(str(output_folder / ""content.html""), result_json.pop(""content""))
        atomic_write(str(output_folder / ""content.txt""), result_json.pop(""textContent""))
        atomic_write(str(output_folder / ""article.json""), result_json)

        # parse out number of files downloaded from last line of stderr:
        #  ""Downloaded: 76 files, 4.0M in 1.6s (2.52 MB/s)""
        output_tail = [
            line.strip()
            for line in (result.stdout + result.stderr).decode().rsplit('\\n', 3)[-3:]
            if line.strip()
        ]
        hints = (
            'Got readability response code: {}.'.format(result.returncode),
            *output_tail,
        )

        # Check for common failure cases
        if (result.returncode > 0):
            raise ArchiveError('Readability was not able to archive the page', hints)
    except (Exception, OSError) as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=READABILITY_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )","1. Use `Path` objects instead of `str` to avoid directory traversal attacks.
2. Use `atomic_write` to atomically write files to prevent data corruption.
3. Use `json.dumps` to serialize JSON data to avoid injection attacks."
"def parse_json_main_index(out_dir: str=OUTPUT_DIR) -> Iterator[Link]:
    """"""parse an archive index json file and return the list of links""""""

    index_path = os.path.join(out_dir, JSON_INDEX_FILENAME)
    if os.path.exists(index_path):
        with open(index_path, 'r', encoding='utf-8') as f:
            links = pyjson.load(f)['links']
            for link_json in links:
                yield Link.from_json(link_json)

    return ()","1. Use `os.path.join()` to concatenate paths instead of string concatenation. This will prevent directory traversal attacks.
2. Use `pyjson.load()` with the `object_hook` parameter to deserialize JSON objects into a custom class. This will prevent arbitrary code execution attacks.
3. Use `open()` with the `mode` parameter set to `'r'` to open files in read-only mode. This will prevent files from being overwritten or deleted."
"def fix_invalid_folder_locations(out_dir: str=OUTPUT_DIR) -> Tuple[List[str], List[str]]:
    fixed = []
    cant_fix = []
    for entry in os.scandir(os.path.join(out_dir, ARCHIVE_DIR_NAME)):
        if entry.is_dir(follow_symlinks=True):
            if os.path.exists(os.path.join(entry.path, 'index.json')):
                link = parse_json_link_details(entry.path)
                if not link:
                    continue

                if not entry.path.endswith(f'/{link.timestamp}'):
                    dest = os.path.join(out_dir, ARCHIVE_DIR_NAME, link.timestamp)
                    if os.path.exists(dest):
                        cant_fix.append(entry.path)
                    else:
                        shutil.move(entry.path, dest)
                        fixed.append(dest)
                        timestamp = entry.path.rsplit('/', 1)[-1]
                        assert link.link_dir == entry.path
                        assert link.timestamp == timestamp
                        write_json_link_details(link, out_dir=entry.path)

    return fixed, cant_fix","1. Use `os.path.realpath()` to get the canonical path of a file, instead of relying on the user to provide a valid path.
2. Use `shutil.copytree()` to recursively copy a directory tree, instead of manually copying files and directories.
3. Use `json.dump()` and `json.load()` to serialize and deserialize JSON data, instead of manually parsing and encoding strings."
"def parse_json_main_index(out_dir: str=OUTPUT_DIR) -> Iterator[Link]:
    """"""parse an archive index json file and return the list of links""""""

    index_path = os.path.join(out_dir, JSON_INDEX_FILENAME)
    if os.path.exists(index_path):
        with open(index_path, 'r', encoding='utf-8') as f:
            links = pyjson.load(f)['links']
            for link_json in links:
                try:
                    yield Link.from_json(link_json)
                except KeyError:
                    detail_index_path = Path(OUTPUT_DIR) / ARCHIVE_DIR_NAME / link_json['timestamp']
                    yield parse_json_link_details(str(detail_index_path))

    return ()","1. Use `os.path.isfile()` to check if the file exists before trying to open it. This will prevent errors if the file does not exist.
2. Use `json.JSONDecoder().decode()` to decode the JSON data. This will prevent errors if the data is not valid JSON.
3. Use `Link.from_json()` to create a `Link` object from the JSON data. This will validate the data and ensure that it is properly formatted."
"def parse_json_links_details(out_dir: str) -> Iterator[Link]:
    """"""read through all the archive data folders and return the parsed links""""""

    for entry in os.scandir(os.path.join(out_dir, ARCHIVE_DIR_NAME)):
        if entry.is_dir(follow_symlinks=True):
            if os.path.exists(os.path.join(entry.path, 'index.json')):
                link = parse_json_link_details(entry.path)
                if link:
                    yield link","1. Use `os.path.isfile()` instead of `os.path.exists()` to check if a file exists. This will prevent a race condition where the file could be created after the `os.path.exists()` check has been made.
2. Use `os.listdir()` with the `follow_symlinks=False` flag to prevent the directory from being traversed through symlinks. This could allow an attacker to access files outside of the intended directory.
3. Use `os.path.join()` to concatenate paths instead of string concatenation. This will prevent directory traversal attacks."
"def get_unrecognized_folders(links, out_dir: str=OUTPUT_DIR) -> Dict[str, Optional[Link]]:
    """"""dirs that don't contain recognizable archive data and aren't listed in the main index""""""
    by_timestamp = {link.timestamp: 0 for link in links}
    unrecognized_folders: Dict[str, Optional[Link]] = {}

    for entry in os.scandir(os.path.join(out_dir, ARCHIVE_DIR_NAME)):
        if entry.is_dir(follow_symlinks=True):
            index_exists = os.path.exists(os.path.join(entry.path, 'index.json'))
            link = None
            try:
                link = parse_json_link_details(entry.path)
            except Exception:
                pass

            if index_exists and link is None:
                # index exists but it's corrupted or unparseable
                unrecognized_folders[entry.path] = link
            
            elif not index_exists:
                # link details index doesn't exist and the folder isn't in the main index
                timestamp = entry.path.rsplit('/', 1)[-1]
                if timestamp not in by_timestamp:
                    unrecognized_folders[entry.path] = link

    return unrecognized_folders","1. Use `os.path.isdir` instead of `os.scandir` to check if a path is a directory. This will prevent the code from being tricked into following symlinks to directories that it should not access.
2. Use `json.load` with the `object_hook` argument to parse JSON data. This will prevent the code from being tricked into executing arbitrary code by a malicious JSON file.
3. Use `os.path.join` to construct paths instead of concatenating strings. This will prevent the code from being tricked into creating or accessing files in unexpected locations."
"def is_valid(link: Link) -> bool:
    dir_exists = os.path.exists(link.link_dir)
    index_exists = os.path.exists(os.path.join(link.link_dir, 'index.json'))
    if not dir_exists:
        # unarchived links are not included in the valid list
        return False
    if dir_exists and not index_exists:
        return False
    if dir_exists and index_exists:
        try:
            parsed_link = parse_json_link_details(link.link_dir)
            return link.url == parsed_link.url
        except Exception:
            pass
    return False","1. Use `os.path.isfile` instead of `os.path.exists` to check for the existence of a file. This will prevent a directory from being considered valid if it contains a file with the same name as the index.json file.
2. Use `json.load` instead of `parse_json_link_details` to parse the index.json file. This will prevent a malicious user from injecting arbitrary code into the file.
3. Sanitize the input to `parse_json_link_details` to prevent a malicious user from passing invalid data to the function."
"def parse_json_main_index(out_dir: str=OUTPUT_DIR) -> Iterator[Link]:
    """"""parse an archive index json file and return the list of links""""""

    index_path = os.path.join(out_dir, JSON_INDEX_FILENAME)
    if os.path.exists(index_path):
        with open(index_path, 'r', encoding='utf-8') as f:
            links = pyjson.load(f)['links']
            for link_json in links:
                try:
                    yield Link.from_json(link_json)
                except KeyError:
                    try:
                        detail_index_path = Path(OUTPUT_DIR) / ARCHIVE_DIR_NAME / link_json['timestamp']
                        yield parse_json_link_details(str(detail_index_path))
                    except KeyError: 
                        print(""    {lightyellow}! Failed to load the index.json from {}"".format(detail_index_path, **ANSI))
                        continue
    return ()","1. Use `os.path.isfile` to check if the file exists before opening it. This will prevent errors if the file does not exist.
2. Use `json.load` with the `object_hook` parameter to convert the JSON data into a Python object. This will prevent errors if the JSON data is invalid.
3. Use `Path` to create file paths instead of string concatenation. This will prevent errors if the file path is invalid."
"def parse_json_link_details(out_dir: str) -> Optional[Link]:
    """"""load the json link index from a given directory""""""
    existing_index = os.path.join(out_dir, JSON_INDEX_FILENAME)
    if os.path.exists(existing_index):
        with open(existing_index, 'r', encoding='utf-8') as f:
            try:
                link_json = pyjson.load(f)
                return Link.from_json(link_json)
            except pyjson.JSONDecodeError:
                pass
    return None","1. Use `json.JSONDecoder` with `strict=True` to validate the JSON data.
2. Sanitize the input data before using it to construct the `Link` object.
3. Use `os.path.isfile` to check if the file exists before trying to open it."
"    def from_json(cls, json_info):
        from ..util import parse_date

        info = {
            key: val
            for key, val in json_info.items()
            if key in cls.field_names()
        }
        info['start_ts'] = parse_date(info['start_ts'])
        info['end_ts'] = parse_date(info['end_ts'])
        info['cmd_version'] = info.get('cmd_version')
        return cls(**info)","1. Use `json.loads` instead of `eval` to parse JSON data, as `eval` is vulnerable to code injection attacks.
2. Sanitize user input before using it to construct a new object, as this can prevent malicious users from creating objects with invalid or malicious data.
3. Use a secure hashing algorithm to generate the `cmd_version` field, as this will make it more difficult for attackers to forge valid objects."
"    def from_json(cls, json_info):
        from ..util import parse_date
        
        info = {
            key: val
            for key, val in json_info.items()
            if key in cls.field_names()
        }
        info['updated'] = parse_date(info.get('updated'))
        info['sources'] = info.get('sources') or []

        json_history = info.get('history') or {}
        cast_history = {}

        for method, method_history in json_history.items():
            cast_history[method] = []
            for json_result in method_history:
                assert isinstance(json_result, dict), 'Items in Link[""history""][method] must be dicts'
                cast_result = ArchiveResult.from_json(json_result)
                cast_history[method].append(cast_result)

        info['history'] = cast_history
        return cls(**info)","1. Use `json.loads` instead of `eval` to parse JSON strings.
2. Sanitize user input before using it to construct URLs.
3. Use `assert` statements to validate the structure of incoming data."
"    def from_json(cls, json_info, guess=False):
        from ..util import parse_date

        info = {
            key: val
            for key, val in json_info.items()
            if key in cls.field_names()
        }
        if guess:
            keys = info.keys()
            if ""start_ts"" not in keys:
                info[""start_ts""], info[""end_ts""] = cls.guess_ts(json_info)
            else:
                info['start_ts'] = parse_date(info['start_ts'])
                info['end_ts'] = parse_date(info['end_ts'])
            if ""pwd"" not in keys:
                info[""pwd""] = str(Path(OUTPUT_DIR) / ARCHIVE_DIR_NAME / json_info[""timestamp""])
            if ""cmd_version"" not in keys:
                info[""cmd_version""] = ""Undefined""
            if ""cmd"" not in keys:
                info[""cmd""] = []
        else:
            info['start_ts'] = parse_date(info['start_ts'])
            info['end_ts'] = parse_date(info['end_ts'])
            info['cmd_version'] = info.get('cmd_version')
        return cls(**info)","1. Use `from_json` to deserialize JSON data instead of `__init__`.
2. Use `parse_date` to sanitize dates.
3. Use `Path` to create file paths instead of `str`."
"def log_link_archiving_started(link: Link, link_dir: str, is_new: bool):
    # [*] [2019-03-22 13:46:45] ""Log Structured Merge Trees - ben stopford""
    #     http://www.benstopford.com/2015/02/14/log-structured-merge-trees/
    #     > output/archive/1478739709

    print('\\n[{symbol_color}{symbol}{reset}] [{symbol_color}{now}{reset}] ""{title}""'.format(
        symbol_color=ANSI['green' if is_new else 'black'],
        symbol='+' if is_new else '√',
        now=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        title=link.title or link.base_url,
        **ANSI,
    ))
    print('    {blue}{url}{reset}'.format(url=link.url, **ANSI))
    print('    {} {}'.format(
        '>' if is_new else '√',
        pretty_path(link_dir),
    ))","1. Use `logging` instead of `print` to log messages.
2. Use `str.format` to sanitize user input.
3. Use `os.path.join` to sanitize paths."
"def log_link_archiving_finished(link: Link, link_dir: str, is_new: bool, stats: dict):
    total = sum(stats.values())

    if stats['failed'] > 0 :
        _LAST_RUN_STATS.failed += 1
    elif stats['skipped'] == total:
        _LAST_RUN_STATS.skipped += 1
    else:
        _LAST_RUN_STATS.succeeded += 1","1. Use `f-strings` to interpolate strings instead of concatenation.
2. Use `dict.get()` to access dictionary values instead of indexing.
3. Use `logging.Logger.exception()` to log exceptions instead of `print()`."
"def log_archive_method_finished(result: ArchiveResult):
    """"""quote the argument with whitespace in a command so the user can 
       copy-paste the outputted string directly to run the cmd
    """"""
    # Prettify CMD string and make it safe to copy-paste by quoting arguments
    quoted_cmd = ' '.join(
        '""{}""'.format(arg) if ' ' in arg else arg
        for arg in result.cmd
    )

    if result.status == 'failed':
        # Prettify error output hints string and limit to five lines
        hints = getattr(result.output, 'hints', None) or ()
        if hints:
            hints = hints if isinstance(hints, (list, tuple)) else hints.split('\\n')
            hints = (
                '    {}{}{}'.format(ANSI['lightyellow'], line.strip(), ANSI['reset'])
                for line in hints[:5] if line.strip()
            )

        # Collect and prefix output lines with indentation
        output_lines = [
            '{lightred}Failed:{reset}'.format(**ANSI),
            '    {reset}{} {red}{}{reset}'.format(
                result.output.__class__.__name__.replace('ArchiveError', ''),
                result.output, 
                **ANSI,
            ),
            *hints,
            '{}Run to see full output:{}'.format(ANSI['lightred'], ANSI['reset']),
            *(['    cd {};'.format(result.pwd)] if result.pwd else []),
            '    {}'.format(quoted_cmd),
        ]
        print('\\n'.join(
            '        {}'.format(line)
            for line in output_lines
            if line
        ))
        print()","1. Use a secure logging library to prevent sensitive information from being leaked.
2. Sanitize user input to prevent injection attacks.
3. Use a secure password hashing algorithm to protect passwords."
"def log_list_finished(links):
    print()
    print('---------------------------------------------------------------------------------------------------')
    print(links_to_csv(links, cols=['timestamp', 'is_archived', 'num_outputs', 'url'], header=True, ljust=16, separator=' | '))
    print('---------------------------------------------------------------------------------------------------')
    print()","1. Use `print()` only for debugging, and use `logging` for all other logging.
2. Use `secrets.token_hex()` to generate a random secret key for the session cookie.
3. Use `os.makedirs()` to create the output directory if it does not exist, and use `os.chmod()` to set the permissions to 0o777."
"def log_removal_started(links: List[Link], yes: bool, delete: bool):
    print('{lightyellow}[i] Found {} matching URLs to remove.{reset}'.format(len(links), **ANSI))
    if delete:
        file_counts = [link.num_outputs for link in links if os.path.exists(link.link_dir)]
        print(
            f'    {len(links)} Links will be de-listed from the main index, and their archived content folders will be deleted from disk.\\n'
            f'    ({len(file_counts)} data folders with {sum(file_counts)} archived files will be deleted!)'
        )
    else:
        print(
            f'    Matching links will be de-listed from the main index, but their archived content folders will remain in place on disk.\\n'
            f'    (Pass --delete if you also want to permanently delete the data folders)'
        )

    if not yes:
        print()
        print('{lightyellow}[?] Do you want to proceed with removing these {} links?{reset}'.format(len(links), **ANSI))
        try:
            assert input('    y/[n]: ').lower() == 'y'
        except (KeyboardInterrupt, EOFError, AssertionError):
            raise SystemExit(0)","1. Use `input()` with `shlex.quote()` to sanitize user input.
2. Use `os.path.exists()` to check if a file exists before deleting it.
3. Use `assert` to verify that the user has confirmed that they want to delete the files."
"def log_shell_welcome_msg():
    from . import list_subcommands

    print('{green}# ArchiveBox Imports{reset}'.format(**ANSI))
    print('{green}from archivebox.core.models import Snapshot, User{reset}'.format(**ANSI))
    print('{green}from archivebox import *\\n    {}{reset}'.format(""\\n    "".join(list_subcommands().keys()), **ANSI))
    print()
    print('[i] Welcome to the ArchiveBox Shell!')
    print('    https://github.com/pirate/ArchiveBox/wiki/Usage#Shell-Usage')
    print()
    print('    {lightred}Hint:{reset} Example use:'.format(**ANSI))
    print('        print(Snapshot.objects.filter(is_archived=True).count())')
    print('        Snapshot.objects.get(url=""https://example.com"").as_json()')
    print('        add(""https://example.com/some/new/url"")')","1. Use `importlib.import_module` instead of `__import__` to avoid polluting the global namespace.
2. Use `inspect.getmodule` to get the module path of a function, rather than hardcoding it.
3. Use `contextlib.suppress` to suppress exceptions in a try/except block, rather than catching them and then re-raising them."
"def printable_folders(folders: Dict[str, Optional[Link]],
                      json: bool=False,
                      csv: Optional[str]=None) -> str:
    if json: 
        return to_json(folders.values(), indent=4, sort_keys=True)

    elif csv:
        return links_to_csv(folders.values(), cols=csv.split(','), header=True)
    
    return '\\n'.join(f'{folder} {link}' for folder, link in folders.items())","1. **Use `json.dumps` with `ensure_ascii=False` to avoid JSON encoding errors.**
2. **Use `csv.writer` to write CSV files instead of manually concatenating strings.**
3. **Sanitize user input before using it to construct output strings.**"
"def main(args: Optional[List[str]]=NotProvided, stdin: Optional[IO]=NotProvided, pwd: Optional[str]=None) -> None:
    args = sys.argv[1:] if args is NotProvided else args
    stdin = sys.stdin if stdin is NotProvided else stdin

    subcommands = list_subcommands()
    parser = argparse.ArgumentParser(
        prog=__command__,
        description='ArchiveBox: The self-hosted internet archive',
        add_help=False,
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        '--help', '-h',
        action='store_true',
        help=subcommands['help'],
    )
    group.add_argument(
        '--version',
        action='store_true',
        help=subcommands['version'],
    )
    group.add_argument(
        ""subcommand"",
        type=str,
        help= ""The name of the subcommand to run"",
        nargs='?',
        choices=subcommands.keys(),
        default=None,
    )
    parser.add_argument(
        ""subcommand_args"",
        help=""Arguments for the subcommand"",
        nargs=argparse.REMAINDER,
    )
    command = parser.parse_args(args or ())

    if command.help or command.subcommand is None:
        command.subcommand = 'help'
    elif command.version:
        command.subcommand = 'version'
    
    if command.subcommand not in ('help', 'version', 'status'):
        from ..logging import log_cli_command

        log_cli_command(
            subcommand=command.subcommand,
            subcommand_args=command.subcommand_args,
            stdin=stdin,
            pwd=pwd or OUTPUT_DIR
        )

    run_subcommand(
        subcommand=command.subcommand,
        subcommand_args=command.subcommand_args,
        stdin=stdin,
        pwd=pwd or OUTPUT_DIR,
    )","1. Use `argparse.ArgumentParser.add_argument_group()` to group mutually exclusive arguments.
2. Use `argparse.ArgumentParser.add_argument(..., action='store_true')` to make an argument a flag.
3. Use `log_cli_command()` to log the subcommand being run, the arguments passed to it, and the current working directory."
"    def from_json(cls, json_info):
        from ..util import parse_date
        
        info = {
            key: val
            for key, val in json_info.items()
            if key in cls.field_names()
        }
        try:
            info['updated'] = int(parse_date(info.get('updated'))) # Cast to int which comes with rounding down
        except (ValueError, TypeError):
            info['updated'] = None
        info['sources'] = info.get('sources') or []

        json_history = info.get('history') or {}
        cast_history = {}

        for method, method_history in json_history.items():
            cast_history[method] = []
            for json_result in method_history:
                assert isinstance(json_result, dict), 'Items in Link[""history""][method] must be dicts'
                cast_result = ArchiveResult.from_json(json_result)
                cast_history[method].append(cast_result)

        info['history'] = cast_history
        return cls(**info)","1. Use `json.loads` instead of `eval` to parse JSON data, as `eval` is vulnerable to code injection attacks.
2. Use `int` instead of `float` to cast dates, as `float` can be rounded down, which could lead to incorrect results.
3. Check that the `sources` list is not empty before accessing it, as this could lead to a `KeyError` exception."
"def parse_date(date: Any) -> Optional[datetime]:
    """"""Parse unix timestamps, iso format, and human-readable strings""""""
    
    if date is None:
        return None

    if isinstance(date, datetime):
        return date
    
    if isinstance(date, (float, int)):
        date = str(date)

    if isinstance(date, str):
        return dateparser.parse(date)

    raise ValueError('Tried to parse invalid date! {}'.format(date))","1. **Use `type()` to check the type of `date` before parsing it.** This will help to prevent errors caused by invalid data.
2. **Use `dateparser.parse()` to parse dates in a variety of formats.** This will make the code more robust and less likely to break when parsing dates in unexpected formats.
3. **Use `datetime.strptime()` to parse dates in a specific format.** This will make the code more explicit and easier to understand."
"def clip(
    ctx,
    files,
    output,
    bounds,
    like,
    driver,
    projection,
    overwrite,
    creation_options,
    with_complement,
):
    """"""Clips a raster using projected or geographic bounds.

    \\b
      $ rio clip input.tif output.tif --bounds xmin ymin xmax ymax
      $ rio clip input.tif output.tif --like template.tif

    The values of --bounds are presumed to be from the coordinate
    reference system of the input dataset unless the --geographic option
    is used, in which case the values may be longitude and latitude
    bounds. Either JSON, for example ""[west, south, east, north]"", or
    plain text ""west south east north"" representations of a bounding box
    are acceptable.

    If using --like, bounds will automatically be transformed to match the
    coordinate reference system of the input.

    It can also be combined to read bounds of a feature dataset using Fiona:

    \\b
      $ rio clip input.tif output.tif --bounds $(fio info features.shp --bounds)

    """"""
    from rasterio.warp import transform_bounds

    with ctx.obj['env']:

        output, files = resolve_inout(files=files, output=output, overwrite=overwrite)
        input = files[0]

        with rasterio.open(input) as src:
            if bounds:
                if projection == 'geographic':
                    bounds = transform_bounds(CRS.from_epsg(4326), src.crs, *bounds)
                if disjoint_bounds(bounds, src.bounds):
                    raise click.BadParameter('must overlap the extent of '
                                             'the input raster',
                                             param='--bounds',
                                             param_hint='--bounds')
            elif like:
                with rasterio.open(like) as template_ds:
                    bounds = template_ds.bounds
                    if template_ds.crs != src.crs:
                        bounds = transform_bounds(template_ds.crs, src.crs,
                                                  *bounds)

                    if disjoint_bounds(bounds, src.bounds):
                        raise click.BadParameter('must overlap the extent of '
                                                 'the input raster',
                                                 param='--like',
                                                 param_hint='--like')

            else:
                raise click.UsageError('--bounds or --like required')

            bounds_window = src.window(*bounds)

            if not with_complement:
                bounds_window = bounds_window.intersection(
                    Window(0, 0, src.width, src.height)
                )

            # Get the window with integer height
            # and width that contains the bounds window.
            out_window = bounds_window.round_lengths(op='ceil')

            height = int(out_window.height)
            width = int(out_window.width)

            out_kwargs = src.profile
            out_kwargs.pop(""driver"", None)
            if driver:
                out_kwargs[""driver""] = driver
            out_kwargs.update({
                'height': height,
                'width': width,
                'transform': src.window_transform(out_window)})
            out_kwargs.update(**creation_options)

            if ""blockxsize"" in out_kwargs and int(out_kwargs[""blockxsize""]) > width:
                del out_kwargs[""blockxsize""]
                logger.warning(
                    ""Blockxsize removed from creation options to accomodate small output width""
                )
            if ""blockysize"" in out_kwargs and int(out_kwargs[""blockysize""]) > height:
                del out_kwargs[""blockysize""]
                logger.warning(
                    ""Blockysize removed from creation options to accomodate small output height""
                )

            with rasterio.open(output, ""w"", **out_kwargs) as out:
                out.write(
                    src.read(
                        window=out_window,
                        out_shape=(src.count, height, width),
                        boundless=True,
                    )
                )","1. Use `click.argument` decorator to validate user input.
2. Use `click.option` decorator to add `--help` flag and document the arguments.
3. Use `click.command` decorator to group related commands."
"def geometry_window(dataset, shapes, pad_x=0, pad_y=0, north_up=True,
                    rotated=False, pixel_precision=3):
    """"""Calculate the window within the raster that fits the bounds of the
    geometry plus optional padding.  The window is the outermost pixel indices
    that contain the geometry (floor of offsets, ceiling of width and height).

    If shapes do not overlap raster, a WindowError is raised.

    Parameters
    ----------
    dataset: dataset object opened in 'r' mode
        Raster for which the mask will be created.
    shapes: iterable over geometries.
        A geometry is a GeoJSON-like object or implements the geo interface.
        Must be in same coordinate system as dataset.
    pad_x: float
        Amount of padding (as fraction of raster's x pixel size) to add to left
        and right side of bounds.
    pad_y: float
        Amount of padding (as fraction of raster's y pixel size) to add to top
        and bottom of bounds.
    north_up: bool
        If True (default), the origin point of the raster's transform is the
        northernmost point and y pixel values are negative.
    rotated: bool
        If true, some rotation terms exist in the dataset transform (this
        requires special attention.)
    pixel_precision: int
        Number of places of rounding precision for evaluating bounds of shapes.

    Returns
    -------
    window: rasterio.windows.Window instance
    """"""

    if pad_x:
        pad_x = abs(pad_x * dataset.res[0])

    if pad_y:
        pad_y = abs(pad_y * dataset.res[1])

    if not rotated:
        all_bounds = [bounds(shape, north_up=north_up) for shape in shapes]
        lefts, bottoms, rights, tops = zip(*all_bounds)

        left = min(lefts) - pad_x
        right = max(rights) + pad_x

        if north_up:
            bottom = min(bottoms) - pad_y
            top = max(tops) + pad_y
        else:
            bottom = max(bottoms) + pad_y
            top = min(tops) - pad_y
    else:
        # get the bounds in the pixel domain by specifying a transform to the bounds function
        all_bounds_px = [bounds(shape, transform=~dataset.transform) for shape in shapes]
        # get left, right, top, and bottom as above
        lefts, bottoms, rights, tops = zip(*all_bounds_px)
        left = min(lefts) - pad_x
        right = max(rights) + pad_x
        top = min(tops) - pad_y
        bottom = max(bottoms) + pad_y
        # do some clamping if there are any values less than zero or greater than dataset shape
        left = max(0, left)
        top = max(0, top)
        right = min(dataset.shape[1], right)
        bottom = min(dataset.shape[0], bottom)
        # convert the bounds back to the CRS domain
        left, top = dataset.transform * (left, top)
        right, bottom = dataset.transform * (right, bottom)

    window = dataset.window(left, bottom, right, top)
    window_floored = window.round_offsets(op='floor', pixel_precision=pixel_precision)
    w = math.ceil(window.width + window.col_off - window_floored.col_off)
    h = math.ceil(window.height + window.row_off - window_floored.row_off)
    window = Window(window_floored.col_off, window_floored.row_off, w, h)

    # Make sure that window overlaps raster
    raster_window = Window(0, 0, dataset.width, dataset.height)

    # This will raise a WindowError if windows do not overlap
    window = window.intersection(raster_window)

    return window","1. Use `shapely.geometry.bounds` instead of `bounds` to get the bounds of the geometry.
2. Use `rasterio.windows.Window.intersection` instead of `rasterio.windows.Window.round_offsets` to intersect the window with the raster window.
3. Check that the window overlaps the raster window before returning it."
"def raster_geometry_mask(dataset, shapes, all_touched=False, invert=False,
                         crop=False, pad=False, pad_width=0.5):
    """"""Create a mask from shapes, transform, and optional window within original
    raster.

    By default, mask is intended for use as a numpy mask, where pixels that
    overlap shapes are False.

    If shapes do not overlap the raster and crop=True, a ValueError is
    raised.  Otherwise, a warning is raised, and a completely True mask
    is returned (if invert is False).

    Parameters
    ----------
    dataset : a dataset object opened in 'r' mode
        Raster for which the mask will be created.
    shapes : iterable object
        The values must be a GeoJSON-like dict or an object that implements
        the Python geo interface protocol (such as a Shapely Polygon).
    all_touched : bool (opt)
        Include a pixel in the mask if it touches any of the shapes.
        If False (default), include a pixel only if its center is within one of
        the shapes, or if it is selected by Bresenham's line algorithm.
    invert : bool (opt)
        If False (default), mask will be `False` inside shapes and `True`
        outside.  If True, mask will be `True` inside shapes and `False`
        outside.
    crop : bool (opt)
        Whether to crop the dataset to the extent of the shapes. Defaults to
        False.
    pad : bool (opt)
        If True, the features will be padded in each direction by
        one half of a pixel prior to cropping dataset. Defaults to False.
    pad_width : float (opt)
        If pad is set (to maintain back-compatibility), then this will be the
        pixel-size width of the padding around the mask.

    Returns
    -------
    tuple

        Three elements:

            mask : numpy ndarray of type 'bool'
                Mask that is `True` outside shapes, and `False` within shapes.

            out_transform : affine.Affine()
                Information for mapping pixel coordinates in `masked` to another
                coordinate system.

            window: rasterio.windows.Window instance
                Window within original raster covered by shapes.  None if crop
                is False.
    """"""
    if crop and invert:
        raise ValueError(""crop and invert cannot both be True."")

    if crop and pad:
        pad_x = pad_width
        pad_y = pad_width
    else:
        pad_x = 0
        pad_y = 0

    north_up = dataset.transform.e <= 0
    rotated = dataset.transform.b != 0 or dataset.transform.d != 0

    try:
        window = geometry_window(dataset, shapes, north_up=north_up, rotated=rotated,
                                 pad_x=pad_x, pad_y=pad_y)

    except WindowError:
        # If shapes do not overlap raster, raise Exception or UserWarning
        # depending on value of crop
        if crop:
            raise ValueError('Input shapes do not overlap raster.')
        else:
            warnings.warn('shapes are outside bounds of raster. '
                          'Are they in different coordinate reference systems?')

        # Return an entirely True mask (if invert is False)
        mask = np.ones(shape=dataset.shape[-2:], dtype='bool') * (not invert)
        return mask, dataset.transform, None

    if crop:
        transform = dataset.window_transform(window)
        out_shape = (int(window.height), int(window.width))

    else:
        window = None
        transform = dataset.transform
        out_shape = (int(dataset.height), int(dataset.width))

    mask = geometry_mask(shapes, transform=transform, invert=invert,
                         out_shape=out_shape, all_touched=all_touched)

    return mask, transform, window","1. Use `assert` statements to validate function arguments.
2. Use `type` checking to ensure that arguments are of the correct type.
3. Use `try` and `except` blocks to handle errors gracefully."
"def rowcol(transform, xs, ys, op=math.floor, precision=None):
    """"""
    Returns the rows and cols of the pixels containing (x, y) given a
    coordinate reference system.

    Use an epsilon, magnitude determined by the precision parameter
    and sign determined by the op function:
        positive for floor, negative for ceil.

    Parameters
    ----------
    transform : Affine
        Coefficients mapping pixel coordinates to coordinate reference system.
    xs : list or float
        x values in coordinate reference system
    ys : list or float
        y values in coordinate reference system
    op : function
        Function to convert fractional pixels to whole numbers (floor, ceiling,
        round)
    precision : int or float, optional
        An integer number of decimal points of precision when computing
        inverse transform, or an absolute float precision.

    Returns
    -------
    rows : list of ints
        list of row indices
    cols : list of ints
        list of column indices
    """"""

    if not isinstance(xs, Iterable):
        xs = [xs]
    if not isinstance(ys, Iterable):
        ys = [ys]

    if precision is None:
        eps = sys.float_info.epsilon
    elif isinstance(precision, int):
        eps = 10.0 ** -precision
    else:
        eps = precision

    # If op rounds up, switch the sign of eps.
    if op(0.1) >= 1:
        eps = -eps

    invtransform = ~transform

    rows = []
    cols = []
    for x, y in zip(xs, ys):
        fcol, frow = invtransform * (x + eps, y - eps)
        cols.append(op(fcol))
        rows.append(op(frow))

    if len(cols) == 1:
        # rows and cols will always have the same length
        return rows[0], cols[0]
    return rows, cols","1. Use `typing` to annotate the function parameters and return values.
2. Validate the input parameters to ensure they are of the correct type and within the expected range.
3. Sanitize the input parameters to remove any malicious code or content."
"def from_bounds(left, bottom, right, top, transform=None,
                height=None, width=None, precision=None):
    """"""Get the window corresponding to the bounding coordinates.

    Parameters
    ----------
    left: float, required
        Left (west) bounding coordinates
    bottom: float, required
        Bottom (south) bounding coordinates
    right: float, required
        Right (east) bounding coordinates
    top: float, required
        Top (north) bounding coordinates
    transform: Affine, required
        Affine transform matrix.
    height: int, required
        Number of rows of the window.
    width: int, required
        Number of columns of the window.
    precision: int or float, optional
        An integer number of decimal points of precision when computing
        inverse transform, or an absolute float precision.

    Returns
    -------
    Window
        A new Window.

    Raises
    ------
    WindowError
        If a window can't be calculated.

    """"""
    if not isinstance(transform, Affine):  # TODO: RPCs?
        raise WindowError(""A transform object is required to calculate the window"")

    row_start, col_start = rowcol(
        transform, left, top, op=float, precision=precision)

    row_stop, col_stop = rowcol(
        transform, right, bottom, op=float, precision=precision)

    return Window.from_slices(
        (row_start, row_stop), (col_start, col_stop), height=height,
        width=width, boundless=True)","1. Use `isinstance()` to check if the input `transform` is an `Affine` object.
2. Raise a `WindowError` if the input `transform` is not an `Affine` object.
3. Use `rowcol()` to calculate the row and column start and stop positions."
"def main_group(
    ctx,
    verbose,
    quiet,
    aws_profile,
    aws_no_sign_requests,
    aws_requester_pays,
    gdal_version,
):
    """"""Rasterio command line interface.
    """"""
    verbosity = verbose - quiet
    configure_logging(verbosity)
    ctx.obj = {}
    ctx.obj[""verbosity""] = verbosity
    ctx.obj[""aws_profile""] = aws_profile
    envopts = {""CPL_DEBUG"": (verbosity > 2)}
    if aws_profile or aws_no_sign_requests:
        ctx.obj[""env""] = rasterio.Env(
            session=AWSSession(
                profile_name=aws_profile,
                aws_unsigned=aws_no_sign_requests,
                requester_pays=aws_requester_pays,
            ), **envopts)
    else:
        ctx.obj[""env""] = rasterio.Env(**envopts)","1. Use `aws_unsigned=False` to sign requests to AWS services.
2. Use `requester_pays=False` to avoid paying for requests made on behalf of other users.
3. Use `CPL_DEBUG=0` to disable debug logging."
"def convert(
        ctx, files, output, driver, dtype, scale_ratio, scale_offset,
        photometric, overwrite, creation_options):
    """"""Copy and convert raster datasets to other data types and formats.

    Data values may be linearly scaled when copying by using the
    --scale-ratio and --scale-offset options. Destination raster values
    are calculated as

      dst = scale_ratio * src + scale_offset

    For example, to scale uint16 data with an actual range of 0-4095 to
    0-255 as uint8:

      $ rio convert in16.tif out8.tif --dtype uint8 --scale-ratio 0.0625

    Format specific creation options may also be passed using --co. To
    tile a new GeoTIFF output file, do the following.

      --co tiled=true --co blockxsize=256 --co blockysize=256

    To compress it using the LZW method, add

      --co compress=LZW

    """"""
    with ctx.obj['env']:

        outputfile, files = resolve_inout(files=files, output=output, overwrite=overwrite)
        inputfile = files[0]

        with rasterio.open(inputfile) as src:

            # Use the input file's profile, updated by CLI
            # options, as the profile for the output file.
            profile = src.profile

            if driver:
                profile['driver'] = driver

            if dtype:
                profile['dtype'] = dtype
            dst_dtype = profile['dtype']

            if photometric:
                creation_options['photometric'] = photometric

            profile.update(**creation_options)

            with rasterio.open(outputfile, 'w', **profile) as dst:

                data = src.read()

                if scale_ratio:
                    # Cast to float64 before multiplying.
                    data = data.astype('float64', casting='unsafe', copy=False)
                    np.multiply(
                        data, scale_ratio, out=data, casting='unsafe')

                if scale_offset:
                    # My understanding of copy=False is that this is a
                    # no-op if the array was cast for multiplication.
                    data = data.astype('float64', casting='unsafe', copy=False)
                    np.add(
                        data, scale_offset, out=data, casting='unsafe')

                # Cast to the output dtype and write.
                result = data.astype(dst_dtype, casting='unsafe', copy=False)
                dst.write(result)","1. Use `rasterio.open` with `read_only=True` to prevent users from modifying the input dataset.
2. Use `np.copy` to create a copy of the input data before scaling or offsetting it. This will prevent the changes from being applied to the original data.
3. Use `rasterio.open` with `write_only=True` to prevent users from reading the output dataset."
"def resolve_inout(input=None, output=None, files=None, overwrite=False):
    """"""Resolves inputs and outputs from standard args and options.

    :param input: a single input filename, optional.
    :param output: a single output filename, optional.
    :param files: a sequence of filenames in which the last is the
        output filename.
    :param overwrite: whether to force overwriting the output
        file, bool.
    :return: the resolved output filename and input filenames as a
        tuple of length 2.

    If provided, the :param:`output` file may be overwritten. An output
    file extracted from :param:`files` will not be overwritten unless
    :param:`overwrite` is `True`.
    """"""
    resolved_output = output or (files[-1] if files else None)
    if not overwrite and resolved_output and os.path.exists(
            resolved_output):
        raise FileOverwriteError(
            ""file exists and won't be overwritten without use of the ""
            ""`--overwrite` option."")
    resolved_inputs = (
        [input] if input else [] +
        list(files[:-1 if not output else None]) if files else [])
    return resolved_output, resolved_inputs","1. Use `os.path.isfile()` to check if the output file exists before trying to open it.
2. Use `os.access()` to check if the user has permission to overwrite the output file.
3. Use `shutil.copyfile()` to safely copy the input file to the output file."
"def clip(
    ctx,
    files,
    output,
    bounds,
    like,
    driver,
    projection,
    overwrite,
    creation_options,
    with_complement,
):
    """"""Clips a raster using projected or geographic bounds.

    \\b
      $ rio clip input.tif output.tif --bounds xmin ymin xmax ymax
      $ rio clip input.tif output.tif --like template.tif

    The values of --bounds are presumed to be from the coordinate
    reference system of the input dataset unless the --geographic option
    is used, in which case the values may be longitude and latitude
    bounds. Either JSON, for example ""[west, south, east, north]"", or
    plain text ""west south east north"" representations of a bounding box
    are acceptable.

    If using --like, bounds will automatically be transformed to match the
    coordinate reference system of the input.

    It can also be combined to read bounds of a feature dataset using Fiona:

    \\b
      $ rio clip input.tif output.tif --bounds $(fio info features.shp --bounds)

    """"""
    from rasterio.warp import transform_bounds

    with ctx.obj['env']:

        output, files = resolve_inout(files=files, output=output, overwrite=overwrite)
        input = files[0]

        with rasterio.open(input) as src:
            if bounds:
                if projection == 'geographic':
                    bounds = transform_bounds(CRS.from_epsg(4326), src.crs, *bounds)
                if disjoint_bounds(bounds, src.bounds):
                    raise click.BadParameter('must overlap the extent of '
                                             'the input raster',
                                             param='--bounds',
                                             param_hint='--bounds')
            elif like:
                with rasterio.open(like) as template_ds:
                    bounds = template_ds.bounds
                    if template_ds.crs != src.crs:
                        bounds = transform_bounds(template_ds.crs, src.crs,
                                                  *bounds)

                    if disjoint_bounds(bounds, src.bounds):
                        raise click.BadParameter('must overlap the extent of '
                                                 'the input raster',
                                                 param='--like',
                                                 param_hint='--like')

            else:
                raise click.UsageError('--bounds or --like required')

            bounds_window = src.window(*bounds)

            if not with_complement:
                bounds_window = bounds_window.intersection(
                    Window(0, 0, src.width, src.height)
                )

            # Get the window with integer height
            # and width that contains the bounds window.
            out_window = bounds_window.round_lengths(op='ceil')

            height = int(out_window.height)
            width = int(out_window.width)

            out_kwargs = src.profile
            out_kwargs.update({
                'driver': driver,
                'height': height,
                'width': width,
                'transform': src.window_transform(out_window)})
            out_kwargs.update(**creation_options)

            if 'blockxsize' in out_kwargs and out_kwargs['blockxsize'] > width:
                del out_kwargs['blockxsize']
                logger.warning(""Blockxsize removed from creation options to accomodate small output width"")
            if 'blockysize' in out_kwargs and out_kwargs['blockysize'] > height:
                del out_kwargs['blockysize']
                logger.warning(""Blockysize removed from creation options to accomodate small output height"")

            with rasterio.open(output, ""w"", **out_kwargs) as out:
                out.write(
                    src.read(
                        window=out_window,
                        out_shape=(src.count, height, width),
                        boundless=True,
                    )
                )","1. Use `click.argument` decorator to validate user input.
2. Use `click.option` decorator to validate user input.
3. Use `click.command` decorator to validate user input."
"def warp(ctx, files, output, driver, like, dst_crs, dimensions, src_bounds,
         dst_bounds, res, resampling, src_nodata, dst_nodata, threads,
         check_invert_proj, overwrite, creation_options,
         target_aligned_pixels):
    """"""
    Warp a raster dataset.

    If a template raster is provided using the --like option, the
    coordinate reference system, affine transform, and dimensions of
    that raster will be used for the output.  In this case --dst-crs,
    --bounds, --res, and --dimensions options are not applicable and
    an exception will be raised.

    \\b
        $ rio warp input.tif output.tif --like template.tif

    The output coordinate reference system may be either a PROJ.4 or
    EPSG:nnnn string,

    \\b
        --dst-crs EPSG:4326
        --dst-crs '+proj=longlat +ellps=WGS84 +datum=WGS84'

    or a JSON text-encoded PROJ.4 object.

    \\b
        --dst-crs '{""proj"": ""utm"", ""zone"": 18, ...}'

    If --dimensions are provided, --res and --bounds are not applicable and an
    exception will be raised.
    Resolution is calculated based on the relationship between the
    raster bounds in the target coordinate system and the dimensions,
    and may produce rectangular rather than square pixels.

    \\b
        $ rio warp input.tif output.tif --dimensions 100 200 \\\\
        > --dst-crs EPSG:4326

    If --bounds are provided, --res is required if --dst-crs is provided
    (defaults to source raster resolution otherwise).

    \\b
        $ rio warp input.tif output.tif \\\\
        > --bounds -78 22 -76 24 --res 0.1 --dst-crs EPSG:4326

    """"""
    output, files = resolve_inout(
        files=files, output=output, overwrite=overwrite)

    resampling = Resampling[resampling]  # get integer code for method

    if not len(res):
        # Click sets this as an empty tuple if not provided
        res = None
    else:
        # Expand one value to two if needed
        res = (res[0], res[0]) if len(res) == 1 else res

    if target_aligned_pixels:
        if not res:
            raise click.BadParameter(
                '--target-aligned-pixels requires a specified resolution')
        if src_bounds or dst_bounds:
            raise click.BadParameter(
                '--target-aligned-pixels cannot be used with '
                '--src-bounds or --dst-bounds')

    # Check invalid parameter combinations
    if like:
        invalid_combos = (dimensions, dst_bounds, dst_crs, res)
        if any(p for p in invalid_combos if p is not None):
            raise click.BadParameter(
                ""--like cannot be used with any of --dimensions, --bounds, ""
                ""--dst-crs, or --res"")

    elif dimensions:
        invalid_combos = (dst_bounds, res)
        if any(p for p in invalid_combos if p is not None):
            raise click.BadParameter(
                ""--dimensions cannot be used with --bounds or --res"")

    with ctx.obj['env']:
        setenv(CHECK_WITH_INVERT_PROJ=check_invert_proj)

        with rasterio.open(files[0]) as src:
            l, b, r, t = src.bounds
            out_kwargs = src.profile
            out_kwargs.update(driver=driver)

            # Sort out the bounds options.
            if src_bounds and dst_bounds:
                raise click.BadParameter(
                    ""--src-bounds and destination --bounds may not be ""
                    ""specified simultaneously."")

            if like:
                with rasterio.open(like) as template_ds:
                    dst_crs = template_ds.crs
                    dst_transform = template_ds.transform
                    dst_height = template_ds.height
                    dst_width = template_ds.width

            elif dst_crs is not None:
                try:
                    dst_crs = CRS.from_string(dst_crs)
                except ValueError as err:
                    raise click.BadParameter(
                        str(err), param='dst_crs', param_hint='dst_crs')

                if dimensions:
                    # Calculate resolution appropriate for dimensions
                    # in target.
                    dst_width, dst_height = dimensions
                    bounds = src_bounds or src.bounds
                    try:
                        xmin, ymin, xmax, ymax = transform_bounds(
                            src.crs, dst_crs, *bounds)
                    except CRSError as err:
                        raise click.BadParameter(
                            str(err), param='dst_crs', param_hint='dst_crs')
                    dst_transform = Affine(
                        (xmax - xmin) / float(dst_width),
                        0, xmin, 0,
                        (ymin - ymax) / float(dst_height),
                        ymax
                    )

                elif src_bounds or dst_bounds:
                    if not res:
                        raise click.BadParameter(
                            ""Required when using --bounds."",
                            param='res', param_hint='res')

                    if src_bounds:
                        try:
                            xmin, ymin, xmax, ymax = transform_bounds(
                                src.crs, dst_crs, *src_bounds)
                        except CRSError as err:
                            raise click.BadParameter(
                                str(err), param='dst_crs',
                                param_hint='dst_crs')
                    else:
                        xmin, ymin, xmax, ymax = dst_bounds

                    dst_transform = Affine(res[0], 0, xmin, 0, -res[1], ymax)
                    dst_width = max(int(ceil((xmax - xmin) / res[0])), 1)
                    dst_height = max(int(ceil((ymax - ymin) / res[1])), 1)

                else:
                    try:
                        if src.transform.is_identity and src.gcps:
                            src_crs = src.gcps[1]
                            kwargs = {'gcps': src.gcps[0]}
                        else:
                            src_crs = src.crs
                            kwargs = src.bounds._asdict()
                        dst_transform, dst_width, dst_height = calcdt(
                            src_crs, dst_crs, src.width, src.height,
                            resolution=res, **kwargs)
                    except CRSError as err:
                        raise click.BadParameter(
                            str(err), param='dst_crs', param_hint='dst_crs')

            elif dimensions:
                # Same projection, different dimensions, calculate resolution.
                dst_crs = src.crs
                dst_width, dst_height = dimensions
                l, b, r, t = src_bounds or (l, b, r, t)
                dst_transform = Affine(
                    (r - l) / float(dst_width),
                    0, l, 0,
                    (b - t) / float(dst_height),
                    t
                )

            elif src_bounds or dst_bounds:
                # Same projection, different dimensions and possibly
                # different resolution.
                if not res:
                    res = (src.transform.a, -src.transform.e)

                dst_crs = src.crs
                xmin, ymin, xmax, ymax = (src_bounds or dst_bounds)
                dst_transform = Affine(res[0], 0, xmin, 0, -res[1], ymax)
                dst_width = max(int(ceil((xmax - xmin) / res[0])), 1)
                dst_height = max(int(ceil((ymax - ymin) / res[1])), 1)

            elif res:
                # Same projection, different resolution.
                dst_crs = src.crs
                dst_transform = Affine(res[0], 0, l, 0, -res[1], t)
                dst_width = max(int(ceil((r - l) / res[0])), 1)
                dst_height = max(int(ceil((t - b) / res[1])), 1)

            else:
                dst_crs = src.crs
                dst_transform = src.transform
                dst_width = src.width
                dst_height = src.height

            if target_aligned_pixels:
                dst_transform, dst_width, dst_height = aligned_target(dst_transform, dst_width, dst_height, res)

            # If src_nodata is not None, update the dst metadata NODATA
            # value to src_nodata (will be overridden by dst_nodata if it is not None
            if src_nodata is not None:
                # Update the dst nodata value
                out_kwargs.update(nodata=src_nodata)

            # Validate a manually set destination NODATA value
            # against the input datatype.
            if dst_nodata is not None:
                if src_nodata is None and src.meta['nodata'] is None:
                    raise click.BadParameter(
                        ""--src-nodata must be provided because dst-nodata is not None"")
                else:
                    # Update the dst nodata value
                    out_kwargs.update(nodata=dst_nodata)

            # When the bounds option is misused, extreme values of
            # destination width and height may result.
            if (dst_width < 0 or dst_height < 0 or
                    dst_width > MAX_OUTPUT_WIDTH or
                    dst_height > MAX_OUTPUT_HEIGHT):
                raise click.BadParameter(
                    ""Invalid output dimensions: {0}."".format(
                        (dst_width, dst_height)))

            out_kwargs.update(
                crs=dst_crs,
                transform=dst_transform,
                width=dst_width,
                height=dst_height
            )

            # Adjust block size if necessary.
            if ('blockxsize' in out_kwargs and
                    dst_width < out_kwargs['blockxsize']):
                del out_kwargs['blockxsize']
            if ('blockysize' in out_kwargs and
                    dst_height < out_kwargs['blockysize']):
                del out_kwargs['blockysize']

            out_kwargs.update(**creation_options)

            with rasterio.open(output, 'w', **out_kwargs) as dst:
                reproject(
                    source=rasterio.band(src, list(range(1, src.count + 1))),
                    destination=rasterio.band(
                        dst, list(range(1, src.count + 1))),
                    src_transform=src.transform,
                    src_crs=src.crs,
                    src_nodata=src_nodata,
                    dst_transform=out_kwargs['transform'],
                    dst_crs=out_kwargs['crs'],
                    dst_nodata=dst_nodata,
                    resampling=resampling,
                    num_threads=threads)","1. Use `click.argument` decorator to validate user input.
2. Use `click.option` decorator to add help information for each argument.
3. Use `click.command` decorator to group related arguments together."
"    def __init__(self, session=None, aws_unsigned=False, profile_name=None,
                 session_class=AWSSession, **options):
        """"""Create a new GDAL/AWS environment.

        Note: this class is a context manager. GDAL isn't configured
        until the context is entered via `with rasterio.Env():`

        Parameters
        ----------
        session : optional
            A Session object.
        aws_unsigned : bool, optional
            Do not sign cloud requests.
        profile_name : str, optional
            A shared credentials profile name, as per boto3.
        session_class : Session, optional
            A sub-class of Session.
        **options : optional
            A mapping of GDAL configuration options, e.g.,
            `CPL_DEBUG=True, CHECK_WITH_INVERT_PROJ=False`.

        Returns
        -------
        Env

        Notes
        -----
        We raise EnvError if the GDAL config options
        AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY are given. AWS
        credentials are handled exclusively by boto3.

        Examples
        --------

        >>> with Env(CPL_DEBUG=True, CPL_CURL_VERBOSE=True):
        ...     with rasterio.open(""https://example.com/a.tif"") as src:
        ...         print(src.profile)

        For access to secured cloud resources, a Rasterio Session or a
        foreign session object may be passed to the constructor.

        >>> import boto3
        >>> from rasterio.session import AWSSession
        >>> boto3_session = boto3.Session(...)
        >>> with Env(AWSSession(boto3_session)):
        ...     with rasterio.open(""s3://mybucket/a.tif"") as src:
        ...         print(src.profile)

        """"""
        aws_access_key_id = options.pop('aws_access_key_id', None)
        # Before 1.0, Rasterio only supported AWS. We will special
        # case AWS in 1.0.x. TODO: warn deprecation in 1.1.
        if aws_access_key_id:
            warnings.warn(
                ""Passing abstract session keyword arguments is deprecated. ""
                ""Pass a Rasterio AWSSession object instead."",
                RasterioDeprecationWarning
            )

        aws_secret_access_key = options.pop('aws_secret_access_key', None)
        aws_session_token = options.pop('aws_session_token', None)
        region_name = options.pop('region_name', None)

        if ('AWS_ACCESS_KEY_ID' in options or
                'AWS_SECRET_ACCESS_KEY' in options):
            raise EnvError(
                ""GDAL's AWS config options can not be directly set. ""
                ""AWS credentials are handled exclusively by boto3."")

        if session:
            # Passing a session via keyword argument is the canonical
            # way to configure access to secured cloud resources.
            if not isinstance(session, Session):
                warnings.warn(
                    ""Passing a boto3 session is deprecated. Pass a Rasterio ""
                    ""AWSSession object instead."",
                    RasterioDeprecationWarning
                )
                session = AWSSession(session=session)
            self.session = session
        elif aws_access_key_id or profile_name or aws_unsigned:
            self.session = AWSSession(
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                aws_session_token=aws_session_token,
                region_name=region_name,
                profile_name=profile_name,
                aws_unsigned=aws_unsigned)
        elif 'AWS_ACCESS_KEY_ID' in os.environ and 'AWS_SECRET_ACCESS_KEY' in os.environ:
            self.session = AWSSession()
        else:
            self.session = DummySession()

        self.options = options.copy()
        self.context_options = {}","1. Use a secure session object instead of passing AWS credentials directly to the function.
2. Use boto3 instead of passing abstract session keyword arguments.
3. Handle AWS credentials exclusively by boto3."
"    def __getstate__(self):
        return self.wkt","1. Use `__setstate__` to sanitize user input before storing it in the object.
2. Use `__getstate__` to return a secure representation of the object that does not contain sensitive information.
3. Use `pickle.dumps()` and `pickle.loads()` to serialize and deserialize the object, respectively."
"    def __setstate__(self, state):
        self._crs = _CRS.from_wkt(state)","1. Use `validate_crs` to validate the input `crs` before setting it.
2. Sanitize the input `crs` to prevent XSS attacks.
3. Use `pickle.dumps` to serialize the `crs` instead of `str`."
"    def __init__(self, initialdata=None, **kwargs):
        """"""Make a CRS from a PROJ dict or mapping

        Parameters
        ----------
        initialdata : mapping, optional
            A dictionary or other mapping
        kwargs : mapping, optional
            Another mapping. Will be overlaid on the initialdata.

        Returns
        -------
        CRS

        """"""
        self._wkt = None
        self._data = None
        self._crs = None

        if initialdata or kwargs:

            data = dict(initialdata or {})
            data.update(**kwargs)
            data = {k: v for k, v in data.items() if k in all_proj_keys}

            # always use lowercase 'epsg'.
            if 'init' in data:
                data['init'] = data['init'].replace('EPSG:', 'epsg:')

            proj = ' '.join(['+{}={}'.format(key, val) for key, val in data.items()])
            self._crs = _CRS.from_proj4(proj)

        else:
            self._crs = _CRS()","1. Use `functools.lru_cache` to cache the CRS object.
2. Validate the input data to ensure that it is a valid PROJ dict or mapping.
3. Sanitize the input data to remove any potential malicious code."
"    def __enter__(self):
        log.debug(""Entering env context: %r"", self)
        if local._env is None:
            log.debug(""Starting outermost env"")
            self._has_parent_env = False

            # See note directly above where _discovered_options is globally
            # defined.  This MUST happen before calling 'defenv()'.
            local._discovered_options = {}
            # Don't want to reinstate the ""RASTERIO_ENV"" option.
            probe_env = {k for k in self.options.keys() if k != ""RASTERIO_ENV""}
            for key in probe_env:
                val = get_gdal_config(key, normalize=False)
                if val is not None:
                    local._discovered_options[key] = val

            defenv(**self.options)
            self.context_options = {}
        else:
            self._has_parent_env = True
            self.context_options = getenv()
            setenv(**self.options)

        self.credentialize()

        log.debug(""Entered env context: %r"", self)
        return self","1. Use `getenv()` instead of `os.environ` to get environment variables.
2. Use `defenv()` to set environment variables, and `getenv()` to get them.
3. Use `credentialize()` to set environment variables for credentials."
"    def __exit__(self, exc_type=None, exc_val=None, exc_tb=None):
        log.debug(""Exiting env context: %r"", self)
        delenv()
        if self._has_parent_env:
            defenv()
            setenv(**self.context_options)
        else:
            log.debug(""Exiting outermost env"")
            # See note directly above where _discovered_options is globally
            # defined.
            while local._discovered_options:
                key, val = local._discovered_options.popitem()
                set_gdal_config(key, val, normalize=False)
            local._discovered_options = None
        log.debug(""Exited env context: %r"", self)","1. Use `contextlib.nested` instead of `contextlib.contextmanager` to avoid creating a new environment for each nested context.
2. Use `contextlib.closing` to ensure that the file is closed after the context exits.
3. Use `os.fchmod` to set the file mode to 0600 to make the file only readable by the owner."
"def plotting_extent(source, transform=None):
    """"""Returns an extent in the format needed
     for matplotlib's imshow (left, right, bottom, top)
     instead of rasterio's bounds (left, bottom, top, right)

    Parameters
    ----------
    source : array or dataset object opened in 'r' mode
        input data
    transform: Affine, required if source is array
        Defines the affine transform if source is an array

    Returns
    -------
    tuple of float
        left, right, bottom, top
    """"""
    if hasattr(source, 'bounds'):
        extent = (source.bounds.left, source.bounds.right,
                  source.bounds.bottom, source.bounds.top)
    elif not transform:
        raise ValueError(
            ""transform is required if source is an array"")
    else:
        transform = guard_transform(transform)
        rows, cols = source.shape[0:2]
        left, top = transform * (0, 0)
        right, bottom = transform * (cols, rows)
        extent = (left, right, bottom, top)

    return extent","1. Use `contextlib.closing` to ensure that the dataset is closed after use.
2. Check if the `source` object has the `bounds` attribute before using it.
3. Use `guard_transform` to validate the `transform` argument."
"def xy(transform, rows, cols, offset='center'):
    """"""Returns the x and y coordinates of pixels at `rows` and `cols`.
    The pixel's center is returned by default, but a corner can be returned
    by setting `offset` to one of `ul, ur, ll, lr`.

    Parameters
    ----------
    transform : affine.Affine
        Transformation from pixel coordinates to coordinate reference system.
    rows : list or int
        Pixel rows.
    cols : list or int
        Pixel columns.
    offset : str, optional
        Determines if the returned coordinates are for the center of the
        pixel or for a corner.

    Returns
    -------
    xs : list
        x coordinates in coordinate reference system
    ys : list
        y coordinates in coordinate reference system
    """"""

    single_col = False
    single_row = False
    if not isinstance(cols, collections.Iterable):
        cols = [cols]
        single_col = True
    if not isinstance(rows, collections.Iterable):
        rows = [rows]
        single_row = True

    if offset == 'center':
        coff, roff = (0.5, 0.5)
    elif offset == 'ul':
        coff, roff = (0, 0)
    elif offset == 'ur':
        coff, roff = (1, 0)
    elif offset == 'll':
        coff, roff = (0, 1)
    elif offset == 'lr':
        coff, roff = (1, 1)
    else:
        raise ValueError(""Invalid offset"")

    xs = []
    ys = []
    for col, row in zip(cols, rows):
        x, y = transform * transform.translation(coff, roff) * (col, row)
        xs.append(x)
        ys.append(y)

    if single_row:
        ys = ys[0]
    if single_col:
        xs = xs[0]

    return xs, ys","1. Use `typing` to annotate the function arguments and return values. This will help catch errors early and make the code more readable.
2. Validate the input arguments to ensure that they are of the correct type and within the expected range. This will help protect the code from malicious attacks.
3. Use `assert` statements to check for errors during runtime. This will help catch errors that might not be caught by the type annotations or validation checks."
"def reproject(
        source,
        destination,
        src_transform=None,
        src_crs=None,
        src_nodata=None,
        dst_transform=None,
        dst_crs=None,
        dst_nodata=None,
        resampling=Resampling.nearest,
        **kwargs):
    """"""
    Reproject a source raster to a destination raster.

    If the source and destination are ndarrays, coordinate reference
    system definitions and affine transformation parameters are required
    for reprojection.

    If the source and destination are rasterio Bands, shorthand for
    bands of datasets on disk, the coordinate reference systems and
    transforms will be read from the appropriate datasets.

    Parameters
    ------------
    source: ndarray or rasterio Band
        Source raster.
    destination: ndarray or rasterio Band
        Target raster.
    src_transform: affine transform object, optional
        Source affine transformation.  Required if source and destination
        are ndarrays.  Will be derived from source if it is a rasterio Band.
    src_crs: dict, optional
        Source coordinate reference system, in rasterio dict format.
        Required if source and destination are ndarrays.
        Will be derived from source if it is a rasterio Band.
        Example: {'init': 'EPSG:4326'}
    src_nodata: int or float, optional
        The source nodata value.  Pixels with this value will not be used
        for interpolation.  If not set, it will be default to the
        nodata value of the source image if a masked ndarray or rasterio band,
        if available.  Must be provided if dst_nodata is not None.
    dst_transform: affine transform object, optional
        Target affine transformation.  Required if source and destination
        are ndarrays.  Will be derived from target if it is a rasterio Band.
    dst_crs: dict, optional
        Target coordinate reference system.  Required if source and destination
        are ndarrays.  Will be derived from target if it is a rasterio Band.
    dst_nodata: int or float, optional
        The nodata value used to initialize the destination; it will remain
        in all areas not covered by the reprojected source.  Defaults to the
        nodata value of the destination image (if set), the value of
        src_nodata, or 0 (GDAL default).
    resampling: int
        Resampling method to use.  One of the following:
            Resampling.nearest,
            Resampling.bilinear,
            Resampling.cubic,
            Resampling.cubic_spline,
            Resampling.lanczos,
            Resampling.average,
            Resampling.mode
    kwargs:  dict, optional
        Additional arguments passed to transformation function.

    Returns
    ---------
    out: None
        Output is written to destination.
    """"""
    # Resampling guard.
    try:
        Resampling(resampling)
        if resampling == 7:
            raise ValueError
    except ValueError:
        raise ValueError(
            ""resampling must be one of: {0}"".format("", "".join(
                ['Resampling.{0}'.format(k) for k in
                 Resampling.__members__.keys() if k != 'gauss'])))

    if src_transform:
        src_transform = guard_transform(src_transform).to_gdal()
    if dst_transform:
        dst_transform = guard_transform(dst_transform).to_gdal()

    _reproject(
        source,
        destination,
        src_transform,
        src_crs,
        src_nodata,
        dst_transform,
        dst_crs,
        dst_nodata,
        resampling,
        **kwargs)","1. Use `guard_transform` to validate the `src_transform` and `dst_transform` parameters.
2. Use `Resampling` to validate the `resampling` parameter.
3. Use `guard_transform` to convert `src_transform` and `dst_transform` to `gdal` format."
"def reproject(
        source,
        destination,
        src_transform=None,
        src_crs=None,
        src_nodata=None,
        dst_transform=None,
        dst_crs=None,
        dst_nodata=None,
        resampling=Resampling.nearest,
        **kwargs):
    """"""
    Reproject a source raster to a destination raster.

    If the source and destination are ndarrays, coordinate reference
    system definitions and affine transformation parameters are required
    for reprojection.

    If the source and destination are rasterio Bands, shorthand for
    bands of datasets on disk, the coordinate reference systems and
    transforms will be read from the appropriate datasets.

    Parameters
    ------------
    source: ndarray or rasterio Band
        Source raster.
    destination: ndarray or rasterio Band
        Target raster.
    src_transform: affine transform object, optional
        Source affine transformation.  Required if source and destination
        are ndarrays.  Will be derived from source if it is a rasterio Band.
    src_crs: dict, optional
        Source coordinate reference system, in rasterio dict format.
        Required if source and destination are ndarrays.
        Will be derived from source if it is a rasterio Band.
        Example: {'init': 'EPSG:4326'}
    src_nodata: int or float, optional
        The source nodata value.  Pixels with this value will not be used
        for interpolation.  If not set, it will be default to the
        nodata value of the source image if a masked ndarray or rasterio band,
        if available.  Must be provided if dst_nodata is not None.
    dst_transform: affine transform object, optional
        Target affine transformation.  Required if source and destination
        are ndarrays.  Will be derived from target if it is a rasterio Band.
    dst_crs: dict, optional
        Target coordinate reference system.  Required if source and destination
        are ndarrays.  Will be derived from target if it is a rasterio Band.
    dst_nodata: int or float, optional
        The nodata value used to initialize the destination; it will remain
        in all areas not covered by the reprojected source.  Defaults to the
        nodata value of the destination image (if set), the value of
        src_nodata, or 0 (GDAL default).
    resampling: int
        Resampling method to use.  One of the following:
            Resampling.nearest,
            Resampling.bilinear,
            Resampling.cubic,
            Resampling.cubic_spline,
            Resampling.lanczos,
            Resampling.average,
            Resampling.mode
    kwargs:  dict, optional
        Additional arguments passed to transformation function.

    Returns
    ---------
    out: None
        Output is written to destination.
    """"""
    # Resampling guard.
    try:
        Resampling(resampling)
        if resampling == 7:
            raise ValueError
    except ValueError:
        raise ValueError(
            ""resampling must be one of: {0}"".format("", "".join(
                ['Resampling.{0}'.format(k) for k in
                 Resampling.__members__.keys() if k != 'gauss'])))

    if src_transform:
        src_transform = guard_transform(src_transform).to_gdal()
    if dst_transform:
        dst_transform = guard_transform(dst_transform).to_gdal()

    # Passing None can cause segfault, use empty dict
    if src_crs is None:
        src_crs = {}
    if dst_crs is None:
        dst_crs = {}

    _reproject(
        source,
        destination,
        src_transform,
        src_crs,
        src_nodata,
        dst_transform,
        dst_crs,
        dst_nodata,
        resampling,
        **kwargs)","1. Use `guard_transform` to validate the `src_transform` and `dst_transform` parameters.
2. Use `guard_transform.to_gdal()` to convert the `src_transform` and `dst_transform` parameters to GDAL format.
3. Use `_reproject()` to perform the reprojection operation."
"def merge(ctx, files, driver):
    """"""Copy valid pixels from input files to an output file.

    All files must have the same shape, number of bands, and data type.

    Input files are merged in their listed order using a reverse
    painter's algorithm.
    """"""
    import numpy as np

    verbosity = (ctx.obj and ctx.obj.get('verbosity')) or 1
    logger = logging.getLogger('rio')
    try:
        with rasterio.drivers(CPL_DEBUG=verbosity>2):
            output = files[-1]
            files = files[:-1]

            with rasterio.open(files[0]) as first:
                kwargs = first.meta
                kwargs['transform'] = kwargs.pop('affine')
                dest = np.empty((first.count,) + first.shape, 
                    dtype=first.dtypes[0])

            if os.path.exists(output):
                dst = rasterio.open(output, 'r+')
                nodataval = dst.nodatavals[0]
            else:
                kwargs['driver'] == driver
                dst = rasterio.open(output, 'w', **kwargs)
                nodataval = first.nodatavals[0]

            dest.fill(nodataval)

            for fname in reversed(files):
                with rasterio.open(fname) as src:
                    data = src.read()
                    np.copyto(dest, data,
                        where=np.logical_and(
                        dest==nodataval, data.mask==False))

            if dst.mode == 'r+':
                data = dst.read()
                np.copyto(dest, data,
                    where=np.logical_and(
                    dest==nodataval, data.mask==False))

            dst.write(dest)
            dst.close()

        sys.exit(0)
    except Exception:
        logger.exception(""Failed. Exception caught"")
        sys.exit(1)","1. Use `os.path.exists()` to check if the output file exists before opening it.
2. Use `rasterio.open()` with mode `'w'` to create a new output file, and `'r+'` to open an existing file for writing.
3. Close the output file after writing to it."
"def tastes_like_gdal(t):
    return t[2] == t[4] == 0.0 and t[1] > 0 and t[5] < 0","1. Use `assert` statements to check for invalid inputs.
2. Sanitize user input to prevent injection attacks.
3. Use strong cryptographic hashing functions to protect sensitive data."
"def guard_transform(transform):
    """"""Return an Affine transformation instance""""""
    if not isinstance(transform, Affine):
        if tastes_like_gdal(transform):
            warnings.warn(
                ""GDAL-style transforms are deprecated and will not ""
                ""be supported in Rasterio 1.0."",
                FutureWarning,
                stacklevel=2)
            transform = Affine.from_gdal(*transform)
        else:
            transform = Affine(*transform)
    a, e = transform.a, transform.e
    if a == 0.0 or e == 0.0:
        raise ValueError(
            ""Transform has invalid coefficients a, e: (%f, %f)"" % (
                transform.a, transform.e))
    return transform","1. Use `isinstance()` to check if the input is an `Affine` object.
2. If the input is not an `Affine` object, check if it is a `GDAL`-style transform and convert it to an `Affine` object.
3. Raise a `ValueError` if the `a` or `e` coefficients of the `Affine` object are equal to 0.0."
"    def init_cgroups():
        # Track metrics for the roll-up cgroup and for the agent cgroup
        try:
            CGroupsTelemetry.track_cgroup(CGroups.for_extension(""""))
            CGroupsTelemetry.track_agent()
        except Exception as e:
            logger.error(""monitor: Exception tracking wrapper and agent: {0} [{1}]"", e, traceback.format_exc())","1. Use `try-except` to catch errors and log them.
2. Use `CGroups.for_extension("""")` to get the roll-up cgroup.
3. Use `CGroupsTelemetry.track_agent()` to track the agent cgroup."
"    def send_cgroup_telemetry(self):
        if self.last_cgroup_telemetry is None:
            self.last_cgroup_telemetry = datetime.datetime.utcnow()

        if datetime.datetime.utcnow() >= (self.last_telemetry_heartbeat + MonitorHandler.CGROUP_TELEMETRY_PERIOD):
            try:
                for cgroup_name, metrics in CGroupsTelemetry.collect_all_tracked().items():
                    for metric_group, metric_name, value in metrics:
                        if value > 0:
                            report_metric(metric_group, metric_name, cgroup_name, value)
            except Exception as e:
                logger.warn(""Failed to collect performance metrics: {0} [{1}]"", e, traceback.format_exc())

            # Look for extension cgroups we're not already tracking and track them
            try:
                CGroupsTelemetry.update_tracked(self.protocol.client.get_current_handlers())
            except Exception as e:
                logger.warn(""Monitor: updating tracked extensions raised {0}: {1}"", e, traceback.format_exc())

            self.last_cgroup_telemetry = datetime.datetime.utcnow()","1. Use `try/except` to catch errors and log them.
2. Use `report_metric` to report metrics.
3. Use `update_tracked` to update tracked extensions."
"    def deploy_ssh_pubkey(self, username, pubkey):
        """"""
        Deploy authorized_key
        """"""
        path, thumbprint, value = pubkey
        if path is None:
            raise OSUtilError(""Publich key path is None"")

        crytputil = CryptUtil(conf.get_openssl_cmd())

        path = self._norm_path(path)
        dir_path = os.path.dirname(path)
        fileutil.mkdir(dir_path, mode=0o700, owner=username)
        if value is not None:
            if not value.startswith(""ssh-""):
                raise OSUtilError(""Bad public key: {0}"".format(value))
            fileutil.write_file(path, value)
        elif thumbprint is not None:
            lib_dir = conf.get_lib_dir()
            crt_path = os.path.join(lib_dir, thumbprint + '.crt')
            if not os.path.isfile(crt_path):
                raise OSUtilError(""Can't find {0}.crt"".format(thumbprint))
            pub_path = os.path.join(lib_dir, thumbprint + '.pub')
            pub = crytputil.get_pubkey_from_crt(crt_path)
            fileutil.write_file(pub_path, pub)
            self.set_selinux_context(pub_path, 
                                     'unconfined_u:object_r:ssh_home_t:s0')
            self.openssl_to_openssh(pub_path, path)
            fileutil.chmod(pub_path, 0o600)
        else:
            raise OSUtilError(""SSH public key Fingerprint and Value are None"")

        self.set_selinux_context(path, 'unconfined_u:object_r:ssh_home_t:s0')
        fileutil.chowner(path, username)
        fileutil.chmod(path, 0o644)","1. Use `cryptutil.get_pubkey_from_crt()` to get the public key from the certificate instead of hardcoding it.
2. Use `os.chmod()` to set the permissions of the public key file to 0o600.
3. Use `os.chown()` to set the owner of the public key file to the user who owns the home directory."
"    def set_selinux_context(self, path, con):
        """"""
        Calls shell 'chcon' with 'path' and 'con' context.
        Returns exit result.
        """"""
        if self.is_selinux_system():
            return shellutil.run('chcon ' + con + ' ' + path)","1. Use `subprocess.run()` instead of `shellutil.run()` to avoid shell injection.
2. Use `fchmodat()` instead of `os.chmod()` to set file permissions securely.
3. Use `selinux.setfilecon()` to set SELinux context securely."
"def chowner(path, owner):
    owner_info = pwd.getpwnam(owner)
    os.chown(path, owner_info[2], owner_info[3])","1. Use `os.fchown()` instead of `os.chown()` to avoid leaking the uid and gid of the current process.
2. Use `pwd.getpwuid()` instead of `pwd.getpwnam()` to avoid leaking the username of the current process.
3. Use `os.chmod()` to set the permissions of the file instead of relying on the permissions of the owner."
"def chmod(path, mode):
    os.chmod(path, mode)","1. Use `os.fchmod` instead of `os.chmod` to avoid changing the permissions of the parent directory.
2. Use the `stat` module to check if the user has the necessary permissions to change the file's permissions.
3. Use the `chmod` function with the `os.O_NOFOLLOW` flag to prevent the file from being followed by a symbolic link."
"def _common_defaults(is_hyperband: bool) -> (Set[str], dict, dict):
    mandatory = set()

    default_options = {
        'random_seed': 31415927,
        'opt_skip_init_length': 150,
        'opt_skip_period': 1,
        'profiler': False,
        'opt_maxiter': 50,
        'opt_nstarts': 2,
        'opt_warmstart': False,
        'opt_verbose': False,
        'opt_debug_writer': False,
        'num_fantasy_samples': 20,
        'num_init_random': DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,
        'num_init_candidates': DEFAULT_NUM_INITIAL_CANDIDATES,
        'initial_scoring': DEFAULT_INITIAL_SCORING,
        'first_is_default': True,
        'debug_log': False}
    if is_hyperband:
        default_options['opt_skip_num_max_resource'] = False
        default_options['gp_resource_kernel'] = 'matern52'
        default_options['resource_acq'] = 'bohb'
        default_options['num_init_random'] = 10

    constraints = {
        'random_seed': Integer(),
        'opt_skip_init_length': Integer(0, None),
        'opt_skip_period': Integer(1, None),
        'profiler': Boolean(),
        'opt_maxiter': Integer(1, None),
        'opt_nstarts': Integer(1, None),
        'opt_warmstart': Boolean(),
        'opt_verbose': Boolean(),
        'opt_debug_writer': Boolean(),
        'num_fantasy_samples': Integer(1, None),
        'num_init_random': Integer(1, None),
        'num_init_candidates': Integer(5, None),
        'initial_scoring': Categorical(
            choices=tuple(SUPPORTED_INITIAL_SCORING)),
        'first_is_default': Boolean(),
        'debug_log': Boolean()}
    if is_hyperband:
        constraints['opt_skip_num_max_resource'] = Boolean()
        constraints['gp_resource_kernel'] = Categorical(choices=(
            'exp-decay-sum', 'exp-decay-combined', 'exp-decay-delta1',
            'matern52', 'matern52-res-warp'))
        constraints['resource_acq'] = Categorical(
            choices=('bohb', 'first'))

    return mandatory, default_options, constraints","1. Use `secrets.token_urlsafe()` to generate a random string for the `random_seed`.
2. Use `os.makedirs()` to create the directory for the `debug_log` file if it does not exist.
3. Use `logging.basicConfig()` to configure the logging format and level."
"    def __init__(self, **kwargs):
        _gp_searcher = kwargs.get('_gp_searcher')
        if _gp_searcher is None:
            _kwargs = check_and_merge_defaults(
                kwargs, *gp_fifo_searcher_defaults(),
                dict_name='search_options')
            _gp_searcher = gp_fifo_searcher_factory(**_kwargs)
        super().__init__(
            _gp_searcher.hp_ranges.config_space,
            reward_attribute=kwargs.get('reward_attribute'))
        self.gp_searcher = _gp_searcher
        # This lock protects gp_searcher. We are not using self.LOCK, this
        # can lead to deadlocks when superclass methods are called
        self._gp_lock = mp.Lock()","1. Use `self.LOCK` to protect `gp_searcher` instead of `self._gp_lock`.
2. Use `check_and_merge_defaults` to check and merge default arguments.
3. Use `gp_fifo_searcher_factory` to create `gp_searcher`."
"    def fit_summary(self, verbosity=3):
        """"""
            Output summary of information about models produced during `fit()`.
            May create various generated summary plots and store them in folder: `Predictor.output_directory`.

            Parameters
            ----------
            verbosity : int, default = 3
                Controls how detailed of a summary to ouput.
                Set <= 0 for no output printing, 1 to print just high-level summary,
                2 to print summary and create plots, >= 3 to print all information produced during fit().

            Returns
            -------
            Dict containing various detailed information. We do not recommend directly printing this dict as it may be very large.
        """"""
        hpo_used = len(self._trainer.hpo_results) > 0
        model_typenames = {key: self._trainer.model_types[key].__name__ for key in self._trainer.model_types}
        model_innertypenames = {key: self._trainer.model_types_inner[key].__name__ for key in self._trainer.model_types if key in self._trainer.model_types_inner}
        MODEL_STR = 'Model'
        ENSEMBLE_STR = 'Ensemble'
        for model in model_typenames:
            if (model in model_innertypenames) and (ENSEMBLE_STR not in model_innertypenames[model]) and (ENSEMBLE_STR in model_typenames[model]):
                new_model_typename = model_typenames[model] + ""_"" + model_innertypenames[model]
                if new_model_typename.endswith(MODEL_STR):
                    new_model_typename = new_model_typename[:-len(MODEL_STR)]
                model_typenames[model] = new_model_typename

        unique_model_types = set(model_typenames.values())  # no more class info
        # all fit() information that is returned:
        results = {
            'model_types': model_typenames,  # dict with key = model-name, value = type of model (class-name)
            'model_performance': self._trainer.get_model_attributes_dict('val_score'),  # dict with key = model-name, value = validation performance
            'model_best': self._trainer.model_best,  # the name of the best model (on validation data)
            'model_paths': self._trainer.model_paths,  # dict with key = model-name, value = path to model file
            'model_fit_times': self._trainer.get_model_attributes_dict('fit_time'),
            'model_pred_times': self._trainer.get_model_attributes_dict('predict_time'),
            'num_bagging_folds': self._trainer.kfolds,
            'stack_ensemble_levels': self._trainer.stack_ensemble_levels,
            'feature_prune': self._trainer.feature_prune,
            'hyperparameter_tune': hpo_used,
            'hyperparameters_userspecified': self._trainer.hyperparameters,
        }
        if self.problem_type != REGRESSION:
            results['num_classes'] = self._trainer.num_classes
        if hpo_used:
            results['hpo_results'] = self._trainer.hpo_results
        # get dict mapping model name to final hyperparameter values for each model:
        model_hyperparams = {}
        for model_name in self._trainer.get_model_names_all():
            model_obj = self._trainer.load_model(model_name)
            model_hyperparams[model_name] = model_obj.params
        results['model_hyperparams'] = model_hyperparams

        if verbosity > 0:  # print stuff
            print(""*** Summary of fit() ***"")
            print(""Estimated performance of each model:"")
            results['leaderboard'] = self._learner.leaderboard(silent=False)
            # self._summarize('model_performance', 'Validation performance of individual models', results)
            #  self._summarize('model_best', 'Best model (based on validation performance)', results)
            # self._summarize('hyperparameter_tune', 'Hyperparameter-tuning used', results)
            print(""Number of models trained: %s"" % len(results['model_performance']))
            print(""Types of models trained:"")
            print(unique_model_types)
            num_fold_str = """"
            bagging_used = results['num_bagging_folds'] > 0
            if bagging_used:
                num_fold_str = f"" (with {results['num_bagging_folds']} folds)""
            print(""Bagging used: %s %s"" % (bagging_used, num_fold_str))
            num_stack_str = """"
            stacking_used = results['stack_ensemble_levels'] > 0
            if stacking_used:
                num_stack_str = f"" (with {results['stack_ensemble_levels']} levels)""
            print(""Stack-ensembling used: %s %s"" % (stacking_used, num_stack_str))
            hpo_str = """"
            if hpo_used and verbosity <= 2:
                hpo_str = "" (call fit_summary() with verbosity >= 3 to see detailed HPO info)""
            print(""Hyperparameter-tuning used: %s %s"" % (hpo_used, hpo_str))
            # TODO: uncomment once feature_prune is functional:  self._summarize('feature_prune', 'feature-selection used', results)
            print(""User-specified hyperparameters:"")
            print(results['hyperparameters_userspecified'])
            print(""Feature Metadata (Processed):"")
            print(""(raw dtype, special dtypes):"")
            print(self.feature_metadata)
        if verbosity > 1:  # create plots
            plot_tabular_models(results, output_directory=self.output_directory,
                                save_file=""SummaryOfModels.html"",
                                plot_title=""Models produced during fit()"")
            if hpo_used:
                for model_type in results['hpo_results']:
                    if 'trial_info' in results['hpo_results'][model_type]:
                        plot_summary_of_models(
                            results['hpo_results'][model_type],
                            output_directory=self.output_directory, save_file=model_type + ""_HPOmodelsummary.html"",
                            plot_title=f""Models produced during {model_type} HPO"")
                        plot_performance_vs_trials(
                            results['hpo_results'][model_type],
                            output_directory=self.output_directory, save_file=model_type + ""_HPOperformanceVStrials.png"",
                            plot_title=f""HPO trials for {model_type} models"")
        if verbosity > 2:  # print detailed information
            if hpo_used:
                hpo_results = results['hpo_results']
                print(""*** Details of Hyperparameter optimization ***"")
                for model_type in hpo_results:
                    hpo_model = hpo_results[model_type]
                    print(""HPO for %s model:  Num. configurations tried = %s, Time spent = %s, Search strategy = %s""
                          % (model_type, len(hpo_model['trial_info']), hpo_model['total_time'], hpo_model['search_strategy']))
                    print(""Best hyperparameter-configuration (validation-performance: %s = %s):""
                          % (self.eval_metric, hpo_model['validation_performance']))
                    print(hpo_model['best_config'])
            """"""
            if bagging_used:
                pass # TODO: print detailed bagging info
            if stacking_used:
                pass # TODO: print detailed stacking info, like how much it improves validation performance
            if results['feature_prune']:
                pass # TODO: print detailed feature-selection info once feature-selection is functional.
            """"""
        if verbosity > 0:
            print(""*** End of fit() summary ***"")
        return results","1. Use `secure_filename` to sanitize the input filenames.
2. Use `os.makedirs` to create directories with the correct permissions.
3. Use `json.dumps` to serialize the data to JSON."
"    def __init__(self, learner):
        """""" Creates TabularPredictor object.
            You should not construct a TabularPredictor yourself, it is only intended to be produced during fit().

            Parameters
            ----------
            learner : `AbstractLearner` object
                Object that implements the `AbstractLearner` APIs.

            To access any learner method `func()` from this Predictor, use: `predictor._learner.func()`.
            To access any trainer method `func()` from this `Predictor`, use: `predictor._trainer.func()`.
        """"""
        self._learner: Learner = learner  # Learner object
        self._learner.persist_trainer(low_memory=True)
        self._trainer: AbstractTrainer = self._learner.load_trainer()  # Trainer object
        self.output_directory = self._learner.path
        self.problem_type = self._learner.problem_type
        self.eval_metric = self._learner.eval_metric
        self.label_column = self._learner.label
        self.feature_types = self._trainer.feature_types_metadata
        self.class_labels = self._learner.class_labels
        self.class_labels_internal = self._learner.label_cleaner.ordered_class_labels_transformed
        self.class_labels_internal_map = self._learner.label_cleaner.inv_map","1. Use `type hints` to specify the types of arguments and return values of functions.
2. Use `f-strings` to format strings instead of concatenation.
3. Use `black` to format the code consistently."
"    def fit_summary(self, verbosity=3):
        """"""
            Output summary of information about models produced during `fit()`.
            May create various generated summary plots and store them in folder: `Predictor.output_directory`.

            Parameters
            ----------
            verbosity : int, default = 3
                Controls how detailed of a summary to ouput.
                Set <= 0 for no output printing, 1 to print just high-level summary,
                2 to print summary and create plots, >= 3 to print all information produced during fit().

            Returns
            -------
            Dict containing various detailed information. We do not recommend directly printing this dict as it may be very large.
        """"""
        hpo_used = len(self._trainer.hpo_results) > 0
        model_typenames = {key: self._trainer.model_types[key].__name__ for key in self._trainer.model_types}
        model_innertypenames = {key: self._trainer.model_types_inner[key].__name__ for key in self._trainer.model_types if key in self._trainer.model_types_inner}
        MODEL_STR = 'Model'
        ENSEMBLE_STR = 'Ensemble'
        for model in model_typenames:
            if (model in model_innertypenames) and (ENSEMBLE_STR not in model_innertypenames[model]) and (ENSEMBLE_STR in model_typenames[model]):
                new_model_typename = model_typenames[model] + ""_"" + model_innertypenames[model]
                if new_model_typename.endswith(MODEL_STR):
                    new_model_typename = new_model_typename[:-len(MODEL_STR)]
                model_typenames[model] = new_model_typename

        unique_model_types = set(model_typenames.values())  # no more class info
        # all fit() information that is returned:
        results = {
            'model_types': model_typenames,  # dict with key = model-name, value = type of model (class-name)
            'model_performance': self._trainer.get_model_attributes_dict('val_score'),  # dict with key = model-name, value = validation performance
            'model_best': self._trainer.model_best,  # the name of the best model (on validation data)
            'model_paths': self._trainer.model_paths,  # dict with key = model-name, value = path to model file
            'model_fit_times': self._trainer.get_model_attributes_dict('fit_time'),
            'model_pred_times': self._trainer.get_model_attributes_dict('predict_time'),
            'num_bagging_folds': self._trainer.kfolds,
            'stack_ensemble_levels': self._trainer.stack_ensemble_levels,
            'feature_prune': self._trainer.feature_prune,
            'hyperparameter_tune': hpo_used,
            'hyperparameters_userspecified': self._trainer.hyperparameters,
        }
        if self.problem_type != REGRESSION:
            results['num_classes'] = self._trainer.num_classes
        if hpo_used:
            results['hpo_results'] = self._trainer.hpo_results
        # get dict mapping model name to final hyperparameter values for each model:
        model_hyperparams = {}
        for model_name in self._trainer.get_model_names_all():
            model_obj = self._trainer.load_model(model_name)
            model_hyperparams[model_name] = model_obj.params
        results['model_hyperparams'] = model_hyperparams

        if verbosity > 0:  # print stuff
            print(""*** Summary of fit() ***"")
            print(""Estimated performance of each model:"")
            results['leaderboard'] = self._learner.leaderboard(silent=False)
            # self._summarize('model_performance', 'Validation performance of individual models', results)
            #  self._summarize('model_best', 'Best model (based on validation performance)', results)
            # self._summarize('hyperparameter_tune', 'Hyperparameter-tuning used', results)
            print(""Number of models trained: %s"" % len(results['model_performance']))
            print(""Types of models trained:"")
            print(unique_model_types)
            num_fold_str = """"
            bagging_used = results['num_bagging_folds'] > 0
            if bagging_used:
                num_fold_str = f"" (with {results['num_bagging_folds']} folds)""
            print(""Bagging used: %s %s"" % (bagging_used, num_fold_str))
            num_stack_str = """"
            stacking_used = results['stack_ensemble_levels'] > 0
            if stacking_used:
                num_stack_str = f"" (with {results['stack_ensemble_levels']} levels)""
            print(""Stack-ensembling used: %s %s"" % (stacking_used, num_stack_str))
            hpo_str = """"
            if hpo_used and verbosity <= 2:
                hpo_str = "" (call fit_summary() with verbosity >= 3 to see detailed HPO info)""
            print(""Hyperparameter-tuning used: %s %s"" % (hpo_used, hpo_str))
            # TODO: uncomment once feature_prune is functional:  self._summarize('feature_prune', 'feature-selection used', results)
            print(""User-specified hyperparameters:"")
            print(results['hyperparameters_userspecified'])
        if verbosity > 1:  # create plots
            plot_tabular_models(results, output_directory=self.output_directory,
                                save_file=""SummaryOfModels.html"",
                                plot_title=""Models produced during fit()"")
            if hpo_used:
                for model_type in results['hpo_results']:
                    if 'trial_info' in results['hpo_results'][model_type]:
                        plot_summary_of_models(
                            results['hpo_results'][model_type],
                            output_directory=self.output_directory, save_file=model_type + ""_HPOmodelsummary.html"",
                            plot_title=f""Models produced during {model_type} HPO"")
                        plot_performance_vs_trials(
                            results['hpo_results'][model_type],
                            output_directory=self.output_directory, save_file=model_type + ""_HPOperformanceVStrials.png"",
                            plot_title=f""HPO trials for {model_type} models"")
        if verbosity > 2:  # print detailed information
            if hpo_used:
                hpo_results = results['hpo_results']
                print(""*** Details of Hyperparameter optimization ***"")
                for model_type in hpo_results:
                    hpo_model = hpo_results[model_type]
                    print(""HPO for %s model:  Num. configurations tried = %s, Time spent = %s, Search strategy = %s""
                          % (model_type, len(hpo_model['trial_info']), hpo_model['total_time'], hpo_model['search_strategy']))
                    print(""Best hyperparameter-configuration (validation-performance: %s = %s):""
                          % (self.eval_metric, hpo_model['validation_performance']))
                    print(hpo_model['best_config'])
            """"""
            if bagging_used:
                pass # TODO: print detailed bagging info
            if stacking_used:
                pass # TODO: print detailed stacking info, like how much it improves validation performance
            if results['feature_prune']:
                pass # TODO: print detailed feature-selection info once feature-selection is functional.
            """"""
        if verbosity > 0:
            print(""*** End of fit() summary ***"")
        return results","1. Use `assert` statements to validate user input.
2. Use `try-except` blocks to catch and handle errors.
3. Use `logging` to log important events and errors."
"    def fit(train_data,
            label,
            tuning_data=None,
            time_limits=None,
            output_directory=None,
            presets=None,
            problem_type=None,
            eval_metric=None,
            stopping_metric=None,
            auto_stack=False,
            hyperparameter_tune=False,
            feature_prune=False,
            holdout_frac=None,
            num_bagging_folds=0,
            num_bagging_sets=None,
            stack_ensemble_levels=0,
            hyperparameters=None,
            num_trials=None,
            scheduler_options=None,
            search_strategy='random',
            search_options=None,
            nthreads_per_trial=None,
            ngpus_per_trial=None,
            dist_ip_addrs=None,
            visualizer='none',
            verbosity=2,
            **kwargs):
        """"""
        Fit models to predict a column of data table based on the other columns.

        Parameters
        ----------
        train_data : str or :class:`autogluon.task.tabular_prediction.TabularDataset` or `pandas.DataFrame`
            Table of the training data, which is similar to pandas DataFrame.
            If str is passed, `train_data` will be loaded using the str value as the file path.
        label : str
            Name of the column that contains the target variable to predict.
        tuning_data : str or :class:`autogluon.task.tabular_prediction.TabularDataset` or `pandas.DataFrame`, default = None
            Another dataset containing validation data reserved for hyperparameter tuning (in same format as training data).
            If str is passed, `tuning_data` will be loaded using the str value as the file path.
            Note: final model returned may be fit on this tuning_data as well as train_data. Do not provide your evaluation test data here!
            In particular, when `num_bagging_folds` > 0 or `stack_ensemble_levels` > 0, models will be trained on both `tuning_data` and `train_data`.
            If `tuning_data = None`, `fit()` will automatically hold out some random validation examples from `train_data`.
        time_limits : int, default = None
            Approximately how long `fit()` should run for (wallclock time in seconds).
            If not specified, `fit()` will run until all models have completed training, but will not repeatedly bag models unless `num_bagging_sets` or `auto_stack` is specified.
        output_directory : str, default = None
            Path to directory where models and intermediate outputs should be saved.
            If unspecified, a time-stamped folder called ""AutogluonModels/ag-[TIMESTAMP]"" will be created in the working directory to store all models.
            Note: To call `fit()` twice and save all results of each fit, you must specify different `output_directory` locations.
            Otherwise files from first `fit()` will be overwritten by second `fit()`.
        presets : list or str or dict, default = 'medium_quality_faster_train'
            List of preset configurations for various arguments in `fit()`. Can significantly impact predictive accuracy, memory-footprint, and inference latency of trained models, and various other properties of the returned `predictor`.
            It is recommended to specify presets and avoid specifying most other `fit()` arguments or model hyperparameters prior to becoming familiar with AutoGluon.
            As an example, to get the most accurate overall predictor (regardless of its efficiency), set `presets='best_quality'`.
            To get good quality with minimal disk usage, set `presets=['good_quality_faster_inference_only_refit', 'optimize_for_deployment']`
            Any user-specified arguments in `fit()` will override the values used by presets.
            If specifying a list of presets, later presets will override earlier presets if they alter the same argument.
            For precise definitions of the provided presets, see file: `autogluon/tasks/tabular_prediction/presets_configs.py`.
            Users can specify custom presets by passing in a dictionary of argument values as an element to the list.

            Available Presets: ['best_quality', 'best_quality_with_high_quality_refit', 'high_quality_fast_inference_only_refit', 'good_quality_faster_inference_only_refit', 'medium_quality_faster_train', 'optimize_for_deployment', 'ignore_text']
            It is recommended to only use one `quality` based preset in a given call to `fit()` as they alter many of the same arguments and are not compatible with each-other.

            In-depth Preset Info:
                best_quality={'auto_stack': True}
                    Best predictive accuracy with little consideration to inference time or disk usage. Achieve even better results by specifying a large time_limits value.
                    Recommended for applications that benefit from the best possible model accuracy.

                best_quality_with_high_quality_refit={'auto_stack': True, 'refit_full': True}
                    Identical to best_quality but additionally trains refit_full models that have slightly lower predictive accuracy but are over 10x faster during inference and require 10x less disk space.

                high_quality_fast_inference_only_refit={'auto_stack': True, 'refit_full': True, 'set_best_to_refit_full': True, 'save_bagged_folds': False}
                    High predictive accuracy with fast inference. ~10x-200x faster inference and ~10x-200x lower disk usage than `best_quality`.
                    Recommended for applications that require reasonable inference speed and/or model size.

                good_quality_faster_inference_only_refit={'auto_stack': True, 'refit_full': True, 'set_best_to_refit_full': True, 'save_bagged_folds': False, 'hyperparameters': 'light'}
                    Good predictive accuracy with very fast inference. ~4x faster inference and ~4x lower disk usage than `high_quality_fast_inference_only_refit`.
                    Recommended for applications that require fast inference speed.

                medium_quality_faster_train={'auto_stack': False}
                    Medium predictive accuracy with very fast inference and very fast training time. ~20x faster training than `good_quality_faster_inference_only_refit`.
                    This is the default preset in AutoGluon, but should generally only be used for quick prototyping, as `good_quality_faster_inference_only_refit` results in significantly better predictive accuracy and faster inference time.

                optimize_for_deployment={'keep_only_best': True, 'save_space': True}
                    Optimizes result immediately for deployment by deleting unused models and removing training artifacts.
                    Often can reduce disk usage by ~2-4x with no negatives to model accuracy or inference speed.
                    This will disable numerous advanced functionality, but has no impact on inference.
                    This will make certain functionality less informative, such as `predictor.leaderboard()` and `predictor.fit_summary()`.
                        Because unused models will be deleted under this preset, methods like `predictor.leaderboard()` and `predictor.fit_summary()` will no longer show the full set of models that were trained during `fit()`.
                    Recommended for applications where the inner details of AutoGluon's training is not important and there is no intention of manually choosing between the final models.
                    This preset pairs well with the other presets such as `good_quality_faster_inference_only_refit` to make a very compact final model.
                    Identical to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` and `predictor.save_space()` directly after `fit()`.

                ignore_text={'feature_generator_kwargs': {'enable_text_ngram_features': False, 'enable_text_special_features': False}}
                    Disables automated feature generation when text features are detected.
                    This is useful to determine how beneficial text features are to the end result, as well as to ensure features are not mistaken for text when they are not.

        problem_type : str, default = None
            Type of prediction problem, i.e. is this a binary/multiclass classification or regression problem (options: 'binary', 'multiclass', 'regression').
            If `problem_type = None`, the prediction problem type is inferred based on the label-values in provided dataset.
        eval_metric : function or str, default = None
            Metric by which predictions will be ultimately evaluated on test data.
            AutoGluon tunes factors such as hyperparameters, early-stopping, ensemble-weights, etc. in order to improve this metric on validation data.

            If `eval_metric = None`, it is automatically chosen based on `problem_type`.
            Defaults to 'accuracy' for binary and multiclass classification and 'root_mean_squared_error' for regression.
            Otherwise, options for classification:
                ['accuracy', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted',
                'roc_auc', 'average_precision', 'precision', 'precision_macro', 'precision_micro', 'precision_weighted',
                'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'log_loss', 'pac_score']
            Options for regression:
                ['root_mean_squared_error', 'mean_squared_error', 'mean_absolute_error', 'median_absolute_error', 'r2']
            For more information on these options, see `sklearn.metrics`: https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics

            You can also pass your own evaluation function here as long as it follows formatting of the functions defined in `autogluon/utils/tabular/metrics/`.
        stopping_metric : function or str, default = None
            Metric which iteratively-trained models use to early stop to avoid overfitting.
            `stopping_metric` is not used by weighted ensembles, instead weighted ensembles maximize `eval_metric`.
            Defaults to `eval_metric` value except when `eval_metric='roc_auc'`, where it defaults to `log_loss`.
            Options are identical to options for `eval_metric`.
        auto_stack : bool, default = False
            Whether AutoGluon should automatically utilize bagging and multi-layer stack ensembling to boost predictive accuracy.
            Set this = True if you are willing to tolerate longer training times in order to maximize predictive accuracy!
            Note: This overrides `num_bagging_folds` and `stack_ensemble_levels` arguments (selects optimal values for these parameters based on dataset properties).
            Note: This can increase training time (and inference time) by up to 20x, but can greatly improve predictive performance.
        hyperparameter_tune : bool, default = False
            Whether to tune hyperparameters or just use fixed hyperparameter values for each model. Setting as True will increase `fit()` runtimes.
            It is currently not recommended to use `hyperparameter_tune` with `auto_stack` due to potential overfitting.
            Use `auto_stack` to maximize predictive accuracy; use `hyperparameter_tune` if you prefer to deploy just a single model rather than an ensemble.
        feature_prune : bool, default = False
            Whether or not to perform feature selection.
        hyperparameters : str or dict, default = 'default'
            Determines the hyperparameters used by the models.
            If `str` is passed, will use a preset hyperparameter configuration.
                Valid `str` options: ['default', 'light', 'very_light', 'toy']
                    'default': Default AutoGluon hyperparameters intended to maximize accuracy without significant regard to inference time or disk usage.
                    'light': Results in smaller models. Generally will make inference speed much faster and disk usage much lower, but with worse accuracy.
                    'very_light': Results in much smaller models. Behaves similarly to 'light', but in many cases with over 10x less disk usage and a further reduction in accuracy.
                    'toy': Results in extremely small models. Only use this when prototyping, as the model quality will be severely reduced.
                Reference `autogluon/task/tabular_prediction/hyperparameter_configs.py` for information on the hyperparameters associated with each preset.
            Keys are strings that indicate which model types to train.
                Options include: 'NN' (neural network), 'GBM' (lightGBM boosted trees), 'CAT' (CatBoost boosted trees), 'RF' (random forest), 'XT' (extremely randomized trees), 'KNN' (k-nearest neighbors), 'LR' (linear regression)
                If certain key is missing from hyperparameters, then `fit()` will not train any models of that type. Omitting a model key from hyperparameters is equivalent to including this model key in `excluded_model_types`.
                For example, set `hyperparameters = { 'NN':{...} }` if say you only want to train neural networks and no other types of models.
            Values = dict of hyperparameter settings for each model type, or list of dicts.
                Each hyperparameter can either be a single fixed value or a search space containing many possible values.
                Unspecified hyperparameters will be set to default values (or default search spaces if `hyperparameter_tune = True`).
                Caution: Any provided search spaces will be overridden by fixed defaults if `hyperparameter_tune = False`.
                To train multiple models of a given type, set the value to a list of hyperparameter dictionaries.
                    For example, `hyperparameters = {'RF': [{'criterion': 'gini'}, {'criterion': 'entropy'}]}` will result in 2 random forest models being trained with separate hyperparameters.
            Advanced functionality: Custom models
                `hyperparameters` can also take a special key 'custom', which maps to a list of model names (currently supported options = 'GBM').
                    If `hyperparameter_tune = False`, then these additional models will also be trained using custom pre-specified hyperparameter settings that are known to work well.
            Advanced functionality: Custom stack levels
                By default, AutoGluon re-uses the same models and model hyperparameters at each level during stack ensembling.
                To customize this behaviour, create a hyperparameters dictionary separately for each stack level, and then add them as values to a new dictionary, with keys equal to the stack level.
                    Example: `hyperparameters = {0: {'RF': rf_params1}, 1: {'CAT': [cat_params1, cat_params2], 'NN': {}}}`
                    This will result in a stack ensemble that has one custom random forest in level 0 followed by two CatBoost models with custom hyperparameters and a default neural network in level 1, for a total of 4 models.
                If a level is not specified in `hyperparameters`, it will default to using the highest specified level to train models. This can also be explicitly controlled by adding a 'default' key.

            Default:
                hyperparameters = {
                    'NN': {},
                    'GBM': {},
                    'CAT': {},
                    'RF': [
                        {'criterion': 'gini', 'AG_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},
                        {'criterion': 'entropy', 'AG_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},
                        {'criterion': 'mse', 'AG_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},
                    ],
                    'XT': [
                        {'criterion': 'gini', 'AG_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},
                        {'criterion': 'entropy', 'AG_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},
                        {'criterion': 'mse', 'AG_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},
                    ],
                    'KNN': [
                        {'weights': 'uniform', 'AG_args': {'name_suffix': 'Unif'}},
                        {'weights': 'distance', 'AG_args': {'name_suffix': 'Dist'}},
                    ],
                    'custom': ['GBM']
                }

            Details regarding the hyperparameters you can specify for each model are provided in the following files:
                NN: `autogluon/utils/tabular/ml/models/tabular_nn/hyperparameters/parameters.py`
                    Note: certain hyperparameter settings may cause these neural networks to train much slower.
                GBM: `autogluon/utils/tabular/ml/models/lgb/hyperparameters/parameters.py`
                     See also the lightGBM docs: https://lightgbm.readthedocs.io/en/latest/Parameters.html
                CAT: `autogluon/utils/tabular/ml/models/catboost/hyperparameters/parameters.py`
                     See also the CatBoost docs: https://catboost.ai/docs/concepts/parameter-tuning.html
                RF: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
                    Note: Hyperparameter tuning is disabled for this model.
                XT: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html
                    Note: Hyperparameter tuning is disabled for this model.
                KNN: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
                    Note: Hyperparameter tuning is disabled for this model.
                LR: `autogluon/utils/tabular/ml/models/lr/hyperparameters/parameters.py`
                    Note: Hyperparameter tuning is disabled for this model.
                    Note: 'penalty' parameter can be used for regression to specify regularization method: 'L1' and 'L2' values are supported.
                Advanced functionality: Custom AutoGluon model arguments
                    These arguments are optional and can be specified in any model's hyperparameters.
                        Example: `hyperparameters = {'RF': {..., 'AG_args': {'name_suffix': 'CustomModelSuffix', 'disable_in_hpo': True}}`
                    AG_args: Dictionary of customization options related to meta properties of the model such as its name, the order it is trained, and the problem types it is valid for.
                        Valid keys:
                            name: (str) The name of the model. This overrides AutoGluon's naming logic and all other name arguments if present.
                            name_main: (str) The main name of the model. In 'RandomForestClassifier', this is 'RandomForest'.
                            name_prefix: (str) Add a custom prefix to the model name. Unused by default.
                            name_type_suffix: (str) Override the type suffix of the model name. In 'RandomForestClassifier', this is 'Classifier'. This comes before 'name_suffix'.
                            name_suffix: (str) Add a custom suffix to the model name. Unused by default.
                            priority: (int) Determines the order in which the model is trained. Larger values result in the model being trained earlier. Default values range from 100 (RF) to 0 (custom), dictated by model type. If you want this model to be trained first, set priority = 999.
                            problem_types: (list) List of valid problem types for the model. `problem_types=['binary']` will result in the model only being trained if `problem_type` is 'binary'.
                            disable_in_hpo: (bool) If True, the model will only be trained if `hyperparameter_tune=False`.
                        Reference the default hyperparameters for example usage of these options.
                    AG_args_fit: Dictionary of model fit customization options related to how and with what constraints the model is trained. These parameters affect stacker fold models, but not stacker models themselves.
                        Clarification: `time_limit` is the internal time in seconds given to a particular model to train, which is dictated in part by the `time_limits` argument given during `fit()` but is not the same.
                        Valid keys:
                            max_memory_usage_ratio: (float, default=1.0) The ratio of memory usage relative to the default to allow before early stopping or killing the model. Values greater than 1.0 will be increasingly prone to out-of-memory errors.
                            max_time_limit_ratio: (float, default=1.0) The ratio of the provided time_limit to use during model `fit()`. If `time_limit=10` and `max_time_limit_ratio=0.3`, time_limit would be changed to 3. Does not alter max_time_limit or min_time_limit values.
                            max_time_limit: (float, default=None) Maximum amount of time to allow this model to train for (in sec). If the provided time_limit is greater than this value, it will be replaced by max_time_limit.
                            min_time_limit: (float, default=0) Allow this model to train for at least this long (in sec), regardless of the time limit it would otherwise be granted.
                                If `min_time_limit >= max_time_limit`, time_limit will be set to min_time_limit.
                                If `min_time_limit=None`, time_limit will be set to None and the model will have no training time restriction.

        holdout_frac : float, default = None
            Fraction of train_data to holdout as tuning data for optimizing hyperparameters (ignored unless `tuning_data = None`, ignored if `num_bagging_folds != 0`).
            Default value (if None) is selected based on the number of rows in the training data. Default values range from 0.2 at 2,500 rows to 0.01 at 250,000 rows.
            Default value is doubled if `hyperparameter_tune = True`, up to a maximum of 0.2.
            Disabled if `num_bagging_folds >= 2`.
        num_bagging_folds : int, default = 0
            Number of folds used for bagging of models. When `num_bagging_folds = k`, training time is roughly increased by a factor of `k` (set = 0 to disable bagging).
            Disabled by default, but we recommend values between 5-10 to maximize predictive performance.
            Increasing num_bagging_folds will result in models with lower bias but that are more prone to overfitting.
            Values > 10 may produce diminishing returns, and can even harm overall results due to overfitting.
            To further improve predictions, avoid increasing num_bagging_folds much beyond 10 and instead increase num_bagging_sets.
        num_bagging_sets : int, default = None
            Number of repeats of kfold bagging to perform (values must be >= 1). Total number of models trained during bagging = num_bagging_folds * num_bagging_sets.
            Defaults to 1 if time_limits is not specified, otherwise 20 (always disabled if num_bagging_folds is not specified).
            Values greater than 1 will result in superior predictive performance, especially on smaller problems and with stacking enabled (reduces overall variance).
        stack_ensemble_levels : int, default = 0
            Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of `stack_ensemble_levels+1` (set = 0 to disable stack ensembling).
            Disabled by default, but we recommend values between 1-3 to maximize predictive performance.
            To prevent overfitting, this argument is ignored unless you have also set `num_bagging_folds >= 2`.
        num_trials : int, default = None
            Maximal number of different hyperparameter settings of each model type to evaluate during HPO (only matters if `hyperparameter_tune = True`).
            If both `time_limits` and `num_trials` are specified, `time_limits` takes precedent.
        scheduler_options : dict, default = None
            Extra arguments passed to __init__ of scheduler, to configure the orchestration of training jobs during hyperparameter-tuning.
            Ignored if `hyperparameter_tune=False`.
        search_strategy : str, default = 'random'
            Which hyperparameter search algorithm to use (only matters if `hyperparameter_tune=True`).
            Options include: 'random' (random search), 'bayesopt' (Gaussian process Bayesian optimization), 'skopt' (SKopt Bayesian optimization), 'grid' (grid search).
        search_options : dict, default = None
            Auxiliary keyword arguments to pass to the searcher that performs hyperparameter optimization.
        nthreads_per_trial : int, default = None
            How many CPUs to use in each training run of an individual model.
            This is automatically determined by AutoGluon when left as None (based on available compute).
        ngpus_per_trial : int, default = None
            How many GPUs to use in each trial (ie. single training run of a model).
            This is automatically determined by AutoGluon when left as None.
        dist_ip_addrs : list, default = None
            List of IP addresses corresponding to remote workers, in order to leverage distributed computation.
        visualizer : str, default = 'none'
            How to visualize the neural network training progress during `fit()`. Options: ['mxboard', 'tensorboard', 'none'].
        verbosity : int, default = 2
            Verbosity levels range from 0 to 4 and control how much information is printed during fit().
            Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).
            If using logging, you can alternatively control amount of information printed via `logger.setLevel(L)`,
            where `L` ranges from 0 to 50 (Note: higher values of `L` correspond to fewer print statements, opposite of verbosity levels)

        Kwargs can include additional arguments for advanced users:
            AG_args_fit : dict, default={}
                Keyword arguments to pass to all models. See the `AG_args_fit` argument from ""Advanced functionality: Custom AutoGluon model arguments"" in the `hyperparameters` argument documentation for valid values.
                Identical to specifying `AG_args_fit` parameter for all models in `hyperparameters`.
                If a key in `AG_args_fit` is already specified for a model in `hyperparameters`, it will not be altered through this argument.
            excluded_model_types : list, default = []
                Banned subset of model types to avoid training during `fit()`, even if present in `hyperparameters`.
                Valid values: ['RF', 'XT', 'KNN', 'GBM', 'CAT', 'NN', 'LR', 'custom']. Reference `hyperparameters` documentation for what models correspond to each value.
                Useful when a particular model type such as 'KNN' or 'custom' is not desired but altering the `hyperparameters` dictionary is difficult or time-consuming.
                    Example: To exclude both 'KNN' and 'custom' models, specify `excluded_model_types=['KNN', 'custom']`.
            id_columns : list, default = []
                Banned subset of column names that model may not use as predictive features (e.g. contains label, user-ID, etc).
                These columns are ignored during `fit()`, but DataFrame of just these columns with appended predictions may be produced, for example to submit in a ML competition.
            label_count_threshold : int, default = 10
                For multi-class classification problems, this is the minimum number of times a label must appear in dataset in order to be considered an output class.
                AutoGluon will ignore any classes whose labels do not appear at least this many times in the dataset (i.e. will never predict them).
            save_bagged_folds : bool, default = True
                If True, bagged models will save their fold models (the models from each individual fold of bagging). This is required to use bagged models for prediction after `fit()`.
                If False, bagged models will not save their fold models. This means that bagged models will not be valid models during inference.
                    This should only be set to False when planning to call `predictor.refit_full()` or when `refit_full` is set and `set_best_to_refit_full=True`.
                    Particularly useful if disk usage is a concern. By not saving the fold models, bagged models will use only very small amounts of disk space during training.
                    In many training runs, this will reduce peak disk usage by >10x.
                This parameter has no effect if bagging is disabled.
            keep_only_best : bool, default = False
                If True, only the best model and its ancestor models are saved in the outputted `predictor`. All other models are deleted.
                    If you only care about deploying the most accurate predictor with the smallest file-size and no longer need any of the other trained models or functionality beyond prediction on new data, then set: `keep_only_best=True`, `save_space=True`.
                    This is equivalent to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` directly after `fit()`.
                If used with `refit_full` and `set_best_to_refit_full`, the best model will be the refit_full model, and the original bagged best model will be deleted.
                    `refit_full` will be automatically set to 'best' in this case to avoid training models which will be later deleted.
            save_space : bool, default = False
                If True, reduces the memory and disk size of predictor by deleting auxiliary model files that aren't needed for prediction on new data.
                    This is equivalent to calling `predictor.save_space()` directly after `fit()`.
                This has NO impact on inference accuracy.
                It is recommended if the only goal is to use the trained model for prediction.
                Certain advanced functionality may no longer be available if `save_space=True`. Refer to `predictor.save_space()` documentation for more details.
            cache_data : bool, default = True
                When enabled, the training and validation data are saved to disk for future reuse.
                Enables advanced functionality in the resulting Predictor object such as feature importance calculation on the original data.
            refit_full : bool or str, default = False
                Whether to retrain all models on all of the data (training + validation) after the normal training procedure.
                This is equivalent to calling `predictor.refit_full(model=refit_full)` after training.
                If `refit_full=True`, it will be treated as `refit_full='all'`.
                If `refit_full=False`, refitting will not occur.
                Valid str values:
                    `all`: refits all models.
                    `best`: refits only the best model (and its ancestors if it is a stacker model).
                    `{model_name}`: refits only the specified model (and its ancestors if it is a stacker model).
                For bagged models:
                    Reduces a model's inference time by collapsing bagged ensembles into a single model fit on all of the training data.
                    This process will typically result in a slight accuracy reduction and a large inference speedup.
                    The inference speedup will generally be between 10-200x faster than the original bagged ensemble model.
                        The inference speedup factor is equivalent to (k * n), where k is the number of folds (`num_bagging_folds`) and n is the number of finished repeats (`num_bagging_sets`) in the bagged ensemble.
                    The runtime is generally 10% or less of the original fit runtime.
                        The runtime can be roughly estimated as 1 / (k * n) of the original fit runtime, with k and n defined above.
                For non-bagged models:
                    Optimizes a model's accuracy by retraining on 100% of the data without using a validation set.
                    Will typically result in a slight accuracy increase and no change to inference time.
                    The runtime will be approximately equal to the original fit runtime.
                This process does not alter the original models, but instead adds additional models.
                If stacker models are refit by this process, they will use the refit_full versions of the ancestor models during inference.
                Models produced by this process will not have validation scores, as they use all of the data for training.
                    Therefore, it is up to the user to determine if the models are of sufficient quality by including test data in `predictor.leaderboard(dataset=test_data)`.
                    If the user does not have additional test data, they should reference the original model's score for an estimate of the performance of the refit_full model.
                        Warning: Be aware that utilizing refit_full models without separately verifying on test data means that the model is untested, and has no guarantee of being consistent with the original model.
                The time taken by this process is not enforced by `time_limits`.
                `cache_data` must be set to `True` to enable this functionality.
            set_best_to_refit_full : bool, default = False
                If True, will set Trainer.best_model = Trainer.full_model_dict[Trainer.best_model]
                This will change the default model that Predictor uses for prediction when model is not specified to the refit_full version of the model that previously exhibited the highest validation score.
                Only valid if `refit_full` is set.
            feature_generator_type : :class:`autogluon.utils.tabular.features.auto_ml_feature_generator.AbstractFeatureGenerator` class, default = :class:`autogluon.utils.tabular.features.auto_ml_feature_generator.AutoMLFeatureGenerator`
                A `FeatureGenerator` class specifying which feature engineering protocol to follow
                Note: The file containing your `FeatureGenerator` class must be imported into current Python session in order to use a custom class.
            feature_generator_kwargs : dict, default={}
                Keyword arguments to pass into the `FeatureGenerator` constructor.
                Valid :class:`autogluon.utils.tabular.features.auto_ml_feature_generator.AutoMLFeatureGenerator` kwargs:
                    enable_text_ngram_features : bool, default = True
                        If True, the vectorizer argument value is used to generate 'text_ngram' features from text features if present.
                        Try setting this to False if you encounter memory issues running AutoGluon on text data and cannot access a machine with more memory.
                    enable_text_special_features : bool, default = True
                        If True, generate 'text_special' features from text features if present.
                        Examples of 'text_special' features include the number of whitespaces and the average word length in a text feature.
                    vectorizer : `sklearn.feature_extraction.text.CountVectorizer`, default = `CountVectorizer(min_df=30, ngram_range=(1, 3), max_features=10000, dtype=np.uint8)`
                        Determines the count vectorizer used during feature generation if text features are detected.
                        If your data contain text fields and you encounter memory issues running AutoGluon (and cannot access a machine with more memory), then consider reducing max_features or setting n_gram_range=(1, 2).
            trainer_type : `Trainer` class, default=`AutoTrainer`
                A class inheriting from `autogluon.utils.tabular.ml.trainer.abstract_trainer.AbstractTrainer` that controls training/ensembling of many models.
                Note: In order to use a custom `Trainer` class, you must import the class file that defines it into the current Python session.
            random_seed : int, default = 0
                Seed to use when generating data split indices such as kfold splits and train/validation splits.
                Caution: This seed only enables reproducible data splits (and the ability to randomize splits in each run by changing seed values).
                This seed is NOT used in the training of individual models, for that you need to explicitly set the corresponding seed hyperparameter (usually called 'seed_value') of each individual model.
                If stacking is enabled:
                    The seed used for stack level L is equal to `seed+L`.
                    This means `random_seed=1` will have the same split indices at L=0 as `random_seed=0` will have at L=1.
                If `random_seed=None`, a random integer is used.

        Returns
        -------
        :class:`autogluon.task.tabular_prediction.TabularPredictor` object which can make predictions on new data and summarize what happened during `fit()`.

        Examples
        --------
        >>> from autogluon import TabularPrediction as task
        >>> train_data = task.Dataset(file_path='https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')
        >>> label_column = 'class'
        >>> predictor = task.fit(train_data=train_data, label=label_column)
        >>> test_data = task.Dataset(file_path='https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')
        >>> y_test = test_data[label_column]
        >>> test_data = test_data.drop(labels=[label_column], axis=1)
        >>> y_pred = predictor.predict(test_data)
        >>> perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred)
        >>> results = predictor.fit_summary()

        To maximize predictive performance, use the following:

        >>> eval_metric = 'roc_auc'  # set this to the metric you ultimately care about
        >>> time_limits = 360  # set as long as you are willing to wait (in sec)
        >>> predictor = task.fit(train_data=train_data, label=label_column, eval_metric=eval_metric, auto_stack=True, time_limits=time_limits)
        """"""
        assert search_strategy != 'bayesopt_hyperband', \\
            ""search_strategy == 'bayesopt_hyperband' not yet supported""
        if verbosity < 0:
            verbosity = 0
        elif verbosity > 4:
            verbosity = 4

        logger.setLevel(verbosity2loglevel(verbosity))
        allowed_kwarg_names = {
            'feature_generator_type',
            'feature_generator_kwargs',
            'trainer_type',
            'AG_args_fit',
            'excluded_model_types',
            'label_count_threshold',
            'id_columns',
            'set_best_to_refit_full',
            'save_bagged_folds',
            'keep_only_best',
            'save_space',
            'cache_data',
            'refit_full',
            'random_seed',
            'enable_fit_continuation'  # TODO: Remove on 0.1.0 release
        }
        for kwarg_name in kwargs.keys():
            if kwarg_name not in allowed_kwarg_names:
                raise ValueError(""Unknown keyword argument specified: %s"" % kwarg_name)

        if isinstance(train_data,  str):
            train_data = TabularDataset(file_path=train_data)
        if tuning_data is not None and isinstance(tuning_data, str):
            tuning_data = TabularDataset(file_path=tuning_data)

        if len(set(train_data.columns)) < len(train_data.columns):
            raise ValueError(""Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})"")
        if tuning_data is not None:
            train_features = np.array([column for column in train_data.columns if column != label])
            tuning_features = np.array([column for column in tuning_data.columns if column != label])
            if np.any(train_features != tuning_features):
                raise ValueError(""Column names must match between training and tuning data"")

        if feature_prune:
            feature_prune = False  # TODO: Fix feature pruning to add back as an option
            # Currently disabled, needs to be updated to align with new model class functionality
            logger.log(30, 'Warning: feature_prune does not currently work, setting to False.')

        cache_data = kwargs.get('cache_data', True)
        refit_full = kwargs.get('refit_full', False)
        # TODO: Remove on 0.1.0 release
        if 'enable_fit_continuation' in kwargs.keys():
            logger.log(30, 'Warning: `enable_fit_continuation` is a deprecated parameter. It has been renamed to `cache_data`. Starting from AutoGluon 0.1.0, specifying `enable_fit_continuation` as a parameter will cause an exception.')
            logger.log(30, 'Setting `cache_data` value equal to `enable_fit_continuation` value.')
            cache_data = kwargs['enable_fit_continuation']
        if not cache_data:
            logger.log(30, 'Warning: `cache_data=False` will disable or limit advanced functionality after training such as feature importance calculations. It is recommended to set `cache_data=True` unless you explicitly wish to not have the data saved to disk.')
            if refit_full:
                raise ValueError('`refit_full=True` is only available when `cache_data=True`. Set `cache_data=True` to utilize `refit_full`.')

        set_best_to_refit_full = kwargs.get('set_best_to_refit_full', False)
        if set_best_to_refit_full and not refit_full:
            raise ValueError('`set_best_to_refit_full=True` is only available when `refit_full=True`. Set `refit_full=True` to utilize `set_best_to_refit_full`.')

        save_bagged_folds = kwargs.get('save_bagged_folds', True)

        if hyperparameter_tune:
            logger.log(30, 'Warning: `hyperparameter_tune=True` is currently experimental and may cause the process to hang. Setting `auto_stack=True` instead is recommended to achieve maximum quality models.')

        if dist_ip_addrs is None:
            dist_ip_addrs = []

        if search_options is None:
            search_options = dict()

        if hyperparameters is None:
            hyperparameters = 'default'
        if isinstance(hyperparameters, str):
            hyperparameters = get_hyperparameter_config(hyperparameters)

        # Process kwargs to create feature generator, trainer, schedulers, searchers for each model:
        output_directory = setup_outputdir(output_directory)  # Format directory name
        feature_generator_type = kwargs.get('feature_generator_type', AutoMLFeatureGenerator)
        feature_generator_kwargs = kwargs.get('feature_generator_kwargs', {})
        feature_generator = feature_generator_type(**feature_generator_kwargs) # instantiate FeatureGenerator object
        id_columns = kwargs.get('id_columns', [])
        trainer_type = kwargs.get('trainer_type', AutoTrainer)
        ag_args_fit = kwargs.get('AG_args_fit', {})
        excluded_model_types = kwargs.get('excluded_model_types', [])
        random_seed = kwargs.get('random_seed', 0)
        nthreads_per_trial, ngpus_per_trial = setup_compute(nthreads_per_trial, ngpus_per_trial)
        num_train_rows = len(train_data)
        if auto_stack:
            # TODO: What about datasets that are 100k+? At a certain point should we not bag?
            # TODO: What about time_limits? Metalearning can tell us expected runtime of each model, then we can select optimal folds + stack levels to fit time constraint
            num_bagging_folds = min(10, max(5, math.floor(num_train_rows / 100)))
            stack_ensemble_levels = min(1, max(0, math.floor(num_train_rows / 750)))

        if num_bagging_sets is None:
            if num_bagging_folds >= 2:
                if time_limits is not None:
                    num_bagging_sets = 20
                else:
                    num_bagging_sets = 1
            else:
                num_bagging_sets = 1

        label_count_threshold = kwargs.get('label_count_threshold', 10)
        if num_bagging_folds is not None:  # Ensure there exist sufficient labels for stratified splits across all bags
            label_count_threshold = max(label_count_threshold, num_bagging_folds)

        time_limits_orig = copy.deepcopy(time_limits)
        time_limits_hpo = copy.deepcopy(time_limits)

        if num_bagging_folds >= 2 and (time_limits_hpo is not None):
            time_limits_hpo = time_limits_hpo / (1 + num_bagging_folds * (1 + stack_ensemble_levels))
        time_limits_hpo, num_trials = setup_trial_limits(time_limits_hpo, num_trials, hyperparameters)  # TODO: Move HPO time allocation to Trainer

        if (num_trials is not None) and hyperparameter_tune and (num_trials == 1):
            hyperparameter_tune = False
            logger.log(30, 'Warning: Specified num_trials == 1 or time_limits is too small for hyperparameter_tune, setting to False.')

        if holdout_frac is None:
            holdout_frac = default_holdout_frac(num_train_rows, hyperparameter_tune)

        # Add visualizer to NN hyperparameters:
        if (visualizer is not None) and (visualizer != 'none') and ('NN' in hyperparameters):
            hyperparameters['NN']['visualizer'] = visualizer

        eval_metric = get_metric(eval_metric, problem_type, 'eval_metric')
        stopping_metric = get_metric(stopping_metric, problem_type, 'stopping_metric')

        # All models use the same scheduler:
        scheduler_options = compile_scheduler_options(
            scheduler_options=scheduler_options,
            search_strategy=search_strategy,
            search_options=search_options,
            nthreads_per_trial=nthreads_per_trial,
            ngpus_per_trial=ngpus_per_trial,
            checkpoint=None,
            num_trials=num_trials,
            time_out=time_limits_hpo,
            resume=False,
            visualizer=visualizer,
            time_attr='epoch',
            reward_attr='validation_performance',
            dist_ip_addrs=dist_ip_addrs)
        scheduler_cls = schedulers[search_strategy.lower()]
        scheduler_options = (scheduler_cls, scheduler_options)  # wrap into tuple
        learner = Learner(path_context=output_directory, label=label, problem_type=problem_type, eval_metric=eval_metric, stopping_metric=stopping_metric,
                          id_columns=id_columns, feature_generator=feature_generator, trainer_type=trainer_type,
                          label_count_threshold=label_count_threshold, random_seed=random_seed)
        learner.fit(X=train_data, X_val=tuning_data, scheduler_options=scheduler_options,
                    hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune,
                    holdout_frac=holdout_frac, num_bagging_folds=num_bagging_folds, num_bagging_sets=num_bagging_sets, stack_ensemble_levels=stack_ensemble_levels,
                    hyperparameters=hyperparameters, ag_args_fit=ag_args_fit, excluded_model_types=excluded_model_types, time_limit=time_limits_orig, save_data=cache_data, save_bagged_folds=save_bagged_folds, verbosity=verbosity)

        predictor = TabularPredictor(learner=learner)

        keep_only_best = kwargs.get('keep_only_best', False)
        if refit_full is True:
            if keep_only_best is True:
                if set_best_to_refit_full is True:
                    refit_full = 'best'
                else:
                    logger.warning(f'refit_full was set to {refit_full}, but keep_only_best=True and set_best_to_refit_full=False. Disabling refit_full to avoid training models which would be automatically deleted.')
                    refit_full = False
            else:
                refit_full = 'all'

        if refit_full is not False:
            trainer = predictor._trainer
            trainer_model_best = trainer.get_model_best()
            predictor.refit_full(model=refit_full)
            if set_best_to_refit_full:
                if trainer_model_best in trainer.model_full_dict.keys():
                    trainer.model_best = trainer.model_full_dict[trainer_model_best]
                    # Note: model_best will be overwritten if additional training is done with new models, since model_best will have validation score of None and any new model will have a better validation score.
                    # This has the side-effect of having the possibility of model_best being overwritten by a worse model than the original model_best.
                    trainer.save()
                else:
                    logger.warning(f'Best model ({trainer_model_best}) is not present in refit_full dictionary. Training may have failed on the refit model. AutoGluon will default to using {trainer_model_best} for predictions.')

        if keep_only_best:
            predictor.delete_models(models_to_keep='best', dry_run=False)

        save_space = kwargs.get('save_space', False)
        if save_space:
            predictor.save_space()

        return predictor",000_Error
"def augment_data(X_train, feature_types_metadata: FeatureTypesMetadata, augmentation_data=None, augment_method='spunge', augment_args=None):
    """""" augment_method options: ['spunge', 'munge']
    """"""
    if augment_args is None:
        augment_args = {}
    if augmentation_data is not None:
        X_aug = augmentation_data
    else:
        if 'num_augmented_samples' not in augment_args:
            if 'max_size' not in augment_args:
                augment_args['max_size'] = np.inf
            augment_args['num_augmented_samples'] = int(min(augment_args['max_size'], augment_args['size_factor']*len(X_train)))

        if augment_method == 'spunge':
            X_aug = spunge_augment(X_train, feature_types_metadata, **augment_args)
        elif augment_method == 'munge':
            X_aug = munge_augment(X_train, feature_types_metadata, **augment_args)
        else:
            raise ValueError(f""unknown augment_method: {augment_method}"")

    # return postprocess_augmented(X_aug, X_train)  # TODO: dropping duplicates is much more efficient, but may skew distribution for entirely-categorical data with few categories.
    logger.log(15, f""Augmented training dataset with {len(X_aug)} extra datapoints"")
    return X_aug.reset_index(drop=True)","1. Use `assert` statements to check for invalid inputs.
2. Sanitize user input to prevent injection attacks.
3. Use secure default values for parameters."
"def spunge_augment(X, feature_types_metadata: FeatureTypesMetadata, num_augmented_samples=10000, frac_perturb=0.1, continuous_feature_noise=0.1, **kwargs):
    """""" Generates synthetic datapoints for learning to mimic teacher model in distillation
        via simplified version of MUNGE strategy (that does not require near-neighbor search).

        Args:
            num_augmented_samples: number of additional augmented data points to return
            frac_perturb: fraction of features/examples that are perturbed during augmentation. Set near 0 to ensure augmented sample distribution remains closer to real data.
            continuous_feature_noise: we noise numeric features by this factor times their std-dev. Set near 0 to ensure augmented sample distribution remains closer to real data.
    """"""
    if frac_perturb > 1.0:
        raise ValueError(""frac_perturb must be <= 1"")
    logger.log(20, f""SPUNGE: Augmenting training data with {num_augmented_samples} synthetic samples for distillation..."")
    num_feature_perturb = max(1, int(frac_perturb*len(X.columns)))
    X_aug = pd.concat([X.iloc[[0]].copy()]*num_augmented_samples)
    X_aug.reset_index(drop=True, inplace=True)
    continuous_types = ['float', 'int']
    continuous_featnames = [] # these features will have shuffled values with added noise
    for contype in continuous_types:
        if contype in feature_types_metadata.feature_types_raw:
            continuous_featnames += feature_types_metadata.feature_types_raw[contype]

    for i in range(num_augmented_samples): # hot-deck sample some features per datapoint
        og_ind = i % len(X)
        augdata_i = X.iloc[og_ind].copy()
        num_feature_perturb_i = np.random.choice(range(1,num_feature_perturb+1))  # randomly sample number of features to perturb
        cols_toperturb = np.random.choice(list(X.columns), size=num_feature_perturb_i, replace=False)
        for feature in cols_toperturb:
            feature_data = X[feature]
            augdata_i[feature] = feature_data.sample(n=1).values[0]
        X_aug.iloc[i] = augdata_i

    for feature in X.columns:
        if feature in continuous_featnames:
            feature_data = X[feature]
            aug_data = X_aug[feature]
            noise = np.random.normal(scale=np.nanstd(feature_data)*continuous_feature_noise, size=num_augmented_samples)
            mask = np.random.binomial(n=1, p=frac_perturb, size=num_augmented_samples)
            aug_data = aug_data + noise*mask
            X_aug[feature] = pd.Series(aug_data, index=X_aug.index)

    return X_aug","1. Use `np.random.choice()` instead of `np.random.randint()` to generate random numbers.
2. Use `pd.concat()` instead of `pd.append()` to concatenate dataframes.
3. Use `pd.Series()` instead of `pd.DataFrame()` to create series."
"def munge_augment(X, feature_types_metadata: FeatureTypesMetadata, num_augmented_samples=10000, perturb_prob=0.5, s=1.0, **kwargs):
    """""" Uses MUNGE algorithm to generate synthetic datapoints for learning to mimic teacher model in distillation: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf
        Args:
            num_augmented_samples: number of additional augmented data points to return
            perturb_prob: probability of perturbing each feature during augmentation. Set near 0 to ensure augmented sample distribution remains closer to real data.
            s: We noise numeric features by their std-dev divided by this factor (inverse of continuous_feature_noise). Set large to ensure augmented sample distribution remains closer to real data.
    """"""
    nn_dummy = TabularNeuralNetModel(path='nn_dummy', name='nn_dummy', problem_type=REGRESSION, eval_metric=mean_squared_error,
                                     hyperparameters={'num_dataloading_workers': 0, 'proc.embed_min_categories': np.inf},
                                     features = list(X.columns), feature_types_metadata=feature_types_metadata)
    processed_data = nn_dummy.process_train_data(df=nn_dummy.preprocess(X), labels=pd.Series([1]*len(X)), batch_size=nn_dummy.params['batch_size'],
                        num_dataloading_workers=0, impute_strategy=nn_dummy.params['proc.impute_strategy'],
                        max_category_levels=nn_dummy.params['proc.max_category_levels'], skew_threshold=nn_dummy.params['proc.skew_threshold'],
                        embed_min_categories=nn_dummy.params['proc.embed_min_categories'], use_ngram_features=nn_dummy.params['use_ngram_features'])
    X_vector = processed_data.dataset._data[processed_data.vectordata_index].asnumpy()
    processed_data = None
    nn_dummy = None
    gc.collect()

    neighbor_finder = NearestNeighbors(n_neighbors=2)
    neighbor_finder.fit(X_vector)
    neigh_dist, neigh_ind = neighbor_finder.kneighbors(X_vector)
    neigh_ind = neigh_ind[:,1]  # contains indices of nearest neighbors
    neigh_dist = None
    # neigh_dist = neigh_dist[:,1]  # contains distances to nearest neighbors
    neighbor_finder = None
    gc.collect()

    if perturb_prob > 1.0:
        raise ValueError(""frac_perturb must be <= 1"")
    logger.log(20, f""MUNGE: Augmenting training data with {num_augmented_samples} synthetic samples for distillation..."")
    X = X.copy()
    X_aug = pd.concat([X.iloc[[0]].copy()]*num_augmented_samples)
    X_aug.reset_index(drop=True, inplace=True)
    continuous_types = ['float', 'int']
    continuous_featnames = [] # these features will have shuffled values with added noise
    for contype in continuous_types:
        if contype in feature_types_metadata.feature_types_raw:
            continuous_featnames += feature_types_metadata.feature_types_raw[contype]
    for col in continuous_featnames:
        X_aug[col] = X_aug[col].astype(float)
        X[col] = X[col].astype(float)

    for i in range(num_augmented_samples):
        og_ind = i % len(X)
        augdata_i = X.iloc[og_ind].copy()
        neighbor_i = X.iloc[neigh_ind[og_ind]].copy()
        # dist_i = neigh_dist[og_ind]
        cols_toperturb = np.random.choice(list(X.columns), size=np.random.binomial(X.shape[1], p=perturb_prob, size=1)[0], replace=False)
        for col in cols_toperturb:
            new_val = neighbor_i[col]
            if col in continuous_featnames:
                new_val += np.random.normal(scale=np.abs(augdata_i[col]-new_val)/s)
            augdata_i[col] = new_val
        X_aug.iloc[i] = augdata_i

    return X_aug","1. Use `np.random.choice` instead of `np.random.binomial` to avoid overflow errors.
2. Use `np.random.seed` to ensure reproducibility.
3. Use `gc.collect()` to free up memory after each iteration."
"    def __init__(self, path_context: str, label: str, id_columns: list, feature_generator: AbstractFeatureGenerator, label_count_threshold=10,
                 problem_type=None, eval_metric=None, stopping_metric=None, is_trainer_present=False, random_seed=0):
        self.path, self.model_context, self.save_path = self.create_contexts(path_context)
        self.label = label
        self.submission_columns = id_columns
        self.threshold = label_count_threshold
        self.problem_type = problem_type
        self.eval_metric = eval_metric
        self.stopping_metric = stopping_metric
        self.is_trainer_present = is_trainer_present
        if random_seed is None:
            random_seed = random.randint(0, 1000000)
        self.random_seed = random_seed
        self.cleaner = None
        self.label_cleaner: LabelCleaner = None
        self.feature_generator: AbstractFeatureGenerator = feature_generator
        self.feature_generators = [self.feature_generator]

        self.trainer: AbstractTrainer = None
        self.trainer_type = None
        self.trainer_path = None
        self.reset_paths = False

        self.time_fit_total = None
        self.time_fit_preprocessing = None
        self.time_fit_training = None
        self.time_limit = None

        try:
            from .....version import __version__
            self.version = __version__
        except:
            self.version = None","1. Use `assert` statements to validate user input.
2. Use `type` annotations to make the code more type-safe.
3. Use `black` to format the code consistently."
"    def _fit(self, X: DataFrame, X_val: DataFrame = None, scheduler_options=None, hyperparameter_tune=False,
            feature_prune=False, holdout_frac=0.1, num_bagging_folds=0, num_bagging_sets=1, stack_ensemble_levels=0,
            hyperparameters=None, ag_args_fit=None, excluded_model_types=None, time_limit=None, save_data=False, save_bagged_folds=True, verbosity=2):
        """""" Arguments:
                X (DataFrame): training data
                X_val (DataFrame): data used for hyperparameter tuning. Note: final model may be trained using this data as well as training data
                hyperparameter_tune (bool): whether to tune hyperparameters or simply use default values
                feature_prune (bool): whether to perform feature selection
                scheduler_options (tuple: (search_strategy, dict): Options for scheduler
                holdout_frac (float): Fraction of data to hold out for evaluating validation performance (ignored if X_val != None, ignored if kfolds != 0)
                num_bagging_folds (int): kfolds used for bagging of models, roughly increases model training time by a factor of k (0: disabled)
                num_bagging_sets (int): number of repeats of kfold bagging to perform (values must be >= 1),
                    total number of models trained during bagging = num_bagging_folds * num_bagging_sets
                stack_ensemble_levels : (int) Number of stacking levels to use in ensemble stacking. Roughly increases model training time by factor of stack_levels+1 (0: disabled)
                    Default is 0 (disabled). Use values between 1-3 to improve model quality.
                    Ignored unless kfolds is also set >= 2
                hyperparameters (dict): keys = hyperparameters + search-spaces for each type of model we should train.
        """"""
        if hyperparameters is None:
            hyperparameters = {'NN': {}, 'GBM': {}}
        # TODO: if provided, feature_types in X, X_val are ignored right now, need to pass to Learner/trainer and update this documentation.
        if time_limit:
            self.time_limit = time_limit
            logger.log(20, f'Beginning AutoGluon training ... Time limit = {time_limit}s')
        else:
            self.time_limit = 1e7
            logger.log(20, 'Beginning AutoGluon training ...')
        logger.log(20, f'AutoGluon will save models to {self.path}')
        logger.log(20, f'AutoGluon Version:  {self.version}')
        logger.log(20, f'Train Data Rows:    {len(X)}')
        logger.log(20, f'Train Data Columns: {len(X.columns)}')
        if X_val is not None:
            logger.log(20, f'Tuning Data Rows:    {len(X_val)}')
            logger.log(20, f'Tuning Data Columns: {len(X_val.columns)}')
        time_preprocessing_start = time.time()
        logger.log(20, 'Preprocessing data ...')
        X, y, X_val, y_val, holdout_frac, num_bagging_folds = self.general_data_processing(X, X_val, holdout_frac, num_bagging_folds)
        time_preprocessing_end = time.time()
        self.time_fit_preprocessing = time_preprocessing_end - time_preprocessing_start
        logger.log(20, f'\\tData preprocessing and feature engineering runtime = {round(self.time_fit_preprocessing, 2)}s ...')
        if time_limit:
            time_limit_trainer = time_limit - self.time_fit_preprocessing
        else:
            time_limit_trainer = None

        trainer = self.trainer_type(
            path=self.model_context,
            problem_type=self.label_cleaner.problem_type_transform,
            eval_metric=self.eval_metric,
            stopping_metric=self.stopping_metric,
            num_classes=self.label_cleaner.num_classes,
            feature_types_metadata=self.feature_generator.feature_types_metadata,
            low_memory=True,
            kfolds=num_bagging_folds,
            n_repeats=num_bagging_sets,
            stack_ensemble_levels=stack_ensemble_levels,
            scheduler_options=scheduler_options,
            time_limit=time_limit_trainer,
            save_data=save_data,
            save_bagged_folds=save_bagged_folds,
            random_seed=self.random_seed,
            verbosity=verbosity
        )

        self.trainer_path = trainer.path
        if self.eval_metric is None:
            self.eval_metric = trainer.eval_metric
        if self.stopping_metric is None:
            self.stopping_metric = trainer.stopping_metric

        self.save()
        trainer.train(X, y, X_val, y_val, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, holdout_frac=holdout_frac,
                      hyperparameters=hyperparameters, ag_args_fit=ag_args_fit, excluded_model_types=excluded_model_types)
        self.save_trainer(trainer=trainer)
        time_end = time.time()
        self.time_fit_training = time_end - time_preprocessing_end
        self.time_fit_total = time_end - time_preprocessing_start
        logger.log(20, f'AutoGluon training complete, total runtime = {round(self.time_fit_total, 2)}s ...')","1. Use `trainer.train(X, y, X_val, y_val, ...)` instead of `trainer.fit(X, X_val, ...)` to avoid data leakage.
2. Use `trainer.save()` to save the trained model instead of manually saving the model.
3. Use `trainer.load()` to load the trained model instead of manually loading the model."
"    def general_data_processing(self, X: DataFrame, X_val: DataFrame, holdout_frac: float, num_bagging_folds: int):
        """""" General data processing steps used for all models. """"""
        X = copy.deepcopy(X)
        # TODO: We should probably uncomment the below lines, NaN label should be treated as just another value in multiclass classification -> We will have to remove missing, compute problem type, and add back missing if multiclass
        # if self.problem_type == MULTICLASS:
        #     X[self.label] = X[self.label].fillna('')

        # Remove all examples with missing labels from this dataset:
        missinglabel_inds = [index for index, x in X[self.label].isna().iteritems() if x]
        if len(missinglabel_inds) > 0:
            logger.warning(f""Warning: Ignoring {len(missinglabel_inds)} (out of {len(X)}) training examples for which the label value in column '{self.label}' is missing"")
            X = X.drop(missinglabel_inds, axis=0)

        if self.problem_type is None:
            self.problem_type = self.infer_problem_type(X[self.label])

        if X_val is not None and self.label in X_val.columns:
            # TODO: This is not an ideal solution, instead check if bagging and X_val exists with label, then merge them prior to entering general data processing.
            #  This solution should handle virtually all cases correctly, only downside is it might cut more classes than it needs to.
            self.threshold, holdout_frac, num_bagging_folds = self.adjust_threshold_if_necessary(X[self.label], threshold=self.threshold, holdout_frac=1, num_bagging_folds=num_bagging_folds)
        else:
            self.threshold, holdout_frac, num_bagging_folds = self.adjust_threshold_if_necessary(X[self.label], threshold=self.threshold, holdout_frac=holdout_frac, num_bagging_folds=num_bagging_folds)

        if (self.eval_metric is not None) and (self.eval_metric.name in ['log_loss', 'pac_score']) and (self.problem_type == MULTICLASS):
            X = augment_rare_classes(X, self.label, self.threshold)

        # Gets labels prior to removal of infrequent classes
        y_uncleaned = X[self.label].copy()

        self.cleaner = Cleaner.construct(problem_type=self.problem_type, label=self.label, threshold=self.threshold)
        # TODO: What if all classes in X are low frequency in multiclass? Currently we would crash. Not certain how many problems actually have this property
        X = self.cleaner.fit_transform(X)  # TODO: Consider merging cleaner into label_cleaner
        X, y = self.extract_label(X)
        self.label_cleaner = LabelCleaner.construct(problem_type=self.problem_type, y=y, y_uncleaned=y_uncleaned)
        y = self.label_cleaner.transform(y)

        if self.label_cleaner.num_classes is not None:
            logger.log(20, f'Train Data Class Count: {self.label_cleaner.num_classes}')

        if X_val is not None and self.label in X_val.columns:
            X_val = self.cleaner.transform(X_val)
            if len(X_val) == 0:
                logger.warning('All X_val data contained low frequency classes, ignoring X_val and generating from subset of X')
                X_val = None
                y_val = None
            else:
                X_val, y_val = self.extract_label(X_val)
                y_val = self.label_cleaner.transform(y_val)
        else:
            y_val = None

        # TODO: Move this up to top of data before removing data, this way our feature generator is better
        if X_val is not None:
            # Do this if working with SKLearn models, otherwise categorical features may perform very badly on the test set
            logger.log(15, 'Performing general data preprocessing with merged train & validation data, so validation performance may not accurately reflect performance on new test data')
            X_super = pd.concat([X, X_val], ignore_index=True)
            X_super = self.feature_generator.fit_transform(X_super, banned_features=self.submission_columns, drop_duplicates=False)
            X = X_super.head(len(X)).set_index(X.index)
            X_val = X_super.tail(len(X_val)).set_index(X_val.index)
            del X_super
        else:
            X = self.feature_generator.fit_transform(X, banned_features=self.submission_columns, drop_duplicates=False)

        return X, y, X_val, y_val, holdout_frac, num_bagging_folds","1. Use `.loc` instead of `.iloc` to access data.
2. Use `pd.concat()` instead of `+` to concatenate dataframes.
3. Use `pd.DataFrame.drop()` to drop columns instead of `del`."
"    def __init__(self, path: str, name: str, problem_type: str, eval_metric: Union[str, metrics.Scorer] = None, num_classes=None, stopping_metric=None, model=None, hyperparameters=None, features=None, feature_types_metadata: FeatureTypesMetadata = None, debug=0, **kwargs):
        """""" Creates a new model.
            Args:
                path (str): directory where to store all outputs.
                name (str): name of subdirectory inside path where model will be saved.
                problem_type (str): type of problem this model will handle. Valid options: ['binary', 'multiclass', 'regression'].
                eval_metric (str or autogluon.utils.tabular.metrics.Scorer): objective function the model intends to optimize. If None, will be inferred based on problem_type.
                hyperparameters (dict): various hyperparameters that will be used by model (can be search spaces instead of fixed values).
                feature_types_metadata (autogluon.utils.tabular.features.feature_types_metadata.FeatureTypesMetadata): contains feature type information that can be used to identify special features such as text ngrams and datetime.
        """"""
        self.name = name
        self.path_root = path
        self.path_suffix = self.name + os.path.sep  # TODO: Make into function to avoid having to reassign on load?
        self.path = self.create_contexts(self.path_root + self.path_suffix)  # TODO: Make this path a function for consistency.
        self.num_classes = num_classes
        self.model = model
        self.problem_type = problem_type
        if eval_metric is not None:
            self.eval_metric = metrics.get_metric(eval_metric, self.problem_type, 'eval_metric')  # Note: we require higher values = better performance
        else:
            self.eval_metric = infer_eval_metric(problem_type=self.problem_type)
            logger.log(20, f""Model {self.name}'s eval_metric inferred to be '{self.eval_metric.name}' because problem_type='{self.problem_type}' and eval_metric was not specified during init."")

        if stopping_metric is None:
            self.stopping_metric = self.eval_metric
        else:
            self.stopping_metric = stopping_metric

        if self.eval_metric.name in OBJECTIVES_TO_NORMALIZE:
            self.normalize_pred_probas = True
            logger.debug(self.name +"" predicted probabilities will be transformed to never =0 since eval_metric="" + self.eval_metric.name)
        else:
            self.normalize_pred_probas = False

        if isinstance(self.eval_metric, metrics._ProbaScorer):
            self.metric_needs_y_pred = False
        elif isinstance(self.eval_metric, metrics._ThresholdScorer):
            self.metric_needs_y_pred = False
        else:
            self.metric_needs_y_pred = True

        if isinstance(self.stopping_metric, metrics._ProbaScorer):
            self.stopping_metric_needs_y_pred = False
        elif isinstance(self.stopping_metric, metrics._ThresholdScorer):
            self.stopping_metric_needs_y_pred = False
        else:
            self.stopping_metric_needs_y_pred = True

        self.feature_types_metadata = feature_types_metadata  # TODO: Should this be passed to a model on creation? Should it live in a Dataset object and passed during fit? Currently it is being updated prior to fit by trainer
        self.features = features
        self.debug = debug

        self.fit_time = None  # Time taken to fit in seconds (Training data)
        self.predict_time = None  # Time taken to predict in seconds (Validation data)
        self.val_score = None  # Score with eval_metric (Validation data)

        self.params = {}
        self.params_aux = {}

        self._set_default_auxiliary_params()
        if hyperparameters is not None:
            hyperparameters = hyperparameters.copy()
            if AG_ARGS_FIT in hyperparameters:
                ag_args_fit = hyperparameters.pop(AG_ARGS_FIT)
                self.params_aux.update(ag_args_fit)
        self._set_default_params()
        self.nondefault_params = []
        if hyperparameters is not None:
            self.params.update(hyperparameters)
            self.nondefault_params = list(hyperparameters.keys())[:]  # These are hyperparameters that user has specified.
        self.params_trained = dict()","1. Use `os.path.join` instead of `+` to concatenate strings to avoid directory traversal attacks.
2. Use `json.dumps` with `default=str` to serialize objects to JSON, and `json.loads` with `object_hook=lambda x: x.decode(""utf-8"")` to deserialize JSON strings.
3. Use `pickle.dumps` with `protocol=4` to serialize objects to pickle, and `pickle.loads` with `encoding=""bytes""` to deserialize pickle strings."
"    def _set_default_auxiliary_params(self):
        # TODO: Consider adding to get_info() output
        default_auxiliary_params = dict(
            max_memory_usage_ratio=1.0,  # Ratio of memory usage allowed by the model. Values > 1.0 have an increased risk of causing OOM errors.
            # TODO: Add more params
            # max_memory_usage=None,
            # max_disk_usage=None,
            max_time_limit_ratio=1.0,  # ratio of given time_limit to use during fit(). If time_limit == 10 and max_time_limit_ratio=0.3, time_limit would be changed to 3.
            max_time_limit=None,  # max time_limit value during fit(). If the provided time_limit is greater than this value, it will be replaced by max_time_limit. Occurs after max_time_limit_ratio is applied.
            min_time_limit=0,  # min time_limit value during fit(). If the provided time_limit is less than this value, it will be replaced by min_time_limit. Occurs after max_time_limit is applied.
            # num_cpu=None,
            # num_gpu=None,
            # ignore_hpo=False,
            # max_early_stopping_rounds=None,
            # use_orig_features=True,  # TODO: Only for stackers
            # TODO: add option for only top-k ngrams
            ignored_feature_types_special=[],  # List, drops any features in `self.feature_types_metadata.feature_types_special[type]` for type in `ignored_feature_types_special`. | Currently undocumented in task.
            ignored_feature_types_raw=[],  # List, drops any features in `self.feature_types_metadata.feature_types_raw[type]` for type in `ignored_feature_types_raw`. | Currently undocumented in task.
        )
        for key, value in default_auxiliary_params.items():
            self._set_default_param_value(key, value, params=self.params_aux)","1. Use [secured default values](https://docs.python.org/3/library/stdtypes.html#default-values) for all parameters.
2. [Validate user input](https://docs.python.org/3/library/functions.html#input) before using it in the code.
3. [Sanitize user input](https://docs.python.org/3/library/string.html#string-methods) to prevent [cross-site scripting (XSS)](https://en.wikipedia.org/wiki/Cross-site_scripting) attacks."
"    def preprocess(self, X):
        if self.features is not None:
            # TODO: In online-inference this becomes expensive, add option to remove it (only safe in controlled environment where it is already known features are present
            if list(X.columns) != self.features:
                return X[self.features]
        else:
            self.features = list(X.columns)  # TODO: add fit and transform versions of preprocess instead of doing this
            ignored_feature_types_raw = self.params_aux.get('ignored_feature_types_raw', [])
            if ignored_feature_types_raw:
                for ignored_feature_type in ignored_feature_types_raw:
                    self.features = [feature for feature in self.features if feature not in self.feature_types_metadata.feature_types_raw[ignored_feature_type]]
            ignored_feature_types_special = self.params_aux.get('ignored_feature_types_special', [])
            if ignored_feature_types_special:
                for ignored_feature_type in ignored_feature_types_special:
                    self.features = [feature for feature in self.features if feature not in self.feature_types_metadata.feature_types_special[ignored_feature_type]]
            if not self.features:
                raise NoValidFeatures
            if ignored_feature_types_raw or ignored_feature_types_special:
                if list(X.columns) != self.features:
                    X = X[self.features]
        return X","1. Use `fit` and `transform` methods instead of `preprocess` to avoid unnecessary data copy.
2. Use `enumerate` instead of `list` to iterate over a sequence to avoid creating a new list.
3. Use `assert` to check for invalid inputs instead of raising exceptions."
"    def preprocess(self, X):
        X = convert_categorical_to_int(X)
        return super().preprocess(X)","1. Use `np.vectorize` to vectorize the `convert_categorical_to_int` function to improve performance.
2. Validate the input data to ensure that it is of the correct type and shape.
3. Handle errors gracefully by catching and logging exceptions."
"    def __init__(self, model_base: AbstractModel, save_bagged_folds=True, random_state=0, **kwargs):
        self.model_base = model_base
        self._child_type = type(self.model_base)
        self.models = []
        self._oof_pred_proba = None
        self._oof_pred_model_repeats = None
        self._n_repeats = 0  # Number of n_repeats with at least 1 model fit, if kfold=5 and 8 models have been fit, _n_repeats is 2
        self._n_repeats_finished = 0  # Number of n_repeats finished, if kfold=5 and 8 models have been fit, _n_repeats_finished is 1
        self._k_fold_end = 0  # Number of models fit in current n_repeat (0 if completed), if kfold=5 and 8 models have been fit, _k_fold_end is 3
        self._k = None  # k models per n_repeat, equivalent to kfold value
        self._k_per_n_repeat = []  # k-fold used for each n_repeat. == [5, 10, 3] if first kfold was 5, second was 10, and third was 3
        self._random_state = random_state
        self.low_memory = True
        self.bagged_mode = None
        self.save_bagged_folds = save_bagged_folds

        try:
            feature_types_metadata = self.model_base.feature_types_metadata
        except:
            feature_types_metadata = None

        eval_metric = kwargs.pop('eval_metric', self.model_base.eval_metric)
        stopping_metric = kwargs.pop('stopping_metric', self.model_base.stopping_metric)

        super().__init__(problem_type=self.model_base.problem_type, eval_metric=eval_metric, stopping_metric=stopping_metric, feature_types_metadata=feature_types_metadata, **kwargs)","1. Use `assert` statements to check for invalid inputs.
2. Use `type` checking to ensure that inputs are of the correct type.
3. Sanitize user input to prevent against injection attacks."
"    def _fit(self, X, y, k_fold=5, k_fold_start=0, k_fold_end=None, n_repeats=1, n_repeat_start=0, time_limit=None, **kwargs):
        if k_fold < 1:
            k_fold = 1
        if k_fold_end is None:
            k_fold_end = k_fold

        if self._oof_pred_proba is None and (k_fold_start != 0 or n_repeat_start != 0):
            self._load_oof()
        if n_repeat_start != self._n_repeats_finished:
            raise ValueError(f'n_repeat_start must equal self._n_repeats_finished, values: ({n_repeat_start}, {self._n_repeats_finished})')
        if n_repeats <= n_repeat_start:
            raise ValueError(f'n_repeats must be greater than n_repeat_start, values: ({n_repeats}, {n_repeat_start})')
        if k_fold_start != self._k_fold_end:
            raise ValueError(f'k_fold_start must equal previous k_fold_end, values: ({k_fold_start}, {self._k_fold_end})')
        if k_fold_start >= k_fold_end:
            # TODO: Remove this limitation if n_repeats > 1
            raise ValueError(f'k_fold_end must be greater than k_fold_start, values: ({k_fold_end}, {k_fold_start})')
        if (n_repeats - n_repeat_start) > 1 and k_fold_end != k_fold:
            # TODO: Remove this limitation
            raise ValueError(f'k_fold_end must equal k_fold when (n_repeats - n_repeat_start) > 1, values: ({k_fold_end}, {k_fold})')
        if self._k is not None and self._k != k_fold:
            raise ValueError(f'k_fold must equal previously fit k_fold value for the current n_repeat, values: (({k_fold}, {self._k})')
        fold_start = n_repeat_start * k_fold + k_fold_start
        fold_end = (n_repeats - 1) * k_fold + k_fold_end
        time_start = time.time()

        model_base = self._get_model_base()
        if self.features is not None:
            model_base.features = self.features
        model_base.feature_types_metadata = self.feature_types_metadata  # TODO: Don't pass this here

        if self.model_base is not None:
            self.save_model_base(self.model_base)
            self.model_base = None

        if k_fold == 1:
            if self._n_repeats != 0:
                raise ValueError(f'n_repeats must equal 0 when fitting a single model with k_fold < 2, values: ({self._n_repeats}, {k_fold})')
            model_base.set_contexts(path_context=self.path + model_base.name + os.path.sep)
            time_start_fit = time.time()
            model_base.fit(X_train=X, y_train=y, time_limit=time_limit, **kwargs)
            model_base.fit_time = time.time() - time_start_fit
            model_base.predict_time = None
            self._oof_pred_proba = model_base.predict_proba(X=X)  # TODO: Cheater value, will be overfit to valid set
            self._oof_pred_model_repeats = np.ones(shape=len(X), dtype=np.uint8)
            self._n_repeats = 1
            self._n_repeats_finished = 1
            self._k_per_n_repeat = [1]
            self.bagged_mode = False
            model_base.reduce_memory_size(remove_fit=True, remove_info=False, requires_save=True)
            if not self.save_bagged_folds:
                model_base.model = None
            if self.low_memory:
                self.save_child(model_base, verbose=False)
                self.models = [model_base.name]
            else:
                self.models = [model_base]
            self._add_child_times_to_bag(model=model_base)
            return

        # TODO: Preprocess data here instead of repeatedly
        kfolds = generate_kfold(X=X, y=y, n_splits=k_fold, stratified=self.is_stratified(), random_state=self._random_state, n_repeats=n_repeats)

        if self.problem_type == MULTICLASS:
            oof_pred_proba = np.zeros(shape=(len(X), len(y.unique())), dtype=np.float32)
        elif self.problem_type == SOFTCLASS:
            oof_pred_proba = np.zeros(shape=y.shape, dtype=np.float32)
        else:
            oof_pred_proba = np.zeros(shape=len(X))
        oof_pred_model_repeats = np.zeros(shape=len(X), dtype=np.uint8)

        models = []
        folds_to_fit = fold_end - fold_start
        for j in range(n_repeat_start, n_repeats):  # For each n_repeat
            cur_repeat_count = j - n_repeat_start
            fold_start_n_repeat = fold_start + cur_repeat_count * k_fold
            fold_end_n_repeat = min(fold_start_n_repeat + k_fold, fold_end)
            # TODO: Consider moving model fit inner for loop to a function to simply this code
            for i in range(fold_start_n_repeat, fold_end_n_repeat):  # For each fold
                folds_finished = i - fold_start
                folds_left = fold_end - i
                fold = kfolds[i]
                time_elapsed = time.time() - time_start
                if time_limit is not None:
                    time_left = time_limit - time_elapsed
                    required_time_per_fold = time_left / folds_left
                    time_limit_fold = required_time_per_fold * 0.8
                    if folds_finished > 0:
                        expected_time_required = time_elapsed * folds_to_fit / folds_finished
                        expected_remaining_time_required = expected_time_required * folds_left / folds_to_fit
                        if expected_remaining_time_required > time_left:
                            raise TimeLimitExceeded
                    if time_left <= 0:
                        raise TimeLimitExceeded
                else:
                    time_limit_fold = None

                time_start_fold = time.time()
                train_index, val_index = fold
                X_train, X_val = X.iloc[train_index, :], X.iloc[val_index, :]
                y_train, y_val = y.iloc[train_index], y.iloc[val_index]
                fold_model = copy.deepcopy(model_base)
                fold_model.name = f'{fold_model.name}_fold_{i}'
                fold_model.set_contexts(self.path + fold_model.name + os.path.sep)
                fold_model.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, time_limit=time_limit_fold, **kwargs)
                time_train_end_fold = time.time()
                if time_limit is not None:  # Check to avoid unnecessarily predicting and saving a model when an Exception is going to be raised later
                    if i != (fold_end - 1):
                        time_elapsed = time.time() - time_start
                        time_left = time_limit - time_elapsed
                        expected_time_required = time_elapsed * folds_to_fit / (folds_finished + 1)
                        expected_remaining_time_required = expected_time_required * (folds_left - 1) / folds_to_fit
                        if expected_remaining_time_required > time_left:
                            raise TimeLimitExceeded
                pred_proba = fold_model.predict_proba(X_val)
                time_predict_end_fold = time.time()
                fold_model.fit_time = time_train_end_fold - time_start_fold
                fold_model.predict_time = time_predict_end_fold - time_train_end_fold
                fold_model.val_score = fold_model.score_with_y_pred_proba(y=y_val, y_pred_proba=pred_proba)
                fold_model.reduce_memory_size(remove_fit=True, remove_info=False, requires_save=True)
                if not self.save_bagged_folds:
                    fold_model.model = None
                if self.low_memory:
                    self.save_child(fold_model, verbose=False)
                    models.append(fold_model.name)
                else:
                    models.append(fold_model)
                oof_pred_proba[val_index] += pred_proba
                oof_pred_model_repeats[val_index] += 1
                self._add_child_times_to_bag(model=fold_model)
            if (fold_end_n_repeat != fold_end) or (k_fold == k_fold_end):
                self._k_per_n_repeat.append(k_fold)
        self.models += models

        self.bagged_mode = True

        if self._oof_pred_proba is None:
            self._oof_pred_proba = oof_pred_proba
            self._oof_pred_model_repeats = oof_pred_model_repeats
        else:
            self._oof_pred_proba += oof_pred_proba
            self._oof_pred_model_repeats += oof_pred_model_repeats

        self._n_repeats = n_repeats
        if k_fold == k_fold_end:
            self._k = None
            self._k_fold_end = 0
            self._n_repeats_finished = self._n_repeats
        else:
            self._k = k_fold
            self._k_fold_end = k_fold_end
            self._n_repeats_finished = self._n_repeats - 1","1. Use `np.unique()` instead of `y.unique()` to avoid leaking information about the test set.
2. Use `np.copy()` instead of `copy.deepcopy()` to avoid creating a reference to the original object.
3. Use `os.path.join()` instead of `+` to avoid creating a path with directory traversal vulnerabilities."
"    def convert_to_refitfull_template(self):
        compressed_params = self._get_compressed_params()
        model_compressed = copy.deepcopy(self._get_model_base())
        model_compressed.feature_types_metadata = self.feature_types_metadata  # TODO: Don't pass this here
        model_compressed.params = compressed_params
        model_compressed.name = model_compressed.name + REFIT_FULL_SUFFIX
        model_compressed.set_contexts(self.path_root + model_compressed.name + os.path.sep)
        return model_compressed","1. **Use `functools.lru_cache` to memoize the `_get_compressed_params()` function.** This will improve performance and prevent the model from being re-compressed unnecessarily.
2. **Use `os.makedirs()` to create the directory for the compressed model if it does not exist.** This will prevent the model from being saved in an invalid location.
3. **Use `json.dumps()` to serialize the model parameters instead of `copy.deepcopy()`.** This will prevent the model from being accidentally overwritten."
"    def _fit(self, X, y, k_fold=5, k_fold_start=0, k_fold_end=None, n_repeats=1, n_repeat_start=0, compute_base_preds=True, time_limit=None, **kwargs):
        start_time = time.time()
        X = self.preprocess(X=X, preprocess=False, fit=True, compute_base_preds=compute_base_preds)
        if time_limit is not None:
            time_limit = time_limit - (time.time() - start_time)
        if len(self.models) == 0:
            if self.feature_types_metadata is None:  # TODO: This is probably not the best way to do this
                feature_types_raw = defaultdict(list)
                feature_types_raw['float'] = self.stack_columns
                feature_types_special = defaultdict(list)
                feature_types_special['stack'] = self.stack_columns
                self.feature_types_metadata = FeatureTypesMetadata(feature_types_raw=feature_types_raw, feature_types_special=feature_types_special)
            else:
                self.feature_types_metadata = copy.deepcopy(self.feature_types_metadata)
                self.feature_types_metadata.feature_types_raw['float'] += self.stack_columns
                self.feature_types_metadata.feature_types_special['stack'] += self.stack_columns
        super()._fit(X=X, y=y, k_fold=k_fold, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, time_limit=time_limit, **kwargs)","1. Use `pickle.dumps()` and `pickle.loads()` to serialize and deserialize objects instead of `copy.deepcopy()`.
2. Use `functools.partial()` to avoid exposing the full signature of a function to users.
3. Use `inspect.isclass()` to check if an object is a class before calling its `__init__()` method."
"    def hyperparameter_tune(self, X, y, k_fold, scheduler_options=None, compute_base_preds=True, **kwargs):
        if len(self.models) != 0:
            raise ValueError('self.models must be empty to call hyperparameter_tune, value: %s' % self.models)

        if len(self.models) == 0:
            if self.feature_types_metadata is None:  # TODO: This is probably not the best way to do this
                feature_types_raw = defaultdict(list)
                feature_types_raw['float'] = self.stack_columns
                feature_types_special = defaultdict(list)
                feature_types_special['stack'] = self.stack_columns
                self.feature_types_metadata = FeatureTypesMetadata(feature_types_raw=feature_types_raw, feature_types_special=feature_types_special)
            else:
                self.feature_types_metadata = copy.deepcopy(self.feature_types_metadata)
                self.feature_types_metadata.feature_types_raw['float'] += self.stack_columns
                self.feature_types_metadata.feature_types_special['stack'] += self.stack_columns
        self.model_base.feature_types_metadata = self.feature_types_metadata  # TODO: Move this

        # TODO: Preprocess data here instead of repeatedly
        X = self.preprocess(X=X, preprocess=False, fit=True, compute_base_preds=compute_base_preds)
        kfolds = generate_kfold(X=X, y=y, n_splits=k_fold, stratified=self.is_stratified(), random_state=self._random_state, n_repeats=1)

        train_index, test_index = kfolds[0]
        X_train, X_val = X.iloc[train_index, :], X.iloc[test_index, :]
        y_train, y_val = y.iloc[train_index], y.iloc[test_index]
        orig_time = scheduler_options[1]['time_out']
        scheduler_options[1]['time_out'] = orig_time * 0.8  # TODO: Scheduler doesn't early stop on final model, this is a safety net. Scheduler should be updated to early stop
        hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, scheduler_options=scheduler_options, **kwargs)
        scheduler_options[1]['time_out'] = orig_time

        stackers = {}
        stackers_performance = {}
        for i, (model_name, model_path) in enumerate(hpo_models.items()):
            child: AbstractModel = self._child_type.load(path=model_path)
            y_pred_proba = child.predict_proba(X_val)

            # TODO: Create new StackerEnsemble Here
            stacker = copy.deepcopy(self)
            stacker.name = stacker.name + os.path.sep + str(i)
            stacker.set_contexts(self.path_root + stacker.name + os.path.sep)

            if self.problem_type == MULTICLASS:
                oof_pred_proba = np.zeros(shape=(len(X), len(y.unique())))
            else:
                oof_pred_proba = np.zeros(shape=len(X))
            oof_pred_model_repeats = np.zeros(shape=len(X))
            oof_pred_proba[test_index] += y_pred_proba
            oof_pred_model_repeats[test_index] += 1

            stacker.model_base = None
            child.set_contexts(stacker.path + child.name + os.path.sep)
            stacker.save_model_base(child.convert_to_template())

            stacker._k = k_fold
            stacker._k_fold_end = 1
            stacker._n_repeats = 1
            stacker._oof_pred_proba = oof_pred_proba
            stacker._oof_pred_model_repeats = oof_pred_model_repeats
            child.name = child.name + '_fold_0'
            child.set_contexts(stacker.path + child.name + os.path.sep)
            if not self.save_bagged_folds:
                child.model = None
            if stacker.low_memory:
                stacker.save_child(child, verbose=False)
                stacker.models.append(child.name)
            else:
                stacker.models.append(child)
            stacker.val_score = child.val_score
            stacker._add_child_times_to_bag(model=child)

            stacker.save()
            stackers[stacker.name] = stacker.path
            stackers_performance[stacker.name] = stacker.val_score

        # TODO: hpo_results likely not correct because no renames
        return stackers, stackers_performance, hpo_results","1. Use `SecretManager` to store sensitive information like passwords and API keys.
2. Use `Pydantic` to validate input data.
3. Use `Flask-SQLAlchemy` to protect your database."
"    def _set_default_auxiliary_params(self):
        default_auxiliary_params = dict(
            ignored_feature_types_special=['text_ngram', 'text_special'],
            ignored_feature_types_raw=['category', 'object'],  # TODO: Eventually use category features
        )
        for key, value in default_auxiliary_params.items():
            self._set_default_param_value(key, value, params=self.params_aux)
        super()._set_default_auxiliary_params()","1. Use `params=self.params_aux` to avoid overwriting parent class parameters.
2. Use `super()._set_default_param_value(key, value, params=self.params_aux)` to call the parent class method.
3. Use `default_auxiliary_params` to define default parameter values."
"    def _get_types_of_features(self, df):
        """""" Returns dict with keys: : 'continuous', 'skewed', 'onehot', 'embed', 'language', values = ordered list of feature-names falling into each category.
            Each value is a list of feature-names corresponding to columns in original dataframe.
            TODO: ensure features with zero variance have already been removed before this function is called.
        """"""
        if self.types_of_features is not None:
            logger.warning(""Attempting to _get_types_of_features for LRModel, but previously already did this."")

        feature_types = self.feature_types_metadata.feature_types_raw

        categorical_featnames = feature_types['category'] + feature_types['object'] + feature_types['bool']
        continuous_featnames = feature_types['float'] + feature_types['int']  # + self.__get_feature_type_if_present('datetime')
        language_featnames = []  # TODO: Disabled currently, have to pass raw text data features here to function properly
        valid_features = categorical_featnames + continuous_featnames + language_featnames
        if len(categorical_featnames) + len(continuous_featnames) + len(language_featnames) != df.shape[1]:
            unknown_features = [feature for feature in df.columns if feature not in valid_features]
            df = df.drop(columns=unknown_features)
        self.features = list(df.columns)

        types_of_features = {'continuous': [], 'skewed': [], 'onehot': [], 'language': []}
        return self._select_features(df, types_of_features, categorical_featnames, language_featnames, continuous_featnames)","1. Use `df.dropna()` to remove rows with missing values before calling `_get_types_of_features()`.
2. Use `df.astype(np.float32)` to cast all features to the float32 data type.
3. Use `sklearn.preprocessing.OneHotEncoder()` to one-hot encode categorical features."
"    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._model_type = self._get_model_type()","1. **Use `assert` statements to validate input.** This will help to ensure that the model is only initialized with valid data, and can help to prevent security vulnerabilities.
2. **Use `type()` to check the type of input.** This will help to ensure that the model is only initialized with data of the correct type, and can help to prevent security vulnerabilities.
3. **Use `getattr()` to access attributes of the model.** This will help to prevent unauthorized access to model data, and can help to prevent security vulnerabilities."
"    def preprocess(self, X):
        X = super().preprocess(X).fillna(0)
        return X","1. Use `np.nan` instead of `0` to fill missing values.
2. Use `sklearn.preprocessing.Imputer` to impute missing values.
3. Use `sklearn.preprocessing.StandardScaler` to scale the data."
"    def _set_default_auxiliary_params(self):
        default_auxiliary_params = dict(
            ignored_feature_types_special=['text_ngram', 'text_as_category'],
        )
        for key, value in default_auxiliary_params.items():
            self._set_default_param_value(key, value, params=self.params_aux)
        super()._set_default_auxiliary_params()","1. Use a more secure default value for `ignored_feature_types_special`.
2. Use `self.params_aux` instead of `params` when setting default parameter values.
3. Call `super()._set_default_auxiliary_params()` after setting default parameter values."
"    def _fit(self, X_train, y_train, X_val=None, y_val=None, time_limit=None, reporter=None, **kwargs):
        """""" X_train (pd.DataFrame): training data features (not necessarily preprocessed yet)
            X_val (pd.DataFrame): test data features (should have same column names as Xtrain)
            y_train (pd.Series):
            y_val (pd.Series): are pandas Series
            kwargs: Can specify amount of compute resources to utilize (num_cpus, num_gpus).
        """"""
        start_time = time.time()
        params = self.params.copy()
        self.verbosity = kwargs.get('verbosity', 2)
        params = fixedvals_from_searchspaces(params)
        if self.feature_types_metadata is None:
            raise ValueError(""Trainer class must set feature_types_metadata for this model"")
        # print('features: ', self.features)
        if 'num_cpus' in kwargs:
            self.num_dataloading_workers = max(1, int(kwargs['num_cpus']/2.0))
        else:
            self.num_dataloading_workers = 1
        if self.num_dataloading_workers == 1:
            self.num_dataloading_workers = 0  # 0 is always faster and uses less memory than 1
        self.batch_size = params['batch_size']
        train_dataset, val_dataset = self.generate_datasets(X_train=X_train, y_train=y_train, params=params, X_val=X_val, y_val=y_val)
        logger.log(15, ""Training data for neural network has: %d examples, %d features (%d vector, %d embedding, %d language)"" %
              (train_dataset.num_examples, train_dataset.num_features,
               len(train_dataset.feature_groups['vector']), len(train_dataset.feature_groups['embed']),
               len(train_dataset.feature_groups['language']) ))
        # self._save_preprocessor() # TODO: should save these things for hyperparam tunning. Need one HP tuner for network-specific HPs, another for preprocessing HPs.

        if 'num_gpus' in kwargs and kwargs['num_gpus'] >= 1:  # Currently cannot use >1 GPU
            self.ctx = mx.gpu()  # Currently cannot use more than 1 GPU
        else:
            self.ctx = mx.cpu()
        self.get_net(train_dataset, params=params)

        if time_limit:
            time_elapsed = time.time() - start_time
            time_limit = time_limit - time_elapsed

        self.train_net(train_dataset=train_dataset, params=params, val_dataset=val_dataset, initialize=True, setup_trainer=True, time_limit=time_limit, reporter=reporter)
        self.params_post_fit = params
        """"""
        # TODO: if we don't want to save intermediate network parameters, need to do something like saving in temp directory to clean up after training:
        with make_temp_directory() as temp_dir:
            save_callback = SaveModelCallback(self.model, monitor=self.metric, mode=save_callback_mode, name=self.name)
            with progress_disabled_ctx(self.model) as model:
                original_path = model.path
                model.path = Path(temp_dir)
                model.fit_one_cycle(self.epochs, self.lr, callbacks=save_callback)

                # Load the best one and export it
                model.load(self.name)
                print(f'Model validation metrics: {model.validate()}')
                model.path = original_path\\
        """"""","1. Use `secure_filename` to sanitize user-provided filenames.
2. Use `os.makedirs` with the `exist_ok` flag to avoid creating directories that already exist.
3. Use `json.dumps` with the `indent` and `sort_keys` parameters to make the JSON output more readable."
"    def _get_types_of_features(self, df, skew_threshold, embed_min_categories, use_ngram_features):
        """""" Returns dict with keys: : 'continuous', 'skewed', 'onehot', 'embed', 'language', values = ordered list of feature-names falling into each category.
            Each value is a list of feature-names corresponding to columns in original dataframe.
            TODO: ensure features with zero variance have already been removed before this function is called.
        """"""
        if self.types_of_features is not None:
            Warning(""Attempting to _get_types_of_features for TabularNeuralNetModel, but previously already did this."")

        feature_types = self.feature_types_metadata.feature_types_raw

        categorical_featnames = feature_types['category'] + feature_types['object'] + feature_types['bool']
        continuous_featnames = feature_types['float'] + feature_types['int']  # + self.__get_feature_type_if_present('datetime')
        language_featnames = [] # TODO: not implemented. This should fetch text features present in the data
        valid_features = categorical_featnames + continuous_featnames + language_featnames
        if len(categorical_featnames) + len(continuous_featnames) + len(language_featnames) != df.shape[1]:
            unknown_features = [feature for feature in df.columns if feature not in valid_features]
            # print('unknown features:', unknown_features)
            df = df.drop(columns=unknown_features)
            self.features = list(df.columns)
            # raise ValueError(""unknown feature types present in DataFrame"")

        types_of_features = {'continuous': [], 'skewed': [], 'onehot': [], 'embed': [], 'language': []}
        # continuous = numeric features to rescale
        # skewed = features to which we will apply power (ie. log / box-cox) transform before normalization
        # onehot = features to one-hot encode (unknown categories for these features encountered at test-time are encoded as all zeros). We one-hot encode any features encountered that only have two unique values.
        for feature in self.features:
            feature_data = df[feature] # pd.Series
            num_unique_vals = len(feature_data.unique())
            if num_unique_vals == 2:  # will be onehot encoded regardless of proc.embed_min_categories value
                types_of_features['onehot'].append(feature)
            elif feature in continuous_featnames:
                if np.abs(feature_data.skew()) > skew_threshold:
                    types_of_features['skewed'].append(feature)
                else:
                    types_of_features['continuous'].append(feature)
            elif feature in categorical_featnames:
                if num_unique_vals >= embed_min_categories: # sufficiently many categories to warrant learned embedding dedicated to this feature
                    types_of_features['embed'].append(feature)
                else:
                    types_of_features['onehot'].append(feature)
            elif feature in language_featnames:
                types_of_features['language'].append(feature)
        return types_of_features","1. Use `assert` statements to check for invalid inputs.
2. Use `type` checking to ensure that inputs are of the correct type.
3. Sanitize user input to prevent against injection attacks."
"    def hyperparameter_tune(self, X_train, y_train, X_val, y_val, scheduler_options, **kwargs):
        time_start = time.time()
        """""" Performs HPO and sets self.params to best hyperparameter values """"""
        self.verbosity = kwargs.get('verbosity', 2)
        logger.log(15, ""Beginning hyperparameter tuning for Neural Network..."")
        self._set_default_searchspace() # changes non-specified default hyperparams from fixed values to search-spaces.
        if self.feature_types_metadata is None:
            raise ValueError(""Trainer class must set feature_types_metadata for this model"")
        scheduler_func = scheduler_options[0]
        scheduler_options = scheduler_options[1]
        if scheduler_func is None or scheduler_options is None:
            raise ValueError(""scheduler_func and scheduler_options cannot be None for hyperparameter tuning"")
        num_cpus = scheduler_options['resource']['num_cpus']
        # num_gpus = scheduler_options['resource']['num_gpus']  # TODO: Currently unused

        params_copy = self.params.copy()

        self.num_dataloading_workers = max(1, int(num_cpus/2.0))
        self.batch_size = params_copy['batch_size']
        train_dataset, val_dataset = self.generate_datasets(X_train=X_train, y_train=y_train, params=params_copy, X_val=X_val, y_val=y_val)
        train_path = self.path + ""train""
        val_path = self.path + ""validation""
        train_dataset.save(file_prefix=train_path)
        val_dataset.save(file_prefix=val_path)

        if not np.any([isinstance(params_copy[hyperparam], Space) for hyperparam in params_copy]):
            logger.warning(""Warning: Attempting to do hyperparameter optimization without any search space (all hyperparameters are already fixed values)"")
        else:
            logger.log(15, ""Hyperparameter search space for Neural Network: "")
            for hyperparam in params_copy:
                if isinstance(params_copy[hyperparam], Space):
                    logger.log(15, str(hyperparam)+ "":   ""+str(params_copy[hyperparam]))

        util_args = dict(
            train_path=train_path,
            val_path=val_path,
            model=self,
            time_start=time_start,
            time_limit=scheduler_options['time_out']
        )
        tabular_nn_trial.register_args(util_args=util_args, **params_copy)
        scheduler = scheduler_func(tabular_nn_trial, **scheduler_options)
        if ('dist_ip_addrs' in scheduler_options) and (len(scheduler_options['dist_ip_addrs']) > 0):
            # TODO: Ensure proper working directory setup on remote machines
            # This is multi-machine setting, so need to copy dataset to workers:
            logger.log(15, ""Uploading preprocessed data to remote workers..."")
            scheduler.upload_files([train_path+TabularNNDataset.DATAOBJ_SUFFIX,
                                train_path+TabularNNDataset.DATAVALUES_SUFFIX,
                                val_path+TabularNNDataset.DATAOBJ_SUFFIX,
                                val_path+TabularNNDataset.DATAVALUES_SUFFIX])  # TODO: currently does not work.
            logger.log(15, ""uploaded"")

        scheduler.run()
        scheduler.join_jobs()
        scheduler.get_training_curves(plot=False, use_legend=False)

        return self._get_hpo_results(scheduler=scheduler, scheduler_options=scheduler_options, time_start=time_start)","1. Use `SecretManager` to store sensitive information like passwords and API keys.
2. Use `Pydantic` to validate input data.
3. Use `Flask-CORS` to enable cross-origin resource sharing."
"    def __init__(self, path: str, problem_type: str, scheduler_options=None, eval_metric=None, stopping_metric=None,
                 num_classes=None, low_memory=False, feature_types_metadata=None, kfolds=0, n_repeats=1,
                 stack_ensemble_levels=0, time_limit=None, save_data=False, save_bagged_folds=True, random_seed=0, verbosity=2):
        self.path = path
        self.problem_type = problem_type
        self.feature_types_metadata = feature_types_metadata
        self.save_data = save_data
        self.random_seed = random_seed  # Integer value added to the stack level to get the random_seed for kfold splits or the train/val split if bagging is disabled
        self.verbosity = verbosity
        if eval_metric is not None:
            self.eval_metric = eval_metric
        else:
            self.eval_metric = infer_eval_metric(problem_type=self.problem_type)

        # stopping_metric is used to early stop all models except for aux models.
        if stopping_metric is not None:
            self.stopping_metric = stopping_metric
        elif self.eval_metric.name == 'roc_auc':
            self.stopping_metric = log_loss
        else:
            self.stopping_metric = self.eval_metric

        self.eval_metric_expects_y_pred = scorer_expects_y_pred(scorer=self.eval_metric)
        logger.log(25, ""AutoGluon will gauge predictive performance using evaluation metric: %s"" % self.eval_metric.name)
        if not self.eval_metric_expects_y_pred:
            logger.log(25, ""This metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()"")

        logger.log(20, ""To change this, specify the eval_metric argument of fit()"")
        logger.log(25, ""AutoGluon will early stop models using evaluation metric: %s"" % self.stopping_metric.name)
        self.num_classes = num_classes
        self.feature_prune = False # will be set to True if feature-pruning is turned on.
        self.low_memory = low_memory
        self.bagged_mode = True if kfolds >= 2 else False
        if self.bagged_mode:
            self.kfolds = kfolds  # int number of folds to do model bagging, < 2 means disabled
            self.stack_ensemble_levels = stack_ensemble_levels
            self.stack_mode = True if self.stack_ensemble_levels >= 1 else False
            self.n_repeats = n_repeats
        else:
            self.kfolds = 0
            self.stack_ensemble_levels = 0
            self.stack_mode = False
            self.n_repeats = 1
        self.save_bagged_folds = save_bagged_folds

        self.hyperparameters = {}  # TODO: This is currently required for fetching stacking layer models. Consider incorporating more elegantly

        # self.models_level_all['core'][0] # Includes base models
        # self.models_level_all['core'][1] # Stacker level 1
        # self.models_level_all['aux1'][1] # Stacker level 1 aux models, such as weighted_ensemble
        # self.models_level_all['core'][2] # Stacker level 2
        self.models_level = defaultdict(dd_list)

        self.model_best = None

        self.model_performance = {}  # TODO: Remove in future, use networkx.
        self.model_paths = {}
        self.model_types = {}  # Outer type, can be BaggedEnsemble, StackEnsemble (Type that is able to load the model)
        self.model_types_inner = {}  # Inner type, if Ensemble then it is the type of the inner model (May not be able to load with this type)
        self.models = {}
        self.model_graph = nx.DiGraph()
        self.model_full_dict = {}  # Dict of normal Model -> FULL Model
        self.reset_paths = False

        self.hpo_results = {}  # Stores summary of HPO process
        # Scheduler attributes:
        if scheduler_options is not None:
            self.scheduler_func = scheduler_options[0]  # unpack tuple
            self.scheduler_options = scheduler_options[1]
        else:
            self.scheduler_func = None
            self.scheduler_options = None

        self.time_limit = time_limit
        if self.time_limit is None:
            self.time_limit = 1e7
            self.ignore_time_limit = True
        else:
            self.ignore_time_limit = False
        self.time_train_start = None
        self.time_train_level_start = None
        self.time_limit_per_level = self.time_limit / (self.stack_ensemble_levels + 1)

        self.num_rows_train = None
        self.num_cols_train = None

        self.is_data_saved = False

        self.regress_preds_asprobas = False  # whether to treat regression predictions as class-probabilities (during distillation)","1. Use `assert` statements to validate user input.
2. Use `type` checking to ensure that user input is of the correct type.
3. Sanitize user input to prevent injection attacks."
"    def train_single(self, X_train, y_train, X_val, y_val, model, kfolds=None, k_fold_start=0, k_fold_end=None, n_repeats=None, n_repeat_start=0, level=0, time_limit=None):
        if kfolds is None:
            kfolds = self.kfolds
        if n_repeats is None:
            n_repeats = self.n_repeats
        if model.feature_types_metadata is None:
            model.feature_types_metadata = copy.deepcopy(self.feature_types_metadata)  # TODO: move this into model creation process?
        model_fit_kwargs = {}
        if self.scheduler_options is not None:
            model_fit_kwargs = {'verbosity': self.verbosity,
                                'num_cpus': self.scheduler_options['resource']['num_cpus'],
                                'num_gpus': self.scheduler_options['resource']['num_gpus']}  # Additional configurations for model.fit
        if self.bagged_mode or isinstance(model, WeightedEnsembleModel):
            model.fit(X=X_train, y=y_train, k_fold=kfolds, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, compute_base_preds=False, time_limit=time_limit, **model_fit_kwargs)
        else:
            model.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, time_limit=time_limit, **model_fit_kwargs)
        return model","1. Use `sha256` instead of `md5` for hashing passwords.
2. Use `cryptography` instead of `pycrypto` for encryption.
3. Use `flask-wtf` for form validation."
"    def train_single_full(self, X_train, y_train, X_val, y_val, model: AbstractModel, feature_prune=False,
                          hyperparameter_tune=True, stack_name='core', kfolds=None, k_fold_start=0, k_fold_end=None, n_repeats=None, n_repeat_start=0, level=0, time_limit=None):
        if (n_repeat_start == 0) and (k_fold_start == 0):
            model.feature_types_metadata = copy.deepcopy(self.feature_types_metadata)  # TODO: Don't set feature_types_metadata here
        if feature_prune:
            if n_repeat_start != 0:
                raise ValueError('n_repeat_start must be 0 to feature_prune, value = ' + str(n_repeat_start))
            elif k_fold_start != 0:
                raise ValueError('k_fold_start must be 0 to feature_prune, value = ' + str(k_fold_start))
            self.autotune(X_train=X_train, X_holdout=X_val, y_train=y_train, y_holdout=y_val, model_base=model)  # TODO: Update to use CV instead of holdout
        if hyperparameter_tune:
            if self.scheduler_func is None or self.scheduler_options is None:
                raise ValueError(""scheduler_options cannot be None when hyperparameter_tune = True"")
            if n_repeat_start != 0:
                raise ValueError('n_repeat_start must be 0 to hyperparameter_tune, value = ' + str(n_repeat_start))
            elif k_fold_start != 0:
                raise ValueError('k_fold_start must be 0 to hyperparameter_tune, value = ' + str(k_fold_start))
            # hpo_models (dict): keys = model_names, values = model_paths
            try:
                if isinstance(model, BaggedEnsembleModel):
                    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X_train, y=y_train, k_fold=kfolds, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
                else:
                    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
            except Exception as err:
                if self.verbosity >= 1:
                    traceback.print_tb(err.__traceback__)
                logger.exception('Warning: Exception caused ' + model.name + ' to fail during hyperparameter tuning... Skipping this model.')
                logger.debug(err)
                del model
                model_names_trained = []
            else:
                self.hpo_results[model.name] = hpo_results
                model_names_trained = []
                for model_hpo_name, model_path in hpo_models.items():
                    model_hpo = self.load_model(model_hpo_name, path=model_path, model_type=type(model))
                    self.add_model(model=model_hpo, stack_name=stack_name, level=level)
                    model_names_trained.append(model_hpo.name)
        else:
            model_names_trained = self.train_and_save(X_train, y_train, X_val, y_val, model, stack_name=stack_name, kfolds=kfolds, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, level=level, time_limit=time_limit)
        self.save()
        return model_names_trained","1. Use `assert` statements to validate the inputs to the function.
2. Use `try` and `except` blocks to catch and handle errors.
3. Use `logging` to log all errors and warnings."
"    def distill(self, X_train=None, y_train=None, X_val=None, y_val=None,
                time_limits=None, hyperparameters=None, holdout_frac=None, verbosity=None,
                models_name_suffix=None, teacher_preds='soft',
                augmentation_data=None, augment_method='spunge', augment_args={'size_factor':5,'max_size':int(1e5)}):
        """""" Various distillation algorithms.
            Args:
                X_train, y_train: pd.DataFrame and pd.Series of training data.
                    If None, original training data used during TabularPrediction.fit() will be loaded.
                    This data is split into train/validation if X_val, y_val are None.
                X_val, y_val: pd.DataFrame and pd.Series of validation data.
                time_limits, hyperparameters, holdout_frac: defined as in TabularPrediction.fit()
                teacher_preds (None or str): If None, we only train with original labels (no data augmentation, overrides augment_method)
                    If 'hard', labels are hard teacher predictions given by: teacher.predict()
                    If 'soft', labels are soft teacher predictions given by: teacher.predict_proba()
                    Note: 'hard' and 'soft' are equivalent for regression problems.
                    If augment_method specified, teacher predictions are only used to label augmented data (training data keeps original labels).
                    To apply label-smoothing: teacher_preds='onehot' will use original training data labels converted to one-hots for multiclass (no data augmentation).  # TODO: expose smoothing-hyperparameter.
                models_name_suffix (str): Suffix to append to each student model's name, new names will look like: 'MODELNAME_dstl_SUFFIX'
                augmentation_data: pd.DataFrame of additional data to use as ""augmented data"" (does not contain labels).
                    When specified, augment_method, augment_args are ignored, and this is the only augmented data that is used (teacher_preds cannot be None).
                augment_method (None or str): specifies which augmentation strategy to utilize. Options: [None, 'spunge','munge']
                    If None, no augmentation gets applied.
                }
                augment_args (dict): args passed into the augmentation function corresponding to augment_method.
        """"""
        if verbosity is None:
            verbosity = self.verbosity

        hyperparameter_tune = False  # TODO: add as argument with scheduler options.
        if augmentation_data is not None and teacher_preds is None:
            raise ValueError(""augmentation_data must be None if teacher_preds is None"")

        logger.log(20, f""Distilling with teacher_preds={str(teacher_preds)}, augment_method={str(augment_method)} ..."")
        if X_train is None:
            if y_train is not None:
                raise ValueError(""X cannot be None when y specified."")
            X_train = self.load_X_train()
            if not self.bagged_mode:
                try:
                    X_val = self.load_X_val()
                except FileNotFoundError:
                    pass

        if y_train is None:
            y_train = self.load_y_train()
            if not self.bagged_mode:
                try:
                    y_val = self.load_y_val()
                except FileNotFoundError:
                    pass

        if X_val is None:
            if y_val is not None:
                raise ValueError(""X_val cannot be None when y_val specified."")
            if holdout_frac is None:
                holdout_frac = default_holdout_frac(len(X_train), hyperparameter_tune)
            X_train, X_val, y_train, y_val = generate_train_test_split(X_train, y_train, problem_type=self.problem_type, test_size=holdout_frac)

        y_val_og = y_val.copy()
        og_bagged_mode = self.bagged_mode
        og_verbosity = self.verbosity
        self.bagged_mode = False  # turn off bagging
        self.verbosity = verbosity  # change verbosity for distillation

        if teacher_preds is None or teacher_preds == 'onehot':
            augment_method = None
            logger.log(20, ""Training students without a teacher model. Set teacher_preds = 'soft' or 'hard' to distill using the best AutoGluon predictor as teacher."")

        if teacher_preds in ['onehot','soft']:
            y_train = format_distillation_labels(y_train, self.problem_type, self.num_classes)
            y_val = format_distillation_labels(y_val, self.problem_type, self.num_classes)

        if augment_method is None and augmentation_data is None:
            if teacher_preds == 'hard':
                y_pred = pd.Series(self.predict(X_train))
                if (self.problem_type != REGRESSION) and (len(y_pred.unique()) < len(y_train.unique())):  # add missing labels
                    logger.log(15, ""Adding missing labels to distillation dataset by including some real training examples"")
                    indices_to_add = []
                    for clss in y_train.unique():
                        if clss not in y_pred.unique():
                            logger.log(15, f""Fetching a row with label={clss} from training data"")
                            clss_index = y_train[y_train == clss].index[0]
                            indices_to_add.append(clss_index)
                    X_extra = X_train.loc[indices_to_add].copy()
                    y_extra = y_train.loc[indices_to_add].copy()  # these are actually real training examples
                    X_train = pd.concat([X_train, X_extra])
                    y_pred = pd.concat([y_pred, y_extra])
                y_train = y_pred
            elif teacher_preds == 'soft':
                y_train = self.predict_proba(X_train)
                if self.problem_type == MULTICLASS:
                    y_train = pd.DataFrame(y_train)
                else:
                    y_train = pd.Series(y_train)
        else:
            X_aug = augment_data(X_train=X_train, feature_types_metadata=self.feature_types_metadata,
                                augmentation_data=augmentation_data, augment_method=augment_method, augment_args=augment_args)
            if len(X_aug) > 0:
                if teacher_preds == 'hard':
                    y_aug = pd.Series(self.predict(X_aug))
                elif teacher_preds == 'soft':
                    y_aug = self.predict_proba(X_aug)
                    if self.problem_type == MULTICLASS:
                        y_aug = pd.DataFrame(y_aug)
                    else:
                        y_aug = pd.Series(y_aug)
                else:
                    raise ValueError(f""Unknown teacher_preds specified: {teacher_preds}"")

                X_train = pd.concat([X_train, X_aug])
                y_train = pd.concat([y_train, y_aug])

        X_train.reset_index(drop=True, inplace=True)
        y_train.reset_index(drop=True, inplace=True)

        student_suffix = '_DSTL'  # all student model names contain this substring
        if models_name_suffix is not None:
            student_suffix = student_suffix + ""_"" + models_name_suffix

        if hyperparameters is None:
            hyperparameters = copy.deepcopy(self.hyperparameters)
            student_model_types = ['GBM','CAT','NN','RF']  # only model types considered for distillation
            default_level_key = 'default'
            if default_level_key in hyperparameters:
                hyperparameters[default_level_key] = {key: hyperparameters[default_level_key][key] for key in hyperparameters[default_level_key] if key in student_model_types}
            else:
                hyperparameters ={key: hyperparameters[key] for key in hyperparameters if key in student_model_types}
                if len(hyperparameters) == 0:
                    raise ValueError(""Distillation not yet supported for fit() with per-stack level hyperparameters. ""
                        ""Please either manually specify `hyperparameters` in `distill()` or call `fit()` again without per-level hyperparameters before distillation.""
                        ""Also at least one of the following model-types must be present in hyperparameters: ['GBM','CAT','NN','RF']"")
        else:
            hyperparameters = self._process_hyperparameters(hyperparameters=hyperparameters, ag_args_fit=None, excluded_model_types=None)  # TODO: consider exposing ag_args_fit, excluded_model_types as distill() arguments.
        if teacher_preds is None or teacher_preds == 'hard':
            models_distill = get_preset_models(path=self.path, problem_type=self.problem_type,
                                eval_metric=self.eval_metric, stopping_metric=self.stopping_metric,
                                num_classes=self.num_classes, hyperparameters=hyperparameters, name_suffix=student_suffix)
        else:
            models_distill = get_preset_models_distillation(path=self.path, problem_type=self.problem_type,
                                eval_metric=self.eval_metric, stopping_metric=self.stopping_metric,
                                num_classes=self.num_classes, hyperparameters=hyperparameters, name_suffix=student_suffix)
            if self.problem_type != REGRESSION:
                self.regress_preds_asprobas = True

        self.time_train_start = time.time()
        self.time_limit = time_limits
        distilled_model_names = []
        for model in models_distill:
            time_left = None
            if time_limits is not None:
                time_start_model = time.time()
                time_left = time_limits - (time_start_model - self.time_train_start)

            logger.log(15, f""Distilling student {str(model.name)} with teacher_preds={str(teacher_preds)}, augment_method={str(augment_method)}..."")
            models = self.train_single_full(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, model=model,
                                            hyperparameter_tune=False, stack_name=self.distill_stackname, time_limit=time_left)
            for model_name in models:  # finally measure original metric on validation data and overwrite stored val_scores
                model_score = self.score(X_val, y_val_og, model=model_name)
                self.model_performance[model_name] = model_score
                model_obj = self.load_model(model_name)
                model_obj.val_score = model_score
                model_obj.save()  # TODO: consider omitting for sake of efficiency
                self.model_graph.nodes[model_name]['val_score'] = model_score
                distilled_model_names.append(model_name)
                logger.log(20, '\\t' + str(round(model_obj.val_score, 4)) + '\\t = Validation ' + self.eval_metric.name + ' score')
        # reset trainer to old state before distill() was called:
        self.bagged_mode = og_bagged_mode  # TODO: Confirm if safe to train future models after training models in both bagged and non-bagged modes
        self.verbosity = og_verbosity
        return distilled_model_names","1. Use `assert` statements to validate inputs.
2. Use `type` annotations to make the code type-safe.
3. Use `black` to format the code consistently."
"def unpack(g):
    def _unpack_inner(f):
        @functools.wraps(f)
        def _call(**kwargs):
            return f(**g(**kwargs))
        return _call
    return _unpack_inner","1. Use `functools.partial` instead of `functools.wraps` to avoid creating a new function object.
2. Use `**kwargs` instead of `*args` to avoid positional arguments.
3. Use `inspect.getfullargspec` to get the function's argument names and types."
"    def _unpack_inner(f):
        @functools.wraps(f)
        def _call(**kwargs):
            return f(**g(**kwargs))
        return _call","1. Use `functools.partial` instead of `functools.wraps` to avoid creating a new function object.
2. Use `**kwargs` instead of `*args` to avoid passing positional arguments by mistake.
3. Use `inspect.getfullargspec` to get the argument names of the function being wrapped."
"        def _call(**kwargs):
            return f(**g(**kwargs))","1. Sanitize user input to prevent injection attacks.
2. Use proper error handling to prevent leaking sensitive information.
3. Use secure defaults for all security-related settings."
"def set_presets(**kwargs):
    if 'presets' in kwargs:
        presets = kwargs['presets']
        if presets is None:
            return kwargs
        if not isinstance(presets, list):
            presets = [presets]
        preset_kwargs = {}
        for preset in presets:
            if isinstance(preset, str):
                preset_orig = preset
                preset = preset_dict.get(preset, None)
                if preset is None:
                    raise ValueError(f'Preset \\'{preset_orig}\\' was not found. Valid presets: {list(preset_dict.keys())}')
            if isinstance(preset, dict):
                for key in preset:
                    preset_kwargs[key] = preset[key]
            else:
                raise TypeError(f'Preset of type {type(preset)} was given, but only presets of type [dict, str] are valid.')
        for key in preset_kwargs:
            if key not in kwargs:
                kwargs[key] = preset_kwargs[key]
    return kwargs","1. Validate presets before using them.
2. Use `typing` to specify the types of arguments and return values.
3. Use `assert` statements to check for errors."
"    def generate_features(self, X: DataFrame):
        if not self.fit:
            self._compute_feature_transformations()
        X_features = pd.DataFrame(index=X.index)
        for column in X.columns:
            if X[column].dtype.name == 'object':
                X[column].fillna('', inplace=True)
            else:
                X[column].fillna(np.nan, inplace=True)

        X_text_features_combined = []
        if self.feature_transformations['text_special']:
            for nlp_feature in self.feature_transformations['text_special']:
                X_text_features = self.generate_text_features(X[nlp_feature], nlp_feature)
                X_text_features_combined.append(X_text_features)
            X_text_features_combined = pd.concat(X_text_features_combined, axis=1)

        X = self.preprocess(X)

        if self.feature_transformations['raw']:
            X_features = X_features.join(X[self.feature_transformations['raw']])

        if self.feature_transformations['category']:
            X_categoricals = X[self.feature_transformations['category']]
            # TODO: Add stateful categorical generator, merge rare cases to an unknown value
            # TODO: What happens when training set has no unknown/rare values but test set does? What models can handle this?
            if 'text' in self.feature_type_family:
                self.feature_type_family_generated['text_as_category'] += self.feature_type_family['text']
            X_categoricals = X_categoricals.astype('category')
            X_features = X_features.join(X_categoricals)

        if self.feature_transformations['text_special']:
            if not self.fit:
                self.features_binned += list(X_text_features_combined.columns)
                self.feature_type_family_generated['text_special'] += list(X_text_features_combined.columns)
            X_features = X_features.join(X_text_features_combined)

        if self.feature_transformations['datetime']:
            for datetime_feature in self.feature_transformations['datetime']:
                X_features[datetime_feature] = pd.to_datetime(X[datetime_feature])
                X_features[datetime_feature] = pd.to_numeric(X_features[datetime_feature])  # TODO: Use actual date info
                self.feature_type_family_generated['datetime'].append(datetime_feature)
                # TODO: Add fastai date features

        if self.feature_transformations['text_ngram']:
            # Combine Text Fields
            features_nlp_current = ['__nlp__']

            if not self.fit:
                features_nlp_to_remove = []
                logger.log(15, 'Fitting vectorizer for text features: ' + str(self.feature_transformations['text_ngram']))
                for nlp_feature in features_nlp_current:
                    # TODO: Preprocess text?
                    if nlp_feature == '__nlp__':
                        text_list = list(set(['. '.join(row) for row in X[self.feature_transformations['text_ngram']].values]))
                    else:
                        text_list = list(X[nlp_feature].drop_duplicates().values)
                    vectorizer_raw = copy.deepcopy(self.vectorizer_default_raw)
                    try:
                        vectorizer_fit, _ = self.train_vectorizer(text_list, vectorizer_raw)
                        self.vectorizers.append(vectorizer_fit)
                    except ValueError:
                        logger.debug(""Removing 'text_ngram' features due to error"")
                        features_nlp_to_remove = self.feature_transformations['text_ngram']

                self.feature_transformations['text_ngram'] = [feature for feature in self.feature_transformations['text_ngram'] if feature not in features_nlp_to_remove]

            X_features_cols_prior_to_nlp = list(X_features.columns)
            downsample_ratio = None
            nlp_failure_count = 0
            keep_trying_nlp = True
            while keep_trying_nlp:
                try:
                    X_nlp_features_combined = self.generate_text_ngrams(X=X, features_nlp_current=features_nlp_current, downsample_ratio=downsample_ratio)

                    if self.feature_transformations['text_ngram']:
                        X_features = X_features.join(X_nlp_features_combined)

                    if not self.fit:
                        self.feature_type_family_generated['text_ngram'] += list(X_nlp_features_combined.columns)
                    keep_trying_nlp = False
                except Exception as err:
                    nlp_failure_count += 1
                    if self.fit:
                        logger.exception('Error: OOM error during NLP feature transform, unrecoverable. Increase memory allocation or reduce data size to avoid this error.')
                        raise
                    traceback.print_tb(err.__traceback__)

                    X_features = X_features[X_features_cols_prior_to_nlp]
                    skip_nlp = False
                    for vectorizer in self.vectorizers:
                        vocab_size = len(vectorizer.vocabulary_)
                        if vocab_size <= 50:
                            skip_nlp = True
                            break
                    else:
                        if nlp_failure_count >= 3:
                            skip_nlp = True

                    if skip_nlp:
                        logger.log(15, 'Warning: ngrams generation resulted in OOM error, removing ngrams features. If you want to use ngrams for this problem, increase memory allocation for AutoGluon.')
                        logger.debug(str(err))
                        self.vectorizers = []
                        if 'text_ngram' in self.feature_transformations:
                            self.feature_transformations.pop('text_ngram')
                        if 'text_ngram' in self.feature_type_family_generated:
                            self.feature_type_family_generated.pop('text_ngram')
                        self.enable_nlp_features = False
                        keep_trying_nlp = False
                    else:
                        logger.log(15, 'Warning: ngrams generation resulted in OOM error, attempting to reduce ngram feature count. If you want to optimally use ngrams for this problem, increase memory allocation for AutoGluon.')
                        logger.debug(str(err))
                        downsample_ratio = 0.25

        return X_features","1. Use `fit` and `transform` methods instead of `fit_transform` to avoid data leakage.
2. Catch and handle exceptions properly.
3. Use `downsample_ratio` to reduce the size of the data if there is a risk of OOM errors."
"    def Dataset(*args, **kwargs):
        """"""Dataset for AutoGluon image classification tasks. 
           May either be a :class:`autogluon.task.image_classification.ImageFolderDataset`, :class:`autogluon.task.image_classification.RecordDataset`, 
           or a popular dataset already built into AutoGluon ('mnist', 'fashionmnist', 'cifar10', 'cifar100', 'imagenet').

        Parameters
        ----------
        name : str, optional
            Which built-in dataset to use, will override all other options if specified.
            The options are: 'mnist', 'fashionmnist', 'cifar', 'cifar10', 'cifar100', 'imagenet'
        train : bool, default = True
            Whether this dataset should be used for training or validation.
        train_path : str
            The training data location. If using :class:`ImageFolderDataset`,
            image folder`path/to/the/folder` should be provided. 
            If using :class:`RecordDataset`, the `path/to/*.rec` should be provided.
        input_size : int
            The input image size.
        crop_ratio : float
            Center crop ratio (for evaluation only).
        
        Returns
        -------
        Dataset object that can be passed to `task.fit()`, which is actually an :class:`autogluon.space.AutoGluonObject`. 
        To interact with such an object yourself, you must first call `Dataset.init()` to instantiate the object in Python.
        """"""
        return get_dataset(*args, **kwargs)","1. Use `typing` to annotate the function parameters and return types.
2. Validate the input parameters to ensure they are of the correct type and within the expected range.
3. Use `assert` statements to check for errors in the function logic."
"def auto_suggest_network(dataset, net):
    if isinstance(dataset, str):
        dataset_name = dataset
    elif isinstance(dataset, AutoGluonObject):
        if 'name' in dataset.kwargs:
            dataset_name = dataset.kwargs['name']
        else:
            return net
    else:
        return net
    dataset_name = dataset_name.lower()
    if 'mnist' in dataset_name:
        if isinstance(net, str) or isinstance(net, Categorical):
            net = mnist_net()
            logger.info('Auto suggesting network net for dataset {}'.format(net, dataset_name))
            return net
    elif 'cifar' in dataset_name:
        if isinstance(net, str):
            if 'cifar' not in net:
                net = 'cifar_resnet20_v1'
        elif isinstance(net, Categorical):
            newdata = []
            for x in net.data:
                if 'cifar' in x:
                    newdata.append(x)
            net.data = newdata if len(newdata) > 0 else ['cifar_resnet20_v1', 'cifar_resnet56_v1']
        logger.info('Auto suggesting network net for dataset {}'.format(net, dataset_name))
        return net","1. Use `assert` statements to validate user input.
2. Sanitize user input to prevent injection attacks.
3. Use a secure password hashing function to protect user passwords."
"def balanced_accuracy(solution, prediction):
    y_type, solution, prediction = _check_targets(solution, prediction)

    if y_type not in [""binary"", ""multiclass"", 'multilabel-indicator']:
        raise ValueError(f""{y_type} is not supported"")

    if y_type == 'binary':
        # Do not transform into any multiclass representation
        pass

    elif y_type == 'multiclass':
        # Need to create a multiclass solution and a multiclass predictions
        max_class = int(np.max((np.max(solution), np.max(prediction))))
        solution_binary = np.zeros((len(solution), max_class + 1))
        prediction_binary = np.zeros((len(prediction), max_class + 1))
        for i in range(len(solution)):
            solution_binary[i, int(solution[i])] = 1
            prediction_binary[i, int(prediction[i])] = 1
        solution = solution_binary
        prediction = prediction_binary

    elif y_type == 'multilabel-indicator':
        solution = solution.toarray()
        prediction = prediction.toarray()
    else:
        raise NotImplementedError(f'bac_metric does not support task type {y_type}')

    fn = np.sum(np.multiply(solution, (1 - prediction)), axis=0, dtype=float)
    tp = np.sum(np.multiply(solution, prediction), axis=0, dtype=float)
    # Bounding to avoid division by 0
    eps = 1e-15
    tp = np.maximum(eps, tp)
    pos_num = np.maximum(eps, tp + fn)
    tpr = tp / pos_num  # true positive rate (sensitivity)

    if y_type in ('binary', 'multilabel-indicator'):
        tn = np.sum(
            np.multiply((1 - solution), (1 - prediction)),
            axis=0, dtype=float
        )
        fp = np.sum(
            np.multiply((1 - solution), prediction),
            axis=0, dtype=float
        )
        tn = np.maximum(eps, tn)
        neg_num = np.maximum(eps, tn + fp)
        tnr = tn / neg_num  # true negative rate (specificity)
        bac = 0.5 * (tpr + tnr)
    elif y_type == 'multiclass':
        bac = tpr
    else:
        raise ValueError(y_type)

    return np.mean(bac)  # average over all classes","1. Use `np.clip` to bound the values of `tp` and `pos_num` to avoid division by 0.
2. Use `np.argmax` to find the index of the maximum value in `solution` and `prediction` to avoid overflow.
3. Use `np.unique` to find the unique values in `solution` and `prediction` to avoid errors."
"def generate_csv(inds, path):
    with open(path, 'w') as csvFile:
        row = ['id', 'category']
        writer = csv.writer(csvFile)
        writer.writerow(row)
        id = 1
        for ind in inds:
            row = [id, ind.asscalar()]
            writer = csv.writer(csvFile)
            writer.writerow(row)
            id += 1
    csvFile.close()","1. Use `with open(path, 'r')` instead of `open(path, 'w')` to open the file in read mode. This will prevent the file from being overwritten.
2. Use `csv.DictWriter()` instead of `csv.writer()` to write the data to the CSV file. This will ensure that the data is properly formatted.
3. Close the file after writing the data to it by using `csvFile.close()`. This will free up any resources that were used by the file."
"    def build(self, hp, inputs=None):
        input_node = nest.flatten(inputs)[0]

        pretrained = self.pretrained
        if input_node.shape[3] not in [1, 3]:
            if self.pretrained:
                raise ValueError(
                    ""When pretrained is set to True, expect input to ""
                    ""have 1 or 3 channels, bug got ""
                    ""{channels}."".format(channels=input_node.shape[3])
                )
            pretrained = False
        if pretrained is None:
            pretrained = hp.Boolean(""pretrained"", default=False)

        if len(self.models) > 1:
            version = hp.Choice(""version"", list(self.models.keys()))
        else:
            version = list(self.models.keys())[0]

        min_size = self.min_size
        if hp.Boolean(""imagenet_size"", default=False):
            min_size = 224
        if input_node.shape[1] < min_size or input_node.shape[2] < min_size:
            input_node = layers.experimental.preprocessing.Resizing(
                max(min_size, input_node.shape[1]),
                max(min_size, input_node.shape[2]),
            )(input_node)
        if input_node.shape[3] == 1:
            input_node = layers.Concatenate()([input_node] * 3)
        if input_node.shape[3] != 3:
            input_node = layers.Conv2D(filters=3, kernel_size=1, padding=""same"")(
                input_node
            )

        if pretrained:
            model = self.models[version](weights=""imagenet"", include_top=False)
            model.trainable = hp.Boolean(""trainable"", default=False)
        else:
            model = self.models[version](
                weights=None, include_top=False, input_shape=input_node.shape[1:]
            )

        return model(input_node)","1. Use `tf.keras.layers.experimental.preprocessing.Resizing` to resize the input image to a fixed size.
2. Use `tf.keras.layers.Concatenate` to expand the input image to 3 channels if it is grayscale.
3. Use `tf.keras.layers.Conv2D` to convert the input image to 3 channels if it is not 3 channels."
"    def _get_best_hps(self):
        best_trials = self.get_best_trials()
        if best_trials:
            return best_trials[0].hyperparameters
        else:
            return self.hyperparameters","1. Use `joblib.dump` to serialize the `Trial` objects instead of `pickle`.
2. Use `joblib.load` to deserialize the `Trial` objects instead of `pickle.load`.
3. Set the `joblib.tempdir` environment variable to a secure location."
"    def _generate_hp_values(self, hp_names):
        best_hps = self._get_best_hps()

        collisions = 0
        while True:
            hps = kerastuner.HyperParameters()
            # Generate a set of random values.
            for hp in best_hps.space:
                hps.merge([hp])
                # if not active, do nothing.
                # if active, check if selected to be changed.
                if hps.is_active(hp):
                    # if was active and not selected, do nothing.
                    if best_hps.is_active(hp.name) and hp.name not in hp_names:
                        continue
                    # if was not active or selected, sample.
                    hps.values[hp.name] = hp.random_sample(self._seed_state)
                    self._seed_state += 1
            values = hps.values
            # Keep trying until the set of values is unique,
            # or until we exit due to too many collisions.
            values_hash = self._compute_values_hash(values)
            if values_hash in self._tried_so_far:
                collisions += 1
                if collisions <= self._max_collisions:
                    continue
                return None
            self._tried_so_far.add(values_hash)
            break
        return values","1. Use `tf.random.set_seed()` to set the random seed for all random operations.
2. Use `tf.keras.utils.experimental.enable_debug_mode()` to enable debug mode.
3. Use `tf.keras.utils.print_model()` to print the model summary."
"    def __init__(self, loss=None, metrics=None, output_shape=None, **kwargs):
        super().__init__(**kwargs)
        self.output_shape = output_shape
        self.loss = loss
        self.metrics = metrics","1. Use `assert` statements to validate input arguments.
2. Use `type` annotations to specify the types of arguments and return values.
3. Use `@staticmethod` to mark methods that do not need to access the class state."
"    def get_config(self):
        config = super().get_config()
        config.update({
            'loss': self.loss,
            'metrics': self.metrics,
            'output_shape': self.output_shape
        })
        return config","1. Use `tf.keras.utils.get_custom_objects()` to register custom losses and metrics.
2. Set the `loss` and `metrics` attributes of the model to the registered objects.
3. Use `tf.keras.utils.get_output_shape()` to get the output shape of the model."
"    def __init__(self, preprocessors=None, **kwargs):
        super().__init__(**kwargs)
        self.preprocessors = nest.flatten(preprocessors)
        self._finished = False
        # Save or load the HyperModel.
        utils.save_json(os.path.join(self.project_dir, 'graph'),
                        graph_module.serialize(self.hypermodel.hypermodel))","1. Use `os.path.join` to concatenate paths instead of string concatenation.
2. Use `json.dump` to save the HyperModel instead of `utils.save_json`.
3. Use `inspect.getfullargspec` to get the argument spec of the `hypermodel.hypermodel` function instead of hardcoding it."
"def feature_encoding_input(block):
    """"""Fetch the column_types and column_names.

    The values are fetched for FeatureEncoding from StructuredDataInput.
    """"""
    if not isinstance(block.inputs[0], nodes.StructuredDataInput):
        raise TypeError('FeatureEncoding block can only be used '
                        'with StructuredDataInput.')
    block.column_types = block.inputs[0].column_types
    block.column_names = block.inputs[0].column_names","1. **Use type checking** to ensure that the input is of the correct type. This will help to prevent errors and protect against malicious attacks.
2. **Sanitize user input** to remove any potential harmful characters. This will help to prevent attackers from injecting malicious code into the system.
3. **Use strong passwords** for all accounts that have access to the system. This will help to prevent attackers from gaining unauthorized access."
"    def get_config(self):
        blocks = [serialize(block) for block in self.blocks]
        nodes = {str(self._node_to_id[node]): serialize(node)
                 for node in self.inputs}
        override_hps = [tf.keras.utils.serialize_keras_object(hp)
                        for hp in self.override_hps]
        block_inputs = {
            str(block_id): [self._node_to_id[node]
                            for node in block.inputs]
            for block_id, block in enumerate(self.blocks)}
        block_outputs = {
            str(block_id): [self._node_to_id[node]
                            for node in block.outputs]
            for block_id, block in enumerate(self.blocks)}

        outputs = [self._node_to_id[node] for node in self.outputs]

        return {
            'override_hps': override_hps,  # List [serialized].
            'blocks': blocks,  # Dict {id: serialized}.
            'nodes': nodes,  # Dict {id: serialized}.
            'outputs': outputs,  # List of node_ids.
            'block_inputs': block_inputs,  # Dict {id: List of node_ids}.
            'block_outputs': block_outputs,  # Dict {id: List of node_ids}.
        }","1. Use `tf.keras.utils.serialize_keras_object` to serialize objects instead of `json.dumps`.
2. Use `tf.keras.utils.deserialize_keras_object` to deserialize objects instead of `json.loads`.
3. Use `tf.keras.utils.get_custom_objects()` to register custom objects before deserializing."
"    def from_config(cls, config):
        blocks = [deserialize(block) for block in config['blocks']]
        nodes = {int(node_id): deserialize(node)
                 for node_id, node in config['nodes'].items()}
        override_hps = [kerastuner.engine.hyperparameters.deserialize(config)
                        for config in config['override_hps']]

        inputs = [nodes[node_id] for node_id in nodes]
        for block_id, block in enumerate(blocks):
            input_nodes = [nodes[node_id]
                           for node_id in config['block_inputs'][str(block_id)]]
            output_nodes = nest.flatten(block(input_nodes))
            for output_node, node_id in zip(
                    output_nodes, config['block_outputs'][str(block_id)]):
                nodes[node_id] = output_node

        outputs = [nodes[node_id] for node_id in config['outputs']]
        return cls(inputs=inputs, outputs=outputs, override_hps=override_hps)","1. Use `tf.io.gfile.GFile` instead of `open` to open files, as it handles file permissions correctly.
2. Use `tf.io.serialize_tensor` to serialize tensors, as it prevents data from being leaked in the serialized form.
3. Use `tf.io.parse_tensor` to deserialize tensors, as it validates the serialized data and prevents invalid data from being deserialized."
"    def __init__(self,
                 num_classes: Optional[int] = None,
                 multi_label: bool = False,
                 loss: Optional[types.LossType] = None,
                 metrics: Optional[types.MetricsType] = None,
                 dropout_rate: Optional[float] = None,
                 **kwargs):
        super().__init__(loss=loss,
                         metrics=metrics,
                         **kwargs)
        self.num_classes = num_classes
        self.multi_label = multi_label
        if not self.metrics:
            self.metrics = ['accuracy']
        self.dropout_rate = dropout_rate
        self.set_loss()","1. Use `typing` to specify the types of arguments and return values. This will help catch errors early and prevent unexpected behavior.
2. Use `validation` to check the arguments passed to the function. This will help ensure that the function is used correctly and that invalid data does not cause problems.
3. Use `encryption` to protect sensitive data. This will help prevent unauthorized access to data and protect users' privacy."
"    def config_from_adapter(self, adapter):
        super().config_from_adapter(adapter)
        self.num_classes = adapter.num_classes
        self.set_loss()","1. Use `torch.jit.script` to make the model's forward pass more secure.
2. Use `torch.jit.freeze` to prevent users from modifying the model's parameters.
3. Use `torch.jit.save` to save the model in a secure format."
"    def __init__(self,
                 output_dim: Optional[int] = None,
                 loss: types.LossType = 'mean_squared_error',
                 metrics: Optional[types.MetricsType] = None,
                 dropout_rate: Optional[float] = None,
                 **kwargs):
        super().__init__(loss=loss,
                         metrics=metrics,
                         **kwargs)
        self.output_dim = output_dim
        if not self.metrics:
            self.metrics = ['mean_squared_error']
        self.loss = loss
        self.dropout_rate = dropout_rate","1. Use `typing` to specify the types of arguments and return values.
2. Validate the input arguments to ensure they are of the correct type and within the expected range.
3. Use `SecretStorage` to securely store sensitive information, such as the model's weights."
"    def fit_before_convert(self, dataset):
        # If in tf.data.Dataset, must be encoded already.
        if isinstance(dataset, tf.data.Dataset):
            if not self.num_classes:
                shape = dataset.take(1).shape[1]
                if shape == 1:
                    self.num_classes = 2
                else:
                    self.num_classes = shape
            return
        if isinstance(dataset, pd.DataFrame):
            dataset = dataset.values
        if isinstance(dataset, pd.Series):
            dataset = dataset.values.reshape(-1, 1)
        # Not label.
        if len(dataset.flatten()) != len(dataset):
            self.num_classes = dataset.shape[1]
            return
        labels = set(dataset.flatten())
        if self.num_classes is None:
            self.num_classes = len(labels)
        if self.num_classes == 2:
            self.label_encoder = encoders.LabelEncoder()
        elif self.num_classes > 2:
            self.label_encoder = encoders.OneHotEncoder()
        elif self.num_classes < 2:
            raise ValueError('Expect the target data for {name} to have '
                             'at least 2 classes, but got {num_classes}.'
                             .format(name=self.name, num_classes=self.num_classes))
        self.label_encoder.fit(dataset)","1. Use `tf.data.Dataset.map()` instead of `tf.data.Dataset.take()` to avoid data leakage.
2. Validate the input data to ensure that it is in the correct format.
3. Use a secure hash function to generate the hash of the data, and compare it with the expected hash to verify the integrity of the data."
"    def __init__(self, data):
        super().__init__()
        self.max_val = data.max()
        data = data / self.max_val
        self.mean = np.mean(data, axis=0, keepdims=True).flatten()
        self.std = np.std(data, axis=0, keepdims=True).flatten()","1. Use `np.nan_to_num()` to replace `np.inf` and `-np.inf` values.
2. Use `np.clip()` to clip the data to a specified range.
3. Use `np.random.seed()` to set the random seed."
"    def transform_train(self, data, targets=None, batch_size=None):
        dataset = self._transform([Normalize(torch.Tensor(self.mean), torch.Tensor(self.std))], data, targets)

        if batch_size is None:
            batch_size = Constant.MAX_BATCH_SIZE
        batch_size = min(len(data), batch_size)

        return DataLoader(dataset, batch_size=batch_size, shuffle=True)","1. Use `torch.jit.script` to create a compiled version of the model. This will make it more difficult for attackers to reverse engineer the model.
2. Use `torch.nn.functional.cross_entropy` instead of `nn.CrossEntropyLoss`. This will prevent attackers from using gradient information to attack the model.
3. Use `torch.utils.data.DataLoader` with a `shuffle=False` flag. This will prevent attackers from using the order of the data to attack the model."
"    def _transform(self, compose_list, data, targets):
        data = data / self.max_val
        args = [0, len(data.shape) - 1] + list(range(1, len(data.shape) - 1))
        data = torch.Tensor(data.transpose(*args))
        data_transforms = Compose(compose_list)
        return MultiTransformDataset(data, targets, data_transforms)","1. Use `torch.jit.script` to make the model's predictions non-differentiable.
2. Validate the input data before feeding it to the model.
3. Use a secure hash function to generate the model's hash, and verify the hash before loading the model."
"    def __init__(self, verbose=False, path=None, resume=False, searcher_args=None, augment=None):
        """"""Initialize the instance.

        The classifier will be loaded from the files in 'path' if parameter 'resume' is True.
        Otherwise it would create a new one.

        Args:
            verbose: A boolean of whether the search process will be printed to stdout.
            path: A string. The path to a directory, where the intermediate results are saved.
            resume: A boolean. If True, the classifier will continue to previous work saved in path.
                Otherwise, the classifier will start a new search.
            augment: A boolean value indicating whether the data needs augmentation. If not define, then it
                will use the value of Constant.DATA_AUGMENTATION which is True by default.

        """"""
        super().__init__(verbose)

        if searcher_args is None:
            searcher_args = {}

        if path is None:
            path = temp_folder_generator()

        if augment is None:
            augment = Constant.DATA_AUGMENTATION

        self.path = path
        if has_file(os.path.join(self.path, 'classifier')) and resume:
            classifier = pickle_from_file(os.path.join(self.path, 'classifier'))
            self.__dict__ = classifier.__dict__
        else:
            self.y_encoder = None
            self.data_transformer = None
            self.verbose = verbose
            self.augment = augment
            self.cnn = CnnModule(self.loss, self.metric, searcher_args, path, verbose)","1. Use `pickle_from_file` instead of `pickle.load` to avoid security vulnerabilities.
2. Use `temp_folder_generator` to generate a temporary folder for the classifier, and delete it when it is not needed anymore.
3. Use `os.path.join` to join paths instead of concatenating strings, to avoid security vulnerabilities."
"    def fit(self, x, y, x_test=None, y_test=None, time_limit=None):
        x = np.array(x)
        y = np.array(y).flatten()
        validate_xy(x, y)
        y = self.transform_y(y)
        if x_test is None or y_test is None:
            # Divide training data into training and testing data.
            validation_set_size = int(len(y) * Constant.VALIDATION_SET_SIZE)
            validation_set_size = min(validation_set_size, 500)
            validation_set_size = max(validation_set_size, 1)
            x_train, x_test, y_train, y_test = train_test_split(x, y,
                                                                test_size=validation_set_size,
                                                                random_state=42)
        else:
            x_train = x
            y_train = y
        # Transform x_train
        if self.data_transformer is None:
            self.data_transformer = ImageDataTransformer(x, augment=self.augment)

        # Wrap the data into DataLoaders
        train_data = self.data_transformer.transform_train(x_train, y_train)
        test_data = self.data_transformer.transform_test(x_test, y_test)

        # Save the classifier
        pickle_to_file(self, os.path.join(self.path, 'classifier'))

        if time_limit is None:
            time_limit = 24 * 60 * 60

        self.cnn.fit(self.get_n_output_node(), x_train.shape, train_data, test_data, time_limit)","1. Use `pickle_to_file` to save the classifier instead of `pickle.dump`.
2. Use `transform_train` and `transform_test` to wrap the data into DataLoaders instead of directly using `DataLoader`.
3. Set `validation_set_size` to a smaller value to prevent overfitting."
"    def evaluate(self, x_test, y_test):
        """"""Return the accuracy score between predict value and `y_test`.""""""
        y_predict = self.predict(x_test)
        return self.metric().evaluate(y_test, y_predict)","1. Use `np.asarray` to convert inputs to numpy arrays to avoid data type errors.
2. Use `np.argmax` to get the index of the maximum value instead of using `max`.
3. Use `np.argsort` to sort the array and then take the first element instead of using `sorted`."
"    def final_fit(self, x_train, y_train, x_test, y_test, trainer_args=None, retrain=False):
        """"""Final training after found the best architecture.

        Args:
            x_train: A numpy.ndarray of training data.
            y_train: A numpy.ndarray of training targets.
            x_test: A numpy.ndarray of testing data.
            y_test: A numpy.ndarray of testing targets.
            trainer_args: A dictionary containing the parameters of the ModelTrainer constructor.
            retrain: A boolean of whether reinitialize the weights of the model.
        """"""
        if trainer_args is None:
            trainer_args = {'max_no_improvement_num': 30}

        y_train = self.transform_y(y_train)
        y_test = self.transform_y(y_test)

        train_data = self.data_transformer.transform_train(x_train, y_train)
        test_data = self.data_transformer.transform_test(x_test, y_test)

        self.cnn.final_fit(train_data, test_data, trainer_args, retrain)","1. Use `torch.jit.script` to make the model's inference process more secure.
2. Use `torch.jit.save` to save the model in a secure format.
3. Use `torch.jit.load` to load the model in a secure way."
"    def export_autokeras_model(self, model_file_name):
        """""" Creates and Exports the AutoKeras model to the given filename. """"""
        portable_model = PortableImageSupervised(graph=self.cnn.best_model,
                                                 y_encoder=self.y_encoder,
                                                 data_transformer=self.data_transformer,
                                                 metric=self.metric,
                                                 inverse_transform_y_method=self.inverse_transform_y)
        pickle_to_file(portable_model, model_file_name)","1. Use `pickle.dump` instead of `pickle.to_file` to avoid pickling the file handle.
2. Use `contextlib.closing` to ensure that the file handle is closed after use.
3. Use `os.chmod` to set the permissions of the exported model file to `0o644`."
"    def __init__(self, graph, data_transformer, y_encoder, metric, inverse_transform_y_method):
        """"""Initialize the instance.
        Args:
            graph: The graph form of the learned model
        """"""
        super().__init__(graph)
        self.data_transformer = data_transformer
        self.y_encoder = y_encoder
        self.metric = metric
        self.inverse_transform_y_method = inverse_transform_y_method","1. Use `torch.jit.script` to make the model's graph unchangeable.
2. Use `torch.jit.save` to save the model in a secure file format.
3. Use `torch.jit.load` to load the model in a secure way."
"def read_image(img_path):
    img = ndimage.imread(fname=img_path)
    return img","1. **Use `os.path.join()` to sanitize the path to the image file.** This will help prevent directory traversal attacks.
2. **Check the file's permissions to make sure it is readable by the user.** This will help prevent unauthorized access to the image file.
3. **Use `PIL.Image.open()` to open the image file instead of `ndimage.imread()`.** This will give you more control over the image loading process and help prevent security vulnerabilities."
"def get_device():
    """""" If Cuda is available, use Cuda device, else use CPU device
        When choosing from Cuda devices, this function will choose the one with max memory available

    Returns: string device name

    """"""
    # TODO: could use gputil in the future
    if torch.cuda.is_available():
        smi_out = os.popen('nvidia-smi -q -d Memory | grep -A4 GPU|grep Free').read()
        # smi_out=
        #       Free                 : xxxxxx MiB
        #       Free                 : xxxxxx MiB
        #                      ....
        memory_available = [int(x.split()[2]) for x in smi_out.splitlines()]
        if not memory_available:
            device = 'cpu'
        else:
            device = 'cuda:' + str(memory_available.index(max(memory_available)))
    else:
        device = 'cpu'
    return device","1. Use `subprocess.check_output` instead of `os.popen` to avoid leaking the command to the shell.
2. Use `subprocess.DEVNULL` to discard the output of the command.
3. Use `torch.cuda.device_count()` to get the number of CUDA devices instead of hard-coding it."
"    def _dense_block_end_node(self, layer_id):
        return self._block_end_node(layer_id, Constant.DENSE_BLOCK_DISTANCE)","1. Use `assert` statements to validate the input arguments.
2. Use `salt` and `pepper` to generate a strong password hash.
3. Use `encryption` to protect sensitive data."
"    def extract_descriptor(self):
        ret = NetworkDescriptor()
        topological_node_list = self.topological_order
        for u in topological_node_list:
            for v, layer_id in self.adj_list[u]:
                layer = self.layer_list[layer_id]
                if is_layer(layer, 'Conv') and layer.kernel_size not in [1, (1,), (1, 1), (1, 1, 1)]:
                    ret.add_conv_width(layer_width(layer))
                if is_layer(layer, 'Dense'):
                    ret.add_dense_width(layer_width(layer))

        # The position of each node, how many Conv and Dense layers before it.
        pos = [0] * len(topological_node_list)
        for v in topological_node_list:
            layer_count = 0
            for u, layer_id in self.reverse_adj_list[v]:
                layer = self.layer_list[layer_id]
                weighted = 0
                if (is_layer(layer, 'Conv') and layer.kernel_size not in [1, (1,), (1, 1), (1, 1, 1)]) \\
                        or is_layer(layer, 'Dense'):
                    weighted = 1
                layer_count = max(pos[u] + weighted, layer_count)
            pos[v] = layer_count

        for u in topological_node_list:
            for v, layer_id in self.adj_list[u]:
                if pos[u] == pos[v]:
                    continue
                layer = self.layer_list[layer_id]
                if is_layer(layer, 'Concatenate'):
                    ret.add_skip_connection(pos[u], pos[v], NetworkDescriptor.CONCAT_CONNECT)
                if is_layer(layer, 'Add'):
                    ret.add_skip_connection(pos[u], pos[v], NetworkDescriptor.ADD_CONNECT)

        return ret","1. Use `torch.jit.trace` to create a traced model instead of manually defining the forward pass. This will prevent attackers from injecting malicious code into the model.
2. Use `torch.jit.save` to save the traced model in a secure location. This will prevent attackers from accessing the model file.
3. Use `torch.jit.load` to load the traced model into a secure environment. This will prevent attackers from executing the model in an insecure environment."
"def to_deeper_graph(graph):
    weighted_layer_ids = graph.deep_layer_ids()
    if len(weighted_layer_ids) >= Constant.MAX_MODEL_DEPTH:
        return None

    deeper_layer_ids = sample(weighted_layer_ids, 1)
    # n_deeper_layer = randint(1, len(weighted_layer_ids))
    # deeper_layer_ids = sample(weighted_layer_ids, n_deeper_layer)

    for layer_id in deeper_layer_ids:
        layer = graph.layer_list[layer_id]
        if is_layer(layer, 'Conv'):
            graph.to_conv_deeper_model(layer_id, randint(1, 2) * 2 + 1)
        else:
            graph.to_dense_deeper_model(layer_id)
    return graph","1. Use `randint` instead of `sample` to avoid leaking information about the size of the dataset.
2. Use `random.shuffle` to randomize the order of the layers, instead of selecting them in a fixed order.
3. Use `assert` statements to check that the input is valid, and raise an exception if it is not."
"    def _raise_passphrase_exception(self):
        if self._passphrase_helper is None:
            _raise_current_error()
        exception = self._passphrase_helper.raise_if_problem(Error)
        if exception is not None:
            raise exception","1. Use `cryptography`'s `SecretKey` class to generate and store the passphrase hash.
2. Use `cryptography`'s `fernet` module to encrypt the data with the passphrase hash.
3. Use `cryptography`'s `InvalidKeyError` exception to handle errors when decrypting the data."
"    def raise_if_problem(self, exceptionType=Error):
        try:
            _exception_from_error_queue(exceptionType)
        except exceptionType as e:
            from_queue = e
        if self._problems:
            raise self._problems[0]
        return from_queue","1. Use `from_queue` variable to store the exception from the error queue.
2. Use `if` statement to check if there is any problem in the error queue.
3. Use `raise` statement to raise the first problem in the error queue."
"    def stats(self):
        """"""
        Returns
            dict?: resource stats
        """"""
        stats = {""hash"": """", ""bytes"": 0, ""fields"": 0, ""rows"": 0}
        return self.metadata_attach(""stats"", self.get(""stats"", stats))","1. Use `getattr` instead of `get` to avoid accessing undefined attributes.
2. Use `json.dumps` to serialize the data before returning it.
3. Sanitize the input data to prevent XSS attacks."
"    def open(self):
        """"""Open the resource as ""io.open"" does

        Raises:
            FrictionlessException: any exception that occurs
        """"""
        self.close()

        # Infer
        self[""name""] = self.name
        self[""profile""] = self.profile
        self[""scheme""] = self.scheme
        self[""format""] = self.format
        self[""hashing""] = self.hashing
        self[""encoding""] = self.encoding
        if self.innerpath:
            self[""innerpath""] = self.innerpath
        if self.compression:
            self[""compression""] = self.compression
        if self.control:
            self[""control""] = self.control
        if self.dialect:
            self[""dialect""] = self.dialect
        self[""stats""] = {""hash"": """", ""bytes"": 0, ""fields"": 0, ""rows"": 0}

        # Validate
        if self.metadata_errors:
            error = self.metadata_errors[0]
            raise FrictionlessException(error)

        # Open
        try:

            # Table
            if self.tabular:
                self.__parser = system.create_parser(self)
                self.__parser.open()
                self.__read_detect_layout()
                self.__read_detect_schema()
                if not self.__nolookup:
                    self.__lookup = self.__read_detect_lookup()
                self.__header = self.__read_header()
                self.__row_stream = self.__read_row_stream()
                return self

            # File
            else:
                self.__loader = system.create_loader(self)
                self.__loader.open()
                return self

        # Error
        except Exception:
            self.close()
            raise","1. Use `try-except` blocks to catch errors and handle them appropriately.
2. Validate user input to prevent against malicious attacks.
3. Use secure coding practices to protect against vulnerabilities."
"    def to_dict(self):
        """"""Convert metadata to a dict

        Returns:
            dict: metadata as a dict
        """"""
        return self.copy()","1. Use `copy.deepcopy()` instead of `copy()` to avoid shallow copies.
2. Use `json.dumps()` to serialize the metadata to a string.
3. Use `json.loads()` to deserialize the metadata from a string."
"    def create_storage(self, name, **options):
        pass","1. Sanitize user input to prevent against injection attacks.
2. Use strong encryption to protect data at rest and in transit.
3. Implement access control to restrict who can access data."
"    def __init__(self, service, project, dataset, prefix=""""):
        self.__service = service
        self.__project = project
        self.__dataset = dataset
        self.__prefix = prefix
        self.__names = None
        self.__tables = {}
        self.__fallbacks = {}","1. Use `os.environ` to get the service account key instead of hardcoding it in the code.
2. Use `contextlib.closing` to ensure that the gRPC connection is closed properly.
3. Use `json.JSONEncoder` to properly escape the table names when sending them to BigQuery."
"    def __read_data_stream(self, name, schema):
        sav = helpers.import_from_plugin(""savReaderWriter"", plugin=""spss"")
        path = self.__write_convert_name(name)
        with sav.SavReader(path, ioUtf8=True, rawMode=False) as reader:
            for item in reader:
                cells = []
                for index, field in enumerate(schema.fields):
                    value = item[index]
                    # Fix decimals that should be integers
                    if field.type == ""integer"" and value is not None:
                        value = int(float(value))
                    # We need to decode bytes to strings
                    if isinstance(value, bytes):
                        value = value.decode(reader.fileEncoding)
                    # Time values need a decimal, add one if missing.
                    if field.type == ""time"" and not re.search(r""\\.\\d*"", value):
                        value = ""{}.0"".format(value)
                    cells.append(value)
                yield cells","1. Use `io.open` instead of `open` to open files with a specific mode.
2. Use `os.path.join` to concatenate paths instead of concatenating strings.
3. Use `sav.SavWriter` to write SPSS files instead of `sav.SavReader`."
"    def prepare(self, stream, schema, extra):

        # Prepare package
        if 'datapackage' not in extra or 'resource-name' not in extra:
            return False
        descriptor = extra['datapackage']
        if descriptor.strip().startswith('{'):
            descriptor = json.loads(descriptor)
        self.__package = Package(descriptor)

        # Prepare schema
        if not schema:
            return False
        if not schema.foreign_keys:
            return False
        self.__schema = schema

        # Prepare foreign keys values
        relations = _get_relations(
            self.__package, self.__schema,
            current_resource_name=extra['resource-name'])
        self.__foreign_keys_values = _get_foreign_keys_values(
            self.__schema, relations)

        return True","1. Use `json.loads()` to parse the datapackage descriptor instead of `eval()`.
2. Validate the datapackage descriptor against a schema to ensure that it is well-formed.
3. Sanitize the foreign keys values to prevent SQL injection attacks."
"    def check_row(self, cells):
        row_number = cells[0]['row-number']

        # Prepare keyed_row
        keyed_row = {}
        for cell in cells:
            if cell.get('field'):
                keyed_row[cell.get('field').name] = cell.get('value')

        # Resolve relations
        errors = []
        for foreign_key in self.__schema.foreign_keys:
            success = _resolve_relations(
                deepcopy(keyed_row), self.__foreign_keys_values, foreign_key)
            if success is None:
                message = 'Foreign key ""{fields}"" violation in row {row_number}'
                message_substitutions = {'fields': foreign_key['fields']}

                # if not a composite foreign-key, add the cell causing the violation to improve the error details
                # with the column-number
                error_cell = None
                if len(foreign_key['fields']) == 1:
                    for cell in cells:
                        if cell['header'] == foreign_key['fields'][0]:
                            error_cell = cell
                            break

                errors.append(Error(
                    self.__code,
                    cell=error_cell,
                    row_number=row_number,
                    message=message,
                    message_substitutions=message_substitutions,
                ))

        return errors","1. Use `validate_input` to sanitize user input.
2. Use `assert` to check for errors.
3. Use `logging` to log errors."
"def _get_relations(package, schema, current_resource_name=None):
    # It's based on the following code:
    # https://github.com/frictionlessdata/datapackage-py/blob/master/datapackage/resource.py#L393

    # Prepare relations
    relations = {}
    for fk in schema.foreign_keys:
        resource_name = fk['reference'].get('resource')
        package_name = fk['reference'].get('package')
        resource = None

        # Self-referenced resource
        if not resource_name:
            for item in package.resources:
                if item.name == current_resource_name:
                    resource = item

        # Internal resource
        elif not package_name:
            resource = package.get_resource(resource_name)

        # External resource (experimental)
        # For now, we rely on uniqueness of resource names and support relative paths
        else:
            descriptor = package_name
            if not descriptor.startswith('http'):
                descriptor = '/'.join([package.base_path, package_name])
            package = Package(descriptor)
            resource = package.get_resource(resource_name)

        # Add to relations (can be None)
        relations[resource_name] = resource
        if resource and resource.tabular:
            relations[resource_name] = resource.read(keyed=True)

    return relations","1. Use `package.get_resource()` instead of `package.resources` to get a resource by name. This will prevent users from accessing resources that they do not have permission to access.
2. Use `package.base_path` to construct the URL for an external resource. This will prevent users from accessing resources that are not part of the package.
3. Use `resource.read(keyed=True)` to read a tabular resource into a keyed DataFrame. This will prevent users from accessing the raw data in the resource."
"def missing_header(cells, sample):
    errors = []

    for cell in copy(cells):

        # Skip if header in cell
        if cell.get('header') is not None:
            continue

        # Add error
        message_substitutions = {
            'field_name': '""{}""'.format(cell['field'].name),
        }
        error = Error(
            'missing-header',
            cell,
            message_substitutions=message_substitutions
        )
        errors.append(error)

        # Remove cell
        cells.remove(cell)

    return errors","1. Use `cell.get()` to check for the existence of a header before accessing it.
2. Use `copy()` to create a new list of cells before removing any cells. This will prevent the original list from being modified.
3. Use `Error()` to create a new error object and add it to the list of errors."
"    def __inspect_table(self, table):

        # Start timer
        start = datetime.datetime.now()

        # Prepare vars
        errors = []
        headers = None
        row_number = 0
        fatal_error = False
        checks = copy(self.__checks)
        source = table['source']
        stream = table['stream']
        schema = table['schema']
        extra = table['extra']

        # Prepare table
        try:
            stream.open()
            sample = stream.sample
            headers = stream.headers
            if self.__filter_checks(checks, type='schema'):
                if schema is None and self.__infer_schema:
                    schema = Schema(infer(headers, sample))
            if schema is None:
                checks = self.__filter_checks(checks, type='schema', inverse=True)
        except Exception as exception:
            fatal_error = True
            message = str(exception)
            if isinstance(exception, tabulator.exceptions.SourceError):
                code = 'source-error'
            elif isinstance(exception, tabulator.exceptions.SchemeError):
                code = 'scheme-error'
            elif isinstance(exception, tabulator.exceptions.FormatError):
                code = 'format-error'
            elif isinstance(exception, tabulator.exceptions.EncodingError):
                code = 'encoding-error'
            elif isinstance(exception, tabulator.exceptions.IOError):
                code = 'io-error'
            elif isinstance(exception, tabulator.exceptions.HTTPError):
                code = 'http-error'
            else:
                raise
            errors.append({
                'row': None,
                'code': code,
                'message': message,
                'row-number': None,
                'column-number': None,
            })

        # Prepare columns
        if not fatal_error:
            columns = []
            fields = [None] * len(headers)
            if schema is not None:
                fields = schema.fields
            iterator = zip_longest(headers, fields, fillvalue=_FILLVALUE)
            for number, (header, field) in enumerate(iterator, start=1):
                column = {'number': number}
                if header is not _FILLVALUE:
                    column['header'] = header
                if field is not _FILLVALUE:
                    column['field'] = field
                columns.append(column)

        # Head checks
        if not fatal_error:
            head_checks = self.__filter_checks(checks, context='head')
            for check in head_checks:
                if not columns:
                    break
                check['func'](errors, columns, sample)
            for error in errors:
                error['row'] = None

        # Body checks
        if not fatal_error:
            states = {}
            colmap = {column['number']: column for column in columns}
            body_checks = self.__filter_checks(checks, context='body')
            with stream:
                for row_number, headers, row in stream.iter(extended=True):
                    columns = []
                    iterator = zip_longest(headers, row, fillvalue=_FILLVALUE)
                    for number, (header, value) in enumerate(iterator, start=1):
                        colref = colmap.get(number, {})
                        column = {'number': number}
                        if header is not _FILLVALUE:
                            column['header'] = colref.get('header', header)
                        if 'field' in colref:
                            column['field'] = colref['field']
                        if value is not _FILLVALUE:
                            column['value'] = value
                        columns.append(column)
                    for check in body_checks:
                        if not columns:
                            break
                        state = states.setdefault(check['code'], {})
                        check['func'](errors, columns, row_number, state)
                    for error in reversed(errors):
                        if 'row' in error:
                            break
                        error['row'] = row
                    if row_number >= self.__row_limit:
                        break
                    if len(errors) >= self.__error_limit:
                        break

        # Stop timer
        stop = datetime.datetime.now()

        # Compose report
        errors = errors[:self.__error_limit]
        report = copy(extra)
        report.update({
            'time': round((stop - start).total_seconds(), 3),
            'valid': not bool(errors),
            'error-count': len(errors),
            'row-count': row_number,
            'headers': headers,
            'source': source,
            'errors': errors,
        })

        return report","1. Use `tabulator.open()` to open the stream before reading from it. This will ensure that the stream is properly closed after reading.
2. Use `tabulator.iter()` to iterate over the rows of the table. This will prevent you from reading more rows than you need.
3. Use `tabulator.sample()` to get a sample of the data before running checks. This will help you to identify potential problems early on."
"def parse(code):
    class_names, code = pre_parse(code)
    o = ast.parse(code)  # python ast
    decorate_ast(o, code, class_names)  # decorated python ast
    o = resolve_negative_literals(o)
    return o.body","1. **Use `ast.literal_eval` instead of `eval`**. `eval` is a dangerous function that can execute arbitrary code, while `ast.literal_eval` only evaluates literal values.
2. **Sanitize user input before using it in code**. This means escaping special characters, checking for malicious code, and validating the input against a whitelist of allowed values.
3. **Use secure coding practices**. This includes using strong passwords, encrypting sensitive data, and following other best practices for secure coding."
"    def add_globals_and_events(self, item):
        item_attributes = {""public"": False}

        # Handle constants.
        if self.get_call_func_name(item) == ""constant"":
            self._constants.add_constant(item, global_ctx=self)
            return

        # Handle events.
        if not (self.get_call_func_name(item) == ""event""):
            item_name, item_attributes = self.get_item_name_and_attributes(item, item_attributes)
            if not all([attr in valid_global_keywords for attr in item_attributes.keys()]):
                raise StructureException('Invalid global keyword used: %s' % item_attributes, item)

        if item.value is not None:
            raise StructureException('May not assign value whilst defining type', item)
        elif self.get_call_func_name(item) == ""event"":
            if self._globals or len(self._defs):
                raise EventDeclarationException(""Events must all come before global declarations and function definitions"", item)
            self._events.append(item)
        elif not isinstance(item.target, ast.Name):
            raise StructureException(""Can only assign type to variable in top-level statement"", item)

        # Is this a custom unit definition.
        elif item.target.id == 'units':
            if not self._custom_units:
                if not isinstance(item.annotation, ast.Dict):
                    raise VariableDeclarationException(""Define custom units using units: { }."", item.target)
                for key, value in zip(item.annotation.keys, item.annotation.values):
                    if not isinstance(value, ast.Str):
                        raise VariableDeclarationException(""Custom unit description must be a valid string"", value)
                    if not isinstance(key, ast.Name):
                        raise VariableDeclarationException(""Custom unit name must be a valid string"", key)
                    check_valid_varname(key.id, self._custom_units, self._structs, self._constants, key, ""Custom unit invalid."")
                    self._custom_units.add(key.id)
                    self._custom_units_descriptions[key.id] = value.s
            else:
                raise VariableDeclarationException(""Custom units can only be defined once"", item.target)

        # Check if variable name is valid.
        # Don't move this check higher, as unit parsing has to happen first.
        elif not self.is_valid_varname(item.target.id, item):
            pass

        elif len(self._defs):
            raise StructureException(""Global variables must all come before function definitions"", item)
        # If the type declaration is of the form public(<type here>), then proceed with
        # the underlying type but also add getters
        elif self.get_call_func_name(item) == ""address"":
            if item.annotation.args[0].id not in premade_contracts:
                raise VariableDeclarationException(""Unsupported premade contract declaration"", item.annotation.args[0])
            premade_contract = premade_contracts[item.annotation.args[0].id]
            self._contracts[item.target.id] = self.make_contract(premade_contract.body)
            self._globals[item.target.id] = VariableRecord(item.target.id, len(self._globals), BaseType('address'), True)

        elif item_name in self._contracts:
            self._globals[item.target.id] = ContractRecord(item.target.id, len(self._globals), ContractType(item_name), True)
            if item_attributes[""public""]:
                typ = ContractType(item_name)
                for getter in self.mk_getter(item.target.id, typ):
                    self._getters.append(self.parse_line('\\n' * (item.lineno - 1) + getter))
                    self._getters[-1].pos = getpos(item)

        elif self.get_call_func_name(item) == ""public"":
            if isinstance(item.annotation.args[0], ast.Name) and item_name in self._contracts:
                typ = ContractType(item_name)
            else:
                typ = parse_type(item.annotation.args[0], 'storage', custom_units=self._custom_units, custom_structs=self._structs, constants=self._constants)
            self._globals[item.target.id] = VariableRecord(item.target.id, len(self._globals), typ, True)
            # Adding getters here
            for getter in self.mk_getter(item.target.id, typ):
                self._getters.append(self.parse_line('\\n' * (item.lineno - 1) + getter))
                self._getters[-1].pos = getpos(item)

        elif isinstance(item.annotation, (ast.Name, ast.Call, ast.Subscript)):
            self._globals[item.target.id] = VariableRecord(
                item.target.id, len(self._globals),
                parse_type(item.annotation, 'storage', custom_units=self._custom_units, custom_structs=self._structs, constants=self._constants),
                True
            )
        else:
            raise InvalidTypeException('Invalid global type specified', item)","1. Use `is_valid_varname` to check if variable names are valid.
2. Use `parse_type` to parse types correctly.
3. Use `make_contract` to create contracts correctly."
"def parse_type(item, location, sigs=None, custom_units=None, custom_structs=None, constants=None):
    # Base and custom types, e.g. num
    if isinstance(item, ast.Name):
        if item.id in base_types:
            return BaseType(item.id)
        elif item.id in special_types:
            return special_types[item.id]
        elif (custom_structs is not None) and (item.id in custom_structs):
            return make_struct_type(item.id, location, custom_structs[item.id], custom_units, custom_structs, constants)
        else:
            raise InvalidTypeException(""Invalid base type: "" + item.id, item)
    # Units, e.g. num (1/sec) or contracts
    elif isinstance(item, ast.Call):
        # Mapping type.
        if item.func.id == 'map':
            if location == 'memory':
                raise InvalidTypeException(""No mappings allowed for in-memory types, only fixed-size arrays"", item)
            keytype = parse_type(item.args[0], None, custom_units=custom_units, custom_structs=custom_structs, constants=constants)
            if not isinstance(keytype, (BaseType, ByteArrayType)):
                raise InvalidTypeException(""Mapping keys must be base or bytes types"", item)
            return MappingType(keytype, parse_type(item.args[1], location, custom_units=custom_units, custom_structs=custom_structs, constants=constants))
        # Contract_types
        if item.func.id == 'address':
            if sigs and item.args[0].id in sigs:
                return ContractType(item.args[0].id)
        # Struct types
        if (custom_structs is not None) and (item.func.id in custom_structs):
            return make_struct_type(item.id, location, custom_structs[item.id], custom_units, custom_structs)
        if not isinstance(item.func, ast.Name):
            raise InvalidTypeException(""Malformed unit type:"", item)
        base_type = item.func.id
        if base_type not in ('int128', 'uint256', 'decimal'):
            raise InvalidTypeException(""You must use int128, uint256, decimal, address, contract, \\
                for variable declarations and indexed for logging topics "", item)
        if len(item.args) == 0:
            raise InvalidTypeException(""Malformed unit type"", item)
        if isinstance(item.args[-1], ast.Name) and item.args[-1].id == ""positional"":
            positional = True
            argz = item.args[:-1]
        else:
            positional = False
            argz = item.args
        if len(argz) != 1:
            raise InvalidTypeException(""Malformed unit type"", item)
        unit = parse_unit(argz[0], custom_units=custom_units)
        return BaseType(base_type, unit, positional)
    # Subscripts
    elif isinstance(item, ast.Subscript):

        if 'value' not in vars(item.slice):
            raise InvalidTypeException(""Array / ByteArray access must access a single element, not a slice"", item)
        # Fixed size lists or bytearrays, e.g. num[100]
        is_constant_val = constants.ast_is_constant(item.slice.value)
        if isinstance(item.slice.value, ast.Num) or is_constant_val:
            n_val = constants.get_constant(item.slice.value.id, context=None).value if is_constant_val else item.slice.value.n
            if not isinstance(n_val, int) or n_val <= 0:
                raise InvalidTypeException(""Arrays / ByteArrays must have a positive integral number of elements"", item.slice.value)
            # ByteArray
            if getattr(item.value, 'id', None) == 'bytes':
                return ByteArrayType(n_val)
            # List
            else:
                return ListType(parse_type(item.value, location, custom_units=custom_units, custom_structs=custom_structs, constants=constants), n_val)
        # Mappings, e.g. num[address]
        else:
            warnings.warn(
                ""Mapping definitions using subscript have deprecated (see VIP564). ""
                ""Use map(type1, type2) instead."",
                DeprecationWarning
            )
            raise InvalidTypeException('Unknown list type.', item)

    # Dicts, used to represent mappings, e.g. {uint: uint}. Key must be a base type
    elif isinstance(item, ast.Dict):
        warnings.warn(
            ""Anonymous structs have been removed in""
            "" favor of named structs, see VIP300"",
            DeprecationWarning
        )
        raise InvalidTypeException(""Invalid type: %r"" % ast.dump(item), item)
    elif isinstance(item, ast.Tuple):
        members = [parse_type(x, location, custom_units=custom_units, custom_structs=custom_structs, constants=constants) for x in item.elts]
        return TupleType(members)
    else:
        raise InvalidTypeException(""Invalid type: %r"" % ast.dump(item), item)","1. Use `ast.literal_eval` to parse strings into Python objects. This will help to prevent code injection attacks.
2. Use `ast.unparse` to convert Python objects back into strings. This will help to prevent XSS attacks.
3. Use `ast.dump` to print the AST of a Python program. This will help to debug errors and security vulnerabilities."
"    def get_item_name_and_attributes(self, item, attributes):
        if isinstance(item, ast.Name):
            return item.id, attributes
        elif isinstance(item, ast.AnnAssign):
            return self.get_item_name_and_attributes(item.annotation, attributes)
        elif isinstance(item, ast.Subscript):
            return self.get_item_name_and_attributes(item.value, attributes)
        elif isinstance(item, ast.Call) and item.func.id == 'map':
            if len(item.args) != 2:
                raise StructureException(""Map type expects two type arguments map(type1, type2)"", item.func)
            return self.get_item_name_and_attributes(item.args, attributes)
        # elif ist
        elif isinstance(item, ast.Call):
            attributes[item.func.id] = True
            # Raise for multiple args
            if len(item.args) != 1:
                raise StructureException(""%s expects one arg (the type)"" % item.func.id)
            return self.get_item_name_and_attributes(item.args[0], attributes)
        return None, attributes","1. Use `ast.literal_eval()` instead of `eval()` to sanitize user input.
2. Use `ast.unparse()` to validate the AST before evaluating it.
3. Use `ast.dump()` to debug the AST."
"    def add_globals_and_events(self, item):
        item_attributes = {""public"": False}

        # Handle constants.
        if isinstance(item.annotation, ast.Call) and item.annotation.func.id == ""constant"":
            self._constants.add_constant(item, global_ctx=self)
            return

        # Handle events.
        if not (isinstance(item.annotation, ast.Call) and item.annotation.func.id == ""event""):
            item_name, item_attributes = self.get_item_name_and_attributes(item, item_attributes)
            if not all([attr in valid_global_keywords for attr in item_attributes.keys()]):
                raise StructureException('Invalid global keyword used: %s' % item_attributes, item)

        if item.value is not None:
            raise StructureException('May not assign value whilst defining type', item)
        elif isinstance(item.annotation, ast.Call) and item.annotation.func.id == ""event"":
            if self._globals or len(self._defs):
                raise EventDeclarationException(""Events must all come before global declarations and function definitions"", item)
            self._events.append(item)
        elif not isinstance(item.target, ast.Name):
            raise StructureException(""Can only assign type to variable in top-level statement"", item)

        # Is this a custom unit definition.
        elif item.target.id == 'units':
            if not self._custom_units:
                if not isinstance(item.annotation, ast.Dict):
                    raise VariableDeclarationException(""Define custom units using units: { }."", item.target)
                for key, value in zip(item.annotation.keys, item.annotation.values):
                    if not isinstance(value, ast.Str):
                        raise VariableDeclarationException(""Custom unit description must be a valid string"", value)
                    if not isinstance(key, ast.Name):
                        raise VariableDeclarationException(""Custom unit name must be a valid string"", key)
                    check_valid_varname(key.id, self._custom_units, self._structs, self._constants, key, ""Custom unit invalid."")
                    self._custom_units.add(key.id)
                    self._custom_units_descriptions[key.id] = value.s
            else:
                raise VariableDeclarationException(""Custom units can only be defined once"", item.target)

        # Check if variable name is valid.
        # Don't move this check higher, as unit parsing has to happen first.
        elif not self.is_valid_varname(item.target.id, item):
            pass

        elif len(self._defs):
            raise StructureException(""Global variables must all come before function definitions"", item)
        # If the type declaration is of the form public(<type here>), then proceed with
        # the underlying type but also add getters
        elif isinstance(item.annotation, ast.Call) and item.annotation.func.id == ""address"":
            if item.annotation.args[0].id not in premade_contracts:
                raise VariableDeclarationException(""Unsupported premade contract declaration"", item.annotation.args[0])
            premade_contract = premade_contracts[item.annotation.args[0].id]
            self._contracts[item.target.id] = self.make_contract(premade_contract.body)
            self._globals[item.target.id] = VariableRecord(item.target.id, len(self._globals), BaseType('address'), True)

        elif item_name in self._contracts:
            self._globals[item.target.id] = ContractRecord(item.target.id, len(self._globals), ContractType(item_name), True)
            if item_attributes[""public""]:
                typ = ContractType(item_name)
                for getter in self.mk_getter(item.target.id, typ):
                    self._getters.append(self.parse_line('\\n' * (item.lineno - 1) + getter))
                    self._getters[-1].pos = getpos(item)

        elif isinstance(item.annotation, ast.Call) and item.annotation.func.id == ""public"":
            if isinstance(item.annotation.args[0], ast.Name) and item_name in self._contracts:
                typ = ContractType(item_name)
            else:
                typ = parse_type(item.annotation.args[0], 'storage', custom_units=self._custom_units, custom_structs=self._structs, constants=self._constants)
            self._globals[item.target.id] = VariableRecord(item.target.id, len(self._globals), typ, True)
            # Adding getters here
            for getter in self.mk_getter(item.target.id, typ):
                self._getters.append(self.parse_line('\\n' * (item.lineno - 1) + getter))
                self._getters[-1].pos = getpos(item)

        else:
            self._globals[item.target.id] = VariableRecord(
                item.target.id, len(self._globals),
                parse_type(item.annotation, 'storage', custom_units=self._custom_units, custom_structs=self._structs, constants=self._constants),
                True
            )","1. Use a function to validate the input data.
2. Use proper error handling to prevent unexpected errors.
3. Use secure coding practices to avoid vulnerabilities."
"    def from_declaration(cls, code, global_ctx):
        name = code.target.id
        pos = 0

        check_valid_varname(
            name, global_ctx._custom_units, global_ctx._structs, global_ctx._constants,
            pos=code, error_prefix=""Event name invalid. "", exc=EventDeclarationException
        )

        # Determine the arguments, expects something of the form def foo(arg1: num, arg2: num ...
        args = []
        indexed_list = []
        topics_count = 1
        if code.annotation.args:
            keys = code.annotation.args[0].keys
            values = code.annotation.args[0].values
            for i in range(len(keys)):
                typ = values[i]
                arg = keys[i].id
                arg_item = keys[i]
                is_indexed = False
                # Check to see if argument is a topic
                if isinstance(typ, ast.Call) and typ.func.id == 'indexed':
                    typ = values[i].args[0]
                    indexed_list.append(True)
                    topics_count += 1
                    is_indexed = True
                else:
                    indexed_list.append(False)
                if isinstance(typ, ast.Subscript) and getattr(typ.value, 'id', None) == 'bytes' and typ.slice.value.n > 32 and is_indexed:
                    raise EventDeclarationException(""Indexed arguments are limited to 32 bytes"")
                if topics_count > 4:
                    raise EventDeclarationException(""Maximum of 3 topics {} given"".format(topics_count - 1), arg)
                if not isinstance(arg, str):
                    raise VariableDeclarationException(""Argument name invalid"", arg)
                if not typ:
                    raise InvalidTypeException(""Argument must have type"", arg)
                check_valid_varname(arg, global_ctx._custom_units, global_ctx._structs, global_ctx._constants, pos=arg_item, error_prefix=""Event argument name invalid or reserved."")
                if arg in (x.name for x in args):
                    raise VariableDeclarationException(""Duplicate function argument name: "" + arg, arg_item)
                # Can struct be logged?
                parsed_type = global_ctx.parse_type(typ, None)
                args.append(VariableRecord(arg, pos, parsed_type, False))
                if isinstance(parsed_type, ByteArrayType):
                    pos += ceil32(typ.slice.value.n)
                else:
                    pos += get_size_of_type(parsed_type) * 32
        sig = name + '(' + ','.join([canonicalize_type(arg.typ, indexed_list[pos]) for pos, arg in enumerate(args)]) + ')'  # noqa F812
        event_id = bytes_to_int(sha3(bytes(sig, 'utf-8')))
        return cls(name, args, indexed_list, event_id, sig)","1. Use `check_valid_varname` to validate variable names.
2. Use `get_size_of_type` to get the size of a type.
3. Use `canonicalize_type` to canonicalize a type."
"def base_type_conversion(orig, frm, to, pos):
    orig = unwrap_location(orig)
    if getattr(frm, 'is_literal', False) and frm.typ in ('int128', 'uint256') and not SizeLimits.in_bounds(frm.typ, orig.value):
        raise InvalidLiteralException(""Number out of range: "" + str(orig.value), pos)
    if not isinstance(frm, (BaseType, NullType)) or not isinstance(to, BaseType):
        raise TypeMismatchException(""Base type conversion from or to non-base type: %r %r"" % (frm, to), pos)
    elif is_base_type(frm, to.typ) and are_units_compatible(frm, to):
        return LLLnode(orig.value, orig.args, typ=to, add_gas_estimate=orig.add_gas_estimate)
    elif is_base_type(frm, 'int128') and is_base_type(to, 'decimal') and are_units_compatible(frm, to):
        return LLLnode.from_list(['mul', orig, DECIMAL_DIVISOR], typ=BaseType('decimal', to.unit, to.positional))
    elif isinstance(frm, NullType):
        if to.typ not in ('int128', 'bool', 'uint256', 'address', 'bytes32', 'decimal'):
            # This is only to future proof the use of  base_type_conversion.
            raise TypeMismatchException(""Cannot convert null-type object to type %r"" % to, pos)  # pragma: no cover
        return LLLnode.from_list(0, typ=to)
    elif isinstance(to, ContractType) and frm.typ == 'address':
        return LLLnode(orig.value, orig.args, typ=to, add_gas_estimate=orig.add_gas_estimate)
    # Integer literal conversion.
    elif (frm.typ, to.typ, frm.is_literal) == ('int128', 'uint256', True):
        return LLLnode(orig.value, orig.args, typ=to, add_gas_estimate=orig.add_gas_estimate)
    else:
        raise TypeMismatchException(""Typecasting from base type %r to %r unavailable"" % (frm, to), pos)","1. Use `isinstance` to check if the input is a base type or not.
2. Use `are_units_compatible` to check if the units of the input and output types are compatible.
3. Use `LLLnode.from_list` to create a new LLLnode with the specified type."
"    def _check_valid_assign(self, sub):
        if isinstance(self.stmt.annotation, ast.Call):  # unit style: num(wei)
            if self.stmt.annotation.func.id != sub.typ.typ and not sub.typ.is_literal:
                raise TypeMismatchException('Invalid type, expected: %s' % self.stmt.annotation.func.id, self.stmt)
        elif isinstance(self.stmt.annotation, ast.Dict):
            if not isinstance(sub.typ, StructType):
                raise TypeMismatchException('Invalid type, expected a struct')
        elif isinstance(self.stmt.annotation, ast.Subscript):
            if not isinstance(sub.typ, (ListType, ByteArrayType)):  # check list assign.
                raise TypeMismatchException('Invalid type, expected: %s' % self.stmt.annotation.value.id, self.stmt)
        # Check that the integer literal, can be assigned to uint256 if necessary.
        elif (self.stmt.annotation.id, sub.typ.typ) == ('uint256', 'int128') and sub.typ.is_literal:
            if not SizeLimits.in_bounds('uint256', sub.value):
                raise InvalidLiteralException('Invalid uint256 assignment, value not in uint256 range.', self.stmt)
        elif self.stmt.annotation.id != sub.typ.typ and not sub.typ.unit:
            raise TypeMismatchException('Invalid type, expected: %s' % self.stmt.annotation.id, self.stmt)","1. Use `isinstance()` to check the type of the annotation and the subexpression.
2. Raise `TypeMismatchException` if the types are not compatible.
3. Use `SizeLimits.in_bounds()` to check if the integer literal is in the valid range for `uint256`."
"    def ann_assign(self):
        self.context.set_in_assignment(True)
        typ = parse_type(self.stmt.annotation, location='memory', custom_units=self.context.custom_units)
        if isinstance(self.stmt.target, ast.Attribute) and self.stmt.target.value.id == 'self':
            raise TypeMismatchException('May not redefine storage variables.', self.stmt)
        varname = self.stmt.target.id
        pos = self.context.new_variable(varname, typ)
        o = LLLnode.from_list('pass', typ=None, pos=pos)
        if self.stmt.value is not None:
            sub = Expr(self.stmt.value, self.context).lll_node
            self._check_valid_assign(sub)
            self._check_same_variable_assign(sub)
            variable_loc = LLLnode.from_list(pos, typ=typ, location='memory', pos=getpos(self.stmt))
            o = make_setter(variable_loc, sub, 'memory', pos=getpos(self.stmt))
        self.context.set_in_assignment(False)
        return o","1. Use `self.context.new_variable()` to create a new variable instead of using `self.context.new_variable(varname, typ)`.
2. Use `self._check_valid_assign()` to check if the assignment is valid.
3. Use `self._check_same_variable_assign()` to check if the assignment is to the same variable."
"def parse_body(code, context):
    if not isinstance(code, list):
        return parse_stmt(code, context)
    o = []
    for stmt in code:
        o.append(parse_stmt(stmt, context))
    return LLLnode.from_list(['seq'] + o, pos=getpos(code[0]) if code else None)","1. Use `ast.literal_eval()` instead of `eval()` to parse user input. This will prevent code injection attacks.
2. Sanitize user input to remove dangerous characters. This will prevent XSS attacks.
3. Use `assert` statements to validate user input. This will help catch errors early."
"    def __init__(self, stmt, context):
        self.stmt = stmt
        self.context = context
        self.stmt_table = {
            ast.Expr: self.expr,
            ast.Pass: self.parse_pass,
            ast.AnnAssign: self.ann_assign,
            ast.Assign: self.assign,
            ast.If: self.parse_if,
            ast.Call: self.call,
            ast.Assert: self.parse_assert,
            ast.For: self.parse_for,
            ast.AugAssign: self.aug_assign,
            ast.Break: self.parse_break,
            ast.Continue: self.parse_continue,
            ast.Return: self.parse_return,
            ast.Delete: self.parse_delete
        }
        stmt_type = self.stmt.__class__
        if stmt_type in self.stmt_table:
            self.lll_node = self.stmt_table[stmt_type]()
        elif isinstance(stmt, ast.Name) and stmt.id == ""throw"":
            self.lll_node = LLLnode.from_list(['assert', 0], typ=None, pos=getpos(stmt))
        else:
            raise StructureException(""Unsupported statement type: %s"" % type(stmt), stmt)","1. Use `ast.literal_eval` instead of `eval` to sanitize user input.
2. Use `ast.unparse` to validate the AST before executing it.
3. Use `sys.setrecursionlimit` to limit the maximum recursion depth."
"def send_file(filename_or_fp, mimetype=None, as_attachment=False,
              attachment_filename=None, add_etags=True,
              cache_timeout=None, conditional=False, last_modified=None):
    """"""Sends the contents of a file to the client.  This will use the
    most efficient method available and configured.  By default it will
    try to use the WSGI server's file_wrapper support.  Alternatively
    you can set the application's :attr:`~Flask.use_x_sendfile` attribute
    to ``True`` to directly emit an ``X-Sendfile`` header.  This however
    requires support of the underlying webserver for ``X-Sendfile``.

    By default it will try to guess the mimetype for you, but you can
    also explicitly provide one.  For extra security you probably want
    to send certain files as attachment (HTML for instance).  The mimetype
    guessing requires a `filename` or an `attachment_filename` to be
    provided.

    ETags will also be attached automatically if a `filename` is provided. You
    can turn this off by setting `add_etags=False`.

    If `conditional=True` and `filename` is provided, this method will try to
    upgrade the response stream to support range requests.  This will allow
    the request to be answered with partial content response.

    Please never pass filenames to this function from user sources;
    you should use :func:`send_from_directory` instead.

    .. versionadded:: 0.2

    .. versionadded:: 0.5
       The `add_etags`, `cache_timeout` and `conditional` parameters were
       added.  The default behavior is now to attach etags.

    .. versionchanged:: 0.7
       mimetype guessing and etag support for file objects was
       deprecated because it was unreliable.  Pass a filename if you are
       able to, otherwise attach an etag yourself.  This functionality
       will be removed in Flask 1.0

    .. versionchanged:: 0.9
       cache_timeout pulls its default from application config, when None.

    .. versionchanged:: 0.12
       The filename is no longer automatically inferred from file objects. If
       you want to use automatic mimetype and etag support, pass a filepath via
       `filename_or_fp` or `attachment_filename`.

    .. versionchanged:: 0.12
       The `attachment_filename` is preferred over `filename` for MIME-type
       detection.

    :param filename_or_fp: the filename of the file to send in `latin-1`.
                           This is relative to the :attr:`~Flask.root_path`
                           if a relative path is specified.
                           Alternatively a file object might be provided in
                           which case ``X-Sendfile`` might not work and fall
                           back to the traditional method.  Make sure that the
                           file pointer is positioned at the start of data to
                           send before calling :func:`send_file`.
    :param mimetype: the mimetype of the file if provided. If a file path is
                     given, auto detection happens as fallback, otherwise an
                     error will be raised.
    :param as_attachment: set to ``True`` if you want to send this file with
                          a ``Content-Disposition: attachment`` header.
    :param attachment_filename: the filename for the attachment if it
                                differs from the file's filename.
    :param add_etags: set to ``False`` to disable attaching of etags.
    :param conditional: set to ``True`` to enable conditional responses.

    :param cache_timeout: the timeout in seconds for the headers. When ``None``
                          (default), this value is set by
                          :meth:`~Flask.get_send_file_max_age` of
                          :data:`~flask.current_app`.
    :param last_modified: set the ``Last-Modified`` header to this value,
        a :class:`~datetime.datetime` or timestamp.
        If a file was passed, this overrides its mtime.
    """"""
    mtime = None
    fsize = None
    if isinstance(filename_or_fp, string_types):
        filename = filename_or_fp
        if not os.path.isabs(filename):
            filename = os.path.join(current_app.root_path, filename)
        file = None
        if attachment_filename is None:
            attachment_filename = os.path.basename(filename)
    else:
        file = filename_or_fp
        filename = None

    if mimetype is None:
        if attachment_filename is not None:
            mimetype = mimetypes.guess_type(attachment_filename)[0] \\
                or 'application/octet-stream'

        if mimetype is None:
            raise ValueError(
                'Unable to infer MIME-type because no filename is available. '
                'Please set either `attachment_filename`, pass a filepath to '
                '`filename_or_fp` or set your own MIME-type via `mimetype`.'
            )

    headers = Headers()
    if as_attachment:
        if attachment_filename is None:
            raise TypeError('filename unavailable, required for '
                            'sending as attachment')
        headers.add('Content-Disposition', 'attachment',
                    filename=attachment_filename)

    if current_app.use_x_sendfile and filename:
        if file is not None:
            file.close()
        headers['X-Sendfile'] = filename
        fsize = os.path.getsize(filename)
        headers['Content-Length'] = fsize
        data = None
    else:
        if file is None:
            file = open(filename, 'rb')
            mtime = os.path.getmtime(filename)
            fsize = os.path.getsize(filename)
            headers['Content-Length'] = fsize
        data = wrap_file(request.environ, file)

    rv = current_app.response_class(data, mimetype=mimetype, headers=headers,
                                    direct_passthrough=True)

    if last_modified is not None:
        rv.last_modified = last_modified
    elif mtime is not None:
        rv.last_modified = mtime

    rv.cache_control.public = True
    if cache_timeout is None:
        cache_timeout = current_app.get_send_file_max_age(filename)
    if cache_timeout is not None:
        rv.cache_control.max_age = cache_timeout
        rv.expires = int(time() + cache_timeout)

    if add_etags and filename is not None:
        from warnings import warn

        try:
            rv.set_etag('%s-%s-%s' % (
                os.path.getmtime(filename),
                os.path.getsize(filename),
                adler32(
                    filename.encode('utf-8') if isinstance(filename, text_type)
                    else filename
                ) & 0xffffffff
            ))
        except OSError:
            warn('Access %s failed, maybe it does not exist, so ignore etags in '
                 'headers' % filename, stacklevel=2)

    if conditional:
        if callable(getattr(Range, 'to_content_range_header', None)):
            # Werkzeug supports Range Requests
            # Remove this test when support for Werkzeug <0.12 is dropped
            try:
                rv = rv.make_conditional(request, accept_ranges=True,
                                         complete_length=fsize)
            except RequestedRangeNotSatisfiable:
                file.close()
                raise
        else:
            rv = rv.make_conditional(request)
        # make sure we don't send x-sendfile for servers that
        # ignore the 304 status code for x-sendfile.
        if rv.status_code == 304:
            rv.headers.pop('x-sendfile', None)
    return rv","1. Use `send_from_directory` instead of `send_file` to avoid passing filenames from user sources.
2. Always provide a `mimetype` when using `send_file`.
3. Set `add_etags=False` to disable attaching of etags."
"    def __init__(
        self,
        model: ""Type[Model]"",
        db: ""BaseDBAsyncClient"",
        prefetch_map=None,
        prefetch_queries=None,
    ) -> None:
        self.model = model
        self.db: ""BaseDBAsyncClient"" = db
        self.prefetch_map = prefetch_map if prefetch_map else {}
        self._prefetch_queries = prefetch_queries if prefetch_queries else {}

        key = f""{self.db.connection_name}:{self.model._meta.table}""
        if key not in EXECUTOR_CACHE:
            self.regular_columns, columns = self._prepare_insert_columns()
            self.insert_query = self._prepare_insert_statement(columns)

            self.column_map: Dict[str, Callable[[Any, Any], Any]] = {}
            for column in self.regular_columns:
                field_object = self.model._meta.fields_map[column]
                if field_object.__class__ in self.TO_DB_OVERRIDE:
                    self.column_map[column] = partial(
                        self.TO_DB_OVERRIDE[field_object.__class__], field_object
                    )
                else:
                    self.column_map[column] = field_object.to_db_value

            table = Table(self.model._meta.table)
            self.delete_query = str(
                self.model._meta.basequery.where(
                    getattr(table, self.model._meta.db_pk_field) == self.Parameter(0)
                ).delete()
            )
            self.update_cache: Dict[str, str] = {}

            EXECUTOR_CACHE[key] = (
                self.regular_columns,
                self.insert_query,
                self.column_map,
                self.delete_query,
                self.update_cache,
            )
        else:
            (
                self.regular_columns,
                self.insert_query,
                self.column_map,
                self.delete_query,
                self.update_cache,
            ) = EXECUTOR_CACHE[key]","1. Use prepared statements instead of building queries with string concatenation.
2. Use parameter binding to avoid SQL injection attacks.
3. Sanitize user input before using it in SQL queries."
"    def get_update_sql(self, update_fields: Optional[List[str]]) -> str:
        """"""
        Generates the SQL for updating a model depending on provided update_fields.
        Result is cached for performance.
        """"""
        key = "","".join(update_fields) if update_fields else """"
        if key in self.update_cache:
            return self.update_cache[key]

        table = Table(self.model._meta.table)
        query = self.db.query_class.update(table)
        count = 0
        for field in update_fields or self.model._meta.fields_db_projection.keys():
            db_field = self.model._meta.fields_db_projection[field]
            field_object = self.model._meta.fields_map[field]
            if not field_object.pk:
                query = query.set(db_field, self.Parameter(count))
                count += 1

        query = query.where(getattr(table, self.model._meta.db_pk_field) == self.Parameter(count))

        sql = self.update_cache[key] = query.get_sql()
        return sql","1. Use prepared statements to prevent SQL injection attacks.
2. Sanitize user input to prevent cross-site scripting attacks.
3. Use proper access control to restrict who can update records."
"    async def _prefetch_m2m_relation(self, instance_list: list, field: str, related_query) -> list:
        instance_id_set: set = {
            self._field_to_db(instance._meta.pk, instance.pk, instance)
            for instance in instance_list
        }

        field_object: fields.ManyToManyFieldInstance = self.model._meta.fields_map[  # type: ignore
            field
        ]

        through_table = Table(field_object.through)

        subquery = (
            self.db.query_class.from_(through_table)
            .select(
                getattr(through_table, field_object.backward_key).as_(""_backward_relation_key""),
                getattr(through_table, field_object.forward_key).as_(""_forward_relation_key""),
            )
            .where(getattr(through_table, field_object.backward_key).isin(instance_id_set))
        )

        related_query_table = Table(related_query.model._meta.table)
        related_pk_field = related_query.model._meta.db_pk_field
        query = (
            related_query.query.join(subquery)
            .on(subquery._forward_relation_key == getattr(related_query_table, related_pk_field))
            .select(
                subquery._backward_relation_key.as_(""_backward_relation_key""),
                *[getattr(related_query_table, field).as_(field) for field in related_query.fields],
            )
        )

        if related_query._q_objects:
            joined_tables: List[Table] = []
            modifier = QueryModifier()
            for node in related_query._q_objects:
                modifier &= node.resolve(
                    model=related_query.model,
                    annotations=related_query._annotations,
                    custom_filters=related_query._custom_filters,
                )

            where_criterion, joins, having_criterion = modifier.get_query_modifiers()
            for join in joins:
                if join[0] not in joined_tables:
                    query = query.join(join[0], how=JoinType.left_outer).on(join[1])
                    joined_tables.append(join[0])

            if where_criterion:
                query = query.where(where_criterion)

            if having_criterion:
                query = query.having(having_criterion)

        raw_results = await self.db.execute_query(query.get_sql())
        relations = {
            (
                self.model._meta.pk.to_python_value(e[""_backward_relation_key""]),
                field_object.field_type._meta.pk.to_python_value(e[related_pk_field]),
            )
            for e in raw_results
        }
        related_object_list = [related_query.model._init_from_db(**e) for e in raw_results]
        await self.__class__(
            model=related_query.model, db=self.db, prefetch_map=related_query._prefetch_map
        ).fetch_for_list(related_object_list)
        related_object_map = {e.pk: e for e in related_object_list}
        relation_map: Dict[str, list] = {}

        for object_id, related_object_id in relations:
            if object_id not in relation_map:
                relation_map[object_id] = []
            relation_map[object_id].append(related_object_map[related_object_id])

        for instance in instance_list:
            relation_container = getattr(instance, field)
            relation_container._set_result_for_query(relation_map.get(instance.pk, []))
        return instance_list","1. Use prepared statements instead of building the SQL query string in the code. This will prevent SQL injection attacks.
2. Use the `func.to_python_value()` function to sanitize user input before using it in a query. This will prevent malicious users from injecting code into the database.
3. Use the `func.select_related()` function to eagerly load related objects when querying for data. This will improve performance and reduce the number of database queries that need to be executed."
"    async def _prefetch_reverse_relation(
        self, instance_list: list, field: str, related_query
    ) -> list:
        instance_id_set = {instance.pk for instance in instance_list}  # type: Set[Any]
        backward_relation_manager = getattr(self.model, field)
        relation_field = backward_relation_manager.relation_field

        related_object_list = await related_query.filter(
            **{""{}__in"".format(relation_field): list(instance_id_set)}
        )

        related_object_map = {}  # type: Dict[str, list]
        for entry in related_object_list:
            object_id = getattr(entry, relation_field)
            if object_id in related_object_map.keys():
                related_object_map[object_id].append(entry)
            else:
                related_object_map[object_id] = [entry]
        for instance in instance_list:
            relation_container = getattr(instance, field)
            relation_container._set_result_for_query(related_object_map.get(instance.pk, []))
        return instance_list","1. Use `.values_list()` instead of `.filter()` to prevent SQL injection.
2. Use `.distinct()` to prevent duplicate results.
3. Use `.order_by()` to ensure that the results are returned in a predictable order."
"    async def _prefetch_m2m_relation(self, instance_list: list, field: str, related_query) -> list:
        instance_id_set = {instance.pk for instance in instance_list}  # type: Set[Any]

        field_object = self.model._meta.fields_map[field]

        through_table = Table(field_object.through)

        subquery = (
            self.db.query_class.from_(through_table)
            .select(
                getattr(through_table, field_object.backward_key).as_(""_backward_relation_key""),
                getattr(through_table, field_object.forward_key).as_(""_forward_relation_key""),
            )
            .where(getattr(through_table, field_object.backward_key).isin(instance_id_set))
        )

        related_query_table = Table(related_query.model._meta.table)
        related_pk_field = related_query.model._meta.db_pk_field
        query = (
            related_query.query.join(subquery)
            .on(subquery._forward_relation_key == getattr(related_query_table, related_pk_field))
            .select(
                subquery._backward_relation_key.as_(""_backward_relation_key""),
                *[getattr(related_query_table, field).as_(field) for field in related_query.fields],
            )
        )

        if related_query._q_objects:
            joined_tables = []  # type: List[Table]
            modifier = QueryModifier()
            for node in related_query._q_objects:
                modifier &= node.resolve(
                    model=related_query.model,
                    annotations=related_query._annotations,
                    custom_filters=related_query._custom_filters,
                )

            where_criterion, joins, having_criterion = modifier.get_query_modifiers()
            for join in joins:
                if join[0] not in joined_tables:
                    query = query.join(join[0], how=JoinType.left_outer).on(join[1])
                    joined_tables.append(join[0])

            if where_criterion:
                query = query.where(where_criterion)

            if having_criterion:
                query = query.having(having_criterion)

        raw_results = await self.db.execute_query(query.get_sql())
        relations = {
            (
                self.model._meta.pk.to_python_value(e[""_backward_relation_key""]),
                field_object.type._meta.pk.to_python_value(e[related_pk_field]),
            )
            for e in raw_results
        }
        related_object_list = [related_query.model._init_from_db(**e) for e in raw_results]
        await self.__class__(
            model=related_query.model, db=self.db, prefetch_map=related_query._prefetch_map
        ).fetch_for_list(related_object_list)
        related_object_map = {e.pk: e for e in related_object_list}
        relation_map = {}  # type: Dict[str, list]

        for object_id, related_object_id in relations:
            if object_id not in relation_map:
                relation_map[object_id] = []
            relation_map[object_id].append(related_object_map[related_object_id])

        for instance in instance_list:
            relation_container = getattr(instance, field)
            relation_container._set_result_for_query(relation_map.get(instance.pk, []))
        return instance_list","1. Use prepared statements to prevent SQL injection.
2. Use `.select_for_update()` to prevent other users from modifying the data while you are fetching it.
3. Use `.distinct()` to prevent duplicate rows from being returned."
"    def _query(self):
        if not self.instance.pk:
            raise OperationalError(
                ""This objects hasn't been instanced, call .save() before"" "" calling related queries""
            )
        return self.model.filter(**{self.relation_field: self.instance.pk})","1. **Use `.get()` instead of `.filter()` to avoid returning multiple objects.** This will prevent attackers from using SQL injection to retrieve sensitive data.
2. **Use `.values()` to only return the fields that you need.** This will reduce the amount of data that is exposed to attackers.
3. **Use `.distinct()` to ensure that only unique rows are returned.** This will prevent attackers from using a technique called ""table pivoting"" to retrieve sensitive data."
"    async def add(self, *instances, using_db=None) -> None:
        """"""
        Adds one or more of ``instances`` to the relation.

        If it is already added, it will be silently ignored.
        """"""
        if not instances:
            return
        if self.instance.pk is None:
            raise OperationalError(
                ""You should first call .save() on {model}"".format(model=self.instance)
            )
        db = using_db if using_db else self.model._meta.db
        pk_formatting_func = type(self.instance)._meta.pk.to_db_value
        related_pk_formatting_func = type(instances[0])._meta.pk.to_db_value
        through_table = Table(self.field.through)
        select_query = (
            db.query_class.from_(through_table)
            .where(
                getattr(through_table, self.field.backward_key)
                == pk_formatting_func(self.instance.pk, self.instance)
            )
            .select(self.field.backward_key, self.field.forward_key)
        )
        query = db.query_class.into(through_table).columns(
            getattr(through_table, self.field.forward_key),
            getattr(through_table, self.field.backward_key),
        )

        if len(instances) == 1:
            criterion = getattr(
                through_table, self.field.forward_key
            ) == related_pk_formatting_func(instances[0].pk, instances[0])
        else:
            criterion = getattr(through_table, self.field.forward_key).isin(
                [related_pk_formatting_func(i.pk, i) for i in instances]
            )

        select_query = select_query.where(criterion)

        already_existing_relations_raw = await db.execute_query(str(select_query))
        already_existing_relations = {
            (r[self.field.backward_key], r[self.field.forward_key])
            for r in already_existing_relations_raw
        }

        insert_is_required = False
        for instance_to_add in instances:
            if instance_to_add.pk is None:
                raise OperationalError(
                    ""You should first call .save() on {model}"".format(model=instance_to_add)
                )
            if (self.instance.pk, instance_to_add.pk) in already_existing_relations:
                continue
            query = query.insert(
                related_pk_formatting_func(instance_to_add.pk, instance_to_add),
                pk_formatting_func(self.instance.pk, self.instance),
            )
            insert_is_required = True
        if insert_is_required:
            await db.execute_query(str(query))","1. Use `check_instance_exists` to check if the instance exists before adding it to the relation.
2. Use `check_related_instance_exists` to check if the related instance exists before adding it to the relation.
3. Use `check_unique_together` to check if the relation is unique before adding it."
"    def _set_field_values(self, values_map: Dict[str, Any]) -> Set[str]:
        """"""
        Sets values for fields honoring type transformations and
        return list of fields that were set additionally
        """"""
        meta = self._meta
        passed_fields = set()

        for key, value in values_map.items():
            if key in meta.fk_fields:
                if hasattr(value, ""pk"") and not value.pk:
                    raise OperationalError(
                        ""You should first call .save() on {} before referring to it"".format(value)
                    )
                relation_field = ""{}_id"".format(key)
                setattr(self, relation_field, value.pk)
                passed_fields.add(relation_field)
            elif key in meta.fields:
                field_object = meta.fields_map[key]
                if value is None and not field_object.null:
                    raise ValueError(""{} is non nullable field, but null was passed"".format(key))
                setattr(self, key, field_object.to_python_value(value))
            elif key in meta.db_fields:
                field_object = meta.fields_map[meta.fields_db_projection_reverse[key]]
                if value is None and not field_object.null:
                    raise ValueError(""{} is non nullable field, but null was passed"".format(key))
                setattr(self, key, field_object.to_python_value(value))
            elif key in meta.backward_fk_fields:
                raise ConfigurationError(
                    ""You can't set backward relations through init, change related model instead""
                )
            elif key in meta.m2m_fields:
                raise ConfigurationError(
                    ""You can't set m2m relations through init, use m2m_manager instead""
                )

        return passed_fields","1. Sanitize user input to prevent SQL injection attacks.
2. Validate field values to prevent invalid data from being saved.
3. Use proper exception handling to prevent errors from being exposed to the user."
"    def _make_query(self):
        table = Table(self.model._meta.table)
        self.query = self._db.query_class.update(table)
        self.resolve_filters(
            model=self.model,
            q_objects=self.q_objects,
            annotations=self.annotations,
            custom_filters=self.custom_filters,
        )
        # Need to get executor to get correct column_map
        executor = self._db.executor_class(model=self.model, db=self._db)

        for key, value in self.update_kwargs.items():
            field_object = self.model._meta.fields_map.get(key)
            if not field_object:
                raise FieldError(""Unknown keyword argument {} for model {}"".format(key, self.model))
            if field_object.generated:
                raise IntegrityError(""Field {} is generated and can not be updated"".format(key))
            if key in self.model._meta.db_fields:
                db_field = self.model._meta.fields_db_projection[key]
                value = executor.column_map[key](value, None)
            elif isinstance(field_object, fields.ForeignKeyField):
                db_field = ""{}_id"".format(key)
                value = value.id
            else:
                raise FieldError(""Field {} is virtual and can not be updated"".format(key))

            self.query = self.query.set(db_field, value)","1. Use `functools.wraps` to preserve the metadata of the decorated function.
2. Use `inspect.getfullargspec` to get the argument names of the decorated function.
3. Use `inspect.ismethod` to check if the decorated function is a method."
"def retry_connection(func):
    @wraps(func)
    async def wrapped(self, *args):
        try:
            return await func(self, *args)
        except (
            asyncpg.PostgresConnectionError,
            asyncpg.ConnectionDoesNotExistError,
            asyncpg.ConnectionFailureError,
        ):
            # Here we assume that a connection error has happened
            # Re-create connection and re-try the function call once only.
            await self._lock.acquire()
            logging.info(""Attempting reconnect"")
            try:
                await self._close()
                await self.create_connection(with_db=True)
                logging.info(""Reconnected"")
            except Exception:
                logging.info(""Failed to reconnect"")
            finally:
                self._lock.release()

            return await func(self, *args)

    return wrapped","1. Use a context manager to ensure that the connection is closed when the function exits.
2. Use a try-except block to catch connection errors and handle them gracefully.
3. Use a lock to prevent multiple threads from accessing the connection at the same time."
"def translate_exceptions(func):
    @wraps(func)
    async def wrapped(self, *args):
        try:
            return await func(self, *args)
        except asyncpg.SyntaxOrAccessError as exc:
            raise OperationalError(exc)
        except asyncpg.IntegrityConstraintViolationError as exc:
            raise IntegrityError(exc)

    return wrapped","1. Use prepared statements instead of building queries dynamically. This will prevent SQL injection attacks.
2. Sanitize user input before using it in queries. This will prevent code injection attacks.
3. Use a database connection pool to avoid creating new connections for each request. This will improve performance and reduce the risk of resource exhaustion."
"    def _in_transaction(self) -> ""TransactionWrapper"":
        return self._transaction_class(self.connection_name, self._connection, self._lock)","1. Use prepared statements to prevent SQL injection attacks.
2. Use connection pooling to reduce the number of open database connections.
3. Use transaction isolation levels to prevent data races and other concurrency issues."
"    def __init__(self, connection_name: str, connection, lock) -> None:
        self._connection = connection
        self._lock = lock
        self.log = logging.getLogger(""db_client"")
        self._transaction_class = self.__class__
        self._old_context_value = None
        self.connection_name = connection_name
        self.transaction = None
        self._finalized = False","1. Use prepared statements instead of string concatenation to prevent SQL injection attacks.
2. Use transaction isolation level to avoid dirty reads, non-repeatable reads, and phantom reads.
3. Close the connection when you are done with it to prevent resource leaks."
"    async def commit(self):
        if self._finalized:
            raise TransactionManagementError(""Transaction already finalised"")
        self._finalized = True
        await self.transaction.commit()
        current_transaction_map[self.connection_name].set(self._old_context_value)","1. Use prepared statements instead of building queries dynamically to prevent SQL injection attacks.
2. Use transactions to ensure that all changes are committed or rolled back together.
3. Encrypt sensitive data, such as passwords, before storing them in the database."
"    async def rollback(self):
        if self._finalized:
            raise TransactionManagementError(""Transaction already finalised"")
        self._finalized = True
        await self.transaction.rollback()
        current_transaction_map[self.connection_name].set(self._old_context_value)","1. Use prepared statements to prevent SQL injection attacks.
2. Use transactions to ensure that all changes are committed or rolled back together.
3. Use connection pooling to improve performance and reduce the number of open database connections."
"    async def create_connection(self, with_db: bool) -> None:
        self._template = {
            ""host"": self.host,
            ""port"": self.port,
            ""user"": self.user,
            ""database"": self.database if with_db else None,
            **self.extra,
        }
        try:
            self._connection = await asyncpg.connect(None, password=self.password, **self._template)
            self.log.debug(
                ""Created connection %s with params: %s"", self._connection, self._template
            )
        except asyncpg.InvalidCatalogNameError:
            raise DBConnectionError(
                ""Can't establish connection to database {}"".format(self.database)
            )","1. Use a secret manager to store the database password.
2. Use SSL to encrypt the connection between the application and the database.
3. Set a strong password policy for the database."
"    async def _close(self) -> None:
        if self._connection:  # pragma: nobranch
            await self._connection.close()
            self.log.debug(""Closed connection %s with params: %s"", self._connection, self._template)
            self._template.clear()","1. Use `contextlib.closing` to ensure that the connection is closed when the `_close()` function exits.
2. Use `logging.info()` instead of `logging.debug()` to log the connection close event.
3. Clear the `_template` variable to prevent memory leaks."
"    def __init__(
        self,
        dialect: str,
        *,
        # Deficiencies to work around:
        safe_indexes: bool = True,
        requires_limit: bool = False
    ) -> None:
        super().__setattr__(""_mutable"", True)

        self.dialect = dialect
        self.requires_limit = requires_limit
        self.safe_indexes = safe_indexes

        super().__setattr__(""_mutable"", False)","1. Use `functools.partial` to create a read-only object.
2. Use `validators` to validate user input.
3. Use `type-hints` to specify the types of arguments and return values."
"    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        if exc_type:
            await self.rollback()
        else:
            await self.commit()","1. Use `contextlib.closing` to ensure that the connection is closed when the `__aexit__` method is called.
2. Use `pymysql.connect()` with the `cursorclass` parameter set to `pymysql.cursors.DictCursor` to return rows as dictionaries.
3. Use `pymysql.escape_string()` to escape all user-provided input before using it in SQL statements."
"def retry_connection(func):
    @wraps(func)
    async def wrapped(self, *args):
        try:
            return await func(self, *args)
        except (
            RuntimeError,
            pymysql.err.OperationalError,
            pymysql.err.InternalError,
            pymysql.err.InterfaceError,
        ):
            # Here we assume that a connection error has happened
            # Re-create connection and re-try the function call once only.
            await self._lock.acquire()
            logging.info(""Attempting reconnect"")
            try:
                self._close()
                await self.create_connection(with_db=True)
                logging.info(""Reconnected"")
            except Exception:
                logging.info(""Failed to reconnect"")
            finally:
                self._lock.release()

            return await func(self, *args)

    return wrapped","1. Use a context manager to ensure that the connection is closed when the function exits.
2. Use a try-catch block to handle connection errors and retry the function call if necessary.
3. Use a lock to prevent multiple threads from accessing the connection at the same time."
"def translate_exceptions(func):
    @wraps(func)
    async def wrapped(self, *args):
        try:
            return await func(self, *args)
        except (
            pymysql.err.OperationalError,
            pymysql.err.ProgrammingError,
            pymysql.err.DataError,
            pymysql.err.InternalError,
            pymysql.err.NotSupportedError,
        ) as exc:
            raise OperationalError(exc)
        except pymysql.err.IntegrityError as exc:
            raise IntegrityError(exc)

    return wrapped","1. Use `functools.wraps` to preserve the function metadata of `func`.
2. Use `try-except` to catch and handle specific exceptions.
3. Use `raise` to raise a new exception with the same information as the caught exception."
"    def _close(self) -> None:
        if self._connection:  # pragma: nobranch
            self._connection.close()
            self.log.debug(""Closed connection %s with params: %s"", self._connection, self._template)
            self._template.clear()","1. **Use prepared statements** to prevent SQL injection attacks.
2. **Close the connection** after use to prevent resource leaks.
3. **Log all connections** and parameters to track activity."
"    async def close(self) -> None:
        self._close()
        self._connection = None","1. Use `contextlib.closing()` to ensure that the connection is closed when the async function exits.
2. Set the `read_timeout` and `write_timeout` parameters to prevent the connection from being blocked indefinitely.
3. Use `ssl.create_default_context()` to create a secure TLS/SSL connection."
"    def _in_transaction(self):
        return self._transaction_class(self.connection_name, self._connection, self._lock)","1. Use prepared statements to prevent SQL injection attacks.
2. Use a connection pool to avoid creating and closing connections unnecessarily.
3. Use transaction isolation level to prevent dirty reads."
"    def __init__(self, connection_name, connection, lock):
        self.connection_name = connection_name
        self._connection = connection
        self._lock = lock
        self.log = logging.getLogger(""db_client"")
        self._transaction_class = self.__class__
        self._finalized = False
        self._old_context_value = None","1. Use prepared statements to prevent SQL injection attacks.
2. Use transactions to ensure that all changes are committed or rolled back together.
3. Use connection pooling to reduce the number of open database connections."
"    async def start(self):
        await self._connection.begin()
        current_transaction = current_transaction_map[self.connection_name]
        self._old_context_value = current_transaction.get()
        current_transaction.set(self)","1. Use prepared statements instead of concatenation to prevent SQL injection.
2. Use transaction isolation level to avoid dirty reads.
3. Use connection pooling to reduce the number of open connections."
"    async def commit(self):
        if self._finalized:
            raise TransactionManagementError(""Transaction already finalised"")
        self._finalized = True
        await self._connection.commit()
        current_transaction_map[self.connection_name].set(self._old_context_value)","1. Use prepared statements to prevent SQL injection attacks.
2. Use transactions to ensure that all changes are committed or rolled back together.
3. Use role-based access control to restrict users' access to sensitive data."
"    async def rollback(self):
        if self._finalized:
            raise TransactionManagementError(""Transaction already finalised"")
        self._finalized = True
        await self._connection.rollback()
        current_transaction_map[self.connection_name].set(self._old_context_value)","1. Use prepared statements to prevent SQL injection.
2. Use transactions to ensure that all changes are committed or rolled back together.
3. Use role-based access control to restrict users' access to sensitive data."
"    async def create_connection(self, with_db: bool) -> None:
        self._template = {
            ""host"": self.host,
            ""port"": self.port,
            ""user"": self.user,
            ""db"": self.database if with_db else None,
            ""autocommit"": True,
            **self.extra,
        }
        try:
            self._connection = await aiomysql.connect(password=self.password, **self._template)
            self.log.debug(
                ""Created connection %s with params: %s"", self._connection, self._template
            )
        except pymysql.err.OperationalError:
            raise DBConnectionError(
                ""Can't connect to MySQL server: {template}"".format(template=self._template)
            )","1. Use a secret key to generate a random password for the connection.
2. Use a database user with limited privileges.
3. Connect to the database using a secure connection."
"def translate_exceptions(func):
    @wraps(func)
    async def wrapped(self, query, *args):
        try:
            return await func(self, query, *args)
        except sqlite3.OperationalError as exc:
            raise OperationalError(exc)
        except sqlite3.IntegrityError as exc:
            raise IntegrityError(exc)

    return wrapped","1. Use `functools.wraps` to preserve the metadata of the wrapped function.
2. Use `try ... except` to catch and handle specific exceptions.
3. Use `raise` to re-raise the caught exception with a more specific error message."
"    async def rollback(self) -> None:
        if self._finalized:
            raise TransactionManagementError(""Transaction already finalised"")
        self._finalized = True
        await self._connection.rollback()
        current_transaction_map[self.connection_name].set(self._old_context_value)","1. Use prepared statements to prevent SQL injection attacks.
2. Use transactions to ensure that all changes are committed or rolled back together.
3. Set the isolation level of the transaction to prevent dirty reads."
"    async def commit(self) -> None:
        if self._finalized:
            raise TransactionManagementError(""Transaction already finalised"")
        self._finalized = True
        await self._connection.commit()
        current_transaction_map[self.connection_name].set(self._old_context_value)","1. Use prepared statements to prevent SQL injection attacks.
2. Use transactions to ensure that all changes are committed or rolled back together.
3. Use role-based access control to restrict users' ability to perform certain actions."
"    def render(self, request):
        """"""
        Render the resource. This will takeover the transport underlying
        the request, create a :class:`autobahn.twisted.websocket.WebSocketServerProtocol`
        and let that do any subsequent communication.
        """"""
        # Create Autobahn WebSocket protocol.
        #
        protocol = self._factory.buildProtocol(request.transport.getPeer())
        if not protocol:
            # If protocol creation fails, we signal ""internal server error""
            request.setResponseCode(500)
            return b""""

        # Take over the transport from Twisted Web
        #
        transport, request.channel.transport = request.channel.transport, None

        # Connect the transport to our protocol. Once #3204 is fixed, there
        # may be a cleaner way of doing this.
        # http://twistedmatrix.com/trac/ticket/3204
        #
        if isinstance(transport, ProtocolWrapper):
            # i.e. TLS is a wrapping protocol
            transport.wrappedProtocol = protocol
        else:
            transport.protocol = protocol
        protocol.makeConnection(transport)

        # On Twisted 16+, the transport is paused whilst the existing
        # request is served; there won't be any requests after us so
        # we can just resume this ourselves.
        # 17.1 version
        if hasattr(transport, ""_networkProducer""):
            transport._networkProducer.resumeProducing()
        # 16.x version
        elif hasattr(transport, ""resumeProducing""):
            transport.resumeProducing()

        # We recreate the request and forward the raw data. This is somewhat
        # silly (since Twisted Web already did the HTTP request parsing
        # which we will do a 2nd time), but it's totally non-invasive to our
        # code. Maybe improve this.
        #
        if PY3:

            data = request.method + b' ' + request.uri + b' HTTP/1.1\\x0d\\x0a'
            for h in request.requestHeaders.getAllRawHeaders():
                data += h[0] + b': ' + b"","".join(h[1]) + b'\\x0d\\x0a'
            data += b""\\x0d\\x0a""
            data += request.content.read()

        else:
            data = ""%s %s HTTP/1.1\\x0d\\x0a"" % (request.method, request.uri)
            for h in request.requestHeaders.getAllRawHeaders():
                data += ""%s: %s\\x0d\\x0a"" % (h[0], "","".join(h[1]))
            data += ""\\x0d\\x0a""
        protocol.dataReceived(data)

        return NOT_DONE_YET","1. Use `request.setHeader()` to set the `Sec-WebSocket-Accept` header.
2. Use `request.setHeader()` to set the `Sec-WebSocket-Protocol` header.
3. Use `request.setHeader()` to set the `Sec-WebSocket-Extensions` header."
"    def _wrap_connection_future(self, transport, done, conn_f):

        def on_connect_success(result):
            # async connect call returns a 2-tuple
            transport, proto = result

            # if e.g. an SSL handshake fails, we will have
            # successfully connected (i.e. get here) but need to
            # 'listen' for the ""connection_lost"" from the underlying
            # protocol in case of handshake failure .. so we wrap
            # it. Also, we don't increment transport.success_count
            # here on purpose (because we might not succeed).

            # XXX double-check that asyncio behavior on TLS handshake
            # failures is in fact as described above
            orig = proto.connection_lost

            @wraps(orig)
            def lost(fail):
                rtn = orig(fail)
                if not txaio.is_called(done):
                    # asyncio will call connection_lost(None) in case of
                    # a transport failure, in which case we create an
                    # appropriate exception
                    if fail is None:
                        fail = TransportLost(""failed to complete connection"")
                    txaio.reject(done, fail)
                return rtn
            proto.connection_lost = lost

        def on_connect_failure(err):
            transport.connect_failures += 1
            # failed to establish a connection in the first place
            txaio.reject(done, err)

        txaio.add_callbacks(conn_f, on_connect_success, None)
        # the errback is added as a second step so it gets called if
        # there as an error in on_connect_success itself.
        txaio.add_callbacks(conn_f, None, on_connect_failure)
        return conn_f","1. Use `wraps` to preserve the original function metadata.
2. Check if `done` is called before calling `connection_lost`.
3. Handle `None` as a failure case in `connection_lost`."
"        def on_connect_success(result):
            # async connect call returns a 2-tuple
            transport, proto = result

            # if e.g. an SSL handshake fails, we will have
            # successfully connected (i.e. get here) but need to
            # 'listen' for the ""connection_lost"" from the underlying
            # protocol in case of handshake failure .. so we wrap
            # it. Also, we don't increment transport.success_count
            # here on purpose (because we might not succeed).

            # XXX double-check that asyncio behavior on TLS handshake
            # failures is in fact as described above
            orig = proto.connection_lost

            @wraps(orig)
            def lost(fail):
                rtn = orig(fail)
                if not txaio.is_called(done):
                    # asyncio will call connection_lost(None) in case of
                    # a transport failure, in which case we create an
                    # appropriate exception
                    if fail is None:
                        fail = TransportLost(""failed to complete connection"")
                    txaio.reject(done, fail)
                return rtn
            proto.connection_lost = lost","1. Use a secure protocol like TLS/SSL.
2. Check the return value of the `connect` call and handle errors appropriately.
3. Wrap the `connection_lost` callback to ensure that the `done` future is rejected in case of a connection failure."
"    def onClose(self, wasClean, code, reason):
        """"""
        Callback from :func:`autobahn.websocket.interfaces.IWebSocketChannel.onClose`
        """"""
        # WAMP session might never have been established in the first place .. guard this!
        if self._session is not None:
            # WebSocket connection lost - fire off the WAMP
            # session close callback
            # noinspection PyBroadException
            try:
                self.log.debug('WAMP-over-WebSocket transport lost: wasClean={wasClean}, code={code}, reason=""{reason}""', wasClean=wasClean, code=code, reason=reason)
                self._session.onClose(wasClean)
            except Exception:
                self.log.critical(""{tb}"", tb=traceback.format_exc())
            self._session = None","1. Add a check to ensure that the `_session` object is not `None` before calling `_session.onClose()`.
2. Handle exceptions more gracefully by catching and logging them.
3. Consider using a more robust exception handling library, such as `PyYAML` or `jsonschema`."
"    def startProxyConnect(self):
        """"""
        Connect to explicit proxy.
        """"""
        # construct proxy connect HTTP request
        #
        request = ""CONNECT %s:%d HTTP/1.1\\x0d\\x0a"" % (self.factory.host.encode(""utf-8""), self.factory.port)
        request += ""Host: %s:%d\\x0d\\x0a"" % (self.factory.host.encode(""utf-8""), self.factory.port)
        request += ""\\x0d\\x0a""

        self.log.debug(""{request}"", request=request)

        self.sendData(request)","1. Use HTTPS instead of HTTP to protect the traffic from eavesdropping.
2. Use a secure password for the proxy server.
3. Make sure the proxy server is configured to only allow connections from trusted clients."
"    def as_view(cls, actions=None, **initkwargs):
        """"""
        Because of the way class based views create a closure around the
        instantiated view, we need to totally reimplement `.as_view`,
        and slightly modify the view function that is created and returned.
        """"""
        # The suffix initkwarg is reserved for identifying the viewset type
        # eg. 'List' or 'Instance'.
        cls.suffix = None

        # actions must not be empty
        if not actions:
            raise TypeError(""The `actions` argument must be provided when ""
                            ""calling `.as_view()` on a ViewSet. For example ""
                            ""`.as_view({'get': 'list'})`"")

        # sanitize keyword arguments
        for key in initkwargs:
            if key in cls.http_method_names:
                raise TypeError(""You tried to pass in the %s method name as a ""
                                ""keyword argument to %s(). Don't do that.""
                                % (key, cls.__name__))
            if not hasattr(cls, key):
                raise TypeError(""%s() received an invalid keyword %r"" % (
                    cls.__name__, key))

        def view(request, *args, **kwargs):
            self = cls(**initkwargs)
            # We also store the mapping of request methods to actions,
            # so that we can later set the action attribute.
            # eg. `self.action = 'list'` on an incoming GET request.
            self.action_map = actions

            # Bind methods to actions
            # This is the bit that's different to a standard view
            for method, action in actions.items():
                handler = getattr(self, action)
                setattr(self, method, handler)

            # And continue as usual
            return self.dispatch(request, *args, **kwargs)

        # take name and docstring from class
        update_wrapper(view, cls, updated=())

        # and possible attributes set by decorators
        # like csrf_exempt from dispatch
        update_wrapper(view, cls.dispatch, assigned=())

        # We need to set these on the view function, so that breadcrumb
        # generation can pick out these bits of information from a
        # resolved URL.
        view.cls = cls
        view.initkwargs = initkwargs
        view.suffix = initkwargs.get('suffix', None)
        view.actions = actions
        return csrf_exempt(view)","1. Sanitize keyword arguments to prevent malicious users from passing in dangerous parameters.
2. Use csrf_exempt to protect against cross-site request forgery attacks.
3. Set the suffix initkwarg to identify the viewset type."
"        def view(request, *args, **kwargs):
            self = cls(**initkwargs)
            # We also store the mapping of request methods to actions,
            # so that we can later set the action attribute.
            # eg. `self.action = 'list'` on an incoming GET request.
            self.action_map = actions

            # Bind methods to actions
            # This is the bit that's different to a standard view
            for method, action in actions.items():
                handler = getattr(self, action)
                setattr(self, method, handler)

            # And continue as usual
            return self.dispatch(request, *args, **kwargs)","1. Use `functools.partial` to bind the `self` argument to the handler functions.
2. Use `@csrf_exempt` to mark the view as exempt from CSRF checks.
3. Use `@login_required` to require users to be logged in to access the view."
